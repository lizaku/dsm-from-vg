<doc id="12" url="https://en.wikipedia.org/wiki?curid=12" title="Anarchism">
Anarchism

Anarchism is a political philosophy that advocates self-governed societies based on voluntary institutions.
These are often described as stateless societies, although several authors have defined them more specifically as institutions based on non-hierarchical or free associations.
Anarchism holds the state to be undesirable, unnecessary and harmful.
While opposition to the state is central, anarchism specifically entails opposing authority or hierarchical organisation in the conduct of all human relations.
Anarchism is usually considered a far-left ideology and much of anarchist economics and anarchist legal philosophy reflects anti-authoritarian interpretations of communism, collectivism, syndicalism, mutualism, or participatory economics.
Anarchism does not offer a fixed body of doctrine from a single particular world view, instead fluxing and flowing as a philosophy.
Many types and traditions of anarchism exist, not all of which are mutually exclusive.
Anarchist schools of thought can differ fundamentally, supporting anything from extreme individualism to complete collectivism.
Strains of anarchism have often been divided into the categories of social and individualist anarchism or similar dual classifications.
The word "anarchism" is composed from the word "anarchy" and the suffix -ism, themselves derived respectively from the Greek , i.e. "anarchy" (from , "anarchos", meaning "one without rulers"; from the privative prefix ἀν- ("an-", i.e. "without") and , "archos", i.e. "leader", "ruler"; (cf.
"archon" or , "arkhē", i.e. "authority", "sovereignty", "realm", "magistracy")) and the suffix or ("-ismos", "-isma", from the verbal infinitive suffix , "-izein").
The first known use of this word was in 1539.
Various factions within the French Revolution labelled opponents as anarchists (as Maximilien Robespierre did the Hébertists) although few shared many views of later anarchists.
There would be many revolutionaries of the early nineteenth century who contributed to the anarchist doctrines of the next generation, such as William Godwin and Wilhelm Weitling, but they did not use the word "anarchist" or "anarchism" in describing themselves or their beliefs.
The first political philosopher to call himself an anarchist was Pierre-Joseph Proudhon, marking the formal birth of anarchism in the mid-nineteenth century.
Since the 1890s and beginning in France, the term "libertarianism" has often been used as a synonym for anarchism and its use as a synonym is still common outside the United States.
On the other hand, some use libertarianism to refer to individualistic free market philosophy only, referring to free market anarchism as libertarian anarchism.
The earliest anarchist themes can be found in the 6th century BC among the works of Taoist philosopher Laozi and in later centuries by Zhuangzi and Bao Jingyan.
Zhuangzi's philosophy has been described by various sources as anarchist.
Zhuangzi wrote: "A petty thief is put in jail.
A great brigand becomes a ruler of a Nation".
Diogenes of Sinope and the Cynics as well as their contemporary Zeno of Citium, the founder of Stoicism, also introduced similar topics.
Jesus is sometimes considered the first anarchist in the Christian anarchist tradition.
Georges Lechartier wrote: "The true founder of anarchy was Jesus Christ and [...] the first anarchist society was that of the apostles".
In early Islamic history, some manifestations of anarchic thought are found during the Islamic civil war over the Caliphate, where the Kharijites insisted that the imamate is a right for each individual within the Islamic society.
The French Renaissance political philosopher Étienne de La Boétie wrote in his most famous work the "Discourse on Voluntary Servitude" what some historians consider an important anarchist precedent.
The radical Protestant Christian Gerrard Winstanley and his group the Diggers are cited by various authors as proposing anarchist social measures in the 17th century in England.
The term "anarchist" first entered the English language in 1642 during the English Civil War as a term of abuse, used by Royalists against their Roundhead opponents.
By the time of the French Revolution, some such as the Enraged Ones began to use the term positively in opposition to Jacobin centralisation of power, seeing "revolutionary government" as oxymoronic.
By the turn of the 19th century, the English word "anarchism" had lost its initial negative connotation.
Modern anarchism emerged from the secular or religious thought of the Enlightenment, particularly Jean-Jacques Rousseau's arguments for the moral centrality of freedom.
As part of the political turmoil of the 1790s in the wake of the French Revolution, William Godwin developed the first expression of modern anarchist thought.
According to Peter Kropotkin, Godwin was "the first to formulate the political and economical conceptions of anarchism, even though he did not give that name to the ideas developed in his work" while Godwin attached his anarchist ideas to an early Edmund Burke.
Godwin is generally regarded as the founder of the school of thought known as philosophical anarchism.
He argued in "Political Justice" (1793) that government has an inherently malevolent influence on society and that it perpetuates dependency and ignorance.
He thought that the spread of the use of reason to the masses would eventually cause government to wither away as an unnecessary force.
Although he did not accord the state with moral legitimacy, he was against the use of revolutionary tactics for removing the government from power.
Rather, he advocated for its replacement through a process of peaceful evolution.
His aversion to the imposition of a rules-based society led him to denounce as a manifestation of the people's "mental enslavement" the foundations of law, property rights and even the institution of marriage.
He considered the basic foundations of society as constraining the natural development of individuals to use their powers of reasoning to arrive at a mutually beneficial method of social organisation.
In each case, government and its institutions are shown to constrain the development of our capacity to live wholly in accordance with the full and free exercise of private judgement.
The French Pierre-Joseph Proudhon is regarded as the first self-proclaimed anarchist, a label he adopted in his groundbreaking work "What is Property?
", published in 1840.
It is for this reason that some claim Proudhon as the founder of modern anarchist theory.
He developed the theory of spontaneous order in society, where organisation emerges without a central coordinator imposing its own idea of order against the wills of individuals acting in their own interests.
His famous quote on the matter is "Liberty is the mother, not the daughter, of order".
In "What is Property?
", Proudhon answers with the famous accusation "Property is theft".
In this work, he opposed the institution of decreed "property" ("propriété"), where owners have complete rights to "use and abuse" their property as they wish.
He contrasted this with what he called "possession", or limited ownership of resources and goods only while in more or less continuous use.
However, Proudhon later added that "Property is Liberty" and argued that it was a bulwark against state power.
His opposition to the state, organised religion and certain capitalist practices inspired subsequent anarchists and made him one of the leading social thinkers of his time.
The anarcho-communist Joseph Déjacque was the first person to describe himself as "libertarian".
Unlike Proudhon, he argued that "it is not the product of his or her labour that the worker has a right to, but to the satisfaction of his or her needs, whatever may be their nature".
In 1844, the post-Hegelian philosopher Max Stirner published in Germany the book, "The Ego and Its Own", which would later be considered an influential early text of individualist anarchism.
French anarchists active in the 1848 Revolution included Anselme Bellegarrigue, Ernest Coeurderoy, Joseph Déjacque and Proudhon himself.
In Europe, harsh reaction followed the revolutions of 1848, during which ten countries had experienced brief or long-term social upheaval as groups carried out nationalist uprisings.
After most of these attempts at systematic change ended in failure, conservative elements took advantage of the divided groups of socialists, liberals and nationalists along with anarchists to prevent further revolt.
In Spain, Ramón de la Sagra established the anarchist journal "El Porvenir" in La Coruña in 1845 which was inspired by Proudhon's ideas.
The Catalan politician Francesc Pi i Margall became the principal translator of Proudhon's works into Spanish and later briefly became President of Spain in 1873 while being the leader of the Federal Democratic Republican Party.
According to George Woodcock: "These translations were to have a profound and lasting effect on the development of Spanish anarchism after 1870, but before that time Proudhonian ideas, as interpreted by Pi, already provided much of the inspiration for the federalist movement which sprang up in the early 1860's".
According to the "Encyclopædia Britannica": "During the Spanish revolution of 1873, Pi y Margall attempted to establish a decentralised, or "cantonalist," political system on Proudhonian lines".
In 1864, the International Workingmen's Association (sometimes called the First International) united diverse revolutionary currents including French followers of Proudhon, Blanquists, Philadelphes, English trade unionists, socialists and social democrats.
Due to its links to active workers' movements, the International became a significant organisation.
Karl Marx became a leading figure in the International and a member of its General Council.
Proudhon's followers, the mutualists, opposed Marx's state socialism, advocating political abstentionism and small property holdings.
Woodcock also reports that the American individualist anarchists Lysander Spooner and William Batchelder Greene had been members of the First International.
In 1868, following their unsuccessful participation in the League of Peace and Freedom (LPF) Russian revolutionary Mikhail Bakunin and his collectivist anarchist associates joined the First International, which had decided not to get involved with the LPF.
They allied themselves with the federalist socialist sections of the International, who advocated the revolutionary overthrow of the state and the collectivisation of property.
At first, the collectivists worked with the Marxists to push the First International in a more revolutionary socialist direction.
Subsequently, the International became polarised into two camps, with Marx and Bakunin as their respective figureheads.
Bakunin characterised Marx's ideas as centralist and predicted that if a Marxist party came to power, its leaders would simply take the place of the ruling class they had fought against.
Anarchist historian George Woodcock reports: "The annual Congress of the International had not taken place in 1870 owing to the outbreak of the Paris Commune, and in 1871 the General Council called only a special conference in London.
One delegate was able to attend from Spain and none from Italy, while a technical excuse – that they had split away from the Fédération Romande – was used to avoid inviting Bakunin's Swiss supporters.
Thus only a tiny minority of anarchists was present, and the General Council's resolutions passed almost unanimously.
Most of them were clearly directed against Bakunin and his followers".
In 1872, the conflict climaxed with a final split between the two groups at the Hague Congress, where Bakunin and James Guillaume were expelled from the International and its headquarters were transferred to New York.
In response, the federalist sections formed their own International at the St.
Imier Congress, adopting a revolutionary anarchist programme.
The Paris Commune was a government that briefly ruled Paris from 18 March (more formally, from 28 March) to 28 May 1871.
The Commune was the result of an uprising in Paris after France was defeated in the Franco-Prussian War.
Anarchists participated actively in the establishment of the Paris Commune.
They included Louise Michel, the Reclus brothers (Élie Reclus and Élisée Reclus) and Eugene Varlin (the latter murdered in the repression afterwards).
As for the reforms initiated by the Commune, such as the re-opening of workplaces as co-operatives, anarchists can see their ideas of associated labour beginning to be realised.
Moreover, the Commune's ideas on federation obviously reflected the influence of Proudhon on French radical ideas.
The Commune's vision of a communal France based on a federation of delegates bound by imperative mandates issued by their electors and subject to recall at any moment echoes Bakunin's and Proudhon's ideas (Proudhon, like Bakunin, had argued in favour of the "implementation of the binding mandate" in 1848 and for federation of communes), thus both economically and politically the Paris Commune was heavily influenced by anarchist ideas.
George Woodcock states that "a notable contribution to the activities of the Commune and particularly to the organization of public services was made by members of various anarchist factions, including the mutualists Courbet, Longuet, and Vermorel, the libertarian collectivists Varlin, Malon, and Lefrangais, and the bakuninists Elie and Elisée Reclus and Louise Michel".
The anti-authoritarian sections of the First International were the precursors of the anarcho-syndicalists, seeking to "replace the privilege and authority of the State" with the "free and spontaneous organization of labour".
In 1886, the Federation of Organized Trades and Labor Unions of the United States and Canada unanimously set 1 May 1886 as the date by which the eight-hour work day would become standard.
In response, unions across the United States prepared a general strike in support of the event.
On 3 May, a fight broke out in Chicago when strikebreakers attempted to cross the picket line and two workers died when police opened fire upon the crowd.
The next day on 4 May, anarchists staged a rally at Chicago's Haymarket Square.
A bomb was thrown by an unknown party near the conclusion of the rally, killing an officer.
In the ensuing panic, police opened fire on the crowd and each other.
Seven police officers and at least four workers were killed.
Eight anarchists directly and indirectly related to the organisers of the rally were arrested and charged with the murder of the deceased officer.
The men became international political celebrities among the labour movement.
Four of the men were executed and a fifth committed suicide prior to his own execution.
The incident became known as the Haymarket affair and was a setback for the labour movement and the struggle for the eight-hour day.
In 1890, a second attempt—this time international in scope—to organise for the eight-hour day was made.
The event also had the secondary purpose of memorialising workers killed as a result of the Haymarket affair.
Although it had initially been conceived as a once-off event, by the following year the celebration of International Workers' Day on May Day had become firmly established as an international worker's holiday.
In 1907, the International Anarchist Congress of Amsterdam gathered delegates from 14 different countries, among which were important figures of the anarchist movement, including Errico Malatesta, Pierre Monatte, Luigi Fabbri, Benoît Broutchoux, Emma Goldman, Rudolf Rocker and Christiaan Cornelissen.
Various themes were treated during the Congress, in particular concerning the organisation of the anarchist movement, popular education issues, the general strike or antimilitarism.
A central debate concerned the relation between anarchism and syndicalism (or trade unionism).
Malatesta and Monatte were in particular disagreement themselves on this issue as the latter thought that syndicalism was revolutionary and would create the conditions of a social revolution while Malatesta did not consider syndicalism by itself sufficient.
He thought that the trade union movement was reformist and even conservative, citing as essentially bourgeois and anti-worker the phenomenon of professional union officials.
Malatesta warned that the syndicalists aims were in perpetuating syndicalism itself, whereas anarchists must always have anarchy as their end and consequently refrain from committing to any particular method of achieving it.
In 1881, the Spanish Workers Federation was the first major anarcho-syndicalist movement—anarchist trade union federations were of special importance in Spain.
The most successful was the Confederación Nacional del Trabajo (National Confederation of Labour, CNT), founded in 1910.
Before the 1940s, the CNT was the major force in Spanish working class politics, attracting 1.58 million members at one point and playing a major role in the Spanish Civil War.
The CNT was affiliated with the International Workers Association, a federation of anarcho-syndicalist trade unions founded in 1922, with delegates representing two million workers from 15 countries in Europe and Latin America.
In Latin America in particular, "[t]he anarchists quickly became active in organising craft and industrial workers throughout South and Central America, and until the early 1920s most of the trade unions in Mexico, Brazil, Peru, Chile, and Argentina were anarcho-syndicalist in general outlook; the prestige of the Spanish C.N.T.
as a revolutionary organisation was undoubtedly to a great extent responsible for this situation.
The largest and most militant of these organisations was the Federación Obrera Regional Argentina [...] it grew quickly to a membership of nearly a quarter of a million, which dwarfed the rival socialdemocratic unions".
Some anarchists, such as Johann Most, advocated publicising violent acts of retaliation against counter-revolutionaries because "we preach not only action in and for itself, but also action as propaganda".
Scholars such as Beverly Gage contend that this was not advocacy of mass murder, but targeted killings of members of the ruling class at times when such actions might garner sympathy from the population, such as during periods of heightened government repression or labor conflicts where workers were killed.
However, Most himself once boasted that "the existing system will be quickest and most radically overthrown by the annihilation of its exponents.
Therefore, massacres of the enemies of the people must be set in motion".
Most is best known for a pamphlet published in 1885, "The Science of Revolutionary Warfare", a how-to manual on the subject of making explosives based on knowledge he acquired while working at an explosives plant in New Jersey.
By the 1880s, people inside and outside the anarchist movement began to use the slogan, "propaganda of the deed" to refer to individual bombings, regicides and tyrannicides.
From 1905 onwards, the Russian counterparts of these anti-syndicalist anarchist-communists become partisans of economic terrorism and illegal "expropriations".
Illegalism as a practice emerged and within it "[t]he acts of the anarchist bombers and assassins ("propaganda by the deed") and the anarchist burglars ("individual reappropriation") expressed their desperation and their personal, violent rejection of an intolerable society.
Moreover, they were clearly meant to be exemplary invitations to revolt".
France's Bonnot Gang was the most famous group to embrace illegalism.
However, important figures in the anarchist movement distanced themselves from such individual acts as soon as 1887.
Peter Kropotkin thus wrote that year in "Le Révolté" that "a structure based on centuries of history cannot be destroyed with a few kilos of dynamite".
A variety of anarchists advocated the abandonment of these sorts of tactics in favour of collective revolutionary action, for example through the trade union movement.
The anarcho-syndicalist Fernand Pelloutier argued in 1895 for renewed anarchist involvement in the labour movement on the basis that anarchism could do very well without "the individual dynamiter".
State repression (including the infamous 1894 French "lois scélérates") of the anarchist and labour movements following the few successful bombings and assassinations may have contributed in the first place to the abandonment of these kinds of tactics, although reciprocally state repression may have played a role in these isolated acts.
The dismemberment of the French socialist movement into many groups and—following the suppression of the 1871—Paris Commune the execution and exile of many "communards" to penal colonies favoured individualist political expression and acts.
Numerous heads of state were assassinated between 1881 and 1914 by members of the anarchist movement, including Tsar Alexander II of Russia, President Sadi Carnot of France, Prime Minister Antonio Cánovas del Castillo of Spain, Empress Elisabeth of Austria, King Umberto I of Italy, President William McKinley of the United States, King Carlos I of Portugal and King George I of Greece.
McKinley's assassin Leon Czolgosz claimed to have been influenced by anarchist and feminist Emma Goldman.
Anarchists participated alongside the Bolsheviks in both February and October revolutions and were initially enthusiastic about the Bolshevik revolution.
However, following a political falling out with the Bolsheviks by the anarchists and other left-wing opposition the conflict culminated in the 1921 Kronstadt rebellion, which the new government repressed.
Anarchists in central Russia were either imprisoned, driven underground or joined the victorious Bolsheviks; the anarchists from Petrograd and Moscow fled to Ukraine.
In the Free Territory, they fought in the civil war against the Whites (a grouping of monarchists and other opponents of the October Revolution) and then the Bolsheviks as part of the Revolutionary Insurrectionary Army of Ukraine led by Nestor Makhno, who established an anarchist society in the region for a number of months.
Expelled American anarchists Emma Goldman and Alexander Berkman were among those agitating in response to Bolshevik policy and the suppression of the Kronstadt uprising, before they left Russia.
Both wrote accounts of their experiences in Russia, criticising the amount of control the Bolsheviks exercised.
For them, Bakunin's predictions about the consequences of Marxist rule that the rulers of the new "socialist" Marxist state would become a new elite had proved all too true.
The victory of the Bolsheviks in the October Revolution and the resulting Russian Civil War did serious damage to anarchist movements internationally.
Many workers and activists saw Bolshevik success as setting an example and communist parties grew at the expense of anarchism and other socialist movements.
In France and the United States, for example, members of the major syndicalist movements of the General Confederation of Labour and Industrial Workers of the World (IWW) left the organisations and joined the Communist International.
The revolutionary wave of 1917–1923 saw the active participation of anarchists in varying degrees of protagonism.
In the German uprising known as the German Revolution of 1918–1919 which established the Bavarian Soviet Republic, the anarchists Gustav Landauer, Silvio Gesell and Erich Mühsam had important leadership positions within the revolutionary councilist structures.
In the Italian events known as the "biennio rosso", the anarcho-syndicalist trade union Unione Sindacale Italiana "grew to 800,000 members and the influence of the Italian Anarchist Union (20,000 members plus "Umanita Nova", its daily paper) grew accordingly [...] Anarchists were the first to suggest occupying workplaces."
In the Mexican Revolution, the Mexican Liberal Party was established and during the early 1910s it led a series of military offensives leading to the conquest and occupation of certain towns and districts in Baja California with the leadership of anarcho-communist Ricardo Flores Magón.
In Paris, the Dielo Truda group of Russian anarchist exiles, which included Nestor Makhno, concluded that anarchists needed to develop new forms of organisation in response to the structures of Bolshevism.
Their 1926 manifesto, called the "Organisational Platform of the General Union of Anarchists (Draft)", was supported.
Platformist groups active today include the Workers Solidarity Movement in Ireland and the North Eastern Federation of Anarchist Communists of North America.
Synthesis anarchism emerged as an organisational alternative to platformism that tries to join anarchists of different tendencies under the principles of anarchism without adjectives.
In the 1920s, this form found as its main proponents Volin and Sebastien Faure.
It is the main principle behind the anarchist federations grouped around the contemporary global International of Anarchist Federations.
In the 1920s and 1930s, the rise of fascism in Europe transformed anarchism's conflict with the state.
Italy saw the first struggles between anarchists and Benito Mussolini's fascists.
Italian anarchists played a key role in the anti-fascist organisation "Arditi del Popolo", which was strongest in areas with anarchist traditions and achieved some success in their activism, such as repelling Blackshirts in the anarchist stronghold of Parma in August 1922.
The veteran Italian anarchist Luigi Fabbri was one of the first critical theorists of fascism, describing it as "the preventive counter-revolution".
In France, where the far-right leagues came close to insurrection in the February 1934 riots, anarchists divided over a united front policy.
Anarchists in France and Italy were active in the Resistance during World War II.
In Germany, the anarchist Erich Mühsam was arrested on charges unknown in the early morning hours of 28 February 1933, within a few hours after the Reichstag fire in Berlin.
Joseph Goebbels, the Nazi propaganda minister, labelled him as one of "those Jewish subversives".
Over the next seventeen months, he would be imprisoned in the concentration camps at Sonnenburg, Brandenburg and finally, Oranienburg.
On 2 February 1934, Mühsam was transferred to the concentration camp at Oranienburg when finally on the night of 9 July 1934, Mühsam was tortured and murdered by the guards, his battered corpse found hanging in a latrine the next morning.
In Spain, the national anarcho-syndicalist trade union CNT initially refused to join a popular front electoral alliance and abstention by CNT supporters led to a right-wing election victory.
In 1936, the CNT changed its policy and anarchist votes helped bring the popular front back to power.
Months later, conservative members of the military, with the support of minority extreme-right parties, responded with an attempted coup, causing the Spanish Civil War (1936–1939).
In response to the army rebellion, an anarchist-inspired movement of peasants and workers, supported by armed militias, took control of Barcelona and of large areas of rural Spain where they collectivised the land.
However, the anarchists were losing ground even before the fascist victory in 1939 in a bitter struggle with the Stalinists, who controlled much of the distribution of military aid to the Republicans cause from the Soviet Union.
According to Noam Chomsky, "the communists were mainly responsible for the destruction of the Spanish anarchists.
Not just in Catalonia—the communist armies mainly destroyed the collectives elsewhere.
The communists basically acted as the police force of the security system of the Republic and were very much opposed to the anarchists, partially because Stalin still hoped at that time to have some kind of pact with Western countries against Adolf Hitler.
That failed and Stalin withdrew the support to the Republic.
They even withdrew the Spanish gold reserves".
The events known as the Spanish Revolution was a workers' social revolution that began during the outbreak of the Spanish Civil War in 1936 and resulted in the widespread implementation of anarchist and more broadly libertarian socialist organisational principles throughout various portions of the country for two to three years, primarily Catalonia, Aragon, Andalusia and parts of Levante.
Much of Spain's economy was put under worker control and in anarchist strongholds like Catalonia the figure was as high as 75%, but lower in areas with heavy Communist Party of Spain influence as the Soviet-allied party actively resisted attempts at collectivisation enactment.
Factories were run through worker committees, agrarian areas became collectivised and run as libertarian communes.
Anarchist historian Sam Dolgoff estimated that about eight million people participated directly or at least indirectly in the Spanish Revolution, which he claimed "came closer to realising the ideal of the free stateless society on a vast scale than any other revolution in history".
Spanish Communist Party-led troops suppressed the collectives and persecuted both dissident Marxists and anarchists.
The prominent Italian anarchist Camillo Berneri, who volunteered to fight against Francisco Franco was killed instead in Spain by gunmen associated with the Spanish Communist Party.
The city of Madrid was turned over to the Francoist forces by the last non-francoist mayor of the city, the anarchist Melchor Rodríguez García.
Anarchism sought to reorganise itself after the war and in this context the organisational debate between synthesis anarchism and platformism took importance once again especially in the anarchist movements of Italy and France.
The Mexican Anarchist Federation was established in 1945 after the Anarchist Federation of the Centre united with the Anarchist Federation of the Federal District.
In the early 1940s, the Antifascist International Solidarity and the Federation of Anarchist Groups of Cuba merged into the large national organisation Asociación Libertaria de Cuba (Cuban Libertarian Association).
From 1944 to 1947, the Bulgarian Anarchist Communist Federation reemerged as part of a factory and workplace committee movement, but was repressed by the new Communist regime.
In 1945 in France the Fédération Anarchiste and the anarchosyndicalist trade union Confédération nationale du travail was established in the next year while the also synthesist Federazione Anarchica Italiana was founded in Italy.
Korean anarchists formed the League of Free Social Constructors in September 1945 and in 1946 the Japanese Anarchist Federation was founded.
An International Anarchist Congress with delegates from across Europe was held in Paris in May 1948.
After World War II, an appeal in the "Fraye Arbeter Shtime" detailing the plight of German anarchists and called for Americans to support them.
By February 1946, the sending of aid parcels to anarchists in Germany was a large-scale operation.
The Federation of Libertarian Socialists was founded in Germany in 1947 and Rudolf Rocker wrote for its organ, "Die Freie Gesellschaft", which survived until 1953.
In 1956, the Uruguayan Anarchist Federation was founded.
In 1955, the Anarcho-Communist Federation of Argentina renamed itself as the Argentine Libertarian Federation.
The Syndicalist Workers' Federation (SWF) was a syndicalist group in active in post-war Britain, and one of Solidarity Federation's earliest predecessors.
It was formed in 1950 by members of the dissolved Anarchist Federation of Britain (AFB).
Unlike the AFB, which was influenced by anarcho-syndicalist ideas but ultimately not syndicalist itself, the SWF decided to pursue a more definitely syndicalist, worker-centred strategy from the outset.
Anarchism continued to influence important literary and intellectual personalities of the time, such as Albert Camus, Herbert Read, Paul Goodman, Dwight Macdonald, Allen Ginsberg, George Woodcock, Leopold Kohr, Julian Beck, John Cage and the French Surrealist group led by André Breton, which now openly embraced anarchism and collaborated in the Fédération Anarchiste.
Anarcho-pacifism became influential in the Anti-nuclear movement and anti war movements of the time as can be seen in the activism and writings of the English anarchist member of Campaign for Nuclear Disarmament Alex Comfort or the similar activism of the American catholic anarcho-pacifists Ammon Hennacy and Dorothy Day.
Anarcho-pacifism became a "basis for a critique of militarism on both sides of the Cold War".
The resurgence of anarchist ideas during this period is well documented in Robert Graham's , "Volume Two: The Emergence of the New Anarchism (1939–1977)".
A surge of popular interest in anarchism occurred in western nations during the 1960s and 1970s.
Anarchism was influential in the Counterculture of the 1960s and anarchists actively participated in the late sixties students and workers revolts.
In 1968, in Carrara, Italy the International of Anarchist Federations was founded during an international anarchist conference held there in 1968 by the three existing European federations of France (the Fédération Anarchiste), the Federazione Anarchica Italiana of Italy and the Iberian Anarchist Federation as well as the Bulgarian federation in French exile.
In the United Kingdom in the 1970s, this was associated with the punk rock movement as exemplified by bands such as Crass and the Sex Pistols.
The housing and employment crisis in most of Western Europe led to the formation of communes and squatter movements like that of Barcelona, Spain.
In Denmark, squatters occupied a disused military base and declared the Freetown Christiania, an autonomous haven in central Copenhagen.
Since the revival of anarchism in the mid-20th century, a number of new movements and schools of thought emerged.
Although feminist tendencies have always been a part of the anarchist movement in the form of anarcha-feminism, they returned with vigour during the second wave of feminism in the 1960s.
Anarchist anthropologist David Graeber and anarchist historian Andrej Grubacic have posited a rupture between generations of anarchism, with those "who often still have not shaken the sectarian habits" of the 19th century contrasted with the younger activists who are "much more informed, among other elements, by indigenous, feminist, ecological and cultural-critical ideas" and who by the turn of the 21st century formed "by far the majority" of anarchists.
Since the 1980s, anarchism has grown into a strong political force in Latin America, with the development of Fejuve (1979), CIPO-RFM (1980s), Zapatistas (1994), Horizontilidad (2001) and the Oaxaca Uprising (2006).
Around the turn of the 21st century, anarchism grew in popularity and influence as part of the anti-war, anti-capitalist, and anti-globalisation movements.
Anarchists became known for their involvement in protests against the meetings of the World Trade Organization (WTO), Group of Eight (G8) and the World Economic Forum (WEF).
Some anarchist factions at these protests engaged in rioting, property destruction, and violent confrontations with police.
These actions were precipitated by ad hoc, leaderless, anonymous cadres known as black blocs—other organisational tactics pioneered in this time include security culture, affinity groups and the use of decentralised technologies such as the internet.
A significant event of this period was the confrontations at WTO conference in Seattle in 1999.
According to anarchist scholar Simon Critchley, "contemporary anarchism can be seen as a powerful critique of the pseudo-libertarianism of contemporary neo-liberalism [...] One might say that contemporary anarchism is about responsibility, whether sexual, ecological or socio-economic; it flows from an experience of conscience about the manifold ways in which the West ravages the rest; it is an ethical outrage at the yawning inequality, impoverishment and disenfranchisment that is so palpable locally and globally".
International anarchist federations in existence include the International of Anarchist Federations, the International Workers' Association and International Libertarian Solidarity.
The largest organised anarchist movement today is in Spain in the form of the Confederación General del Trabajo (CGT) and the CNT.
CGT membership was estimated at around 100,000 for 2003.
Anarchist ideas have been influential in the development of the Democratic Federation of Northern Syria (DFNS), more commonly known as Rojava, a "de facto" autonomous region in northern Syria.
Abdullah Öcalan—a founding member of the Kurdistan Workers' Party (PKK) who is currently imprisoned in Turkey—is an iconic and popular figure in the DFNS whose ideas shaped the region's society and politics.
While in prison, Öcalan corresponded with (and was influenced by) Murray Bookchin, an anarcho-communist theorist and philosopher who developed Communalism and libertarian municipalism.
Modelled after Bookchin's ideas, Öcalan developed the theory of democratic confederalism.
In March 2005, he issued his "Declaration of Democratic Confederalism in Kurdistan", calling upon citizens "to stop attacking the government and instead create municipal assemblies, which he called 'democracy without the state'".
Anarchist schools of thought had been generally grouped in two main historical traditions, individualist anarchism and social anarchism, which have some different origins, values and evolution.
The individualist wing of anarchism emphasises negative liberty, i.e. opposition to state or social control over the individual, while those in the social wing emphasise positive liberty to achieve one's potential and argue that humans have needs that society ought to fulfil, "recognising equality of entitlement".
In a chronological and theoretical sense, there are classical—those created throughout the 19th century—and post-classical anarchist schools—those created since the mid-20th century and after.
Beyond the specific factions of anarchist thought is philosophical anarchism, which embodies the theoretical stance that the state lacks moral legitimacy without accepting the imperative of revolution to eliminate it.
A component especially of individualist anarchism philosophical anarchism may accept the existence of a minimal state as unfortunate, and usually temporary, "necessary evil" but argue that citizens do not have a moral obligation to obey the state when its laws conflict with individual autonomy.
One reaction against sectarianism within the anarchist milieu was "anarchism without adjectives", a call for toleration first adopted by Fernando Tarrida del Mármol in 1889 in response to the "bitter debates" of anarchist theory at the time.
In abandoning the hyphenated anarchisms (i.e. collectivist-, communist-, mutualist– and individualist-anarchism), it sought to emphasise the anti-authoritarian beliefs common to all anarchist schools of thought.
Mutualism began in 18th-century English and French labour movements before taking an anarchist form associated with Pierre-Joseph Proudhon in France and others in the United States.
Proudhon proposed spontaneous order, whereby organisation emerges without central authority, a "positive anarchy" where order arises when everybody does "what he wishes and only what he wishes" and where "business transactions alone produce the social order."
Proudhon distinguished between ideal political possibilities and practical governance.
For this reason, much in contrast to some of his theoretical statements concerning ultimate spontaneous self-governance, Proudhon was heavily involved in French parliamentary politics and allied himself not with anarchist but socialist factions of workers' movements and, in addition to advocating state-protected charters for worker-owned cooperatives, promoted certain nationalisation schemes during his life of public service.
Mutualist anarchism is concerned with reciprocity, free association, voluntary contract, federation, and credit and currency reform.
According to the American mutualist William Batchelder Greene, each worker in the mutualist system would receive "just and exact pay for his work; services equivalent in cost being exchangeable for services equivalent in cost, without profit or discount".
Mutualism has been retrospectively characterised as ideologically situated between individualist and collectivist forms of anarchism.
Proudhon first characterised his goal as a "third form of society, the synthesis of communism and property".
Social anarchism calls for a system with common ownership of means of production and democratic control of all organisations, without any government authority or coercion.
It is the largest school of thought in anarchism.
Social anarchism rejects private property, seeing it as a source of social inequality (while retaining respect for personal property) and emphasises cooperation and [[mutual aid (organization)|mutual aid]].
[[Collectivist anarchism]], also referred to as revolutionary socialism or a form of such, is a revolutionary form of anarchism, commonly associated with Mikhail Bakunin and Johann Most.
Collectivist anarchists oppose all private ownership of the means of production, instead advocating that ownership be collectivised.
This was to be achieved through violent revolution, first starting with a small cohesive group through acts of violence, or propaganda by the deed, which would inspire the workers as a whole to revolt and forcibly collectivise the means of production.
However, collectivisation was not to be extended to the distribution of income as workers would be paid according to time worked, rather than receiving goods being distributed "according to need" as in anarcho-communism.
This position was criticised by anarchist communists as effectively "uphold[ing] the wages system".
Collectivist anarchism arose contemporaneously with [[Marxism]], but opposed the Marxist dictatorship of the proletariat despite the stated Marxist goal of a collectivist stateless society.
Anarchist, communist and collectivist ideas are not [[mutually exclusive]]—although the collectivist anarchists advocated compensation for labour, some held out the possibility of a post-revolutionary transition to a communist system of distribution according to need.
[[File:Kropotkin2.jpg|thumb|upright|[[Peter Kropotkin]] was influential in the development of [[anarcho-communism]]]]
[[Anarcho-communism]] (also known as anarchist-communism, libertarian communism and occasionally as free communism) is a theory of anarchism that advocates abolition of the state, [[market (economics)|markets]], money, [[private property]] (while retaining respect for personal property) and capitalism in favour of [[common ownership]] of the [[means of production]], [[direct democracy]] and a horizontal network of [[voluntary association]]s and [[workers' council]]s with production and consumption based on the guiding principle: "[[From each according to his ability, to each according to his needs|From each according to his ability, to each according to his need]]".
Some forms of anarchist communism such as [[insurrectionary anarchism]] are strongly influenced by egoism and radical individualism, believing anarcho-communism is the best social system for the realisation of individual freedom.
Most anarcho-communists view anarcho-communism as a way of reconciling the opposition between the individual and society.
Anarcho-communism developed out of radical socialist currents after the French Revolution but was first formulated as such in the Italian section of the First International.
The theoretical work of Peter Kropotkin took importance later as it expanded and developed pro-organisationalist and insurrectionary anti-organisationalist sections.
To date, the best known examples of an anarchist communist society (i.e. established around the ideas as they exist today and achieving worldwide attention and knowledge in the historical canon), are the anarchist territories during the [[Spanish Revolution of 1936|Spanish Revolution]] and the [[Free Territory]] during the [[Russian Revolution (1917)|Russian Revolution]].
Through the efforts and influence of the [[Anarchism in Spain|Spanish anarchists]] during the Spanish Revolution within the Spanish Civil War, starting in 1936 anarchist communism existed in most of Aragon, parts of the Levante and Andalusia as well as in the stronghold of [[Revolutionary Catalonia|anarchist Catalonia]] before being crushed by the combined forces of the [[Francoist Spain|regime that won the war]], Hitler, Mussolini, Communist Party of Spain repression (backed by the Soviet Union) as well as economic and armaments blockades from the capitalist countries and the Spanish Republic itself.
During the Russian Revolution, anarchists such as Nestor Makhno worked to create and defend—through the Revolutionary Insurrectionary Army of Ukraine—anarcho-communism in the Free Territory of the Ukraine from 1919 before being conquered by the Bolsheviks in 1921.
[[File:Manifestación CNT Bilbao.jpg|thumb|left|[[International Workers' Day|May Day]] 2010 demonstration of Spanish [[Anarcho-syndicalism|anarcho-syndicalist]] trade union [[Confederación Nacional del Trabajo|CNT]] in Bilbao, Basque Country]]
[[Anarcho-syndicalism]] is a branch of anarchism that focuses on the labour movement.
Anarcho-syndicalists view labour unions as a potential force for revolutionary social change, replacing capitalism and the state with a new society democratically self-managed by workers.
The basic principles of anarcho-syndicalism are workers' [[wikt:Solidarity|solidarity]], [[direct action]] and [[workers' self-management]].
Anarcho-syndicalists believe that only direct action—that is, action concentrated on directly attaining a goal as opposed to indirect action, such as electing a representative to a government position—will allow workers to liberate themselves.
Moreover, anarcho-syndicalists believe that workers' organisations (the organisations that struggle against the wage system, which in anarcho-syndicalist theory will eventually form the basis of a new society) should be self-managing.
They should not have bosses or "business agents"—rather, the workers should be able to make all the decisions that affect them themselves.
Rudolf Rocker was one of the most popular voices in the anarcho-syndicalist movement.
He outlined a view of the origins of the movement, what it sought and why it was important to the future of labour in his 1938 pamphlet "Anarcho-Syndicalism".
The International Workers Association is an international anarcho-syndicalist federation of various labour unions from different countries.
The Spanish CNT played and still plays a major role in the Spanish labour movement.
It was also an important force in the Spanish Civil War.
[[Individualist anarchism]] refers to several traditions of thought within the anarchist movement that emphasise the [[individual]] and their [[Will (philosophy)|will]] over any kinds of external determinants such as groups, society, traditions and ideological systems.
Individualist anarchism is not a single philosophy, but it instead refers to a group of individualistic philosophies that sometimes are in conflict.
In 1793, William Godwin, who has often been cited as the first anarchist, wrote "[[Political Justice]]", which some consider the first expression of anarchism.
Godwin was a philosophical anarchist and from a [[rationalist]] and [[utilitarian]] basis opposed revolutionary action and saw a [[Limited government|minimal state]] as a present "necessary evil" that would become increasingly irrelevant and powerless by the gradual spread of knowledge.
Godwin advocated individualism, proposing that all cooperation in labour be eliminated on the premise that this would be most conducive with the general good.
[[File:Max stirner.jpg|thumb|left|[[Max Stirner]] (here in a sketch by [[Friedrich Engels]]) is usually considered a prominent early individualist anarchist]]
An influential form of individualist anarchism, called "egoism", or [[egoist anarchism]], was expounded by one of the earliest and best-known proponents of individualist anarchism, the German [[Max Stirner]].
Stirner's "[[The Ego and Its Own]]", published in 1844, is a founding text of the philosophy.
According to Stirner, the only limitation on the rights of individuals is their power to obtain what they desire, without regard for God, state, or morality.
To Stirner, rights were "[[Reification (fallacy)|spooks]]" in the mind and he held that society does not exist, but "the individuals are its reality".
Stirner advocated self-assertion and foresaw [[Union of egoists|unions of egoists]], non-systematic associations continually renewed by all parties' support through an act of will, which Stirner proposed as a form of organisation in place of the state.
Egoist anarchists argue that egoism will foster genuine and spontaneous union between individuals.
"Egoism" has inspired many interpretations of Stirner's philosophy.
It was re-discovered and promoted by German philosophical anarchist and [[homosexual]] activist [[John Henry Mackay]].
[[Josiah Warren]] is widely regarded as the first American anarchist, and the four-page weekly paper he edited during 1833, "The Peaceful Revolutionist", was the first anarchist periodical published.
For American anarchist historian Eunice Minette Schuster, "[i]t is apparent [...] that Proudhonian Anarchism was to be found in the United States at least as early as 1848 and that it was not conscious of its affinity to the Individualist Anarchism of Josiah Warren and [[Stephen Pearl Andrews]] [...] William B. Greene presented this Proudhonian Mutualism in its purest and most systematic form".
[[Henry David Thoreau]] (1817–1862) was an important early influence in individualist anarchist thought in the United States and Europe.
Thoreau was an American author, poet, naturalist, tax resister, [[Development criticism|development critic]], surveyor, historian, philosopher and leading [[transcendentalist]].
He is best known for his books "[[Walden]]", a reflection upon [[simple living]] in natural surroundings, as well as his essay, "[[Civil Disobedience (Thoreau)|Civil Disobedience]]", an argument for individual resistance to civil government in moral opposition to an unjust state.
Benjamin Tucker later fused Stirner's egoism with the economics of Warren and Proudhon in his eclectic influential publication "[[Liberty (1881–1908)|Liberty]]".
From these early influences, individualist anarchism in different countries attracted a small yet diverse following of Bohemian artists and intellectuals, [[free love]] and [[birth control]] advocates (see [[anarchism and issues related to love and sex]]), individualist [[naturist]]s and [[nudist]]s (see [[anarcho-naturism]]), [[freethought]] and [[anti-clerical]] activists as well as young anarchist outlaws in what became known as illegalism and [[individual reclamation]] (see [[European individualist anarchism]] and [[individualist anarchism in France]]).
These authors and activists included [[Oscar Wilde]], [[Emile Armand]], [[Han Ryner]], [[Henri Zisly]], [[Renzo Novatore]], [[Miguel Gimenez Igualada]], [[Adolf Brand]] and [[Lev Chernyi]] among others.
[[File:Jarach and Zerzan.JPG|thumb|left|[[Lawrence Jarach]] (left) and [[John Zerzan]] (right), two prominent [[Contemporary anarchism|contemporary anarchist]] authors: Zerzan is known as prominent voice within [[anarcho-primitivism]] while Jarach is a noted advocate of [[post-left anarchy]]]]
Anarchism continues to generate many philosophies and movements, at times eclectic, drawing upon various sources and [[Syncretic politics|syncretic]], combining disparate concepts to create new philosophical approaches.
[[Insurrectionary anarchism]] is a revolutionary theory, practice, and tendency within the anarchist movement which emphasises [[insurrection]] within anarchist practice.
It is critical of formal organisations such as [[labour unions]] and federations that are based on a political programme and periodic congresses.
Instead, insurrectionary anarchists advocate informal organisation and small [[affinity group]] based organisation.
Insurrectionary anarchists put value in attack, permanent [[class conflict]] and a refusal to negotiate or compromise with class enemies.
[[Green anarchism]] (or eco-anarchism) is a school of thought within anarchism that emphasises environmental issues, with an important precedent in anarcho-naturism and whose main contemporary currents are anarcho-primitivism and [[Social ecology (theory)|social ecology]].
Writing from a green anarchist perspective, [[John Zerzan]] attributes the ills of today's social degradation to technology and the birth of agricultural civilization.
While [[Layla AbdelRahim]] argues that "the shift in human consciousness was also a shift in human subsistence strategies, whereby some human animals reinvented their narrative to center murder and predation and thereby institutionalize violence".
Thus, according to her, civilization was the result of the human development of technologies and grammar for predatory economics.
Language and literacy, she claims, are some of these technologies.
[[Anarcha-feminism]] (also called anarchist feminism and anarcho-feminism) combines anarchism with feminism.
It generally views [[patriarchy]] as a manifestation of involuntary coercive hierarchy that should be replaced by [[decentralised]] free association.
Anarcha-feminists believe that the struggle against patriarchy is an essential part of [[class struggle]], and the anarchist struggle against the state.
In essence, the philosophy sees anarchist struggle as a necessary component of feminist struggle and vice versa.
[[L. Susan Brown]] claims that "as anarchism is a political philosophy that opposes all relationships of power, it is inherently feminist".
Anarcha-feminism began with the late 19th-century writings of early feminist anarchists such as Emma Goldman and [[Voltairine de Cleyre]].
[[Anarcho-pacifism]] is a tendency that rejects violence in the struggle for social change (see [[non-violence]]).
It developed mostly in the Netherlands, Britain and the United States before and during the Second World War.
Christian anarchism is a [[Christian movement|movement]] in [[political theology]] that combines anarchism and Christianity.
Its main proponents included [[Leo Tolstoy]], Dorothy Day, Ammon Hennacy and [[Jacques Ellul]].
[[Religious anarchism]] refers to a set of related anarchist ideologies that are inspired by the teachings of (organized) religions, but many anarchists have traditionally been skeptical of and opposed to [[organized religion]].
Many different religions have served as inspiration for religious forms of anarchism, most notably Christianity as Christian anarchists believe that biblical teachings give credence to anarchist philosophy.
Non-Christian forms of religious anarchism include [[Buddhist anarchism]], [[Jewish anarchism]] and most recently [[Neopaganism]].
[[Synthesis anarchism]] is a form of anarchism that tries to join anarchists of different tendencies under the principles of anarchism without adjectives.
In the 1920s, this form found as its main proponents the [[anarcho-communists]] [[Voline]] and [[Sébastien Faure]].
It is the main principle behind the anarchist federations grouped around the contemporary global [[International of Anarchist Federations]].
[[Platformism]] is a tendency within the wider anarchist movement based on the organisational theories in the tradition of Dielo Truda's "Organisational Platform of the General Union of Anarchists (Draft)".
The document was based on the experiences of [[Anarchism in Russia|Russian anarchists]] in the 1917 October Revolution, which led eventually to the victory of the [[Bolsheviks]] over the anarchists and other groups.
The "Platform" attempted to address and explain the anarchist movement's failures during the Russian Revolution.
[[Post-left anarchy]] is a recent current in anarchist thought that promotes a critique of anarchism's relationship to traditional [[left-wing politics]].
Some post-leftists seek to escape the confines of [[ideology]] in general also presenting a critique of organisations and [[morality]].
Influenced by the work of Max Stirner and by the Marxist [[Situationist International]], post-left anarchy is marked by a focus on social insurrection and a rejection of leftist social organisation.
[[Post-anarchism]] is a theoretical move towards a synthesis of classical anarchist theory and [[poststructuralist]] thought, drawing from diverse ideas including post-left anarchy, [[postmodernism]], [[autonomism]], [[postcolonialism]] and the [[Situationist International]].
[[Queer anarchism]] is a form of [[socialism]] which suggests anarchism as a solution to the issues faced by the [[LGBT community]], mainly [[heteronormativity]], [[homophobia]], [[transphobia]] and [[biphobia]].
Anarcho-queer arose during the late 20th century based on the work of Michel Foucault "[[The History of Sexuality]]".
[[Left-wing market anarchism]] strongly affirm the classical liberal ideas of self-ownership and free markets while maintaining that taken to their logical conclusions, these ideas support strongly anti-corporatist, anti-hierarchical, pro-labour positions and anti-capitalism in economics and anti-imperialism in foreign policy.
[[Anarcho-capitalism]] advocates the elimination of the state in favour of [[self-ownership]] in a [[free market]].
Anarcho-capitalism developed from radical anti-state libertarianism and individualist anarchism, drawing from [[Austrian School]] economics, study of [[law and economics]] and [[public choice theory]].
There is a strong current within anarchism which believes that anarcho-capitalism cannot be considered a part of the anarchist movement due to the fact that anarchism has historically been an [[Anti-capitalism|anti-capitalist]] movement and for definitional reasons which see anarchism as [[Anarchism and capitalism|incompatible]] with capitalist forms.
[[Anarcho-transhumanism]] is a recently new branch of anarchism that takes traditional and modern anarchism, typically drawing from [[anarcho-syndicalism]], [[left-libertarianism]] or [[Libertarian socialist|libertarian socialism]] and combines it with [[transhumanism]] and [[post-humanism]].
It can be described as a "liberal democratic revolution, at its core the idea that people are happiest when they have rational control over their lives.
Reason, science, and technology provide one kind of control, slowly freeing us from ignorance, toil, pain, disease and limited lifespans (aging)".
Some anarcho-transhumanists might also follow [[technogaianism]].
[[File:Gadewar.jpg|thumb|left|Which forms of violence (if any) are [[anarchism and violence|consistent with anarchist values]] is a controversial subject among anarchists]]

Anarchism is a [[philosophy]] that embodies many diverse attitudes, tendencies and schools of thought and as such disagreement over questions of values, ideology and tactics is common.
The compatibility of [[Anarchism and capitalism|capitalism]], [[anarchism and nationalism|nationalism]] and [[Anarchism and religion|religion]] with anarchism is widely disputed.
Similarly, anarchism enjoys complex relationships with ideologies such as [[Anarchism and Marxism|Marxism]], [[Issues in anarchism#Communism|communism]], [[collectivism]], [[syndicalism]]/[[trade unionism]] and [[capitalism]].
Anarchists may be motivated by [[humanism]], [[God|divine authority]], [[enlightened self-interest]], [[veganarchism|veganism]] or any number of alternative ethical doctrines.
Phenomena such as [[civilisation]], [[technology]] (e.g.
within anarcho-primitivism) and [[Issues in anarchism#Participation in statist democracy|the democratic process]] may be sharply criticised within some anarchist tendencies and simultaneously lauded in others.
On a tactical level, while [[propaganda of the deed]] was a tactic used by anarchists in the 19th century (e.g.
the [[nihilist movement]]), some contemporary anarchists espouse alternative direct action methods such as [[nonviolence]], [[counter-economics]] and [[Crypto-anarchism|anti-state cryptography]] to bring about an anarchist society.
About the scope of an anarchist society, some anarchists advocate a global one, while others do so by local ones.
The diversity in anarchism has led to widely different use of identical terms among different anarchist traditions, which has led to many [[definitional concerns in anarchist theory]].
Intersecting and overlapping between various schools of thought, certain topics of interest and internal disputes have proven perennial within anarchist theory.
[[File:Emilearmand01.jpg|thumb|upright|[[Individualist anarchism in France|French individualist anarchist]] [[Émile Armand]] propounded the virtues of [[free love]] in the [[Anarchism in France|Parisian anarchist milieu]] of the early 20th century]]
An important current within anarchism is [[free love]].
Free love advocates sometimes traced their roots back to [[Josiah Warren]] and to experimental communities, viewed sexual freedom as a clear, direct expression of an individual's sovereignty.
Free love particularly stressed [[women's rights]] since most sexual laws discriminated against women, see for example marriage laws and anti-birth control measures.
The most important American free love journal was "[[Lucifer the Lightbearer]]" (1883–1907), edited by [[Moses Harman]] and [[Lois Waisbrooker]], but also there existed [[Ezra Heywood]] and Angela Heywood's "[[The Word (free love)|The Word]]" (1872–1890, 1892–1893).
"[[Free Society]]" (1895–1897 as "The Firebrand"; 1897–1904 as "Free Society") was a major anarchist newspaper in the United States at the end of the 19th and beginning of the 20th centuries.
The publication advocated free love and women's rights and critiqued "[[Comstockery]]"—i.e.
censorship of sexual information.
Also [[M. E. Lazarus]] was an important American individualist anarchist who promoted free love.
In New York City's [[Greenwich Village]], [[Bohemianism|bohemian]] feminists and socialists advocated self-realisation and pleasure for women (and also men) in the here and now.
They encouraged playing with sexual roles and sexuality and the openly bisexual radical [[Edna St.
Vincent Millay]] and the lesbian anarchist [[Margaret C. Anderson|Margaret Anderson]] were prominent among them.
Discussion groups organised by the Villagers were frequented by Emma Goldman, among others.
Magnus Hirschfeld noted in 1923 that Goldman "has campaigned boldly and steadfastly for individual rights, and especially for those deprived of their rights.
Thus it came about that she was the first and only woman, indeed the first and only American, to take up the defence of homosexual love before the general public".
Before Goldman, [[heterosexual]] anarchist Robert Reitzel (1849–1898) spoke positively of homosexuality from the beginning of the 1890s in his Detroit-based [[German language]] journal "Der arme Teufel" (English: The Poor Devil).
In Argentina, anarcha-feminist [[Virginia Bolten]] published the newspaper called "" (English: "The Woman's Voice"), which was published nine times in Rosario between 8 January 1896 and 1 January 1897 and was revived briefly in 1901.
In Europe, the main propagandist of free love within individualist anarchism was Emile Armand.
He proposed the concept of "la camaraderie amoureuse" to speak of free love as the possibility of voluntary sexual encounter between consenting adults.
He was also a consistent proponent of [[polyamory]].
In Germany, the [[Stirnerism|Stirnerists]] Adolf Brand and John Henry Mackay were pioneering campaigners for the acceptance of male [[bisexuality]] and [[homosexuality]].
[[Mujeres Libres]] was an anarchist women's organisation in Spain that aimed to empower working class women.
It was founded in 1936 by [[Lucía Sánchez Saornil]], Mercedes Comaposada and [[Amparo Poch y Gascón]] and had approximately 30,000 members.
The organisation was based on the idea of a "double struggle" for [[Feminist movement|women's liberation]] and social revolution and argued that the two objectives were equally important and should be pursued in parallel.
In order to gain mutual support, they created networks of women anarchists.
Lucía Sánchez Saornil was a main founder of the Spanish anarcha-feminist federation Mujeres Libres who was open about her [[lesbian]]ism.
She was published in a variety of literary journals while working under a male pen name, she was able to explore lesbian themes at a time when homosexuality was criminalised and subject to [[censorship]] and punishment.
More recently, the British anarcho-pacifist Alex Comfort gained notoriety during the [[sexual revolution]] for writing the bestseller sex manual "[[The Joy of Sex]]".
The issue of free love has a dedicated treatment in the work of French anarcho-[[hedonist]] philosopher [[Michel Onfray]] in such works as "Théorie du corps amoureux.
Pour une érotique solaire" (2000) and "L'invention du plaisir.
Fragments cyréaniques" (2002).
[[File:Francisco Ferrer Guardia.jpg|thumb|left|upright|[[Francesc Ferrer i Guàrdia]], [[Anarchism in Spain|Catalan anarchist]] pedagogue and [[Freethought|free thinker]]]]
For English anarchist William Godwin, education was "the main means by which change would be achieved".
Godwin saw that the main goal of education should be the promotion of happiness.
For Godwin, education had to have a "respect for the child's autonomy which precluded any form of coercion", a "pedagogy that respected this and sought to build on the child's own motivation and initiatives" and a "concern about the child's capacity to resist an ideology transmitted through the school".
In his "[[Political Justice]]", he criticises state sponsored schooling "on account of its obvious alliance with national government".
Early American anarchist Josiah Warren advanced alternative education experiences in the libertarian communities he established.
Max Stirner wrote in 1842 a long essay on education called "[[The False Principle of our Education]]" in which Stirner names his educational principle "personalist", explaining that self-understanding consists in hourly self-creation.
Education for him is to create "free men, sovereign characters", by which he means "eternal characters [...] who are therefore eternal because they form themselves each moment".
In the United States, freethought was a basically [[Anti-Christianity|anti-Christian]], [[anti-clerical]] movement, whose purpose was to make the individual politically and spiritually free to decide for himself on religious matters.
A number of contributors to "Liberty" (anarchist publication) were prominent figures in both freethought and anarchism.
The individualist anarchist George MacDonald was a co-editor of "Freethought" and, for a time, "The Truth Seeker".
E.C.
Walker was co-editor of "[[Lucifer, the Light-Bearer]]" and many anarchists were "ardent freethinkers; reprints from freethought papers such as "Lucifer, the Light-Bearer", "Freethought" and "The Truth Seeker" appeared in "Liberty"... The church was viewed as a common ally of the state and as a repressive force in and of itself".
In 1901, Catalan anarchist and free thinker Francesc Ferrer i Guàrdia established "modern" or [[Progressive education|progressive schools]] in Barcelona in defiance of an educational system controlled by the Catholic Church.
The schools' stated goal was to "educate the working class in a rational, secular and non-coercive setting".
Fiercely anti-clerical, Ferrer believed in "freedom in education", education free from the authority of church and state.
Murray Bookchin wrote: "This period [1890s] was the heyday of libertarian schools and pedagogical projects in all areas of the country where Anarchists exercised some degree of influence.
Perhaps the best-known effort in this field was Francisco Ferrer's Modern School (Escuela Moderna), a project which exercised a considerable influence on Catalan education and on experimental techniques of teaching generally".
La Escuela Moderna and Ferrer's ideas generally formed the inspiration for a series of "[[Modern School (United States)|Modern Schools]]" in the United States, Cuba, South America and London.
The first of these was started in New York City in 1911.
It also inspired the Italian newspaper "[[Università popolare (Italian newspaper)|Università popolare]]", founded in 1901.
Russian christian anarchist Leo Tolstoy established a school for peasant children on his estate.
Tolstoy's educational experiments were short-lived due to harassment by the Tsarist secret police.
Tolstoy established a conceptual difference between education and culture.
He thought that "[e]ducation is the tendency of one man to make another just like himself [...] Education is culture under restraint, culture is free.
[Education is] when the teaching is forced upon the pupil, and when then instruction is exclusive, that is when only those subjects are taught which the educator regards as necessary".
For him, "without compulsion, education was transformed into culture".
A more recent libertarian tradition on education is that of [[unschooling]] and the [[anarchist free school|free school]] in which child-led activity replaces pedagogic approaches.
Experiments in Germany led to [[A. S. Neill]] founding what became [[Summerhill School]] in 1921.
Summerhill is often cited as an example of anarchism in practice.
However, although Summerhill and other free schools are radically libertarian, they differ in principle from those of Ferrer by not advocating an overtly political class struggle-approach.
In addition to organising schools according to libertarian principles, anarchists have also questioned the concept of schooling per se.
The term [[deschooling]] was popularised by [[Ivan Illich]], who argued that the school as an institution is dysfunctional for self-determined learning and serves the creation of a consumer society instead.
Criticisms of anarchism include moral criticisms and pragmatic criticisms.
Anarchism is often evaluated as unfeasible or [[utopian]] by its critics.
[[Category:Anarchism| ]]
[[Category:Anti-capitalism]]
[[Category:Anti-fascism]]
[[Category:Far-left politics]]
[[Category:Libertarian socialism]]
[[Category:Political culture]]
[[Category:Political ideologies]]
[[Category:Social theories]]

</doc>
<doc id="25" url="https://en.wikipedia.org/wiki?curid=25" title="Autism">
Autism

Autism is a developmental disorder characterized by troubles with social interaction and communication, and by restricted and repetitive behavior.
Parents usually notice signs during the first two or three years of their child's life.
These signs often develop gradually, though some children with autism reach their developmental milestones at a normal pace before worsening.
Autism is associated with a combination of genetic and environmental factors.
Risk factors during pregnancy include certain infections, such as rubella, and toxins including valproic acid, alcohol, cocaine, pesticides and air pollution.
Controversies surround other proposed environmental causes; for example, the vaccine hypotheses, which have been disproven.
Autism affects information processing in the brain by altering how nerve cells and their synapses connect and organize; how this occurs is not well understood.
In the DSM-5, autism and less severe forms of the condition, including Asperger syndrome and pervasive developmental disorder not otherwise specified (PDD-NOS), have been combined into the diagnosis of autism spectrum disorder (ASD).
Early speech or behavioral interventions can help children with autism gain self-care, social, and communication skills.
Although there is no known cure, there have been cases of children who recovered.
Not many children with autism live independently after reaching adulthood, though some are successful.
An autistic culture has developed, with some individuals seeking a cure and others believing autism should be accepted as a difference and not treated as a disorder.
Globally, autism is estimated to affect 24.8 million people as of 2015.
In the 2000s, the number of people affected was estimated at 1–2 per 1,000 people worldwide.
In the developed countries, about 1.5% of children are diagnosed with ASD , a more than doubling from 0.7% in 2000 in the United States.
It occurs four-to-five times more often in boys than girls.
The number of people diagnosed has increased dramatically since the 1960s, partly due to changes in diagnostic practice; the question of whether actual rates have increased is unresolved.
Autism is a highly variable neurodevelopmental disorder that first appears during infancy or childhood, and generally follows a steady course without remission.
People with autism may be severely impaired in some respects but normal, or even superior, in others.
Overt symptoms gradually begin after the age of six months, become established by age two or three years and tend to continue through adulthood, although often in more muted form.
It is distinguished not by a single symptom but by a characteristic triad of symptoms: impairments in social interaction; impairments in communication; and restricted interests and repetitive behavior.
Other aspects, such as atypical eating, are also common but are not essential for diagnosis.
Individual symptoms of autism occur in the general population and appear not to associate highly, without a sharp line separating pathologically severe from common traits.
Social deficits distinguish autism and the related autism spectrum disorders (ASD; see Classification) from other developmental disorders.
People with autism have social impairments and often lack the intuition about others that many people take for granted.
Noted autistic Temple Grandin described her inability to understand the social communication of neurotypicals, or people with normal neural development, as leaving her feeling "like an anthropologist on Mars".
Unusual social development becomes apparent early in childhood.
Autistic infants show less attention to social stimuli, smile and look at others less often, and respond less to their own name.
Autistic toddlers differ more strikingly from social norms; for example, they have less eye contact and turn-taking, and do not have the ability to use simple movements to express themselves, such as pointing at things.
Three- to five-year-old children with autism are less likely to exhibit social understanding, approach others spontaneously, imitate and respond to emotions, communicate nonverbally, and take turns with others.
However, they do form attachments to their primary caregivers.
Most children with autism display moderately less attachment security than neurotypical children, although this difference disappears in children with higher mental development or less severe ASD.
Older children and adults with ASD perform worse on tests of face and emotion recognition although this may be partly due to a lower ability to define a person's own emotions.
Children with high-functioning autism suffer from more intense and frequent loneliness compared to non-autistic peers, despite the common belief that children with autism prefer to be alone.
Making and maintaining friendships often proves to be difficult for those with autism.
For them, the quality of friendships, not the number of friends, predicts how lonely they feel.
Functional friendships, such as those resulting in invitations to parties, may affect the quality of life more deeply.
There are many anecdotal reports, but few systematic studies, of aggression and violence in individuals with ASD.
The limited data suggest that, in children with intellectual disability, autism is associated with aggression, destruction of property, and tantrums.
About a third to a half of individuals with autism do not develop enough natural speech to meet their daily communication needs.
Differences in communication may be present from the first year of life, and may include delayed onset of babbling, unusual gestures, diminished responsiveness, and vocal patterns that are not synchronized with the caregiver.
In the second and third years, children with autism have less frequent and less diverse babbling, consonants, words, and word combinations; their gestures are less often integrated with words.
Children with autism are less likely to make requests or share experiences, and are more likely to simply repeat others' words (echolalia) or reverse pronouns.
Joint attention seems to be necessary for functional speech, and deficits in joint attention seem to distinguish infants with ASD: for example: they may look at a pointing hand instead of the pointed-at object, and they consistently fail to point at objects in order to comment on or share an experience.
Children with autism may have difficulty with imaginative play and with developing symbols into language.
In a pair of studies, high-functioning children with autism aged 8–15 performed equally well as, and as adults better than, individually matched controls at basic language tasks involving vocabulary and spelling.
Both autistic groups performed worse than controls at complex language tasks such as figurative language, comprehension and inference.
As people are often sized up initially from their basic language skills, these studies suggest that people speaking to autistic individuals are more likely to overestimate what their audience comprehends.
Autistic individuals can display many forms of repetitive or restricted behavior, which the Repetitive Behavior Scale-Revised (RBS-R) categorizes as follows.
No single repetitive or self-injurious behavior seems to be specific to autism, but autism appears to have an elevated pattern of occurrence and severity of these behaviors.
Autistic individuals may have symptoms that are independent of the diagnosis, but that can affect the individual or the family.
An estimated 0.5% to 10% of individuals with ASD show unusual abilities, ranging from splinter skills such as the memorization of trivia to the extraordinarily rare talents of prodigious autistic savants.
Many individuals with ASD show superior skills in perception and attention, relative to the general population.
Sensory abnormalities are found in over 90% of those with autism, and are considered core features by some, although there is no good evidence that sensory symptoms differentiate autism from other developmental disorders.
Differences are greater for under-responsivity (for example, walking into things) than for over-responsivity (for example, distress from loud noises) or for sensation seeking (for example, rhythmic movements).
An estimated 60–80% of autistic people have motor signs that include poor muscle tone, poor motor planning, and toe walking; deficits in motor coordination are pervasive across ASD and are greater in autism proper.
Unusual eating behavior occurs in about three-quarters of children with ASD, to the extent that it was formerly a diagnostic indicator.
Selectivity is the most common problem, although eating rituals and food refusal also occur; this does not appear to result in malnutrition.
Although some children with autism also have gastrointestinal symptoms, there is a lack of published rigorous data to support the theory that children with autism have more or different gastrointestinal symptoms than usual; studies report conflicting results, and the relationship between gastrointestinal problems and ASD is unclear.
Parents of children with ASD have higher levels of stress.
Siblings of children with ASD report greater admiration of and less conflict with the affected sibling than siblings of unaffected children and were similar to siblings of children with Down syndrome in these aspects of the sibling relationship.
However, they reported lower levels of closeness and intimacy than siblings of children with Down syndrome; siblings of individuals with ASD have greater risk of negative well-being and poorer sibling relationships as adults.
It has long been presumed that there is a common cause at the genetic, cognitive, and neural levels for autism's characteristic triad of symptoms.
However, there is increasing suspicion that autism is instead a complex disorder whose core aspects have distinct causes that often co-occur.
Autism has a strong genetic basis, although the genetics of autism are complex and it is unclear whether ASD is explained more by rare mutations with major effects, or by rare multigene interactions of common genetic variants.
Complexity arises due to interactions among multiple genes, the environment, and epigenetic factors which do not change DNA sequencing but are heritable and influence gene expression.
Many genes have been associated with autism through sequencing the genomes of affected individuals and their parents.
Studies of twins suggest that heritability is 0.7 for autism and as high as 0.9 for ASD, and siblings of those with autism are about 25 times more likely to be autistic than the general population.
However, most of the mutations that increase autism risk have not been identified.
Typically, autism cannot be traced to a Mendelian (single-gene) mutation or to a single chromosome abnormality, and none of the genetic syndromes associated with ASDs have been shown to selectively cause ASD.
Numerous candidate genes have been located, with only small effects attributable to any particular gene.
Most loci individually explain less than 1% of cases of autism.
The large number of autistic individuals with unaffected family members may result from spontaneous structural variation — such as deletions, duplications or inversions in genetic material during meiosis.
Hence, a substantial fraction of autism cases may be traceable to genetic causes that are highly heritable but not inherited: that is, the mutation that causes the autism is not present in the parental genome.
Autism may be underdiagnosed in women and girls due to an assumption that it is primarily a male condition.
Several lines of evidence point to synaptic dysfunction as a cause of autism.
Some rare mutations may lead to autism by disrupting some synaptic pathways, such as those involved with cell adhesion.
Gene replacement studies in mice suggest that autistic symptoms are closely related to later developmental steps that depend on activity in synapses and on activity-dependent changes.
All known teratogens (agents that cause birth defects) related to the risk of autism appear to act during the first eight weeks from conception, and though this does not exclude the possibility that autism can be initiated or affected later, there is strong evidence that autism arises very early in development.
Exposure to air pollution during pregnancy, especially heavy metals and particulates, may increase the risk of autism.
Environmental factors that have been claimed without evidence to contribute to or exacerbate autism include certain foods, infectious diseases, solvents, PCBs, phthalates and phenols used in plastic products, pesticides, brominated flame retardants, alcohol, smoking, illicit drugs, vaccines, and prenatal stress.
Some such as the MMR vaccine have been completely disproven.
Parents may first become aware of autistic symptoms in their child around the time of a routine vaccination.
This has led to unsupported theories blaming vaccine "overload", a vaccine preservative, or the MMR vaccine for causing autism.
The latter theory was supported by a litigation-funded study that has since been shown to have been "an elaborate fraud".
Although these theories lack convincing scientific evidence and are biologically implausible, parental concern about a potential vaccine link with autism has led to lower rates of childhood immunizations, outbreaks of previously controlled childhood diseases in some countries, and the preventable deaths of several children.
Autism's symptoms result from maturation-related changes in various systems of the brain.
How autism occurs is not well understood.
Its mechanism can be divided into two areas: the pathophysiology of brain structures and processes associated with autism, and the neuropsychological linkages between brain structures and behaviors.
The behaviors appear to have multiple pathophysiologies.
Unlike many other brain disorders, such as Parkinson's, autism does not have a clear unifying mechanism at either the molecular, cellular, or systems level; it is not known whether autism is a few disorders caused by mutations converging on a few common molecular pathways, or is (like intellectual disability) a large set of disorders with diverse mechanisms.
Autism appears to result from developmental factors that affect many or all functional brain systems, and to disturb the timing of brain development more than the final product.
Neuroanatomical studies and the associations with teratogens strongly suggest that autism's mechanism includes alteration of brain development soon after conception.
This anomaly appears to start a cascade of pathological events in the brain that are significantly influenced by environmental factors.
Just after birth, the brains of children with autism tend to grow faster than usual, followed by normal or relatively slower growth in childhood.
It is not known whether early overgrowth occurs in all children with autism.
It seems to be most prominent in brain areas underlying the development of higher cognitive specialization.
Hypotheses for the cellular and molecular bases of pathological early overgrowth include the following:

The immune system is thought to play an important role in autism.
Children with autism have been found by researchers to have inflammation of both the peripheral and central immune systems as indicated by increased levels of pro-inflammatory cytokines and significant activation of microglia.
Biomarkers of abnormal immune function have also been associated with increased impairments in behaviors that are characteristic of the core features of autism such as, deficits in social interactions and communication.
Interactions between the immune system and the nervous system begin early during the embryonic stage of life, and successful neurodevelopment depends on a balanced immune response.
It is thought that activation of a pregnant mother's immune system such as from environmental toxicants or infection can contribute to causing autism through causing a disruption of brain development.
This is supported by recent studies that have found that infection during pregnancy is associated with an increased risk of autism.
The relationship of neurochemicals to autism is not well understood; several have been investigated, with the most evidence for the role of serotonin and of genetic differences in its transport.
The role of group I metabotropic glutamate receptors (mGluR) in the pathogenesis of fragile X syndrome, the most common identified genetic cause of autism, has led to interest in the possible implications for future autism research into this pathway.
Some data suggests neuronal overgrowth potentially related to an increase in several growth hormones or to impaired regulation of growth factor receptors.
Also, some inborn errors of metabolism are associated with autism, but probably account for less than 5% of cases.
The mirror neuron system (MNS) theory of autism hypothesizes that distortion in the development of the MNS interferes with imitation and leads to autism's core features of social impairment and communication difficulties.
The MNS operates when an animal performs an action or observes another animal perform the same action.
The MNS may contribute to an individual's understanding of other people by enabling the modeling of their behavior via embodied simulation of their actions, intentions, and emotions.
Several studies have tested this hypothesis by demonstrating structural abnormalities in MNS regions of individuals with ASD, delay in the activation in the core circuit for imitation in individuals with Asperger syndrome, and a correlation between reduced MNS activity and severity of the syndrome in children with ASD.
However, individuals with autism also have abnormal brain activation in many circuits outside the MNS and the MNS theory does not explain the normal performance of children with autism on imitation tasks that involve a goal or object.
ASD-related patterns of low function and aberrant activation in the brain differ depending on whether the brain is doing social or nonsocial tasks.
In autism there is evidence for reduced functional connectivity of the default network (a large-scale brain network involved in social and emotional processing), with intact connectivity of the task-positive network (used in sustained attention and goal-directed thinking).
In people with autism the two networks are not negatively correlated in time, suggesting an imbalance in toggling between the two networks, possibly reflecting a disturbance of self-referential thought.
The underconnectivity theory of autism hypothesizes that autism is marked by underfunctioning high-level neural connections and synchronization, along with an excess of low-level processes.
Evidence for this theory has been found in functional neuroimaging studies on autistic individuals and by a brainwave study that suggested that adults with ASD have local overconnectivity in the cortex and weak functional connections between the frontal lobe and the rest of the cortex.
Other evidence suggests the underconnectivity is mainly within each hemisphere of the cortex and that autism is a disorder of the association cortex.
From studies based on event-related potentials, transient changes to the brain's electrical activity in response to stimuli, there is considerable evidence for differences in autistic individuals with respect to attention, orientation to auditory and visual stimuli, novelty detection, language and face processing, and information storage; several studies have found a preference for nonsocial stimuli.
For example, magnetoencephalography studies have found evidence in children with autism of delayed responses in the brain's processing of auditory signals.
In the genetic area, relations have been found between autism and schizophrenia based on duplications and deletions of chromosomes; research showed that schizophrenia and autism are significantly more common in combination with 1q21.1 deletion syndrome.
Research on autism/schizophrenia relations for chromosome 15 (15q13.3), chromosome 16 (16p13.1) and chromosome 17 (17p12) are inconclusive.
Functional connectivity studies have found both hypo- and hyper-connectivity in brains of people with autism.
Hypo-connectivity seems to dominate, especially for interhemispheric and cortico-cortical functional connectivity.
Two major categories of cognitive theories have been proposed about the links between autistic brains and behavior.
The first category focuses on deficits in social cognition.
Simon Baron-Cohen's empathizing–systemizing theory postulates that autistic individuals can systemize—that is, they can develop internal rules of operation to handle events inside the brain—but are less effective at empathizing by handling events generated by other agents.
An extension, the extreme male brain theory, hypothesizes that autism is an extreme case of the male brain, defined psychometrically as individuals in whom systemizing is better than empathizing.
These theories are somewhat related to Baron-Cohen's earlier theory of mind approach, which hypothesizes that autistic behavior arises from an inability to ascribe mental states to oneself and others.
The theory of mind hypothesis is supported by the atypical responses of children with autism to the Sally–Anne test for reasoning about others' motivations, and the mirror neuron system theory of autism described in "Pathophysiology" maps well to the hypothesis.
However, most studies have found no evidence of impairment in autistic individuals' ability to understand other people's basic intentions or goals; instead, data suggests that impairments are found in understanding more complex social emotions or in considering others' viewpoints.
The second category focuses on nonsocial or general processing: the executive functions such as working memory, planning, inhibition.
In his review, Kenworthy states that "the claim of executive dysfunction as a causal factor in autism is controversial", however, "it is clear that executive dysfunction plays a role in the social and cognitive deficits observed in individuals with autism".
Tests of core executive processes such as eye movement tasks indicate improvement from late childhood to adolescence, but performance never reaches typical adult levels.
A strength of the theory is predicting stereotyped behavior and narrow interests; two weaknesses are that executive function is hard to measure and that executive function deficits have not been found in young children with autism.
Weak central coherence theory hypothesizes that a limited ability to see the big picture underlies the central disturbance in autism.
One strength of this theory is predicting special talents and peaks in performance in autistic people.
A related theory—enhanced perceptual functioning—focuses more on the superiority of locally oriented and perceptual operations in autistic individuals.
Yet another, monotropism, posits that autism stems from a different cognitive style, tending to focus attention (or processing resources) intensely, to the exclusion of other stimuli.
These theories map well from the underconnectivity theory of autism.
Neither category is satisfactory on its own; social cognition theories poorly address autism's rigid and repetitive behaviors, while the nonsocial theories have difficulty explaining social impairment and communication difficulties.
A combined theory based on multiple deficits may prove to be more useful.
Diagnosis is based on behavior, not cause or mechanism.
Under the DSM-5, autism is characterized by persistent deficits in social communication and interaction across multiple contexts, as well as restricted, repetitive patterns of behavior, interests, or activities.
These deficits are present in early childhood, typically before age three, and lead to clinically significant functional impairment.
Sample symptoms include lack of social or emotional reciprocity, stereotyped and repetitive use of language or idiosyncratic language, and persistent preoccupation with unusual objects.
The disturbance must not be better accounted for by Rett syndrome, intellectual disability or global developmental delay.
ICD-10 uses essentially the same definition.
Several diagnostic instruments are available.
Two are commonly used in autism research: the Autism Diagnostic Interview-Revised (ADI-R) is a semistructured parent interview, and the Autism Diagnostic Observation Schedule (ADOS) uses observation and interaction with the child.
The Childhood Autism Rating Scale (CARS) is used widely in clinical environments to assess severity of autism based on observation of children.
The Diagnostic interview for social and communication disorders (DISCO) may also be used.
A pediatrician commonly performs a preliminary investigation by taking developmental history and physically examining the child.
If warranted, diagnosis and evaluations are conducted with help from ASD specialists, observing and assessing cognitive, communication, family, and other factors using standardized tools, and taking into account any associated medical conditions.
A pediatric neuropsychologist is often asked to assess behavior and cognitive skills, both to aid diagnosis and to help recommend educational interventions.
A differential diagnosis for ASD at this stage might also consider intellectual disability, hearing impairment, and a specific language impairment such as Landau–Kleffner syndrome.
The presence of autism can make it harder to diagnose coexisting psychiatric disorders such as depression.
Clinical genetics evaluations are often done once ASD is diagnosed, particularly when other symptoms already suggest a genetic cause.
Although genetic technology allows clinical geneticists to link an estimated 40% of cases to genetic causes, consensus guidelines in the US and UK are limited to high-resolution chromosome and fragile X testing.
A genotype-first model of diagnosis has been proposed, which would routinely assess the genome's copy number variations.
As new genetic tests are developed several ethical, legal, and social issues will emerge.
Commercial availability of tests may precede adequate understanding of how to use test results, given the complexity of autism's genetics.
Metabolic and neuroimaging tests are sometimes helpful, but are not routine.
ASD can sometimes be diagnosed by age 14 months, although diagnosis becomes increasingly stable over the first three years of life: for example, a one-year-old who meets diagnostic criteria for ASD is less likely than a three-year-old to continue to do so a few years later.
In the UK the National Autism Plan for Children recommends at most 30 weeks from first concern to completed diagnosis and assessment, though few cases are handled that quickly in practice.
Although the symptoms of autism and ASD begin early in childhood, they are sometimes missed; years later, adults may seek diagnoses to help them or their friends and family understand themselves, to help their employers make adjustments, or in some locations to claim disability living allowances or other benefits.
Girls are often diagnosed later than boys.
Underdiagnosis and overdiagnosis are problems in marginal cases, and much of the recent increase in the number of reported ASD cases is likely due to changes in diagnostic practices.
The increasing popularity of drug treatment options and the expansion of benefits has given providers incentives to diagnose ASD, resulting in some overdiagnosis of children with uncertain symptoms.
Conversely, the cost of screening and diagnosis and the challenge of obtaining payment can inhibit or delay diagnosis.
It is particularly hard to diagnose autism among the visually impaired, partly because some of its diagnostic criteria depend on vision, and partly because autistic symptoms overlap with those of common blindness syndromes or blindisms.
Autism is one of the five pervasive developmental disorders (PDD), which are characterized by widespread abnormalities of social interactions and communication, and severely restricted interests and highly repetitive behavior.
These symptoms do not imply sickness, fragility, or emotional disturbance.
Of the five PDD forms, Asperger syndrome is closest to autism in signs and likely causes; Rett syndrome and childhood disintegrative disorder share several signs with autism, but may have unrelated causes; PDD not otherwise specified (PDD-NOS; also called "atypical autism") is diagnosed when the criteria are not met for a more specific disorder.
Unlike with autism, people with Asperger syndrome have no substantial delay in language development.
The terminology of autism can be bewildering, with autism, Asperger syndrome and PDD-NOS often called the "autism spectrum disorders" (ASD) or sometimes the "autistic disorders", whereas autism itself is often called "autistic disorder", "childhood autism", or "infantile autism".
In this article, "autism" refers to the classic autistic disorder; in clinical practice, though, "autism", "ASD", and "PDD" are often used interchangeably.
ASD, in turn, is a subset of the broader autism phenotype, which describes individuals who may not have ASD but do have autistic-like traits, such as avoiding eye contact.
The manifestations of autism cover a wide spectrum, ranging from individuals with severe impairments—who may be silent, developmentally disabled, and locked into hand flapping and rocking—to high functioning individuals who may have active but distinctly odd social approaches, narrowly focused interests, and verbose, pedantic communication.
Because the behavior spectrum is continuous, boundaries between diagnostic categories are necessarily somewhat arbitrary.
Sometimes the syndrome is divided into low-, medium- or high-functioning autism (LFA, MFA, and HFA), based on IQ thresholds, or on how much support the individual requires in daily life; these subdivisions are not standardized and are controversial.
Autism can also be divided into syndromal and non-syndromal autism; the syndromal autism is associated with severe or profound intellectual disability or a congenital syndrome with physical symptoms, such as tuberous sclerosis.
Although individuals with Asperger syndrome tend to perform better cognitively than those with autism, the extent of the overlap between Asperger syndrome, HFA, and non-syndromal autism is unclear.
Some studies have reported diagnoses of autism in children due to a loss of language or social skills, as opposed to a failure to make progress, typically from 15 to 30 months of age.
The validity of this distinction remains controversial; it is possible that regressive autism is a specific subtype, or that there is a continuum of behaviors between autism with and without regression.
Research into causes has been hampered by the inability to identify biologically meaningful subgroups within the autistic population and by the traditional boundaries between the disciplines of psychiatry, psychology, neurology and pediatrics.
Newer technologies such as fMRI and diffusion tensor imaging can help identify biologically relevant phenotypes (observable traits) that can be viewed on brain scans, to help further neurogenetic studies of autism; one example is lowered activity in the fusiform face area of the brain, which is associated with impaired perception of people versus objects.
It has been proposed to classify autism using genetics as well as behavior.
About half of parents of children with ASD notice their child's unusual behaviors by age 18 months, and about four-fifths notice by age 24 months.
According to an article, failure to meet any of the following milestones "is an absolute indication to proceed with further evaluations.
Delay in referral for such testing may delay early diagnosis and treatment and affect the long-term outcome".
The United States Preventive Services Task Force in 2016 found it was unclear if screening was beneficial or harmful among children in whom there is no concerns.
The Japanese practice is to screen all children for ASD at 18 and 24 months, using autism-specific formal screening tests.
In contrast, in the UK, children whose families or doctors recognize possible signs of autism are screened.
It is not known which approach is more effective.
Screening tools include the Modified Checklist for Autism in Toddlers (M-CHAT), the Early Screening of Autistic Traits Questionnaire, and the First Year Inventory; initial data on M-CHAT and its predecessor, the Checklist for Autism in Toddlers (CHAT), on children aged 18–30 months suggests that it is best used in a clinical setting and that it has low sensitivity (many false-negatives) but good specificity (few false-positives).
It may be more accurate to precede these tests with a broadband screener that does not distinguish ASD from other developmental disorders.
Screening tools designed for one culture's norms for behaviors like eye contact may be inappropriate for a different culture.
Although genetic screening for autism is generally still impractical, it can be considered in some cases, such as children with neurological symptoms and dysmorphic features.
While infection with rubella during pregnancy causes fewer than 1% of cases of autism, vaccination against rubella can prevent many of those cases.
The main goals when treating children with autism are to lessen associated deficits and family distress, and to increase quality of life and functional independence.
In general, higher IQs are correlated with greater responsiveness to treatment and improved treatment outcomes.
No single treatment is best and treatment is typically tailored to the child's needs.
Families and the educational system are the main resources for treatment.
Services should be carried out by behavior analysts, special education teachers, speech pathologists, and licensed psychologists.
Studies of interventions have methodological problems that prevent definitive conclusions about efficacy.
However, the development of evidence-based interventions has advanced in recent years.
Although many psychosocial interventions have some positive evidence, suggesting that some form of treatment is preferable to no treatment, the methodological quality of systematic reviews of these studies has generally been poor, their clinical results are mostly tentative, and there is little evidence for the relative effectiveness of treatment options.
Intensive, sustained special education programs and behavior therapy early in life can help children acquire self-care, social communication, and job skills, and often improve functioning and decrease symptom severity and maladaptive behaviors; claims that intervention by around age three years is crucial are not substantiated.
While medications have not been found to help with core symptoms, they may be used for associated symptoms, such as irritability, inattention, or repetitive behavior patterns.
Educational interventions often used include applied behavior analysis (ABA), developmental models, structured teaching, speech and language therapy, social skills therapy, and occupational therapy.
Among these approaches, interventions either treat autistic features comprehensively, or focalize treatment on a specific area of deficit.
The quality of research for early intensive behavioral intervention (EIBI)—a treatment procedure encompassing over thirty hours per week of the structured type of ABA that is carried out with very young children—is currently low, and more vigorous research designs with larger sample sizes are needed.
Two theoretical frameworks outlined for early childhood intervention include structured and naturalistic ABA interventions, and developmental social pragmatic models (DSP).
One interventional strategy utilizes a parent training model, which teaches parents how to implement various ABA and DSP techniques, allowing for parents to disseminate interventions themselves.
Various DSP programs have been developed to explicitly deliver intervention systems through at-home parent implementation.
Despite the recent development of parent training models, these interventions have demonstrated effectiveness in numerous studies, being evaluated as a probable efficacious mode of treatment.
Early, intensive ABA therapy has demonstrated effectiveness in enhancing global functioning in preschool children, and is well-established for improving the intellectual performance of that age group.
Similarly, a teacher-implemented intervention that utilizes a more naturalistic form of ABA combined with a developmental social pragmatic approach has been found to be beneficial in improving social-communication skills in young children, although there is less evidence in its treatment of global symptoms.
Neuropsychological reports are often poorly communicated to educators, resulting in a gap between what a report recommends and what education is provided.
It is not known whether treatment programs for children lead to significant improvements after the children grow up, and the limited research on the effectiveness of adult residential programs shows mixed results.
The appropriateness of including children with varying severity of autism spectrum disorders in the general education population is a subject of current debate among educators and researchers.
Medications may be used to treat ASD symptoms that interfere with integrating a child into home or school when behavioral treatment fails.
They may also be used for associated health problems, such as ADHD or anxiety.
More than half of US children diagnosed with ASD are prescribed psychoactive drugs or anticonvulsants, with the most common drug classes being antidepressants, stimulants, and antipsychotics.
The atypical antipsychotic drugs risperidone and aripiprazole are FDA-approved for treating associated aggressive and self-injurious behaviors.
However, their side effects must be weighed against their potential benefits, and people with autism may respond atypically.
Side effects, for example, may include weight gain, tiredness, drooling, and aggression.
SSRI antidepressants, such as fluoxetine and fluvoxamine, have been shown to be effective in reducing repetitive and ritualistic behaviors, while the stimulant medication methylphenidate is beneficial for some children with co-morbid inattentiveness or hyperactivity.
There is scant reliable research about the effectiveness or safety of drug treatments for adolescents and adults with ASD.
No known medication relieves autism's core symptoms of social and communication impairments.
Experiments in mice have reversed or reduced some symptoms related to autism by replacing or modulating gene function, suggesting the possibility of targeting therapies to specific rare mutations known to cause autism.
Although many alternative therapies and interventions are available, few are supported by scientific studies.
Treatment approaches have little empirical support in quality-of-life contexts, and many programs focus on success measures that lack predictive validity and real-world relevance.
Some alternative treatments may place the child at risk.
A 2008 study found that compared to their peers, autistic boys have significantly thinner bones if on casein-free diets; in 2005, botched chelation therapy killed a five-year-old child with autism.
Another alternative medicine practice with no evidence is CEASE therapy, a mixture of homeopathy, supplements, and 'vaccine detoxing'.
Although popularly used as an alternative treatment for people with autism, there is no good evidence that a gluten-free diet is of benefit.
In the subset of people who have gluten sensitivity there is limited evidence that suggests that a gluten free diet may improve some autistic behaviors.
There is tentative evidence that music therapy may improve social interactions, verbal communication, and non-verbal communication skills.
There has been early research looking at hyperbaric treatments in children with autism.
The emergence of the autism rights movement has served as an attempt to encourage people to be more tolerant of those with autism.
Through this movement, people hope to cause others to think of autism as a difference instead of a disease.
Proponents of this movement wish to seek "acceptance, not cures."
There have also been many worldwide events promoting autism awareness such as World Autism Awareness Day, Light It Up Blue, Autism Sunday, Autistic Pride Day, Autreat, and others.
There have also been many organizations dedicated to increasing the awareness of autism and the effects that autism has on someone's life.
These organizations include Autism Speaks, Autism National Committee, Autism Society of America, and many others.
Social-science scholars have had an increased focused on studying those with autism in hopes to learn more about "autism as a culture, transcultural comparisons... and research on social movements."
Media has had an influence on how the public perceives those with autism.
"Rain Man", a film that won 4 Oscars including Best Picture, depicts a character with autism who has incredible talents and abilities.
While many autistic individuals don't have these special abilities, there are some who have been successful in their fields.
Treatment is expensive; indirect costs are more so.
For someone born in 2000, a US study estimated an average lifetime cost of $ (net present value in dollars, inflation-adjusted from 2003 estimate), with about 10% medical care, 30% extra education and other care, and 60% lost economic productivity.
Publicly supported programs are often inadequate or inappropriate for a given child, and unreimbursed out-of-pocket medical or therapy expenses are associated with likelihood of family financial problems; one 2008 US study found a 14% average loss of annual income in families of children with ASD, and a related study found that ASD is associated with higher probability that child care problems will greatly affect parental employment.
US states increasingly require private health insurance to cover autism services, shifting costs from publicly funded education programs to privately funded health insurance.
After childhood, key treatment issues include residential care, job training and placement, sexuality, social skills, and estate planning.
There is no known cure.
Children recover occasionally, so that they lose their diagnosis of ASD; this occurs sometimes after intensive treatment and sometimes not.
It is not known how often recovery happens; reported rates in unselected samples have ranged from 3% to 25%.
Most children with autism acquire language by age five or younger, though a few have developed communication skills in later years.
Most children with autism lack social support, meaningful relationships, future employment opportunities or self-determination.
Although core difficulties tend to persist, symptoms often become less severe with age.
Few high-quality studies address long-term prognosis.
Some adults show modest improvement in communication skills, but a few decline; no study has focused on autism after midlife.
Acquiring language before age six, having an IQ above 50, and having a marketable skill all predict better outcomes; independent living is unlikely with severe autism.
Most people with autism face significant obstacles in transitioning to adulthood.
Most recent reviews tend to estimate a prevalence of 1–2 per 1,000 for autism and close to 6 per 1,000 for ASD, and 11 per 1,000 children in the United States for ASD as of 2008; because of inadequate data, these numbers may underestimate ASD's true rate.
Globally, autism affects an estimated 24.8 million people as of 2015, while Asperger syndrome affects a further 37.2 million.
In 2012, the NHS estimated that the overall prevalence of autism among adults aged 18 years and over in the UK was 1.1%.
Rates of PDD-NOS's has been estimated at 3.7 per 1,000, Asperger syndrome at roughly 0.6 per 1,000, and childhood disintegrative disorder at 0.02 per 1,000.
CDC's most recent estimate is that 1 out of every 68 children, or 14.7 per 1,000, has an ASD as of 2010.
The number of reported cases of autism increased dramatically in the 1990s and early 2000s.
This increase is largely attributable to changes in diagnostic practices, referral patterns, availability of services, age at diagnosis, and public awareness, though unidentified environmental risk factors cannot be ruled out.
The available evidence does not rule out the possibility that autism's true prevalence has increased; a real increase would suggest directing more attention and funding toward changing environmental factors instead of continuing to focus on genetics.
Boys are at higher risk for ASD than girls.
The sex ratio averages 4.3:1 and is greatly modified by cognitive impairment: it may be close to 2:1 with intellectual disability and more than 5.5:1 without.
Several theories about the higher prevalence in males have been investigated, but the cause of the difference is unconfirmed; one theory is that females are underdiagnosed.
Although the evidence does not implicate any single pregnancy-related risk factor as a cause of autism, the risk of autism is associated with advanced age in either parent, and with diabetes, bleeding, and use of psychiatric drugs in the mother during pregnancy.
The risk is greater with older fathers than with older mothers; two potential explanations are the known increase in mutation burden in older sperm, and the hypothesis that men marry later if they carry genetic liability and show some signs of autism.
Most professionals believe that race, ethnicity, and socioeconomic background do not affect the occurrence of autism.
Several other conditions are common in children with autism.
They include:

A few examples of autistic symptoms and treatments were described long before autism was named.
The "Table Talk" of Martin Luther, compiled by his notetaker, Mathesius, contains the story of a 12-year-old boy who may have been severely autistic.
Luther reportedly thought the boy was a soulless mass of flesh possessed by the devil, and suggested that he be suffocated, although a later critic has cast doubt on the veracity of this report.
The earliest well-documented case of autism is that of Hugh Blair of Borgue, as detailed in a 1747 court case in which his brother successfully petitioned to annul Blair's marriage to gain Blair's inheritance.
The Wild Boy of Aveyron, a feral child caught in 1798, showed several signs of autism; the medical student Jean Itard treated him with a behavioral program designed to help him form social attachments and to induce speech via imitation.
The New Latin word "autismus" (English translation "autism") was coined by the Swiss psychiatrist Eugen Bleuler in 1910 as he was defining symptoms of schizophrenia.
He derived it from the Greek word "autós" (αὐτός, meaning "self"), and used it to mean morbid self-admiration, referring to "autistic withdrawal of the patient to his fantasies, against which any influence from outside becomes an intolerable disturbance".
The word "autism" first took its modern sense in 1938 when Hans Asperger of the Vienna University Hospital adopted Bleuler's terminology "autistic psychopaths" in a lecture in German about child psychology.
Asperger was investigating an ASD now known as Asperger syndrome, though for various reasons it was not widely recognized as a separate diagnosis until 1981.
Leo Kanner of the Johns Hopkins Hospital first used "autism" in its modern sense in English when he introduced the label "early infantile autism" in a 1943 report of 11 children with striking behavioral similarities.
Almost all the characteristics described in Kanner's first paper on the subject, notably "autistic aloneness" and "insistence on sameness", are still regarded as typical of the autistic spectrum of disorders.
It is not known whether Kanner derived the term independently of Asperger.
Donald Triplett was the first person diagnosed with autism.
He was diagnosed by Kanner after being first examined in 1938, and was labeled as "case 1".
Triplett was noted for his savant abilities, particularly being able to name musical notes played on a piano and to mentally multiply numbers.
His father, Oliver, described him as socially withdrawn but interested in number patterns, music notes, letters of the alphabet, and U.S.
president pictures.
By the age of 2, he had the ability to recite the 23rd Psalm and memorized 25 questions and answers from the Presbyterian catechism.
He was also interested in creating musical chords.
Kanner's reuse of "autism" led to decades of confused terminology like "infantile schizophrenia", and child psychiatry's focus on maternal deprivation led to misconceptions of autism as an infant's response to "refrigerator mothers".
Starting in the late 1960s autism was established as a separate syndrome.
As late as the mid-1970s there was little evidence of a genetic role in autism; while in 2007 it was believed to be one of the most heritable psychiatric conditions.
Although the rise of parent organizations and the destigmatization of childhood ASD have affected how ASD is viewed, parents continue to feel social stigma in situations where their child's autistic behavior is perceived negatively, and many primary care physicians and medical specialists express some beliefs consistent with outdated autism research.
It took until 1980 for the DSM-III to differentiate autism from childhood schizophrenia.
In 1987, the DSM-III-R provided a checklist for diagnosing autism.
In May 2013, the DSM-5 was released, updating the classification for pervasive developmental disorders.
The grouping of disorders, including PDD-NOS, autism, Asperger syndrome, Rett syndrome, and CDD, has been removed and replaced with the general term of Autism Spectrum Disorders.
The two categories that exist are impaired social communication and/or interaction, and restricted and/or repetitive behaviors.
The Internet has helped autistic individuals bypass nonverbal cues and emotional sharing that they find difficult to deal with, and has given them a way to form online communities and work remotely.
Societal and cultural aspects of autism have developed: some in the community seek a cure, while others believe that autism is simply another way of being.
</doc>
<doc id="39" url="https://en.wikipedia.org/wiki?curid=39" title="Albedo">
Albedo

Albedo () (, meaning 'whiteness') is the measure of the diffuse reflection of solar radiation out of the total solar radiation received by an astronomical body (e.g.
a planet like Earth).
It is dimensionless and measured on a scale from 0 (corresponding to a black body that absorbs all incident radiation) to 1 (corresponding to a body that reflects all incident radiation).
Surface albedo is defined as the ratio of irradiance reflected to the irradiance received by a surface.
The proportion reflected is not only determined by properties of the surface itself, but also by the spectral and angular distribution of solar radiation reaching the Earth's surface.
These factors vary with atmospheric composition, geographic location and time (see position of the Sun).
While bi-hemispherical reflectance is calculated for a single angle of incidence (i.e., for a given position of the Sun), albedo is the directional integration of reflectance over all solar angles in a given period.
The temporal resolution may range from seconds (as obtained from flux measurements) to daily, monthly, or annual averages.
Unless given for a specific wavelength (spectral albedo), albedo refers to the entire spectrum of solar radiation.
Due to measurement constraints, it is often given for the spectrum in which most solar energy reaches the surface (between 0.3 and 3 μm).
This spectrum includes visible light (0.39–0.7 μm), which explains why surfaces with a low albedo appear dark (e.g., trees absorb most radiation), whereas surfaces with a high albedo appear bright (e.g., snow reflects most radiation).
Albedo is an important concept in climatology, astronomy, and environmental management (e.g., as part of the Leadership in Energy and Environmental Design (LEED) program for sustainable rating of buildings).
The average albedo of the Earth from the upper atmosphere, its "planetary albedo", is 30–35% because of cloud cover, but widely varies locally across the surface because of different geological and environmental features.
The term albedo was introduced into optics by Johann Heinrich Lambert in his 1760 work "Photometria".
Any albedo in visible light falls within a range of about 0.9 for fresh snow to about 0.04 for charcoal, one of the darkest substances.
Deeply shadowed cavities can achieve an effective albedo approaching the zero of a black body.
When seen from a distance, the ocean surface has a low albedo, as do most forests, whereas desert areas have some of the highest albedos among landforms.
Most land areas are in an albedo range of 0.1 to 0.4.
The average albedo of Earth is about 0.3.
This is far higher than for the ocean primarily because of the contribution of clouds.
Earth's surface albedo is regularly estimated via Earth observation satellite sensors such as NASA's MODIS instruments on board the Terra and Aqua satellites, and the CERES instrument on the Suomi NPP and JPSS.
As the amount of reflected radiation is only measured for a single direction by satellite, not all directions, a mathematical model is used to translate a sample set of satellite reflectance measurements into estimates of directional-hemispherical reflectance and bi-hemispherical reflectance (e.g.,).
These calculations are based on the bidirectional reflectance distribution function (BRDF), which describes how the reflectance of a given surface depends on the view angle of the observer and the solar angle.
BDRF can facilitate translations of observations of reflectance into albedo.
Earth's average surface temperature due to its albedo and the greenhouse effect is currently about 15 °C.
If Earth were frozen entirely (and hence be more reflective), the average temperature of the planet would drop below −40 °C.
If only the continental land masses became covered by glaciers, the mean temperature of the planet would drop to about 0 °C.
In contrast, if the entire Earth was covered by water — a so-called aquaplanet — the average temperature on the planet would rise to almost 27 °C.
For land surfaces, it has been shown that the albedo at a particular solar zenith angle "θ" can be approximated by the proportionate sum of two terms: the directional-hemispherical reflectance at that solar zenith angle, formula_1, and the bi-hemispherical reflectance, formula_2, with formula_3 being the proportion of direct radiation from a given solar angle, and formula_4 being the proportion of diffuse illumination.
Hence, the actual albedo formula_5 (also called blue-sky albedo) can then be given as:

Directional-hemispherical reflectance is sometimes referred to as black-sky albedo and bi-hemispherical reflectance as white-sky albedo.
These terms are important because they allow the albedo to be calculated for any given illumination conditions from a knowledge of the intrinsic properties of the surface.
The albedos of planets, satellites and minor planets such as asteroids can be used to infer much about their properties.
The study of albedos, their dependence on wavelength, lighting angle ("phase angle"), and variation in time comprises a major part of the astronomical field of photometry.
For small and far objects that cannot be resolved by telescopes, much of what we know comes from the study of their albedos.
For example, the absolute albedo can indicate the surface ice content of outer Solar System objects, the variation of albedo with phase angle gives information about regolith properties, whereas unusually high radar albedo is indicative of high metal content in asteroids.
Enceladus, a moon of Saturn, has one of the highest known albedos of any body in the Solar System, with 99% of EM radiation reflected.
Another notable high-albedo body is Eris, with an albedo of 0.96.
Many small objects in the outer Solar System and asteroid belt have low albedos down to about 0.05.
A typical comet nucleus has an albedo of 0.04.
Such a dark surface is thought to be indicative of a primitive and heavily space weathered surface containing some organic compounds.
The overall albedo of the Moon is measured to be around 0.136, but it is strongly directional and non-Lambertian, displaying also a strong opposition effect.
Although such reflectance properties are different from those of any terrestrial terrains, they are typical of the regolith surfaces of airless Solar System bodies.
Two common albedos that are used in astronomy are the (V-band) geometric albedo (measuring brightness when illumination comes from directly behind the observer) and the Bond albedo (measuring total proportion of electromagnetic energy reflected).
Their values can differ significantly, which is a common source of confusion.
In detailed studies, the directional reflectance properties of astronomical bodies are often expressed in terms of the five Hapke parameters which semi-empirically describe the variation of albedo with phase angle, including a characterization of the opposition effect of regolith surfaces.
The correlation between astronomical (geometric) albedo, absolute magnitude and diameter is:
formula_7,

where formula_8 is the astronomical albedo, formula_9 is the diameter in kilometers, and formula_10 is the absolute magnitude.
Albedo is not directly dependent on illumination because changing the amount of incoming light proportionally changes the amount of reflected light, except in circumstances where a change in illumination induces a change in the Earth's surface at that location (e.g.
through albedo-temperature feedback).
That said, albedo and illumination both vary by latitude.
Albedo is highest near the poles and lowest in the subtropics, with a local maximum in the tropics.
The intensity of albedo temperature effects depend on the amount of albedo and the level of local insolation (solar irradiance); high albedo areas in the arctic and antarctic regions are cold due to low insolation, where areas such as the Sahara Desert, which also have a relatively high albedo, will be hotter due to high insolation.
Tropical and sub-tropical rainforest areas have low albedo, and are much hotter than their temperate forest counterparts, which have lower insolation.
Because insolation plays such a big role in the heating and cooling effects of albedo, high insolation areas like the tropics will tend to show a more pronounced fluctuation in local temperature when local albedo changes.
Arctic regions notably release more heat back into space than what they absorb, effectively cooling the Earth.
This has been a concern since arctic ice and snow has been melting at higher rates due to higher temperatures, creating regions in the arctic that are notably darker (being water or ground which is darker color) and reflects less heat back into space.
This feedback loop results in a reduced albedo effect.
Albedo affects climate by determining how much radiation a planet absorbs.
The uneven heating of Earth from albedo variations between land, ice, or ocean surfaces can drive weather.
When an area's albedo changes due to snowfall, a snow–temperature feedback results.
A layer of snowfall increases local albedo, reflecting away sunlight, leading to local cooling.
In principle, if no outside temperature change affects this area (e.g., a warm air mass), the raised albedo and lower temperature would maintain the current snow and invite further snowfall, deepening the snow–temperature feedback.
However, because local weather is dynamic due to the change of seasons, eventually warm air masses and a more direct angle of sunlight (higher insolation) cause melting.
When the melted area reveals surfaces with lower albedo, such as grass or soil, the effect is reversed: the darkening surface lowers albedo, increasing local temperatures, which induces more melting and thus reducing the albedo further, resulting in still more heating.
Snow albedo is highly variable, ranging from as high as 0.9 for freshly fallen snow, to about 0.4 for melting snow, and as low as 0.2 for dirty snow.
Over Antarctica snow albedo averages a little more than 0.8.
If a marginally snow-covered area warms, snow tends to melt, lowering the albedo, and hence leading to more snowmelt because more radiation is being absorbed by the snowpack (the ice–albedo positive feedback).
Just as fresh snow has a higher albedo than does dirty snow, the albedo of snow-covered sea ice is far higher than that of sea water.
Sea water absorbs more solar radiation than would the same surface covered with reflective snow.
When sea ice melts, either due to a rise in sea temperature or in response to increased solar radiation from above, the snow-covered surface is reduced, and more surface of sea water is exposed, so the rate of energy absorption increases.
The extra absorbed energy heats the sea water, which in turn increases the rate at which sea ice melts.
As with the preceding example of snowmelt, the process of melting of sea ice is thus another example of a positive feedback.
Both positive feedback loops have long been recognized as important to the modern theory of Global warming.
Cryoconite, powdery windblown dust containing soot, sometimes reduces albedo on glaciers and ice sheets.
The dynamical nature of albedo in response to positive feedback, together with the effects of small errors in the measurement of albedo, can lead to large errors in energy estimates.
Because of this, in order to reduce the error of energy estimates, it is important to measure the albedo of snow-covered areas through remote sensing techniques rather than applying a single value for albedo over broad regions.
Albedo works on a smaller scale, too.
In sunlight, dark clothes absorb more heat and light-coloured clothes reflect it better, thus allowing some control over body temperature by exploiting the albedo effect of the colour of external clothing.
Albedo can affect the electrical energy output of solar photovoltaic devices.
For example, the effects of a spectrally responsive albedo are illustrated by the differences between the spectrally weighted albedo of solar photovoltaic technology based on hydrogenated amorphous silicon (a-Si:H) and crystalline silicon (c-Si)-based compared to traditional spectral-integrated albedo predictions.
Research showed impacts of over 10%.
More recently, the analysis was extended to the effects of spectral bias due to the specular reflectivity of 22 commonly occurring surface materials (both human-made and natural) and analyzes the albedo effects on the performance of seven photovoltaic materials covering three common photovoltaic system topologies: industrial (solar farms), commercial flat rooftops and residential pitched-roof applications.
Because forests generally have a low albedo, (the majority of the ultraviolet and visible spectrum is absorbed through photosynthesis), some scientists have suggested that greater heat absorption by trees could offset some of the carbon benefits of afforestation (or offset the negative climate impacts of deforestation).
In the case of evergreen forests with seasonal snow cover albedo reduction may be great enough for deforestation to cause a net cooling effect.
Trees also impact climate in extremely complicated ways through evapotranspiration.
The water vapor causes cooling on the land surface, causes heating where it condenses, acts a strong greenhouse gas, and can increase albedo when it condenses into clouds Scientists generally treat evapotranspiration as a net cooling impact, and the net climate impact of albedo and evapotranspiration changes from deforestation depends greatly on local climate 

In seasonally snow-covered zones, winter albedos of treeless areas are 10% to 50% higher than nearby forested areas because snow does not cover the trees as readily.
Deciduous trees have an albedo value of about 0.15 to 0.18 whereas coniferous trees have a value of about 0.09 to 0.15.
Studies by the Hadley Centre have investigated the relative (generally warming) effect of albedo change and (cooling) effect of carbon sequestration on planting forests.
They found that new forests in tropical and midlatitude areas tended to cool; new forests in high latitudes (e.g., Siberia) were neutral or perhaps warming.
Water reflects light very differently from typical terrestrial materials.
The reflectivity of a water surface is calculated using the Fresnel equations (see graph).
At the scale of the wavelength of light even wavy water is always smooth so the light is reflected in a locally specular manner (not diffusely).
The glint of light off water is a commonplace effect of this.
At small angles of incident light, waviness results in reduced reflectivity because of the steepness of the reflectivity-vs.-incident-angle curve and a locally increased average incident angle.
Although the reflectivity of water is very low at low and medium angles of incident light, it becomes very high at high angles of incident light such as those that occur on the illuminated side of Earth near the terminator (early morning, late afternoon, and near the poles).
However, as mentioned above, waviness causes an appreciable reduction.
Because light specularly reflected from water does not usually reach the viewer, water is usually considered to have a very low albedo in spite of its high reflectivity at high angles of incident light.
Note that white caps on waves look white (and have high albedo) because the water is foamed up, so there are many superimposed bubble surfaces which reflect, adding up their reflectivities.
Fresh 'black' ice exhibits Fresnel reflection.
Snow on top of this sea ice increases the albedo to 0.9.
Cloud albedo has substantial influence over atmospheric temperatures.
Different types of clouds exhibit different reflectivity, theoretically ranging in albedo from a minimum of near 0 to a maximum approaching 0.8.
"On any given day, about half of Earth is covered by clouds, which reflect more sunlight than land and water.
Clouds keep Earth cool by reflecting sunlight, but they can also serve as blankets to trap warmth."
Albedo and climate in some areas are affected by artificial clouds, such as those created by the contrails of heavy commercial airliner traffic.
A study following the burning of the Kuwaiti oil fields during Iraqi occupation showed that temperatures under the burning oil fires were as much as 10 °C colder than temperatures several miles away under clear skies.
Aerosols (very fine particles/droplets in the atmosphere) have both direct and indirect effects on Earth's radiative balance.
The direct (albedo) effect is generally to cool the planet; the indirect effect (the particles act as cloud condensation nuclei and thereby change cloud properties) is less certain.
As per the effects are:
Another albedo-related effect on the climate is from black carbon particles.
The size of this effect is difficult to quantify: the Intergovernmental Panel on Climate Change estimates that the global mean radiative forcing for black carbon aerosols from fossil fuels is +0.2 W m, with a range +0.1 to +0.4 W m. Black carbon is a bigger cause of the melting of the polar ice cap in the Arctic than carbon dioxide due to its effect on the albedo.
Human activities (e.g., deforestation, farming, and urbanization) change the albedo of various areas around the globe.
However, quantification of this effect on the global scale is difficult.
Single-scattering albedo is used to define scattering of electromagnetic waves on small particles.
It depends on properties of the material (refractive index); the size of the particle or particles; and the wavelength of the incoming radiation.
</doc>
<doc id="290" url="https://en.wikipedia.org/wiki?curid=290" title="A">
A

A (named , plural "As", "A's", "a"s, "a's" or "aes") is the first letter and the first vowel of the ISO basic Latin alphabet.
It is similar to the Ancient Greek letter alpha, from which it derives.
The uppercase version consists of the two slanting sides of a triangle, crossed in the middle by a horizontal bar.
The lowercase version can be written in two forms: the double-storey a and single-storey ɑ. The latter is commonly used in handwriting and fonts based on it, especially fonts intended to be read by children, and is also found in italic type.
The earliest certain ancestor of "A" is aleph (also written 'aleph), the first letter of the Phoenician alphabet, which consisted entirely of consonants (for that reason, it is also called an abjad to distinguish it from a true alphabet).
In turn, the ancestor of aleph may have been a pictogram of an ox head in proto-Sinaitic script influenced by Egyptian hieroglyphs, styled as a triangular head with two horns extended.
By 1600 BC, the Phoenician alphabet letter had a linear form that served as the base for some later forms.
Its name is thought to have corresponded closely to the Hebrew or Arabic aleph.
When the ancient Greeks adopted the alphabet, they had no use for a letter to represent the glottal stop—the consonant sound that the letter denoted in Phoenician and other Semitic languages, and that was the first phoneme of the Phoenician pronunciation of the letter—so they used their version of the sign to represent the vowel , and called it by the similar name of alpha.
In the earliest Greek inscriptions after the Greek Dark Ages, dating to the 8th century BC, the letter rests upon its side, but in the Greek alphabet of later times it generally resembles the modern capital letter, although many local varieties can be distinguished by the shortening of one leg, or by the angle at which the cross line is set.
The Etruscans brought the Greek alphabet to their civilization in the Italian Peninsula and left the letter unchanged.
The Romans later adopted the Etruscan alphabet to write the Latin language, and the resulting letter was preserved in the Latin alphabet that would come to be used to write many languages, including English.
During Roman times, there were many variant forms of the letter "A".
First was the monumental or lapidary style, which was used when inscribing on stone or other "permanent" media.
There was also a cursive style used for everyday or utilitarian writing, which was done on more perishable surfaces.
Due to the "perishable" nature of these surfaces, there are not as many examples of this style as there are of the monumental, but there are still many surviving examples of different types of cursive, such as majuscule cursive, minuscule cursive, and semicursive minuscule.
Variants also existed that were intermediate between the monumental and cursive styles.
The known variants include the early semi-uncial, the uncial, and the later semi-uncial.
At the end of the Roman Empire (5th century AD), several variants of the cursive minuscule developed through Western Europe.
Among these were the semicursive minuscule of Italy, the Merovingian script in France, the Visigothic script in Spain, and the Insular or Anglo-Irish semi-uncial or Anglo-Saxon majuscule of Great Britain.
By the 9th century, the Caroline script, which was very similar to the present-day form, was the principal form used in book-making, before the advent of the printing press.
This form was derived through a combining of prior forms.
15th-century Italy saw the formation of the two main variants that are known today.
These variants, the "Italic" and "Roman" forms, were derived from the Caroline Script version.
The Italic form, also called "script a," is used in most current handwriting and consists of a circle and vertical stroke.
This slowly developed from the fifth-century form resembling the Greek letter tau in the hands of medieval Irish and English writers.
The Roman form is used in most printed material; it consists of a small loop with an arc over it ("a").
Both derive from the majuscule (capital) form.
In Greek handwriting, it was common to join the left leg and horizontal stroke into a single loop, as demonstrated by the uncial version shown.
Many fonts then made the right leg vertical.
In some of these, the serif that began the right leg stroke developed into an arc, resulting in the printed form, while in others it was dropped, resulting in the modern handwritten form.
Italic type is commonly used to mark emphasis or more generally to distinguish one part of a text from the rest (set in Roman type).
There are some other cases aside from italic type where "script a" ("ɑ"), also called Latin alpha, is used in contrast with Latin "a" (such as in the International Phonetic Alphabet).
In modern English orthography, the letter represents at least seven different vowel sounds:

The double sequence does not occur in native English words, but is found in some words derived from foreign languages such as "Aaron" and "aardvark".
However, occurs in many common digraphs, all with their own sound or sounds, particularly , , , , and .
In most languages that use the Latin alphabet, denotes an open unrounded vowel, such as , , or .
An exception is Saanich, in which (and the glyph Á) stands for a close-mid front unrounded vowel .
In phonetic and phonemic notation:

In algebra, the letter "a" along with other letters at the beginning of the alphabet is used to represent known quantities, whereas the letters at the end of the alphabet ("x", "y", "z") are used to denote unknown quantities.
In geometry, capital A, B, C etc.
are used to denote segments, lines, rays, etc.
A capital A is also typically used as one of the letters to represent an angle in a triangle, the lowercase a representing the side opposite angle A.
"A" is often used to denote something or someone of a better or more prestigious quality or status: A-, A or A+, the best grade that can be assigned by teachers for students' schoolwork; "A grade" for clean restaurants; A-list celebrities, etc.
Such associations can have a motivating effect, as exposure to the letter A has been found to improve performance, when compared with other letters.
"A" is used as a prefix on some words, such as asymmetry, to mean "not" or "without" (from Greek).
Finally, the letter A is used to denote size, as in a narrow size shoe, or a small cup size in a brassiere.
</doc>
<doc id="303" url="https://en.wikipedia.org/wiki?curid=303" title="Alabama">
Alabama

Alabama is a state in the southeastern region of the United States.
It is bordered by Tennessee to the north, Georgia to the east, Florida and the Gulf of Mexico to the south, and Mississippi to the west.
Alabama is the 30th largest by area and the 24th-most populous of the U.S.
states.
With a total of of inland waterways, Alabama has among the most of any state.
Alabama is nicknamed the "Yellowhammer State", after the state bird.
Alabama is also known as the "Heart of Dixie" and the "Cotton State".
The state tree is the longleaf pine, and the state flower is the camellia.
Alabama's capital is Montgomery.
The largest city by population is Birmingham, which has long been the most industrialized city; the largest city by land area is Huntsville.
The oldest city is Mobile, founded by French colonists in 1702 as the capital of French Louisiana.
From the American Civil War until World War II, Alabama, like many states in the southern U.S., suffered economic hardship, in part because of its continued dependence on agriculture.
Similar to other former slave states, Alabamian legislators employed Jim Crow laws to disenfranchise and otherwise discriminate against African Americans from the end of the Reconstruction Era up until at least the 1970s.
Despite the growth of major industries and urban centers, white rural interests dominated the state legislature from 1901 to the 1960s.
During this time, urban interests and African Americans were markedly under-represented.
Following World War II, Alabama grew as the state's economy changed from one primarily based on agriculture to one with diversified interests.
The state's economy in the 21st century is based on management, automotive, finance, manufacturing, aerospace, mineral extraction, healthcare, education, retail, and technology.
The European-American naming of the Alabama River and state was derived from the Alabama people, a Muskogean-speaking tribe whose members lived just below the confluence of the Coosa and Tallapoosa rivers on the upper reaches of the river.
In the Alabama language, the word for a person of Alabama lineage is "Albaamo" (or variously "Albaama" or "Albàamo" in different dialects; the plural form is "Albaamaha").
The suggestion that "Alabama" was borrowed from the Choctaw language is unlikely.
The word's spelling varies significantly among historical sources.
The first usage appears in three accounts of the Hernando de Soto expedition of 1540: Garcilaso de la Vega used "Alibamo", while the Knight of Elvas and Rodrigo Ranjel wrote "Alibamu" and "Limamu", respectively, in transliterations of the term.
As early as 1702, the French called the tribe the "Alibamon", with French maps identifying the river as "Rivière des Alibamons".
Other spellings of the name have included "Alibamu", "Alabamo", "Albama", "Alebamon", "Alibama", "Alibamou", "Alabamu", "Allibamou".
Sources disagree on the word's meaning.
Some scholars suggest the word comes from the Choctaw "alba" (meaning "plants" or "weeds") and "amo" (meaning "to cut", "to trim", or "to gather").
The meaning may have been "clearers of the thicket" or "herb gatherers", referring to clearing land for cultivation or collecting medicinal plants.
The state has numerous place names of Native American origin.
However, there are no correspondingly similar words in the Alabama language.
An 1842 article in the "Jacksonville Republican" proposed it meant "Here We Rest."
This notion was popularized in the 1850s through the writings of Alexander Beaufort Meek.
Experts in the Muskogean languages have not found any evidence to support such a translation.
Indigenous peoples of varying cultures lived in the area for thousands of years before the advent of European colonization.
Trade with the northeastern tribes by the Ohio River began during the Burial Mound Period (1000 BC–AD 700) and continued until European contact.
The agrarian Mississippian culture covered most of the state from 1000 to 1600 AD, with one of its major centers built at what is now the Moundville Archaeological Site in Moundville, Alabama.
This is the second-largest complex of the classic Middle Mississippian era, after Cahokia in present-day Illinois, which was the center of the culture.
Analysis of artifacts from archaeological excavations at Moundville were the basis of scholars' formulating the characteristics of the Southeastern Ceremonial Complex (SECC).
Contrary to popular belief, the SECC appears to have no direct links to Mesoamerican culture, but developed independently.
The Ceremonial Complex represents a major component of the religion of the Mississippian peoples; it is one of the primary means by which their religion is understood.
Among the historical tribes of Native American people living in present-day Alabama at the time of European contact were the Cherokee, an Iroquoian language people; and the Muskogean-speaking Alabama ("Alibamu"), Chickasaw, Choctaw, Creek, and Koasati.
While part of the same large language family, the Muskogee tribes developed distinct cultures and languages.
With exploration in the 16th century, the Spanish were the first Europeans to reach Alabama.
The expedition of Hernando de Soto passed through Mabila and other parts of the state in 1540.
More than 160 years later, the French founded the region's first European settlement at Old Mobile in 1702.
The city was moved to the current site of Mobile in 1711.
This area was claimed by the French from 1702 to 1763 as part of La Louisiane.
After the French lost to the British in the Seven Years' War, it became part of British West Florida from 1763 to 1783.
After the United States victory in the American Revolutionary War, the territory was divided between the United States and Spain.
The latter retained control of this western territory from 1783 until the surrender of the Spanish garrison at Mobile to U.S.
forces on April 13, 1813.
Thomas Bassett, a loyalist to the British monarchy during the Revolutionary era, was one of the earliest white settlers in the state outside Mobile.
He settled in the Tombigbee District during the early 1770s.
The district's boundaries were roughly limited to the area within a few miles of the Tombigbee River and included portions of what is today southern Clarke County, northernmost Mobile County, and most of Washington County.
What is now the counties of Baldwin and Mobile became part of Spanish West Florida in 1783, part of the independent Republic of West Florida in 1810, and was finally added to the Mississippi Territory in 1812.
Most of what is now the northern two-thirds of Alabama was known as the Yazoo lands beginning during the British colonial period.
It was claimed by the Province of Georgia from 1767 onwards.
Following the Revolutionary War, it remained a part of Georgia, although heavily disputed.
With the exception of the area around Mobile and the Yazoo lands, what is now the lower one-third Alabama was made part of the Mississippi Territory when it was organized in 1798.
The Yazoo lands were added to the territory in 1804, following the Yazoo land scandal.
Spain kept a claim on its former Spanish West Florida territory in what would become the coastal counties until the Adams–Onís Treaty officially ceded it to the United States in 1819.
Before Mississippi's admission to statehood on December 10, 1817, the more sparsely settled eastern half of the territory was separated and named the Alabama Territory.
The United States Congress created the Alabama Territory on March 3, 1817.
St.
Stephens, now abandoned, served as the territorial capital from 1817 to 1819.
Alabama was admitted as the 22nd state on December 14, 1819, with Congress selecting Huntsville as the site for the first Constitutional Convention.
From July 5 to August 2, 1819, delegates met to prepare the new state constitution.
Huntsville served as temporary capital from 1819 to 1820, when the seat of government moved to Cahaba in Dallas County.
Cahaba, now a ghost town, was the first permanent state capital from 1820 to 1825.
Alabama Fever was underway when the state was admitted to the Union, with settlers and land speculators pouring into the state to take advantage of fertile land suitable for cotton cultivation.
Part of the frontier in the 1820s and 1830s, its constitution provided for universal suffrage for white men.
Southeastern planters and traders from the Upper South brought slaves with them as the cotton plantations in Alabama expanded.
The economy of the central Black Belt (named for its dark, productive soil) was built around large cotton plantations whose owners' wealth grew mainly from slave labor.
The area also drew many poor, disfranchised people who became subsistence farmers.
Alabama had an estimated population of under 10,000 people in 1810, but it increased to more than 300,000 people by 1830.
Most Native American tribes were completely removed from the state within a few years of the passage of the Indian Removal Act by Congress in 1830.
From 1826 to 1846, Tuscaloosa served as Alabama's capital.
On January 30, 1846, the Alabama legislature announced it had voted to move the capital city from Tuscaloosa to Montgomery.
The first legislative session in the new capital met in December 1847.
A new capitol building was erected under the direction of Stephen Decatur Button of Philadelphia.
The first structure burned down in 1849, but was rebuilt on the same site in 1851.
This second capitol building in Montgomery remains to the present day.
It was designed by Barachias Holt of Exeter, Maine.
By 1860, the population had increased to 964,201 people, of which nearly half, 435,080, were enslaved African Americans, and 2,690 were free people of color.
On January 11, 1861, Alabama declared its secession from the Union.
After remaining an independent republic for a few days, it joined the Confederate States of America.
The Confederacy's capital was initially at Montgomery.
Alabama was heavily involved in the American Civil War.
Although comparatively few battles were fought in the state, Alabama contributed about 120,000 soldiers to the war effort.
A company of cavalry soldiers from Huntsville, Alabama, joined Nathan Bedford Forrest's battalion in Hopkinsville, Kentucky.
The company wore new uniforms with yellow trim on the sleeves, collar and coat tails.
This led to them being greeted with "Yellowhammer", and the name later was applied to all Alabama troops in the Confederate Army.
Alabama's slaves were freed by the 13th Amendment in 1865.
Alabama was under military rule from the end of the war in May 1865 until its official restoration to the Union in 1868.
From 1867 to 1874, with most white citizens barred temporarily from voting and freedmen enfranchised, many African Americans emerged as political leaders in the state.
Alabama was represented in Congress during this period by three African-American congressmen: Jeremiah Haralson, Benjamin S. Turner, and James T. Rapier.
Following the war, the state remained chiefly agricultural, with an economy tied to cotton.
During Reconstruction, state legislators ratified a new state constitution in 1868 that created the state's first public school system and expanded women's rights.
Legislators funded numerous public road and railroad projects, although these were plagued with allegations of fraud and misappropriation.
Organized insurgent, resistance groups tried to suppress the freedmen and Republicans.
Besides the short-lived original Ku Klux Klan, these included the Pale Faces, Knights of the White Camellia, Red Shirts, and the White League.
Reconstruction in Alabama ended in 1874, when the Democrats regained control of the legislature and governor's office through an election dominated by fraud and violence.
They wrote another constitution in 1875, and the legislature passed the Blaine Amendment, prohibiting public money from being used to finance religious-affiliated schools.
The same year, legislation was approved that called for racially segregated schools.
Railroad passenger cars were segregated in 1891.
After disfranchising most African Americans and many poor whites in the 1901 constitution, the Alabama legislature passed more Jim Crow laws at the beginning of the 20th century to impose segregation in everyday life.
The new 1901 Constitution of Alabama included provisions for voter registration that effectively disenfranchised large portions of the population, including nearly all African Americans and Native Americans, and tens of thousands of poor whites, through making voter registration difficult, requiring a poll tax and literacy test.
The 1901 constitution required racial segregation of public schools.
By 1903, only 2,980 African Americans were registered in Alabama, although at least 74,000 were literate.
This compared to more than 181,000 African Americans eligible to vote in 1900.
The numbers dropped even more in later decades.
The state legislature passed additional racial segregation laws related to public facilities into the 1950s: jails were segregated in 1911; hospitals in 1915; toilets, hotels, and restaurants in 1928; and bus stop waiting rooms in 1945.
While the planter class had persuaded poor whites to vote for this legislative effort to suppress black voting, the new restrictions resulted in their disenfranchisement as well, due mostly to the imposition of a cumulative poll tax.
By 1941, whites constituted a slight majority of those disenfranchised by these laws: 600,000 whites vs. 520,000 African-Americans.
Nearly all African Americans had lost the ability to vote.
Despite numerous legal challenges that succeeded in overturning certain provisions, the state legislature would create new ones to maintain disenfranchisement.
The exclusion of blacks from the political system persisted until after passage of federal civil rights legislation in 1965 to enforce their constitutional rights as citizens.
The rural-dominated Alabama legislature consistently underfunded schools and services for the disenfranchised African Americans, but it did not relieve them of paying taxes.
Partially as a response to chronic underfunding of education for African Americans in the South, the Rosenwald Fund began funding the construction of what came to be known as Rosenwald Schools.
In Alabama these schools were designed and the construction partially financed with Rosenwald funds, which paid one-third of the construction costs.
The fund required the local community and state to raise matching funds to pay the rest.
Black residents effectively taxed themselves twice, by raising additional monies to supply matching funds for such schools, which were built in many rural areas.
They often donated land and labor as well.
Beginning in 1913, the first 80 Rosenwald Schools were built in Alabama for African-American children.
A total of 387 schools, seven teachers' houses, and several vocational buildings were completed by 1937 in the state.
Several of the surviving school buildings in the state are now listed on the National Register of Historic Places.
Continued racial discrimination and lynchings, agricultural depression, and the failure of the cotton crops due to boll weevil infestation led tens of thousands of African Americans from rural Alabama and other states to seek opportunities in northern and midwestern cities during the early decades of the 20th century as part of the Great Migration out of the South.
Reflecting this emigration, the population growth rate in Alabama (see "historical populations" table below) dropped by nearly half from 1910 to 1920.
At the same time, many rural people migrated to the city of Birmingham to work in new industrial jobs.
Birmingham experienced such rapid growth that it was called the "Magic City".
By 1920, Birmingham was the 36th-largest city in the United States.
Heavy industry and mining were the basis of its economy.
Its residents were under-represented for decades in the state legislature, which refused to redistrict after each decennial census according to population changes, as it was required by the state constitution.
This did not change until the late 1960s following a lawsuit and court order.
Beginning in the 1940s, when the courts started taking the first steps to recognize the voting rights of black voters, the Alabama legislature took several counter-steps designed to disfranchise black voters.
The legislature passed, and the voters ratified [as these were mostly white voters], a state constitutional amendment that gave local registrars greater latitude to disqualify voter registration applicants.
Black citizens in Mobile successfully challenged this amendment as a violation of the Fifteenth Amendment.
The legislature also changed the boundaries of Tuskegee to a 28-sided figure designed to fence out blacks from the city limits.
The Supreme Court unanimously held that this racial "gerrymandering" violated the Constitution.
In 1961, ... the Alabama legislature also intentionally diluted the effect of the black vote by instituting numbered place requirements for local elections.
Industrial development related to the demands of World War II brought a level of prosperity to the state not seen since before the civil war.
Rural workers poured into the largest cities in the state for better jobs and a higher standard of living.
One example of this massive influx of workers occurred in Mobile.
Between 1940 and 1943, more than 89,000 people moved into the city to work for war-related industries.
Cotton and other cash crops faded in importance as the state developed a manufacturing and service base.
Despite massive population changes in the state from 1901 to 1961, the rural-dominated legislature refused to reapportion House and Senate seats based on population, as required by the state constitution to follow the results of decennial censuses.
They held on to old representation to maintain political and economic power in agricultural areas.
One result was that Jefferson County, containing Birmingham's industrial and economic powerhouse, contributed more than one-third of all tax revenue to the state, but did not receive a proportional amount in services.
Urban interests were consistently underrepresented in the legislature.
A 1960 study noted that because of rural domination, "a minority of about 25 per cent of the total state population is in majority control of the Alabama legislature."
In the United States Supreme Court cases of "Baker v. Carr" (1962) and "Reynolds v. Sims" (1964), the court ruled ruled that the principle of "one man, one vote" needed to be the basis of both houses of state legislatures as well, and that their districts had to be based on population, rather than geographic counties, as Alabama had used for its senate.
In 1972, for the first time since 1901, the legislature completed the congressional redistricting based on the decennial census.
This benefited the urban areas that had developed, as well as all in the population who had been underrepresented for more than 60 years.
Other changes were made to implement representative state house and senate districts.
African Americans continued to press in the 1950s and 1960s to end disenfranchisement and segregation in the state through the civil rights movement, including legal challenges.
In 1954, the US Supreme Court ruled in "Brown v. Board of Education" that public schools had to be desegregated, but Alabama was slow to comply.
During the 1960s, under Governor George Wallace, Alabama resisted compliance with federal demands for desegregation.
The civil rights movement had notable events in Alabama, including the Montgomery Bus Boycott (1955–56), Freedom Rides in 1961, and 1965 Selma to Montgomery marches.
These contributed to Congressional passage and enactment of the Civil Rights Act of 1964 and Voting Rights Act of 1965 by the U.S.
Congress.
Legal segregation ended in the states in 1964, but Jim Crow customs often continued until specifically challenged in court.
According to "The New York Times", by 2017, many of Alabama's African-Americans were living in Alabama's cities such as Birmingham and Montgomery.
Also, the Black Belt region across central Alabama "is home to largely poor counties that are predominantly African-American.
These counties include Dallas, Lowndes, Marengo and Perry."
Alabama has made some changes since the late 20th century and has used new types of voting to increase representation.
In the 1980s, an omnibus redistricting case, "Dillard v. Crenshaw County", challenged the at-large voting for representative seats of 180 Alabama jurisdictions, including counties and school boards.
At-large voting had diluted the votes of any minority in a county, as the majority tended to take all seats.
Despite African Americans making up a significant minority in the state, they had been unable to elect any representatives in most of the at-large jurisdictions.
As part of settlement of this case, five Alabama cities and counties, including Chilton County, adopted a system of cumulative voting for election of representatives in multi-seat jurisdictions.
This has resulted in more proportional representation for voters.
In another form of proportional representation, 23 jurisdictions use limited voting, as in Conecuh County.
In 1982, limited voting was first tested in Conecuh County.
Together use of these systems has increased the number of African Americans and women being elected to local offices, resulting in governments that are more representative of their citizens.
Alabama is the thirtieth-largest state in the United States with of total area: 3.2% of the area is water, making Alabama 23rd in the amount of surface water, also giving it the second-largest inland waterway system in the United States.
About three-fifths of the land area is a gentle plain with a general descent towards the Mississippi River and the Gulf of Mexico.
The North Alabama region is mostly mountainous, with the Tennessee River cutting a large valley and creating numerous creeks, streams, rivers, mountains, and lakes.
Alabama is bordered by the states of Tennessee to the north, Georgia to the east, Florida to the south, and Mississippi to the west.
Alabama has coastline at the Gulf of Mexico, in the extreme southern edge of the state.
The state ranges in elevation from sea level at Mobile Bay to over 1,800 feet (550 m) in the Appalachian Mountains in the northeast.
The highest point is Mount Cheaha, at a height of .
Alabama's land consists of of forest or 67% of total land area.
Suburban Baldwin County, along the Gulf Coast, is the largest county in the state in both land area and water area.
Areas in Alabama administered by the National Park Service include Horseshoe Bend National Military Park near Alexander City; Little River Canyon National Preserve near Fort Payne; Russell Cave National Monument in Bridgeport; Tuskegee Airmen National Historic Site in Tuskegee; and Tuskegee Institute National Historic Site near Tuskegee.
Additionally, Alabama has four National Forests: Conecuh, Talladega, Tuskegee, and William B. Bankhead.
Alabama also contains the Natchez Trace Parkway, the Selma To Montgomery National Historic Trail, and the Trail Of Tears National Historic Trail.
A notable natural wonder in Alabama is "Natural Bridge" rock, the longest natural bridge east of the Rockies, located just south of Haleyville.
A -wide meteorite impact crater is located in Elmore County, just north of Montgomery.
This is the Wetumpka crater, the site of "Alabama's greatest natural disaster."
A -wide meteorite hit the area about 80 million years ago.
The hills just east of downtown Wetumpka showcase the eroded remains of the impact crater that was blasted into the bedrock, with the area labeled the Wetumpka crater or astrobleme ("star-wound") because of the concentric rings of fractures and zones of shattered rock that can be found beneath the surface.
In 2002, Christian Koeberl with the Institute of Geochemistry University of Vienna published evidence and established the site as the 157th recognized impact crater on Earth.
The state is classified as humid subtropical ("Cfa") under the Koppen Climate Classification.
The average annual temperature is 64 °F (18 °C).
Temperatures tend to be warmer in the southern part of the state with its proximity to the Gulf of Mexico, while the northern parts of the state, especially in the Appalachian Mountains in the northeast, tend to be slightly cooler.
Generally, Alabama has very hot summers and mild winters with copious precipitation throughout the year.
Alabama receives an average of of rainfall annually and enjoys a lengthy growing season of up to 300 days in the southern part of the state.
Summers in Alabama are among the hottest in the U.S., with high temperatures averaging over throughout the summer in some parts of the state.
Alabama is also prone to tropical storms and even hurricanes.
Areas of the state far away from the Gulf are not immune to the effects of the storms, which often dump tremendous amounts of rain as they move inland and weaken.
South Alabama reports many thunderstorms.
The Gulf Coast, around Mobile Bay, averages between 70 and 80 days per year with thunder reported.
This activity decreases somewhat further north in the state, but even the far north of the state reports thunder on about 60 days per year.
Occasionally, thunderstorms are severe with frequent lightning and large hail; the central and northern parts of the state are most vulnerable to this type of storm.
Alabama ranks ninth in the number of deaths from lightning and tenth in the number of deaths from lightning strikes per capita.
Alabama, along with Oklahoma, has the most reported EF5 tornadoes of any state, according to statistics from the National Climatic Data Center for the period January 1, 1950, to June 2013.
Several long-tracked F5/EF5 tornadoes have contributed to Alabama reporting more tornado fatalities than any other state.
The state was affected by the 1974 Super Outbreak and was devastated tremendously by the 2011 Super Outbreak.
The 2011 Super Outbreak produced a record amount of tornadoes in the state.
The tally reached 62.
The peak season for tornadoes varies from the northern to southern parts of the state.
Alabama is one of the few places in the world that has a secondary tornado season in November and December, along with the spring severe weather season.
The northern part of the state—along the Tennessee Valley—is one of the areas in the U.S.
most vulnerable to violent tornadoes.
The area of Alabama and Mississippi most affected by tornadoes is sometimes referred to as Dixie Alley, as distinct from the Tornado Alley of the Southern Plains.
Winters are generally mild in Alabama, as they are throughout most of the Southeastern United States, with average January low temperatures around in Mobile and around in Birmingham.
Although snow is a rare event in much of Alabama, areas of the state north of Montgomery may receive a dusting of snow a few times every winter, with an occasional moderately heavy snowfall every few years.
Historic snowfall events include New Year's Eve 1963 snowstorm and the 1993 Storm of the Century.
The annual average snowfall for the Birmingham area is per year.
In the southern Gulf coast, snowfall is less frequent, sometimes going several years without any snowfall.
Alabama's highest temperature of was recorded on September 5, 1925, in the unincorporated community of Centerville.
The record low of occurred on January 30, 1966, in New Market.
Alabama is home to a diverse array of flora and fauna, due largely to a variety of habitats that range from the Tennessee Valley, Appalachian Plateau, and Ridge-and-Valley Appalachians of the north to the Piedmont, Canebrake, and Black Belt of the central region to the Gulf Coastal Plain and beaches along the Gulf of Mexico in the south.
The state is usually ranked among the top in nation for its range of overall biodiversity.
Alabama is in the subtropical coniferous forest biome and once boasted huge expanses of pine forest, which still form the largest proportion of forests in the state.
It currently ranks fifth in the nation for the diversity of its flora.
It is home to nearly 4,000 pteridophyte and spermatophyte plant species.
Indigenous animal species in the state include 62 mammal species, 93 reptile species, 73 amphibian species, roughly 307 native freshwater fish species, and 420 bird species that spend at least part of their year within the state.
Invertebrates include 97 crayfish species and 383 mollusk species.
113 of these mollusk species have never been collected outside the state.
The United States Census Bureau estimates that the population of Alabama was 4,858,979 on July 1, 2015, which represents an increase of 79,243, or 1.66%, since the 2010 Census.
This includes a natural increase since the last census of 121,054 people (that is 502,457 births minus 381,403 deaths) and an increase due to net migration of 104,991 people into the state.
Immigration from outside the U.S.
resulted in a net increase of 31,180 people, and migration within the country produced a net gain of 73,811 people.
The state had 108,000 foreign-born (2.4% of the state population), of which an estimated 22.2% were undocumented (24,000).
The center of population of Alabama is located in Chilton County, outside the town of Jemison.
According to the 2010 Census, Alabama had a population of 4,779,736.
The racial composition of the state was 68.5% White (67.0% Non-Hispanic White and 1.5% Hispanic White), 26.2% Black or African American, 3.9% Hispanic or Latino of any race, 1.1% Asian, 0.6% American Indian and Alaska Native, 0.1% Native Hawaiian and Other Pacific Islander, 2.0% from Some Other Race, and 1.5% from Two or More Races.
In 2011, 46.6% of Alabama's population younger than age 1 were minorities.
The largest reported ancestry groups in Alabama are: African (26.2%), English (23.6%), Irish (7.7%), German (5.7%), and Scots-Irish (2.0%).
Those citing "American" ancestry in Alabama are generally of English or British ancestry; many Anglo-Americans identify as having American ancestry because their roots have been in North America for so long, in some cases since the 1600s.
Demographers estimate that a minimum of 20–23% of people in Alabama are of predominantly English ancestry and that the figure is likely higher.
In the 1980 census, 41% of the people in Alabama identified as being of English ancestry, making them the largest ethnic group at the time.
Based on historic migration and settlement patterns in the southern colonies and states, demographers estimated there are more people in Alabama of Scots-Irish origins than self-reported.
Many people in Alabama claim Irish ancestry because of the term Scots-Irish but, based on historic immigration and settlement, their ancestors were more likely Protestant Scots-Irish coming from northern Ireland, where they had been for a few generations as part of the English colonization.
The Scots-Irish were the largest non-English immigrant group from the British Isles before the American Revolution, and many settled in the South, later moving into the Deep South as it was developed.
In 1984, under the Davis–Strong Act, the state legislature established the Alabama Indian Affairs Commission.
Native American groups within the state had increasingly been demanding recognition as ethnic groups and seeking an end to discrimination.
Given the long history of slavery and associated racial segregation, the Native American peoples, who have sometimes been of mixed race, have insisted on having their cultural identification respected.
In the past, their self-identification was often overlooked as the state tried to impose a binary breakdown of society into white and black.
The state has officially recognized nine American Indian tribes in the state, descended mostly from the Five Civilized Tribes of the American Southeast.
These are:

The state government has promoted recognition of Native American contributions to the state, including the designation in 2000 for Columbus Day to be jointly celebrated as American Indian Heritage Day.
95.1% of all Alabama residents five years old or older spoke only English at home in 2010, a minor decrease from 96.1% in 2000.
Alabama English is predominantly Southern, and is related to South Midland speech which was taken across the border from Tennessee.
In the major Southern speech region, there is the decreasing loss of the final /r/, for example the /boyd/ pronunciation of 'bird'.
In the northern third of the state, there is a South Midland 'arm' and 'barb' rhyming with 'form' and 'orb'.
Unique words in Alabama English include: redworm (earthworm), peckerwood (woodpecker), snake doctor and snake feeder (dragonfly), tow sack (burlap bag), plum peach (clingstone), French harp (harmonica), and dog irons (andirons).
In the 2008 American Religious Identification Survey, 86% of Alabama respondents reported their religion as Christian, including 6% Catholic, with 11% as having no religion.
The composition of other traditions is 0.5% Mormon, 0.5% Jewish, 0.5% Muslim, 0.5% Buddhist, and 0.5% Hindu.
Alabama is located in the middle of the Bible Belt, a region of numerous Protestant Christians.
Alabama has been identified as one of the most religious states in the United States, with about 58% of the population attending church regularly.
A majority of people in the state identify as Evangelical Protestant.
, the three largest denominational groups in Alabama are the Southern Baptist Convention, The United Methodist Church, and non-denominational Evangelical Protestant.
In Alabama, the Southern Baptist Convention has the highest number of adherents with 1,380,121; this is followed by the United Methodist Church with 327,734 adherents, non-denominational Evangelical Protestant with 220,938 adherents, and the Catholic Church with 150,647 adherents.
Many Baptist and Methodist congregations became established in the Great Awakening of the early 19th century, when preachers proselytized across the South.
The Assemblies of God had almost 60,000 members, the Churches of Christ had nearly 120,000 members.
The Presbyterian churches, strongly associated with Scots-Irish immigrants of the 18th century and their descendants, had a combined membership around 75,000 (PCA – 28,009 members in 108 congregations, PC(USA) – 26,247 members in 147 congregations, the Cumberland Presbyterian Church – 6,000 members in 59 congregations, the Cumberland Presbyterian Church in America – 5,000 members and 50 congregations plus the EPC and Associate Reformed Presbyterians with 230 members and 9 congregations).
In a 2007 survey, nearly 70% of respondents could name all four of the Christian Gospels.
Of those who indicated a religious preference, 59% said they possessed a "full understanding" of their faith and needed no further learning.
In a 2007 poll, 92% of Alabamians reported having at least some confidence in churches in the state.
Although in much smaller numbers, many other religious faiths are represented in the state as well, including Judaism, Islam, Hinduism, Buddhism, Sikhism, the Bahá'í Faith, and Unitarian Universalism.
Jews have been present in what is now Alabama since 1763, during the colonial era of Mobile, when Sephardic Jews immigrated from London.
The oldest Jewish congregation in the state is Congregation Sha'arai Shomayim in Mobile.
It was formally recognized by the state legislature on January 25, 1844.
Later immigrants in the nineteenth and twentieth centuries tended to be Ashkenazi Jews from eastern Europe.
Jewish denominations in the state include two Orthodox, four Conservative, ten Reform, and one Humanistic synagogue.
Muslims have been increasing in Alabama, with 31 mosques built by 2011, many by African-American converts.
Several Hindu temples and cultural centers in the state have been founded by Indian immigrants and their descendants, the best-known being the Shri Swaminarayan Mandir in Birmingham, the Hindu Temple and Cultural Center of Birmingham in Pelham, the Hindu Cultural Center of North Alabama in Capshaw, and the Hindu Mandir and Cultural Center in Tuscaloosa.
There are six Dharma centers and organizations for Theravada Buddhists.
Most monastic Buddhist temples are concentrated in southern Mobile County, near Bayou La Batre.
This area has attracted an influx of refugees from Cambodia, Laos, and Vietnam during the 1970s and thereafter.
The four temples within a ten-mile radius of Bayou La Batre, include Chua Chanh Giac, Wat Buddharaksa, and Wat Lao Phoutthavihan.
The first community of adherents of the Bahá'í Faith in Alabama was founded in 1896 by Paul K. Dealy, who moved from Chicago to Fairhope.
Bahá'í centers in Alabama exist in Birmingham, Huntsville, and Florence.
A Centers for Disease Control and Prevention study in 2008 showed that obesity in Alabama was a problem, with most counties having over 29% of adults obese, except for ten which had a rate between 26% and 29%.
Residents of the state, along with those in five other states, were least likely in the nation to be physically active during leisure time.
Alabama, and the southeastern U.S.
in general, has one of the highest incidences of adult onset diabetes in the country, exceeding 10% of adults.
The state has invested in aerospace, education, health care, banking, and various heavy industries, including automobile manufacturing, mineral extraction, steel production and fabrication.
By 2006, crop and animal production in Alabama was valued at $1.5 billion.
In contrast to the primarily agricultural economy of the previous century, this was only about 1% of the state's gross domestic product.
The number of private farms has declined at a steady rate since the 1960s, as land has been sold to developers, timber companies, and large farming conglomerates.
Non-agricultural employment in 2008 was 121,800 in management occupations; 71,750 in business and financial operations; 36,790 in computer-related and mathematical occupation; 44,200 in architecture and engineering; 12,410 in life, physical, and social sciences; 32,260 in community and social services; 12,770 in legal occupations; 116,250 in education, training, and library services; 27,840 in art, design and media occupations; 121,110 in healthcare; 44,750 in fire fighting, law enforcement, and security; 154,040 in food preparation and serving; 76,650 in building and grounds cleaning and maintenance; 53,230 in personal care and services; 244,510 in sales; 338,760 in office and administration support; 20,510 in farming, fishing, and forestry; 120,155 in construction and mining, gas, and oil extraction; 106,280 in installation, maintenance, and repair; 224,110 in production; and 167,160 in transportation and material moving.
According to the U.S.
Bureau of Economic Analysis, the 2008 total gross state product was $170 billion, or $29,411 per capita.
Alabama's 2012 GDP increased 1.2% from the previous year.
The single largest increase came in the area of information.
In 2010, per capita income for the state was $22,984.
The state's seasonally adjusted unemployment rate was 5.8% in April 2015.
This compared to a nationwide seasonally adjusted rate of 5.4%.
Alabama has no state minimum wage and uses the federal minimum wage of $7.25.
In February 2016, the state passed legislation that prevents Alabama municipalities from raising the minimum wage in their locality.
The legislation voids a Birmingham city ordinance that was to raise the city's minimum wage to $10.10.
, Alabama has the sixth highest poverty rate among states in the U.S.
In 2017, United Nations Special Rapporteur Philip Alston toured parts of rural Alabama and observed environmental conditions that he said were poorer than anywhere he had seen in the developed world.
The five employers that employed the most employees in Alabama in April 2011 were:

The next twenty largest employers, , included:
Alabama's agricultural outputs include poultry and eggs, cattle, fish, plant nursery items, peanuts, cotton, grains such as corn and sorghum, vegetables, milk, soybeans, and peaches.
Although known as "The Cotton State", Alabama ranks between eighth and tenth in national cotton production, according to various reports, with Texas, Georgia and Mississippi comprising the top three.
Alabama's industrial outputs include iron and steel products (including cast-iron and steel pipe); paper, lumber, and wood products; mining (mostly coal); plastic products; cars and trucks; and apparel.
In addition, Alabama produces aerospace and electronic products, mostly in the Huntsville area, the location of NASA's George C. Marshall Space Flight Center and the U.S.
Army Materiel Command, headquartered at Redstone Arsenal.
A great deal of Alabama's economic growth since the 1990s has been due to the state's expanding automotive manufacturing industry.
Located in the state are Honda Manufacturing of Alabama, Hyundai Motor Manufacturing Alabama, Mercedes-Benz U.S.
International, and Toyota Motor Manufacturing Alabama, as well as their various suppliers.
Since 1993, the automobile industry has generated more than 67,800 new jobs in the state.
Alabama currently ranks 4th in the nation for vehicle exports.
Automakers accounted for approximately a third of the industrial expansion in the state in 2012.
The eight models produced at the state's auto factories totaled combined sales of 74,335 vehicles for 2012.
The strongest model sales during this period were the Hyundai Elantra compact car, the Mercedes-Benz GL-Class sport utility vehicle and the Honda Ridgeline sport utility truck.
Steel producers Outokumpu, Nucor, SSAB, ThyssenKrupp, and U.S.
Steel have facilities in Alabama and employ over 10,000 people.
In May 2007, German steelmaker ThyssenKrupp selected Calvert in Mobile County for a 4.65 billion combined stainless and carbon steel processing facility.
ThyssenKrupp's stainless steel division, Inoxum, including the stainless portion of the Calvert plant, was sold to Finnish stainless steel company Outokumpu in 2012.
The remaining portion of the ThyssenKrupp plant had final bids submitted by ArcelorMittal and Nippon Steel for $1.6 billion in March 2013.
Companhia Siderúrgica Nacional submitted a combined bid for the mill at Calvert, plus a majority stake in the ThyssenKrupp mill in Brazil, for $3.8 billion.
In July 2013, the plant was sold to ArcelorMittal and Nippon Steel.
The Hunt Refining Company, a subsidiary of Hunt Consolidated, Inc., is based in Tuscaloosa and operates a refinery there.
The company also operates terminals in Mobile, Melvin, and Moundville.
JVC America, Inc. operates an optical disc replication and packaging plant in Tuscaloosa.
The Goodyear Tire and Rubber Company operates a large plant in Gadsden that employs about 1,400 people.
It has been in operation since 1929.
Construction of an Airbus A320 family aircraft assembly plant in Mobile was formally announced by Airbus CEO Fabrice Brégier from the Mobile Convention Center on July 2, 2012.
The plans include a $600 million factory at the Brookley Aeroplex for the assembly of the A319, A320 and A321 aircraft.
Construction began in 2013, with plans for it to become operable by 2015 and produce up to 50 aircraft per year by 2017.
The assembly plant is the company's first factory to be built within the United States.
It was announced on February 1, 2013, that Airbus had hired Alabama-based Hoar Construction to oversee construction of the facility.
An estimated 20 million tourists visit the state each year.
Over 100,000 of these are from other countries, including from Canada, the United Kingdom, Germany and Japan.
In 2006, 22.3 million travellers spent $8.3 billion providing an estimated 162,000 jobs in the state.
Some of the most popular areas include the Rocket City of Huntsville, the beaches along the Gulf, and the state's capitol in Montgomery.
UAB Hospital is the only Level I trauma center in Alabama.
UAB is the largest state government employer in Alabama, with a workforce of about 18,000.
Alabama has the headquarters of Regions Financial Corporation, BBVA Compass, Superior Bancorp, and the former Colonial Bancgroup.
Birmingham-based Compass Banchshares was acquired by Spanish-based BBVA in September 2007, although the headquarters of BBVA Compass remains in Birmingham.
In November 2006, Regions Financial completed its merger with AmSouth Bancorporation, which was also headquartered in Birmingham.
SouthTrust Corporation, another large bank headquartered in Birmingham, was acquired by Wachovia in 2004 for $14.3 billion.
The city still has major operations for Wachovia and its now post-operating bank Wells Fargo, which includes a regional headquarters, an operations center campus and a $400 million data center.
Nearly a dozen smaller banks are also headquartered in the Birmingham, such as Superior Bancorp, ServisFirst, and New South Federal Savings Bank.
Birmingham also serves as the headquarters for several large investment management companies, including Harbert Management Corporation.
Telecommunications provider AT&T, formerly BellSouth, has a major presence in Alabama with several large offices in Birmingham.
The company has over 6,000 employees and more than 1,200 contract employees.
Many commercial technology companies are headquartered in Huntsville, such as network access company ADTRAN, computer graphics company Intergraph, and IT infrastructure company Avocent.
Cinram manufactures and distributes 20th Century Fox DVDs and Blu-ray Discs out of its Huntsville plant.
Rust International has grown to include Brasfield & Gorrie, BE&K, Hoar Construction, and B.L.
Harbert International, which all routinely are included in the Engineering News-Record lists of top design, international construction, and engineering firms.
(Rust International was acquired in 2000 by Washington Group International, which was in turn acquired by San-Francisco based URS Corporation in 2007.)
The foundational document for Alabama's government is the Alabama Constitution, which was ratified in 1901.
At almost 800 amendments and 310,000 words, it is by some accounts the world's longest constitution and is roughly forty times the length of the United States Constitution.
There has been a significant movement to rewrite and modernize Alabama's constitution.
Critics argue that Alabama's constitution maintains highly centralized power with the state legislature, leaving practically no power in local hands.
Most counties do not have home rule.
Any policy changes proposed in different areas of the state must be approved by the entire Alabama legislature and, frequently, by state referendum.
One criticism of the current constitution claims that its complexity and length intentionally codify segregation and racism.
Alabama's government is divided into three coequal branches.
The legislative branch is the Alabama Legislature, a bicameral assembly composed of the Alabama House of Representatives, with 105 members, and the Alabama Senate, with 35 members.
The Legislature is responsible for writing, debating, passing, or defeating state legislation.
The Republican Party currently holds a majority in both houses of the Legislature.
The Legislature has the power to override a gubernatorial veto by a simple majority (most state Legislatures require a two-thirds majority to override a veto).
Until 1964, the state elected state senators on a geographic basis by county, with one per county.
It had not redistricted congressional districts since passage of its constitution in 1901; as a result, urbanized areas were grossly underrepresented.
It had not changed legislative districts to reflect the decennial censuses, either.
In "Reynolds v. Sims" (1964), the US Supreme Court implemented the principle of "one man, one vote", ruling that congressional districts had to be reapportioned based on censuses (as the state already included in its constitution but had not implemented.)
Further, the court ruled that both houses of bicameral state legislatures had to be apportioned by population, as there was no constitutional basis for states to have geographically based systems.
At that time, Alabama and many other states had to change their legislative districting, as many across the country had systems that underrepresented urban areas and districts.
This had caused decades of underinvestment in such areas.
For instance, Birmingham and Jefferson County taxes had supplied one-third of the state budget, but Jefferson County received only 1/67th of state services in funding.
Through the legislative delegations, the Alabama legislature kept control of county governments.
The executive branch is responsible for the execution and oversight of laws.
It is headed by the Governor of Alabama.
Other members of executive branch include the cabinet, the Attorney General of Alabama, the Alabama Secretary of State, the Alabama State Treasurer, and the State Auditor of Alabama.
The current governor of the state is Republican Kay Ivey.
The office of lieutenant governor is currently vacant.
The members of the Legislature take office immediately after the November elections.
Statewide officials, such as the governor, lieutenant governor, attorney general, and other constitutional officers, take office the following January.
The judicial branch is responsible for interpreting the state's Constitution and applying the law in state criminal and civil cases.
The state's highest court is the Supreme Court of Alabama.
Alabama uses partisan elections to select judges.
Since the 1980s judicial campaigns have become increasingly politicized.
The current chief justice of the Alabama Supreme Court is Republican Lyn Stuart.
All sitting justices on the Alabama Supreme Court are members of the Republican Party.
There are two intermediate appellate courts, the Court of Civil Appeals and the Court of Criminal Appeals, and four trial courts: the circuit court (trial court of general jurisdiction), and the district, probate, and municipal courts.
Some critics believe that the election of judges has contributed to an exceedingly high rate of executions.
Alabama has the highest per capita death penalty rate in the country.
In some years, it imposes more death sentences than does Texas, a state which has a population five times larger.
Some of its cases have been highly controversial; the Supreme Court has overturned 24 convictions in death penalty cases.
It was the only state to allow judges to override jury decisions in whether or not to use a death sentence; in 10 cases judges overturned sentences of life imprisonment without parole (LWOP) that were voted unanimously by juries.
This judicial authority was removed in April 2017.
Alabama levies a 2, 4, or 5 percent personal income tax, depending upon the amount earned and filing status.
Taxpayers are allowed to deduct their federal income tax from their Alabama state tax, and can do so even if taking the standard deduction.
Taxpayers who file itemized deductions are also allowed to deduct the Federal Insurance Contributions Act tax (Social Security and Medicare tax).
The state's general sales tax rate is 4%.
Sales tax rates for cities and counties are also added to purchases.
For example, the total sales tax rate in Mobile is 10% and there is an additional restaurant tax of 1%, which means that a diner in Mobile would pay an 11% tax on a meal.
, sales and excise taxes in Alabama account for 51% of all state and local revenue, compared with an average of about 36% nationwide.
Alabama is one of seven states that levy a tax on food at the same rate as other goods, and one of two states (the other being neighboring Mississippi) which fully taxes groceries without any offsetting relief for low-income families.
(Most states exempt groceries from sales tax or apply a lower tax rate.)
Alabama's income tax on poor working families is among the highest in the United States.
Alabama is the only state that levies income tax on a family of four with income as low as $4,600, which is barely one-quarter of the federal poverty line.
Alabama's threshold is the lowest among the 41 states and the District of Columbia with income taxes.
The corporate income tax rate is currently 6.5%.
The overall federal, state, and local tax burden in Alabama ranks the state as the second least tax-burdened state in the country.
Property taxes are the lowest in the U.S.
The current state constitution requires a voter referendum to raise property taxes.
Since Alabama's tax structure largely depends on consumer spending, it is subject to high variable budget structure.
For example, in 2003, Alabama had an annual budget deficit as high as $670 million.
Alabama has 67 counties.
Each county has its own elected legislative branch, usually called the county commission.
It also has limited executive authority in the county.
Because of the constraints of the Alabama Constitution, which centralizes power in the state legislature, only seven counties (Jefferson, Lee, Mobile, Madison, Montgomery, Shelby, and Tuscaloosa) in the state have limited home rule.
Instead, most counties in the state must lobby the Local Legislation Committee of the state legislature to get simple local policies approved, ranging from waste disposal to land use zoning.
The state legislature has retained power over local governments by refusing to pass a constitutional amendment establishing home rule for counties, as recommended by the 1973 Alabama Constitutional Commission.
Legislative delegations retain certain powers over each county.
United States Supreme Court decisions in "Baker v. Carr" (1964) required that both houses have districts established on the basis of population, and redistricted after each census, in order to implement the principle of "one man, one vote".
Before that, each county was represented by one state senator, leading to under-representation in the state senate for more urbanized, populous counties.
The rural bias of the state legislature, which had also failed to redistrict seats in the state house, affected politics well into the 20th century, failing to recognize the rise of industrial cities and urbanized areas.
"The lack of home rule for counties in Alabama has resulted in the proliferation of local legislation permitting counties to do things not authorized by the state constitution.
Alabama's constitution has been amended more than 700 times, and almost one-third of the amendments are local in nature, applying to only one county or city.
A significant part of each legislative session is spent on local legislation, taking away time and attention of legislators from issues of statewide importance."
Alabama is an alcoholic beverage control state, meaning that the state government holds a monopoly on the sale of alcohol.
The Alabama Alcoholic Beverage Control Board controls the sale and distribution of alcoholic beverages in the state.
Twenty-five of the 67 counties are "dry counties" which ban the sale of alcohol, and there are many dry municipalities even in counties which permit alcohol sales.
During Reconstruction following the American Civil War, Alabama was occupied by federal troops of the Third Military District under General John Pope.
In 1874, the political coalition of white Democrats known as the Redeemers took control of the state government from the Republicans, in part by suppressing the black vote through violence, fraud and intimidation.
After 1890, a coalition of White Democratic politicians passed laws to segregate and disenfranchise African American residents, a process completed in provisions of the 1901 constitution.
Provisions which disenfranchised blacks resulted in excluding many poor Whites.
By 1941 more Whites than Blacks had been disenfranchised: 600,000 to 520,000.
The total effects were greater on the black community, as almost all of its citizens were disfranchised and relegated to separate and unequal treatment under the law.
From 1901 through the 1960s, the state did not redraw election districts as population grew and shifted within the state during urbanization and industrialization of certain areas.
As counties were the basis of election districts, the result was a rural minority that dominated state politics through nearly three-quarters of the century, until a series of federal court cases required redistricting in 1972 to meet equal representation.
Alabama state politics gained nationwide and international attention in the 1950s and 1960s during the civil rights movement, when whites bureaucratically, and at times violently, resisted protests for electoral and social reform.
Governor George Wallace, the state's only four-term governor, was a controversial figure who vowed to maintain segregation.
Only after passage of the federal Civil Rights Act of 1964 and Voting Rights Act of 1965 did African Americans regain the ability to exercise suffrage, among other civil rights.
In many jurisdictions, they continued to be excluded from representation by at-large electoral systems, which allowed the majority of the population to dominate elections.
Some changes at the county level have occurred following court challenges to establish single-member districts that enable a more diverse representation among county boards.
In 2007, the Alabama Legislature passed, and Republican Governor Bob Riley signed a resolution expressing "profound regret" over slavery and its lingering impact.
In a symbolic ceremony, the bill was signed in the Alabama State Capitol, which housed Congress of the Confederate States of America.
In 2010, Republicans won control of both houses of the legislature for the first time in 136 years.
, there are a total of 3,326,812 registered voters, with 2,979,576 active, and the others inactive in the state.
With the disfranchisement of Blacks in 1901, the state became part of the "Solid South", a system in which the Democratic Party operated as effectively the only viable political party in every Southern state.
For nearly 100 years, local and state elections in Alabama were decided in the Democratic Party primary, with generally only token Republican challengers running in the General Election.
Since the mid to late 20th century, however, there has been a realignment among the two major political parties, and white conservatives started shifting to the Republican Party.
In Alabama, majority-white districts are now expected to regularly elect Republican candidates to federal, state and local office.
Members of the nine seats on the Supreme Court of Alabama and all ten seats on the state appellate courts are elected to office.
Until 1994, no Republicans held any of the court seats.
In that general election, the then-incumbent Chief Justice, Ernest C. Hornsby, refused to leave office after losing the election by approximately 3,000 votes to Republican Perry O. Hooper, Sr.. Hornsby sued Alabama and defiantly remained in office for nearly a year before finally giving up the seat after losing in court.
This ultimately led to a collapse of support for Democrats at the ballot box in the next three or four election cycles.
The Democrats lost the last of the nineteen court seats in August 2011 with the resignation of the last Democrat on the bench.
In the early 21st century, Republicans hold all seven of the statewide elected executive branch offices.
Republicans hold six of the eight elected seats on the Alabama State Board of Education.
In 2010, Republicans took large majorities of both chambers of the state legislature, giving them control of that body for the first time in 136 years.
The last remaining statewide Democrat, who served on the Alabama Public Service Commission was defeated in 2012.
Only two Republican Lieutenant Governors have been elected since the end of Reconstruction, when Republicans generally represented Reconstruction government, including the newly emancipated freedmen who had gained the franchise.
The two GOP Lt.
Governors were Steve Windom (1999–2003) and Kay Ivey (2011-2017).
Many local offices (County Commissioners, Boards of Education, Tax Assessors, Tax Collectors, etc.)
in the state are still held by Democrats.
Many rural counties have voters who are majority Democrats, resulting in local elections being decided in the Democratic primary.
Similarly many metropolitan and suburban counties are majority-Republican and elections are effectively decided in the Republican Primary, although there are exceptions.
Alabama's 67 County Sheriffs are elected in partisan, at-large races, and Democrats still retain the narrow majority of those posts.
The current split is 35 Democrats, 31 Republicans, and one Independent Fayette.
However, most of the Democratic sheriffs preside over rural and less populated counties.
The majority of Republican sheriffs have been elected in the more urban/suburban and heavily populated counties.
, the state of Alabama has one female sheriff, in Morgan County, Alabama, and ten African-American sheriffs.
The state's two U.S.
senators are Republican Richard C. Shelby and Democrat Doug Jones.
Shelby was originally elected to the Senate as a Democrat in 1986 and re-elected in 1992, but switched parties immediately following the November 1994 general election.
In the U.S.
House of Representatives, the state is represented by seven members, six of whom are Republicans: (Bradley Byrne, Mike D. Rogers, Robert Aderholt, Morris J. Brooks, Martha Roby, and Gary Palmer) and one Democrat: Terri Sewell who represents the Black Belt as well as most of the predominantly black portions of Birmingham, Tuscaloosa and Montgomery.
Public primary and secondary education in Alabama is under the purview of the Alabama State Board of Education as well as local oversight by 67 county school boards and 60 city boards of education.
Together, 1,496 individual schools provide education for 744,637 elementary and secondary students.
Public school funding is appropriated through the Alabama Legislature through the Education Trust Fund.
In FY 2006–2007, Alabama appropriated $3,775,163,578 for primary and secondary education.
That represented an increase of $444,736,387 over the previous fiscal year.
In 2007, over 82 percent of schools made adequate yearly progress (AYP) toward student proficiency under the National No Child Left Behind law, using measures determined by the state of Alabama.
While Alabama's public education system has improved in recent decades, it lags behind in achievement compared to other states.
According to U.S.
Census data (2000), Alabama's high school graduation rate—75%—is the fourth lowest in the U.S.
(after Kentucky, Louisiana and Mississippi).
The largest educational gains were among people with some college education but without degrees.
Although unusual in the West, school corporal punishment is not uncommon in Alabama, with 27,260 public school students paddled at least one time, according to government data for the 2011–2012 school year.
The rate of school corporal punishment in Alabama is surpassed only by Mississippi and Arkansas.
Alabama's programs of higher education include 14 four-year public universities, two-year community colleges, and 17 private, undergraduate and graduate universities.
In the state are four medical schools (as of fall 2015) (University of Alabama School of Medicine, University of South Alabama and Alabama College of Osteopathic Medicine and The Edward Via College of Osteopathic Medicine – Auburn Campus), two veterinary colleges (Auburn University and Tuskegee University), a dental school (University of Alabama School of Dentistry), an optometry college (University of Alabama at Birmingham), two pharmacy schools (Auburn University and Samford University), and five law schools (University of Alabama School of Law, Birmingham School of Law, Cumberland School of Law, Miles Law School, and the Thomas Goode Jones School of Law).
Public, post-secondary education in Alabama is overseen by the Alabama Commission on Higher Education and the Alabama Department of Postsecondary Education.
Colleges and universities in Alabama offer degree programs from two-year associate degrees to a multitude of doctoral level programs.
The largest single campus is the University of Alabama, located in Tuscaloosa, with 37,665 enrolled for fall 2016.
Troy University was the largest institution in the state in 2010, with an enrollment of 29,689 students across four Alabama campuses (Troy, Dothan, Montgomery, and Phenix City), as well as sixty learning sites in seventeen other states and eleven other countries.
The oldest institutions are the public University of North Alabama in Florence and the Catholic Church-affiliated Spring Hill College in Mobile, both founded in 1830.
Accreditation of academic programs is through the Southern Association of Colleges and Schools (SACS) as well as other subject-focused national and international accreditation agencies such as the Association for Biblical Higher Education (ABHE), the Council on Occupational Education (COE), and the Accrediting Council for Independent Colleges and Schools (ACICS).
According to the 2011 "U.S.
News & World Report", Alabama had three universities ranked in the top 100 Public Schools in America (University of Alabama at 31, Auburn University at 36, and University of Alabama at Birmingham at 73).
According to the 2012 "U.S.
News & World Report", Alabama had four tier 1 universities (University of Alabama, Auburn University, University of Alabama at Birmingham and University of Alabama in Huntsville).
Major newspapers include "Birmingham News", Mobile "Press-Register", and "Montgomery Advertiser".
Political websites include Alabama Political Reporter, Left in Alabama, and Yellowhammer News.
Major television network affiliates in Alabama include:

ABC

CBS

Fox

NBC

PBS/Alabama Public Television

The CW

Viewers in eastern Alabama are served by stations in Atlanta and Columbus, Georgia.
College football is extremely popular in Alabama, particularly the University of Alabama Crimson Tide and Auburn University Tigers, rivals in the Southeastern Conference.
In the 2013 season, Alabama averaged over 100,000 fans per game and Auburn averaged over 80,000 fans, both numbers among the top 20 in the nation in average attendance.
Bryant–Denny Stadium is the home of the Alabama football team, and has a seating capacity of 101,821, and is the fifth largest stadium in America.
Jordan-Hare Stadium is the home field of the Auburn football team and seats up to 87,451.
Legion Field is home for the UAB Blazers football program and the Birmingham Bowl.
It seats 71,594.
Ladd–Peebles Stadium in Mobile is the home of the University of South Alabama football team, and serves as the home of the NCAA Senior Bowl, Dollar General Bowl (formerly GoDaddy.com Bowl), and Alabama-Mississippi All Star Classic; the stadium seats 40,646.
In 2009, Bryant–Denny Stadium and Jordan-Hare Stadium became the homes of the Alabama High School Athletic Association state football championship games, after previously being held at Legion Field in Birmingham.
Alabama has several professional and semi-professional sports teams, including three minor league baseball teams.
The Talladega Superspeedway motorsports complex hosts a series of NASCAR events.
It has a seating capacity of 143,000 and is the thirteenth largest stadium in the world and sixth largest stadium in America.
Also, the Barber Motorsports Park has hosted IndyCar Series and Rolex Sports Car Series races.
The ATP Birmingham was a World Championship Tennis tournament held from 1973 to 1980.
Alabama has hosted several professional golf tournaments, such as the 1984 and 1990 PGA Championship at Shoal Creek, the Barbasol Championship (PGA Tour), the Mobile LPGA Tournament of Champions, Airbus LPGA Classic, and Yokohama Tire LPGA Classic (LPGA Tour), and The Tradition (Champions Tour).
Major airports with sustained commercial operations in Alabama include Birmingham-Shuttlesworth International Airport (BHM), Huntsville International Airport (HSV), Dothan Regional Airport (DHN), Mobile Regional Airport (MOB), Montgomery Regional Airport (MGM), and Muscle Shoals – Northwest Alabama Regional Airport (MSL).
For rail transport, Amtrak schedules the "Crescent", a daily passenger train, running from New York to New Orleans with station stops at Anniston, Birmingham, and Tuscaloosa.
Alabama has six major interstate roads that cross the state: Interstate 65 (I-65) travels north–south roughly through the middle of the state; I-20/I-59 travel from the central west Mississippi state line to Birmingham, where I-59 continues to the north-east corner of the state and I-20 continues east towards Atlanta; I-85 originates in Montgomery and travels east-northeast to the Georgia state line, providing a main thoroughfare to Atlanta; and I-10 traverses the southernmost portion of the state, traveling from west to east through Mobile.
I-22 enters the state from Mississippi and connects Birmingham with Memphis, Tennessee.
In addition, there are currently five auxiliary interstate routes in the state: I-165 in Mobile, I-359 in Tuscaloosa, I-459 around Birmingham, I-565 in Decatur and Huntsville, and I-759 in Gadsden.
A sixth route, I-685, will be formed when I-85 is rerouted along a new southern bypass of Montgomery.
A proposed northern bypass of Birmingham will be designated as I-422.
Since a direct connection from I-22 to I-422 will not be possible, I-222 has been proposed, as well.
Several U.S.
Highways also pass through the state, such as U.S.
Route 11 (US-11), US-29, US-31, US-43, US-45, US-72, US-78, US-80, US-82, US-84, US-90, US-98, US-231, US-278, US-280, US-331, US-411, and US-431.
There are four toll roads in the state: Montgomery Expressway in Montgomery; Tuscaloosa Bypass in Tuscaloosa; Emerald Mountain Expressway in Wetumpka; and Beach Express in Orange Beach.
The Port of Mobile, Alabama's only saltwater port, is a large seaport on the Gulf of Mexico with inland waterway access to the Midwest by way of the Tennessee-Tombigbee Waterway.
The Port of Mobile was ranked 12th by tons of traffic in the United States during 2009.
The newly expanded container terminal at the Port of Mobile was ranked as the 25th busiest for container traffic in the nation during 2011.
The state's other ports are on rivers with access to the Gulf of Mexico.
Water ports of Alabama, listed from north to south:






</doc>
<doc id="305" url="https://en.wikipedia.org/wiki?curid=305" title="Achilles">
Achilles

In Greek mythology, Achilles or Achilleus ( ; , "Achilleus" ) was a Greek hero of the Trojan War and the central character and greatest warrior of Homer's "Iliad".
His mother was the immortal Nereid Thetis, and his father, the mortal Peleus, was the king of the Myrmidons.
Achilles' most notable feat during the Trojan War was the slaying of the Trojan hero Hector outside the gates of Troy.
Although the death of Achilles is not presented in the "Iliad", other sources concur that he was killed near the end of the Trojan War by Paris, who shot him in the heel with an arrow.
Later legends (beginning with a poem by Statius in the 1st century AD) state that Achilles was invulnerable in all of his body except for his heel because, when his mother Thetis dipped him in the river Styx as an infant, she held him by one of his heels.
Alluding to these legends, the term "Achilles' heel" has come to mean a point of weakness, especially in someone or something with an otherwise strong constitution.
Linear B tablets attest to the personal name "Achilleus" in the forms "a-ki-re-u" and "a-ki-re-we", the latter being the dative of the former.
The name grew more popular, even becoming common soon after the seventh century BC and was also turned into the female form Ἀχιλλεία ("Achilleía"), attested in Attica in the fourth century BC (IG II² 1617) and, in the form "Achillia", on a stele in Halicarnassus as the name of a female gladiator fighting an "Amazon".
Achilles' name can be analyzed as a combination of (') "distress, pain, sorrow, grief" and (') "people, soldiers, nation", resulting in a proto-form "*Akhí-lāu̯os" "he who has the people distressed" or "he whose people have distress".
The grief or distress of the people is a theme raised numerous times in the "Iliad" (and frequently by Achilles himself).
Achilles' role as the hero of grief or distress forms an ironic juxtaposition with the conventional view of him as the hero of "" ("glory", usually in war).
Furthermore, "laós" has been construed by Gregory Nagy, following Leonard Palmer, to mean "a corps of soldiers", a muster.
With this derivation, the name obtains a double meaning in the poem: when the hero is functioning rightly, his men bring distress to the enemy, but when wrongly, his men get the grief of war.
The poem is in part about the misdirection of anger on the part of leadership.
Another etymology relates the name to a Proto-Indo-European compound "*h₂eḱ-pṓds" "sharp foot" which first gave an Illyrian "*āk̂pediós", evolving through time into "*ākhpdeós" and then "*akhiddeús".
The shift from "-dd-" to "-ll-" is then ascribed to the passing of the name into Greek via a Pre-Greek source.
The first root part "*h₂eḱ-" "sharp, pointed" also gave Greek ἀκή ("akḗ" "point, silence, healing"), ἀκμή ("akmḗ" "point, edge, zenith") and ὀξύς ("oxús" "sharp, pointed, keen, quick, clever"), whereas ἄχος stems from the root "*h₂egʰ-" "to be upset, afraid".
The whole expression would be comparable to the Latin "acupedius" "swift of foot".
Compare also the Latin word family of "aciēs" "sharp edge or point, battle line, battle, engagement", "acus" "needle, pin, bodkin", and "acuō" "to make pointed, sharpen, whet; to exercise; to arouse" (whence "acute").
Some topical epitheta of Achilles in the "Iliad" point to this "swift-footedness", namely ποδάρκης δῖος Ἀχιλλεὺς ("podárkēs dĩos Achilleús" "swift-footed divine Achilles") or, even more frequently, πόδας ὠκὺς Ἀχιλλεύς ("pódas ōkús Achilleús" "quick-footed Achilles").
Some researchers deem the name a loan word, possibly from a Pre-Greek language.
Achilles' descent from the Nereid Thetis and a similarity of his name with those of river deities such as Acheron and Achelous have led to speculations about him being an old water divinity (see below Worship).
Robert S. P. Beekes has suggested a Pre-Greek origin of the name, based among other things on the coexistence of "-λλ-" and "-λ-" in epic language, which may account for a palatalized phoneme /l/ in the original language.
Achilles was the son of the Nereid Thetis and of Peleus, the king of the Myrmidons.
Zeus and Poseidon had been rivals for the hand of Thetis until Prometheus, the fore-thinker, warned Zeus of a prophecy (originally uttered by Themis, goddess of divine law) that Thetis would bear a son greater than his father.
For this reason, the two gods withdrew their pursuit, and had her wed Peleus.
There is a tale which offers an alternative version of these events: In the "Argonautica" (4.760) Zeus' sister and wife Hera alludes to Thetis' chaste resistance to the advances of Zeus, pointing out that Thetis was so loyal to Hera's marriage bond that she coolly rejected the father of gods.
Thetis, although a daughter of the sea-god Nereus, was also brought up by Hera, further explaining her resistance to the advances of Zeus.
Zeus was furious and decreed that she would never marry an immortal.
According to the "Achilleid", written by Statius in the 1st century AD, and to non-surviving previous sources, when Achilles was born Thetis tried to make him immortal by dipping him in the river Styx.
However, he was left vulnerable at the part of the body by which she held him: his left heel (see Achilles' heel, Achilles' tendon).
It is not clear if this version of events was known earlier.
In another version of this story, Thetis anointed the boy in ambrosia and put him on top of a fire in order to burn away the mortal parts of his body.
She was interrupted by Peleus and abandoned both father and son in a rage.
However, none of the sources before Statius make any reference to this general invulnerability.
To the contrary, in the "Iliad" Homer mentions Achilles being wounded: in Book 21 the Paeonian hero Asteropaeus, son of Pelagon, challenged Achilles by the river Scamander.
He cast two spears at once, one grazed Achilles' elbow, "drawing a spurt of blood".
Also, in the fragmentary poems of the Epic Cycle in which one can find description of the hero's death (i.e. the "Cypria", the "Little Iliad" by Lesches of Pyrrha, the "Aithiopis" and "Iliou persis" by Arctinus of Miletus), there is no trace of any reference to his general invulnerability or his famous weakness at the heel; in the later vase paintings presenting the death of Achilles, the arrow (or in many cases, arrows) hit his body.
Peleus entrusted Achilles to Chiron the Centaur, on Mount Pelion, to be reared.
Thetis foretold that her son's fate was either to gain glory and die young, or to live a long but uneventful life in obscurity.
Achilles chose the former, and decided to take part in the Trojan war.
According to Homer, Achilles grew up in Phthia together with his companion Patroclus.
According to Photius, the sixth book of the "New History" by Ptolemy Hephaestion reported that Thetis burned in a secret place the children she had by Peleus; but when she had Achilles, Peleus noticed, tore him from the flames with only a burnt foot, and confided him to the centaur Chiron.
Later Chiron exhumed the body of the Damysus, who was the fastest of all the giants, removed the ankle, and incorporated it into Achilles' burnt foot.
Some post-Homeric sources claim that in order to keep Achilles safe from the war, Thetis (or, in some versions, Peleus) hid the young man at the court of Lycomedes, king of Skyros.
There, Achilles is disguised as a girl and lives among Lycomedes' daughters, perhaps under the name "Pyrrha" (the red-haired girl).
With Lycomedes' daughter Deidamia, whom in the account of Statius he rapes, Achilles there fathers a son, Neoptolemus (also called Pyrrhus, after his father's possible alias).
According to this story, Odysseus learns from the prophet Calchas that the Achaeans would be unable to capture Troy without Achilles' aid.
Odysseus goes to Skyros in the guise of a peddler selling women's clothes and jewelry and places a shield and spear among his goods.
When Achilles instantly takes up the spear, Odysseus sees through his disguise and convinces him to join the Greek campaign.
In another version of the story, Odysseus arranges for a trumpet alarm to be sounded while he was with Lycomedes' women; while the women flee in panic, Achilles prepares to defend the court, thus giving his identity away.
According to the "Iliad", Achilles arrived at Troy with 50 ships, each carrying 50 Myrmidons.
He appointed five leaders (each leader commanding 500 Myrmidons): Menesthius, Eudorus, Peisander, Phoenix and Alcimedon.
When the Greeks left for the Trojan War, they accidentally stopped in Mysia, ruled by King Telephus.
In the resulting battle, Achilles gave Telephus a wound that would not heal; Telephus consulted an oracle, who stated that "he that wounded shall heal".
Guided by the oracle, he arrived at Argos, where Achilles healed him in order that he might become their guide for the voyage to Troy.
According to other reports in Euripides' lost play about Telephus, he went to Aulis pretending to be a beggar and asked Achilles to heal his wound.
Achilles refused, claiming to have no medical knowledge.
Alternatively, Telephus held Orestes for ransom, the ransom being Achilles' aid in healing the wound.
Odysseus reasoned that the spear had inflicted the wound; therefore, the spear must be able to heal it.
Pieces of the spear were scraped off onto the wound and Telephus was healed.
According to the "Cypria" (the part of the Epic Cycle that tells the events of the Trojan War before Achilles' wrath), when the Achaeans desired to return home, they were restrained by Achilles, who afterwards attacked the cattle of Aeneas, sacked neighbouring cities (like Pedasus and Lyrnessus, where the Greeks capture the queen Briseis) and killed Tenes, a son of Apollo, as well as Priam's son Troilus in the sanctuary of Apollo Thymbraios.
However, the romance between Troilus and Chryseis described in Geoffrey Chaucer's "Troilus and Criseyde" and in William Shakespeare's "Troilus and Cressida" is a medieval invention.
In Dares Phrygius' "Account of the Destruction of Troy", the Latin summary through which the story of Achilles was transmitted to medieval Europe, as well as in older accounts, Troilus was a young Trojan prince, the youngest of King Priam's and Hecuba's five legitimate sons (or according other sources, another son of Apollo).
Despite his youth, he was one of the main Trojan war leaders, a "horse fighter" or "chariot fighter" according to Homer.
Prophecies linked Troilus' fate to that of Troy and so he was ambushed in an attempt to capture him.
Yet Achilles, struck by the beauty of both Troilus and his sister Polyxena, and overcome with lust, directed his sexual attentions on the youth – who, refusing to yield, instead found himself decapitated upon an altar-omphalos of Apollo Thymbraios.
Later versions of the story suggested Troilus was accidentally killed by Achilles in an over-ardent lovers' embrace.
In this version of the myth, Achilles' death therefore came in retribution for this sacrilege.
Ancient writers treated Troilus as the epitome of a dead child mourned by his parents.
Had Troilus lived to adulthood, the First Vatican Mythographer claimed, Troy would have been invincible.
Homer's "Iliad" is the most famous narrative of Achilles' deeds in the Trojan War.
Achilles' wrath (μῆνις Ἀχιλλέως, "mênis Achilléōs") is the central theme of the poem.
The first two lines of the "Iliad" read:
The Homeric epic only covers a few weeks of the decade-long war, and does not narrate Achilles' death.
It begins with Achilles' withdrawal from battle after being dishonoured by Agamemnon, the commander of the Achaean forces.
Agamemnon has taken a woman named Chryseis as his slave.
Her father Chryses, a priest of Apollo, begs Agamemnon to return her to him.
Agamemnon refuses, and Apollo sends a plague amongst the Greeks.
The prophet Calchas correctly determines the source of the troubles but will not speak unless Achilles vows to protect him.
Achilles does so, and Calchas declares that Chryseis must be returned to her father.
Agamemnon consents, but then commands that Achilles' battle prize Briseis, the daughter of Briseus, be brought to him to replace Chryseis.
Angry at the dishonour of having his plunder and glory taken away (and, as he says later, because he loves Briseis), with the urging of his mother Thetis, Achilles refuses to fight or lead his troops alongside the other Greek forces.
At the same time, burning with rage over Agamemnon's theft, Achilles prays to Thetis to convince Zeus to help the Trojans gain ground in the war, so that he may regain his honour.
As the battle turns against the Greeks, thanks to the influence of Zeus, Nestor declares that the Trojans are winning because Agamemnon has angered Achilles, and urges the king to appease the warrior.
Agamemnon agrees and sends Odysseus and two other chieftains, Ajax and Phoenix, to Achilles with the offer of the return of Briseis and other gifts.
Achilles rejects all Agamemnon offers him and simply urges the Greeks to sail home as he was planning to do.
The Trojans, led by Hector, subsequently push the Greek army back toward the beaches and assault the Greek ships.
With the Greek forces on the verge of absolute destruction, Patroclus leads the Myrmidons into battle, wearing Achilles' armour, though Achilles remains at his camp.
Patroclus succeeds in pushing the Trojans back from the beaches, but is killed by Hector before he can lead a proper assault on the city of Troy.
After receiving the news of the death of Patroclus from Antilochus, the son of Nestor, Achilles grieves over his beloved companion's death.
His mother Thetis comes to comfort the distraught Achilles.
She persuades Hephaestus to make new armour for him, in place of the armour that Patroclus had been wearing, which was taken by Hector.
The new armour includes the Shield of Achilles, described in great detail in the poem.
Enraged over the death of Patroclus, Achilles ends his refusal to fight and takes the field, killing many men in his rage but always seeking out Hector.
Achilles even engages in battle with the river god Scamander, who has become angry that Achilles is choking his waters with all the men he has killed.
The god tries to drown Achilles but is stopped by Hera and Hephaestus.
Zeus himself takes note of Achilles' rage and sends the gods to restrain him so that he will not go on to sack Troy itself before the time allotted for its destruction, seeming to show that the unhindered rage of Achilles can defy fate itself.
Finally, Achilles finds his prey.
Achilles chases Hector around the wall of Troy three times before Athena, in the form of Hector's favorite and dearest brother, Deiphobus, persuades Hector to stop running and fight Achilles face to face.
After Hector realizes the trick, he knows the battle is inevitable.
Wanting to go down fighting, he charges at Achilles with his only weapon, his sword, but misses.
Accepting his fate, Hector begs Achilles, not to spare his life, but to treat his body with respect after killing him.
Achilles tells Hector it is hopeless to expect that of him, declaring that "my rage, my fury would drive me now to hack your flesh away and eat you raw – such agonies you have caused me".
Achilles then kills Hector and drags his corpse by its heels behind his chariot.
After having a dream where Patroclus begs Achilles to hold his funeral, Achilles hosts a series of funeral games in his honour.
With the assistance of the god Hermes, Hector's father, Priam, goes to Achilles' tent to plead with Achilles for the return of Hector's body so that he can be buried.
Achilles relents and promises a truce for the duration of the funeral.
The poem ends with a description of Hector's funeral, with the doom of Troy and Achilles himself still to come.
The "Aethiopis" (7th century BC) and a work named "Posthomerica", composed by Quintus of Smyrna in the fourth century AD, relate further events from the Trojan War.
When Penthesilea, queen of the Amazons and daughter of Ares, arrives in Troy, Priam hopes that she will defeat Achilles.
After his temporary truce with Priam, Achilles fights and kills the warrior queen, only to grieve over her death later.
At first, he was so distracted by her beauty, he did not fight as intensely as usual.
Once he realized that his distraction was endangering his life, he refocused and killed her.
Following the death of Patroclus, Nestor's son Antilochus becomes Achilles' closest companion.
When Memnon, son of the Dawn Goddess Eos and king of Ethiopia, slays Antilochus, Achilles once more obtains revenge on the battlefield, killing Memnon.
Consequently, Eos will not let the sun rise, until Zeus persuades her.
The fight between Achilles and Memnon over Antilochus echoes that of Achilles and Hector over Patroclus, except that Memnon (unlike Hector) was also the son of a goddess.
Many Homeric scholars argued that episode inspired many details in the "Iliad"'s description of the death of Patroclus and Achilles' reaction to it.
The episode then formed the basis of the cyclic epic "Aethiopis", which was composed after the "Iliad", possibly in the 7th century BC.
The "Aethiopis" is now lost, except for scattered fragments quoted by later authors.
The exact nature of Achilles' relationship with Patroclus has been a subject of dispute in both the classical period and modern times.
In the "Iliad", it appears to be the model of a deep and loyal friendship.
Homer does not suggest that Achilles and his close friend Patroclus were lovers.
Despite there being no direct evidence in the text of the "Iliad" that Achilles and Patroclus were lovers, this theory was expressed by some later authors.
Commentators from classical antiquity to the present have often interpreted the relationship through the lens of their own cultures.
In 5th-century BC Athens, the intense bond was often viewed in light of the Greek custom of "paiderasteia".
In Plato's "Symposium", the participants in a dialogue about love assume that Achilles and Patroclus were a couple; Phaedrus argues that Achilles was the younger and more beautiful one so he was the beloved and Patroclus was the lover.
But ancient Greek had no words to distinguish heterosexual and homosexual, and it was assumed that a man could both desire handsome young men and have sex with women.
The death of Achilles, as predicted by Hector with his dying breath, was brought about by Paris with an arrow (to the heel according to Statius).
In some versions, the god Apollo guided Paris' arrow.
Some retellings also state that Achilles was scaling the gates of Troy and was hit with a poisoned arrow.
All of these versions deny Paris any sort of valour, owing to the common conception that Paris was a coward and not the man his brother Hector was, and Achilles remained undefeated on the battlefield.
His bones were mingled with those of Patroclus, and funeral games were held.
He was represented in the "Aethiopis" as living after his death in the island of Leuke at the mouth of the river Danube.
Another version of Achilles' death is that he fell deeply in love with one of the Trojan princesses, Polyxena.
Achilles asks Priam for Polyxena's hand in marriage.
Priam is willing because it would mean the end of the war and an alliance with the world's greatest warrior.
But while Priam is overseeing the private marriage of Polyxena and Achilles, Paris, who would have to give up Helen if Achilles married his sister, hides in the bushes and shoots Achilles with a divine arrow, killing him.
In the "Odyssey", Agamemnon informs Achilles of his pompous burial and the erection of his mound at the Hellespont while they are receiving the dead suitors in Hades.
He claims they built a massive burial mound on the beach of Ilion that could be seen by anyone approaching from the Ocean.
Achilles was cremated and his ashes buried in the same urn as those of Patroclus.
Paris was later killed by Philoctetes using the enormous bow of Heracles.
In Book 11 of Homer's "Odyssey", Odysseus sails to the underworld and converses with the shades.
One of these is Achilles, who when greeted as "blessed in life, blessed in death", responds that he would rather be a slave to the worst of masters than be king of all the dead.
But Achilles then asks Odysseus of his son's exploits in the Trojan war, and when Odysseus tells of Neoptolemus' heroic actions, Achilles is filled with satisfaction.
This leaves the reader with an ambiguous understanding of how Achilles felt about the heroic life.
According to some accounts, he had married Medea in life, so that after both their deaths they were united in the Elysian Fields of Hades – as Hera promised Thetis in Apollonius' "Argonautica" (3rd century BC).
Achilles' armour was the object of a feud between Odysseus and Telamonian Ajax (Ajax the greater).
They competed for it by giving speeches on why they were the bravest after Achilles to their Trojan prisoners, who after considering both men, decided Odysseus was more deserving of the armour.
Furious, Ajax cursed Odysseus, which earned him the ire of Athena.
Athena temporarily made Ajax so mad with grief and anguish that he began killing sheep, thinking them his comrades.
After a while, when Athena lifted his madness and Ajax realized that he had actually been killing sheep, Ajax was left so ashamed that he committed suicide.
Odysseus eventually gave the armour to Neoptolemus, the son of Achilles.
A relic claimed to be Achilles' bronze-headed spear was for centuries preserved in the temple of Athena on the acropolis of Phaselis, Lycia, a port on the Pamphylian Gulf.
The city was visited in 333 BC by Alexander the Great, who envisioned himself as the new Achilles and carried the "Iliad" with him, but his court biographers do not mention the spear.
However, it was shown in the time of Pausanias in the 2nd century AD.
Numerous paintings on pottery have suggested a tale not mentioned in the literary traditions.
At some point in the war, Achilles and Ajax were playing a board game ("petteia").
They were absorbed in the game and oblivious to the surrounding battle.
The Trojans attacked and reached the heroes, who were saved only by an intervention of Athena.
The tomb of Achilles, extant throughout antiquity in Troad, was venerated by Thessalians, but also by Persian expeditionary forces, as well as by Alexander the Great and the Roman emperor Caracalla.
Achilles' cult was also to be found at other places, e. g. on the island of Astypalaea in the Sporades, in Sparta which had a sanctuary, in Elis and in Achilles' homeland Thessaly, as well as in the Magna Graecia cities of Tarentum, Locri and Croton, accounting for an almost Panhellenic cult to the hero.
The spread and intensity of the hero's veneration among the Greeks that had settled on the northern coast of the Pontus Euxinus, today's Black Sea, appears to have been remarkable.
An archaic cult is attested for the Milesian colony of Olbia as well as for an island in the middle of the Black Sea, today identified with Snake Island (Ukrainian Зміїний, "Zmiinyi", near Kiliya, Ukraine).
Early dedicatory inscriptions from the Greek colonies on the Black Sea (graffiti and inscribed clay disks, these possibly being votive offerings, from Olbia, the area of Berezan Island and the Tauric Chersonese) attest the existence of a heroic cult of Achilles from the sixth century BC onwards.
The cult was still thriving in the third century AD, when dedicatory stelae from Olbia refer to an "Achilles Pontárchēs" (Ποντάρχης, roughly "lord of the Sea," or "of the Pontus Euxinus"), who was invoked as a protector of the city of Olbia, venerated on par with Olympian gods such as the local Apollo Prostates, Hermes Agoraeus, or Poseidon.
Pliny the Elder (23–79 AD) in his "Natural History" mentions a "port of the Achæi" and an "island of Achilles", famous for the tomb of that "man" (portus Achaeorum, insula Achillis, tumulo eius viri clara), situated somewhat nearby Olbia and the Dnieper-Bug Estuary; furthermore, at 125 Roman miles from this island, he places a peninsula "which stretches forth in the shape of a sword" obliquely, called "Dromos Achilleos" (Ἀχιλλέως δρόμος, "Achilléōs drómos" "the Race-course of Achilles") and considered the place of the hero's exercise or of games instituted by him.
This last feature of Pliny's account is considered to be the iconic spit, called today "Tendra" (or "Kosa Tendra" and "Kosa Djarilgatch"), situated between the mouth of the Dnieper and Karkinit Bay, but which is hardly 125 Roman miles (c.
185 km) away from the Dnieper-Bug estuary, as Pliny states.
(To the "Race-course" he gives a length of 80 miles, c.
120 km, whereas the spit measures c.
70 km today.)
In the following chapter of his book, Pliny refers to the same island as "Achillea" and introduces two further names for it: "Leuce" or "Macaron" (from Greek [νῆσος] μακαρῶν "island of the blest").
The "present day" measures, he gives at this point, seem to account for an identification of "Achillea" or "Leuce" with today's Snake Island.
Pliny's contemporary Pomponius Mela (c.
43 AD) tells that Achilles was buried on an island named "Achillea", situated between the Borysthenes and the Ister, adding to the geographical confusion.
Ruins of a square temple, measuring 30 meters to a side, possibly that dedicated to Achilles, were discovered by Captain Kritzikly in 1823 on Snake Island.
A second exploration in 1840 showed that the construction of a lighthouse had destroyed all traces of this temple.
A fifth century BC black-glazed lekythos inscription, found on the island in 1840, reads: "Glaukos, son of Poseidon, dedicated me to Achilles, lord of Leuke."
In another inscription from the fifth or fourth century BC, a statue is dedicated to Achilles, lord of Leuke, by a citizen of Olbia, while in a further dedication, the city of Olbia confirms its continuous maintenance of the island's cult, again suggesting its quality as a place of a supra-regional hero veneration.
The heroic cult dedicated to Achilles on "Leuce" seems to go back to an account from the lost epic "Aethiopis" according to which, after his untimely death, Thetis had snatched her son from the funeral pyre and removed him to a mythical Λεύκη Νῆσος ("Leúkē Nêsos" "White Island").
Already in the fifth century BC, Pindar had mentioned a cult of Achilles on a "bright island" (φαεννά νᾶσος, "phaenná nâsos") of the Black Sea, while in another of his works, Pindar would retell the story of the immortalized Achilles living on a geographically indefinite Island of the Blest together with other heroes such as his father Peleus and Cadmus.
Well known is the connection of these mythological Fortunate Isles (μακαρῶν νῆσοι, "makárôn nêsoi") or the Homeric Elysium with the stream Oceanus which according to Greek mythology surrounds the inhabited world, which should have accounted for the identification of the northern strands of the Euxine with it.
Guy Hedreen has found further evidence for this connection of Achilles with the northern margin of the inhabited world in a poem by Alcaeus, speaking of "Achilles lord of Scythia" and the opposition of North and South, as evoked by Achilles' fight against the Aethiopian prince Memnon, who in his turn would be removed to his homeland by his mother Eos after his death.
The "Periplus of the Euxine Sea" (c.
130 AD) gives the following details:

The Greek geographer Dionysius Periegetes, who lived probably during the first century AD, wrote that the island was called "Leuce" "because the wild animals which live there are white.
It is said that there, in Leuce island, reside the souls of Achilles and other heroes, and that they wander through the uninhabited valleys of this island; this is how Jove rewarded the men who had distinguished themselves through their virtues, because through virtue they had acquired everlasting honour".
Similarly, others relate the island's name to its white cliffs, snakes or birds dwelling there.
Pausanias has been told that the island is "covered with forests and full of animals, some wild, some tame.
In this island there is also Achilles' temple and his statue".
Leuce had also a reputation as a place of healing.
Pausanias reports that the Delphic Pythia sent a lord of Croton to be cured of a chest wound.
Ammianus Marcellinus attributes the healing to waters ("aquae") on the island.
A number of important commercial port cities of the Greek waters were dedicated to Achilles.
Herodotus, Pliny the Elder and Strabo reported on the existence of a town "Achílleion" (Ἀχίλλειον), built by settlers from Mytilene in the sixth century BC, close to the hero's presumed burial mound in the Troad.
Later attestations point to an "Achílleion" in Messenia (according to Stephanus Byzantinus) and an "Achílleios" (Ἀχίλλειος) in Laconia.
Nicolae Densuşianu recognized a connection to Achilles in the names of Aquileia and of the northern arm of the Danube delta, called Chilia (presumably from an older "Achileii"), though his conclusion, that Leuce had sovereign rights over the Black Sea, evokes modern rather than archaic sea-law.
The kings of Epirus claimed to be descended from Achilles through his son, Neoptolemus.
Alexander the Great, son of the Epirote princess Olympias, could therefore also claim this descent, and in many ways strove to be like his great ancestor.
He is said to have visited the tomb of Achilles at Achilleion while passing Troy.
In AD 216 the Roman Emperor Caracalla, while on his way to war against Parthia, emulated Alexander by holding games around Achilles' tumulus.
The Greek tragedian Aeschylus wrote a trilogy of plays about Achilles, given the title "Achilleis" by modern scholars.
The tragedies relate the deeds of Achilles during the Trojan War, including his defeat of Hector and eventual death when an arrow shot by Paris and guided by Apollo punctures his heel.
Extant fragments of the "Achilleis" and other Aeschylean fragments have been assembled to produce a workable modern play.
The first part of the "Achilleis" trilogy, "The Myrmidons", focused on the relationship between Achilles and chorus, who represent the Achaean army and try to convince Achilles to give up his quarrel with Agamemnon; only a few lines survive today.
In Plato's "Symposium", Phaedrus points out that Aeschylus portrayed Achilles as the lover and Patroclus as the beloved; Phaedrus argues that this is incorrect because Achilles, being the younger and more beautiful of the two, was the beloved, who loved his lover so much that he chose to die to revenge him.
The tragedian Sophocles also wrote "The Lovers of Achilles", a play with Achilles as the main character.
Only a few fragments survive.
Towards the end of the 5th century BC, a more negative view of Achilles emerges in Greek drama; Euripides refers to Achilles in a bitter or ironic tone in "Hecuba", "Electra", and "Iphigenia in Aulis".
The philosopher Zeno of Elea centered one of his paradoxes on an imaginary footrace between "swift-footed" Achilles and a tortoise, by which he attempted to show that Achilles could not catch up to a tortoise with a head start, and therefore that motion and change were impossible.
As a student of the monist Parmenides and a member of the Eleatic school, Zeno believed time and motion to be illusions.
The Romans, who traditionally traced their lineage to Troy, took a highly negative view of Achilles.
Virgil refers to Achilles as a savage and a merciless butcher of men, while Horace portrays Achilles ruthlessly slaying women and children.
Other writers, such as Catullus, Propertius, and Ovid, represent a second strand of disparagement, with an emphasis on Achilles' erotic career.
This strand continues in Latin accounts of the Trojan War by writers such as Dictys Cretensis and Dares Phrygius and in Benoît de Sainte-Maure's "Roman de Troie" and Guido delle Colonne's "Historia destructionis Troiae", which remained the most widely read and retold versions of the Matter of Troy until the 17th century.
Achilles was described by the Byzantine chronicler Leo the Deacon, not as Hellene, but as Scythian, while according to the Byzantine author John Malalas, his army was made up of a tribe previously known as Myrmidons and later as Bulgars.
Achilles has been frequently the subject of operas, ballets and related genres.
In films Achilles has been portrayed in the following films and television series:

In 1890, Elisabeth of Bavaria, Empress of Austria, had a summer palace built in Corfu.
The building is named the "Achilleion", after Achilles.
Its paintings and statuary depict scenes from the Trojan War, with particular focus on Achilles.
</doc>
<doc id="307" url="https://en.wikipedia.org/wiki?curid=307" title="Abraham Lincoln">
Abraham Lincoln

Abraham Lincoln (February 12, 1809 – April 15, 1865) was an American lawyer and politician who served as the 16th president of the United States from 1861 until his assassination in April 1865.
Lincoln led the U.S.
through the American Civil War, its bloodiest war and perhaps its greatest moral, constitutional, and political crisis.
In doing so, he preserved the Union, abolished slavery, strengthened the federal government, and modernized the economy.
Born in Hodgenville, Kentucky, Lincoln grew up on the western frontier in Kentucky and Indiana.
Largely self-educated, he became a lawyer in Illinois, a Whig Party leader, and was elected to the Illinois House of Representatives, in which he served for eight years.
Elected to the United States House of Representatives in 1846, Lincoln promoted rapid modernization of the economy and opposed the Mexican–American War.
After a single term, he returned to Illinois and resumed his successful law practice.
Reentering politics in 1854, he became a leader in building the new Republican Party, which had a statewide majority in Illinois.
As part of the 1858 campaign for US Senator from Illinois, Lincoln took part in a series of highly publicized debates with his opponent and rival, Democrat Stephen A. Douglas; Lincoln spoke out against the expansion of slavery, but lost the race to Douglas.
In 1860, Lincoln secured the Republican Party presidential nomination as a moderate from a swing state, though most delegates originally favored other candidates.
Though he gained very little support in the slaveholding states of the South, he swept the North and was elected president in 1860.
Though there were attempts to bridge the differences between North and South, ultimately Lincoln's victory prompted seven southern slave states to secede from the United States and form the Confederate States of America before he moved into the White House.
U.S.
troops refused to leave Fort Sumter, a fort located in Charleston, South Carolina, after the secession of the Southern States.
The resulting Confederate attack on Fort Sumter inspired the North to rally behind the Union.
As the leader of the moderate faction of the Republican Party, Lincoln confronted Radical Republicans, who demanded harsher treatment of the South; War Democrats, who rallied a large faction of former opponents into his camp; anti-war Democrats (called Copperheads), who despised him; and irreconcilable secessionists, who plotted his assassination.
Lincoln fought back by pitting his opponents against each other, by carefully planned political patronage and by appealing to the American people with his powers of oratory.
His Gettysburg Address became an iconic endorsement of nationalism, republicanism, equal rights, liberty, and democracy.
He suspended "habeas corpus", leading to the controversial "Ex parte Merryman" decision, and he averted potential British intervention by defusing the "Trent" Affair.
Lincoln closely supervised the war effort, especially the selection of generals, including his most successful general, Ulysses S. Grant.
He made major decisions on Union war strategy, including a naval blockade that shut down the South's trade.
As the war progressed, his complex moves toward ending slavery included the Emancipation Proclamation of 1863; Lincoln used the U.S.
Army to protect escaped slaves, encouraged the border states to outlaw slavery, and pushed through Congress the Thirteenth Amendment to the United States Constitution, which permanently outlawed slavery.
An astute politician deeply involved with power issues in each state, Lincoln reached out to the War Democrats and managed his own re-election campaign in the 1864 presidential election.
Anticipating the war's conclusion, Lincoln pushed a moderate view of Reconstruction, seeking to reunite the nation speedily through a policy of generous reconciliation in the face of lingering and bitter divisiveness.
On April 14, 1865, five days after the surrender of Confederate general Robert E. Lee, Lincoln was shot by Confederate sympathizer John Wilkes Booth and died the next day.
Lincoln has been consistently ranked both by scholars and the public as among the greatest U.S.
presidents.
Abraham Lincoln was born on February 12, 1809, as the second child of Thomas and Nancy Hanks Lincoln, in a one-room log cabin on the Sinking Spring Farm near Hodgenville, Kentucky.
He was a descendant of Samuel Lincoln, an Englishman who migrated from Hingham, Norfolk, to its namesake of Hingham, Massachusetts, in 1638.
Samuel's grandson and great-grandson began the family's western migration, which passed through New Jersey, Pennsylvania, and Virginia.
Lincoln's paternal grandfather and namesake, Captain Abraham Lincoln, moved the family from Virginia to Jefferson County, Kentucky, in the 1780s.
Captain Lincoln was killed in an Indian raid in 1786.
His children, including eight-year-old Thomas, the future president's father, witnessed the attack.
After his father's murder, Thomas was left to make his own way on the frontier, working at odd jobs in Kentucky and in Tennessee, before settling with members of his family in Hardin County, Kentucky, in the early 1800s.
Lincoln's mother, Nancy, is widely assumed to have been the daughter of Lucy Hanks, although no record of Nancy Hanks' birth has ever been found.
According to William Ensign Lincoln's book "The Ancestry of Abraham Lincoln", Nancy was the daughter of Joseph Hanks; however, the debate continues over whether she was born out of wedlock.
Still another researcher, Adin Baber, claims that Nancy Hanks was the daughter of Abraham Hanks and Sarah Harper of Virginia.
Thomas Lincoln and Nancy Hanks were married on June 12, 1806, in Washington County, and moved to Elizabethtown, Kentucky, following their marriage.
They became the parents of three children: Sarah, born on February 10, 1807; Abraham, on February 12, 1809; and another son, Thomas, who died in infancy.
Thomas Lincoln bought or leased several farms in Kentucky, including the Sinking Spring farm, where Abraham was born; however, a land title dispute soon forced the Lincolns to move.
In 1811, the family moved north, to Knob Creek Farm, where Thomas acquired title to of land.
In 1815 a claimant in another land dispute sought to eject the family from the farm.
Of the that Thomas held in Kentucky, he lost all but of his land in court disputes over property titles.
Frustrated over the lack of security provided by the Kentucky title survey system in the courts, Thomas sold the remaining land he held in Kentucky in 1814, and began planning a move to Indiana, where the land survey process was more reliable and the ability for an individual to retain land titles was more secure.
In 1816, the family moved north across the Ohio River to Indiana, a free, non-slaveholding territory, where they settled in an "unbroken forest" in Hurricane Township, Perry County.
(Their land in southern Indiana became part of Spencer County, Indiana, when the county was established in 1818.)
The farm is preserved as part of the Lincoln Boyhood National Memorial.
In 1860, Lincoln noted that the family's move to Indiana was "partly on account of slavery"; but mainly due to land title difficulties in Kentucky.
During the family's years in Kentucky and Indiana, Thomas Lincoln worked as a farmer, cabinetmaker, and carpenter.
He owned farms, several town lots and livestock, paid taxes, sat on juries, appraised estates, served on country slave patrols, and guarded prisoners.
Thomas and Nancy Lincoln were also members of a Separate Baptists church, which had restrictive moral standards and opposed alcohol, dancing, and slavery.
Within a year of the family's arrival in Indiana, Thomas claimed title to of Indiana land.
Despite some financial challenges he eventually obtained clear title to of land in what became known as the Little Pigeon Creek Community in Spencer County.
Prior to the family's move to Illinois in 1830, Thomas had acquired an additional twenty acres of land adjacent to his property.
Several significant family events took place during Lincoln's youth in Indiana.
On October 5, 1818, Nancy Lincoln died of milk sickness, leaving 11-year-old Sarah in charge of a household that included her father, 9-year-old Abraham, and Dennis Hanks, Nancy's 19-year-old orphaned cousin.
On December 2, 1819, Lincoln's father married Sarah "Sally" Bush Johnston, a widow from Elizabethtown, Kentucky, with three children of her own.
Abraham became very close to his stepmother, whom he referred to as "Mother".
Those who knew Lincoln as a teenager later recalled him being very distraught over his sister Sarah's death on January 20, 1828, while giving birth to a stillborn son.
As a youth, Lincoln disliked the hard labor associated with frontier life.
Some of his neighbors and family members thought for a time that he was lazy for all his "reading, scribbling, writing, ciphering, writing Poetry, etc.
", and must have done it to avoid manual labor.
His stepmother also acknowledged he did not enjoy "physical labor", but loved to read.
Lincoln was largely self-educated.
His formal schooling from several itinerant teachers was intermittent, the aggregate of which may have amounted to less than a year; however, he was an avid reader and retained a lifelong interest in learning.
Family, neighbors, and schoolmates of Lincoln's youth recalled that he read and reread the King James Bible, Aesop's Fables, John Bunyan's "The Pilgrim's Progress", Daniel Defoe's "Robinson Crusoe", Mason Locke Weems's "The Life of Washington", and "The Autobiography of Benjamin Franklin", among others.
As he grew into his teens, Lincoln took responsibility for the chores expected of him as one of the boys in the household.
He also complied with the customary obligation of a son giving his father all earnings from work done outside the home until the age of 21.
Abraham became adept at using an axe.
Tall for his age, Lincoln was also strong and athletic.
He attained a reputation for brawn and audacity after a very competitive wrestling match with the renowned leader of a group of ruffians known as "the Clary's Grove boys".
In early March 1830, partly out of fear of a milk sickness outbreak along the Ohio River, several members of the extended Lincoln family moved west to Illinois, a non-slaveholding state, and settled in Macon County, west of Decatur.
Historians disagree on who initiated the move; Thomas Lincoln had no obvious reason to leave Indiana, and one possibility is that other members of the family, including Dennis Hanks, might not have attained the stability and steady income that Thomas Lincoln had.
After the family relocated to Illinois, Abraham became increasingly distant from his father, in part because of his father's lack of education, but occasionally lent him money.
In 1831, as Thomas and other members of the family prepared to move to a new homestead in Coles County, Illinois, Abraham was old enough to make his own decisions and struck out on his own.
Traveling down the Sangamon River, he ended up in the village of New Salem in Sangamon County.
Later that spring, Denton Offutt, a New Salem merchant, hired Lincoln and some friends to take goods by flatboat from New Salem to New Orleans via the Sangamon, Illinois, and Mississippi rivers.
After arriving in New Orleans—and witnessing slavery firsthand—Lincoln returned to New Salem, where he remained for the next six years.
According to some sources, Lincoln's first romantic interest was Ann Rutledge, whom he met when he first moved to New Salem; these sources indicate that by 1835, they were in a relationship but not formally engaged.
She died at the age of 22 on August 25, 1835, most likely of typhoid fever.
In the early 1830s, he met Mary Owens from Kentucky when she was visiting her sister.
Late in 1836, Lincoln agreed to a match with Mary if she returned to New Salem.
Mary did return in November 1836, and Lincoln courted her for a time; however, they both had second thoughts about their relationship.
On August 16, 1837, Lincoln wrote Mary a letter suggesting he would not blame her if she ended the relationship.
She never replied and the courtship ended.
In 1840, Lincoln became engaged to Mary Todd, who was from a wealthy slave-holding family in Lexington, Kentucky.
They met in Springfield, Illinois in December 1839 and were engaged the following December.
A wedding set for January 1, 1841, was canceled when the two broke off their engagement at Lincoln's initiative.
They later met again at a party and married on November 4, 1842, in the Springfield mansion of Mary's married sister.
While preparing for the nuptials and feeling anxiety again, Lincoln, when asked where he was going, replied, "To hell, I suppose."
In 1844, the couple bought a house in Springfield near Lincoln's law office.
Mary Todd Lincoln kept house, often with the help of a relative or hired servant girl.
He was an affectionate, though often absent, husband and father of four children.
Robert Todd Lincoln was born in 1843 and Edward Baker Lincoln (Eddie) in 1846.
Edward died on February 1, 1850, in Springfield, probably of tuberculosis.
"Willie" Lincoln was born on December 21, 1850, and died of a fever on February 20, 1862.
The Lincolns' fourth son, Thomas "Tad" Lincoln, was born on April 4, 1853, and died of heart failure at the age of 18 on July 16, 1871.
Robert was the only child to live to adulthood and have children.
The Lincolns' last descendant, great-grandson Robert Todd Lincoln Beckwith, died in 1985.
Lincoln "was remarkably fond of children", and the Lincolns were not considered to be strict with their own.
The deaths of their sons had profound effects on both parents.
Abraham Lincoln suffered from "melancholy", a condition which now is referred to as clinical depression.
Later in life, Mary struggled with the stresses of losing her husband and sons, and Robert Lincoln committed her temporarily to a mental health asylum in 1875.
Lincoln's father-in-law and others of the Todd family were either slave owners or slave traders.
Lincoln was close to the Todds, and he and his family occasionally visited the Todd estate in Lexington.
During his term as president of the United States, Mary was known to cook for Lincoln often.
Since she was raised by a wealthy family, her cooking abilities were simple, but satisfied Lincoln's tastes, which included, particularly, imported oysters.
In 1832, at age 23, Lincoln and a partner (Denton Offutt) bought a small general store on credit in New Salem, Illinois.
Although the economy was booming in the region, the business struggled and Lincoln eventually sold his share.
That March he began his political career with his first campaign for the Illinois General Assembly.
He had attained local popularity and could draw crowds as a natural raconteur in New Salem, though he lacked an education, powerful friends, and money, which may be why he lost.
He advocated navigational improvements on the Sangamon River.
Before the election, Lincoln served as a captain in the Illinois Militia during the Black Hawk War.
Following his return, Lincoln continued his campaign for the August 6 election for the Illinois General Assembly.
At , he was tall and "strong enough to intimidate any rival".
At his first speech, when he saw a supporter in the crowd being attacked, Lincoln grabbed the assailant by his "neck and the seat of his trousers" and threw him.
Lincoln finished eighth out of 13 candidates (the top four were elected), though he received 277 of the 300 votes cast in the New Salem precinct.
Lincoln served as New Salem's postmaster and later as county surveyor, all the while reading voraciously.
He then decided to become a lawyer and began teaching himself law by reading Blackstone's "Commentaries on the Laws of England" and other law books.
Of his learning method, Lincoln stated: "I studied with nobody".
His second campaign in 1834 was successful.
He won election to the state legislature; though he ran as a Whig, many Democrats favored him over a more powerful Whig opponent.
Admitted to the Illinois bar in 1836, he moved to Springfield, Illinois, and began to practice law under John T. Stuart, Mary Todd's cousin.
Lincoln became an able and successful lawyer with a reputation as a formidable adversary during cross-examinations and closing arguments.
He partnered with Stephen T. Logan from 1841 until 1844.
Then Lincoln began his practice with William Herndon, whom Lincoln thought "a studious young man".
Successful on his second run for office, Lincoln served four successive terms in the Illinois House of Representatives as a Whig representative from Sangamon County.
He supported the construction of the Illinois and Michigan Canal, which he remained involved with later as a Canal Commissioner.
In the 1835–36 legislative session, he voted to expand suffrage to white males, whether landowners or not.
He was known for his "free soil" stance of opposing both slavery and abolitionism.
He first articulated this in 1837, saying, "[The] Institution of slavery is founded on both injustice and bad policy, but the promulgation of abolition doctrines tends rather to increase than abate its evils."
His stance closely followed Henry Clay in supporting the American Colonization Society program of making the abolition of slavery practical by its advocation and helping the freed slaves to settle in Liberia in Africa.
From the early 1830s, Lincoln was a steadfast Whig and professed to friends in 1861 to be "an old line Whig, a disciple of Henry Clay".
The party, including Lincoln, favored economic modernization in banking, protective tariffs to fund internal improvements including railroads, and espoused urbanization as well.
Lincoln ran for the Whig nomination for Illinois's 7th district of the U.S.
House of Representatives in 1843, but was defeated by John J. Hardin.
However, Lincoln won support for the principle of rotation, whereby Hardin would retire after only one term to allow for the nomination of another candidate.
Lincoln hoped that this arrangement would lead to his nomination in 1846.
Lincoln was indeed elected to the House of Representatives in 1846, where he served one two-year term.
He was the only Whig in the Illinois delegation, but he showed his party loyalty by participating in almost all votes and making speeches that echoed the party line.
Lincoln, in collaboration with abolitionist Congressman Joshua R. Giddings, wrote a bill to abolish slavery in the District of Columbia with compensation for the owners, enforcement to capture fugitive slaves, and a popular vote on the matter.
He abandoned the bill when it failed to garner sufficient Whig supporters.
On foreign and military policy, Lincoln spoke out against the Mexican–American War, which he attributed to President Polk's desire for "military glory—that attractive rainbow, that rises in showers of blood".
Lincoln also supported the Wilmot Proviso, which, if it had been adopted, would have banned slavery in any U.S.
territory won from Mexico.
Lincoln emphasized his opposition to Polk by drafting and introducing his Spot Resolutions.
The war had begun with a Mexican slaughter of American soldiers in territory disputed by Mexico and the U.S.
Polk insisted that Mexican soldiers had "invaded "our territory" and shed the blood of our fellow-citizens on our "own soil"".
Lincoln demanded that Polk show Congress the exact spot on which blood had been shed and prove that the spot was on American soil.
Congress never enacted the resolution or even debated it, the national papers ignored it, and it resulted in a loss of political support for Lincoln in his district.
One Illinois newspaper derisively nicknamed him "spotty Lincoln".
Lincoln later regretted some of his statements, especially his attack on the presidential war-making powers.
Realizing Clay was unlikely to win the presidency, Lincoln, who had pledged in 1846 to serve only one term in the House, supported General Zachary Taylor for the Whig nomination in the 1848 presidential election.
Taylor won and Lincoln hoped to be appointed Commissioner of the General Land Office, but that lucrative patronage job went to an Illinois rival, Justin Butterfield, considered by the administration to be a highly skilled lawyer, but in Lincoln's view, an "old fossil".
The administration offered him the consolation prize of secretary or governor of the Oregon Territory.
This distant territory was a Democratic stronghold, and acceptance of the post would have effectively ended his legal and political career in Illinois, so he declined and resumed his law practice.
Lincoln returned to practicing law in Springfield, handling "every kind of business that could come before a prairie lawyer".
Twice a year for 16 years, 10 weeks at a time, he appeared in county seats in the midstate region when the county courts were in session.
Lincoln handled many transportation cases in the midst of the nation's western expansion, particularly the conflicts arising from the operation of river barges under the many new railroad bridges.
As a riverboat man, Lincoln initially favored those interests, but ultimately represented whoever hired him.
In fact, he later represented a bridge company against a riverboat company in a landmark case involving a canal boat that sank after hitting a bridge.
In 1849, he received a patent for a flotation device for the movement of boats in shallow water.
The idea was never commercialized, but Lincoln is the only president to hold a patent.
In 1851, he represented the Alton & Sangamon Railroad in a dispute with one of its shareholders, James A. Barret, who had refused to pay the balance on his pledge to buy shares in the railroad on the grounds that the company had changed its original train route.
Lincoln successfully argued that the railroad company was not bound by its original charter extant at the time of Barret's pledge; the charter was amended in the public interest to provide a newer, superior, and less expensive route, and the corporation retained the right to demand Barret's payment.
The decision by the Illinois Supreme Court has been cited by numerous other courts in the nation.
Lincoln appeared before the Illinois Supreme Court in 175 cases, in 51 as sole counsel, of which 31 were decided in his favor.
From 1853 to 1860, another of Lincoln's largest clients was the Illinois Central Railroad.
Lincoln's reputation with clients gave rise to his nickname "Honest Abe".
Lincoln's most notable criminal trial occurred in 1858 when he defended William "Duff" Armstrong, who was on trial for the murder of James Preston Metzker.
The case is famous for Lincoln's use of a fact established by judicial notice in order to challenge the credibility of an eyewitness.
After an opposing witness testified seeing the crime in the moonlight, Lincoln produced a "Farmers' Almanac" showing the moon was at a low angle, drastically reducing visibility.
Based on this evidence, Armstrong was acquitted.
Lincoln rarely raised objections in the courtroom; but in an 1859 case, where he defended a cousin, Peachy Harrison, who was accused of stabbing another to death, Lincoln angrily protested the judge's decision to exclude evidence favorable to his client.
Instead of holding Lincoln in contempt of court as was expected, the judge, a Democrat, reversed his ruling, allowing the evidence and acquitting Harrison.
The debate over the status of slavery in the territories exacerbated sectional tensions between the slave-holding South and the North, and the Compromise of 1850 failed to defuse the issue.
In the early 1850s, Lincoln supported efforts for sectional mediation, and his 1852 eulogy for Henry Clay focused on the latter's support for gradual emancipation and opposition to "both extremes" on the slavery issue.
As the 1850s progressed, the debate over slavery in the Nebraska Territory and Kansas Territory became particularly acrimonious, and Senator Stephen A. Douglas of Illinois proposed popular sovereignty as a compromise measure; the proposal would take the issue of slavery out of the hands of Congress by allowing the electorate of each territory to decide the status of slavery themselves.
The proposal alarmed many Northerners, who hoped to stop the spread of slavery into the territories.
Despite this Northern opposition, Douglas's Kansas–Nebraska Act narrowly passed Congress in May 1854.
For months after its passage, Lincoln did not publicly comment on the Kansas–Nebraska Act, but he came to strongly oppose it.
On October 16, 1854, in his "Peoria Speech", Lincoln declared his opposition to slavery, which he repeated en route to the presidency.
Speaking in his Kentucky accent, with a very powerful voice, he said the Kansas Act had a ""declared" indifference, but as I must think, a covert "real" zeal for the spread of slavery.
I cannot but hate it.
I hate it because of the monstrous injustice of slavery itself.
I hate it because it deprives our republican example of its just influence in the world ..." Lincoln's attacks on the Kansas–Nebraska Act marked his return to political life.
Nationally, the Whigs were irreparably split by the Kansas–Nebraska Act and other efforts to compromise on the slavery issue.
Reflecting the demise of his party, Lincoln would write in 1855, "I think I am a Whig, but others say there are no Whigs, and that I am an abolitionist [...] I do no more than oppose the "extension" of slavery."
Drawing on the antislavery portion of the Whig Party, and combining Free Soil, Liberty, and antislavery Democratic Party members, the new Republican Party formed as a northern party dedicated to antislavery.
Lincoln resisted early attempts to recruit him to the new party, fearing that it would serve as a platform for extreme abolitionists.
Lincoln also still hoped to rejuvenate the ailing Whig Party, though he bemoaned his party's growing closeness with the nativist Know Nothing movement.
In the 1854 elections, Lincoln was elected to the Illinois legislature but declined to take his seat.
In the aftermath of the elections, which showed the power and popularity of the movement opposed to the Kansas–Nebraska Act, Lincoln instead sought election to the United States Senate.
At that time, senators were elected by the state legislature.
After leading in the first six rounds of voting, but unable to obtain a majority, Lincoln instructed his backers to vote for Lyman Trumbull.
Trumbull was an antislavery Democrat, and had received few votes in the earlier ballots; his supporters, also antislavery Democrats, had vowed not to support any Whig.
Lincoln's decision to withdraw enabled his Whig supporters and Trumbull's antislavery Democrats to combine and defeat the mainstream Democratic candidate, Joel Aldrich Matteson.
In part due to the ongoing violent political confrontations in the Kansas, opposition to the Kansas–Nebraska Act remained strong in Illinois and throughout the North.
As the 1856 elections approached, Lincoln abandoned the defunct Whig Party in favor of the Republicans.
He attended the May 1856 Bloomington Convention, which formally established the Illinois Republican Party.
The convention platform asserted that Congress had the right to regulate slavery in the territories and called for the immediate admission of Kansas as a free state.
Lincoln gave the final speech of the convention, in which he endorsed the party platform and called for the preservation of the Union.
At the June 1856 Republican National Convention, Lincoln received significant support on the vice presidential ballot, though the party nominated a ticket of John C. Frémont and William Dayton.
Lincoln strongly supported the Republican ticket, campaigning for the party throughout Illinois.
The Democrats nominated former Ambassador James Buchanan, who had been out of the country since 1853 and thus had avoided the debate over slavery in the territories, while the Know Nothings nominated former Whig President Millard Fillmore.
In the 1856 elections, Buchanan defeated both his challengers, but Frémont won several Northern states and Republican William Henry Bissell won election as Governor of Illinois.
Though Lincoln did not himself win office, his vigorous campaigning had made him the leading Republican in Illinois.
Eric Foner (2010) contrasts the abolitionists and anti-slavery Radical Republicans of the Northeast who saw slavery as a sin, with the conservative Republicans who thought it was bad because it hurt white people and blocked progress.
Foner argues that Lincoln was a moderate in the middle, opposing slavery primarily because it violated the republicanism principles of the Founding Fathers, especially the equality of all men and democratic self-government as expressed in the Declaration of Independence.
In March 1857, the Supreme Court issued its decision in "Dred Scott v. Sandford".
The opinion by Chief Justice Roger B. Taney held that blacks were not citizens and derived no rights from the Constitution.
While many Democrats hoped that "Dred Scott" would end the dispute over slavery in the territories, the decision sparked further outrage in the North.
Lincoln denounced the decision, alleging it was the product of a conspiracy of Democrats to support the Slave Power.
Lincoln argued, "The authors of the Declaration of Independence never intended 'to say all were equal in color, size, intellect, moral developments, or social capacity', but they 'did consider all men created equal—equal in certain inalienable rights, among which are life, liberty, and the pursuit of happiness'."
Douglas was up for re-election in 1858, and Lincoln hoped to defeat the powerful Illinois Democrat.
With the former Democrat Trumbull now serving as a Republican Senator, many in the party felt that a former Whig should be nominated in 1858, and Lincoln's 1856 campaigning and willingness to support Trumbull in 1854 had earned him favor in the party.
Some eastern Republicans favored the reelection of Douglas for the Senate in 1858, since he had led the opposition to the Lecompton Constitution, which would have admitted Kansas as a slave state.
But many Illinois Republicans resented this eastern interference.
For the first time, Illinois Republicans held a convention to agree upon a Senate candidate, and Lincoln won the party's Senate nomination with little opposition.
Accepting the nomination, Lincoln delivered his House Divided Speech, drawing on Mark 3:25, "A house divided against itself cannot stand.
I believe this government cannot endure permanently half slave and half free.
I do not expect the Union to be dissolved—I do not expect the house to fall—but I do expect it will cease to be divided.
It will become all one thing, or all the other."
The speech created an evocative image of the danger of disunion caused by the slavery debate, and rallied Republicans across the North.
The stage was then set for the campaign for statewide election of the Illinois legislature which would, in turn, select Lincoln or Douglas as its U.S.
senator.
On being informed of Lincoln's nomination, Douglas stated, "[Lincoln] is the strong man of the party ... and if I beat him, my victory will be hardly won."
The Senate campaign featured the seven Lincoln–Douglas debates of 1858, the most famous political debates in American history.
The principals stood in stark contrast both physically and politically.
Lincoln warned that "The Slave Power" was threatening the values of republicanism, and accused Douglas of distorting the values of the Founding Fathers that all men are created equal, while Douglas emphasized his Freeport Doctrine, that local settlers were free to choose whether to allow slavery or not, and accused Lincoln of having joined the abolitionists.
The debates had an atmosphere of a prize fight and drew crowds in the thousands.
Lincoln stated Douglas' popular sovereignty theory was a threat to the nation's morality and that Douglas represented a conspiracy to extend slavery to free states.
Douglas said that Lincoln was defying the authority of the U.S.
Supreme Court and the "Dred Scott" decision.
Though the Republican legislative candidates won more popular votes, the Democrats won more seats, and the legislature re-elected Douglas to the Senate.
Despite the bitterness of the defeat for Lincoln, his articulation of the issues gave him a national political reputation.
In May 1859, Lincoln purchased the "Illinois Staats-Anzeiger", a German-language newspaper which was consistently supportive; most of the state's 130,000 German Americans voted Democratic but there was Republican support that a German-language paper could mobilize.
In the aftermath of the 1858 election, newspapers frequently mentioned Lincoln as a potential Republican presidential candidate in 1860, with William H. Seward, Salmon P. Chase, Edward Bates, and Simon Cameron looming as rivals for the nomination.
While Lincoln was popular in the Midwest, he lacked support in the Northeast, and was unsure as to whether he should seek the presidency.
In January 1860, Lincoln told a group of political allies that he would accept the 1860 presidential nomination if offered, and in the following months several local papers endorsed Lincoln for president.
On February 27, 1860, New York party leaders invited Lincoln to give a speech at Cooper Union to a group of powerful Republicans.
Lincoln argued that the Founding Fathers had little use for popular sovereignty and had repeatedly sought to restrict slavery.
Lincoln insisted the moral foundation of the Republicans required opposition to slavery, and rejected any "groping for some middle ground between the right and the wrong".
Despite his inelegant appearance—many in the audience thought him awkward and even ugly—Lincoln demonstrated an intellectual leadership that brought him into the front ranks of the party and into contention for the Republican presidential nomination.
Journalist Noah Brooks reported, "No man ever before made such an impression on his first appeal to a New York audience."
Historian Donald described the speech as a "superb political move for an unannounced candidate, to appear in one rival's (Seward) own state at an event sponsored by the second rival's (Chase) loyalists, while not mentioning either by name during its delivery".
In response to an inquiry about his presidential intentions, Lincoln said, "The taste "is" in my mouth a little."
On May 9–10, 1860, the Illinois Republican State Convention was held in Decatur.
Lincoln's followers organized a campaign team led by David Davis, Norman Judd, Leonard Swett, and Jesse DuBois, and Lincoln received his first endorsement to run for the presidency.
Exploiting the embellished legend of his frontier days with his father (clearing the land and splitting fence rails with an ax), Lincoln's supporters adopted the label of "The Rail Candidate".
In 1860 Lincoln described himself: "I am in height, six feet, four inches, nearly; lean in flesh, weighing, on an average, one hundred and eighty pounds; dark complexion, with coarse black hair, and gray eyes."
On May 18, at the Republican National Convention in Chicago, Lincoln became the Republican candidate on the third ballot, beating candidates such as Seward and Chase.
A former Democrat, Hannibal Hamlin of Maine, was nominated for Vice President to balance the ticket.
Lincoln's success depended on his campaign team, his reputation as a moderate on the slavery issue, and his strong support for Whiggish programs of internal improvements and the protective tariff.
On the third ballot Pennsylvania put him over the top.
Pennsylvania iron interests were reassured by his support for protective tariffs.
Lincoln's managers had been adroitly focused on this delegation as well as the others, while following Lincoln's strong dictate to "Make no contracts that bind me".
Most Republicans agreed with Lincoln that the North was the aggrieved party, as the Slave Power tightened its grasp on the national government with the "Dred Scott" decision and the presidency of James Buchanan.
Throughout the 1850s, Lincoln doubted the prospects of civil war, and his supporters rejected claims that his election would incite secession.
Meanwhile, Douglas was selected as the candidate of the Northern Democrats.
Delegates from 11 slave states walked out of the Democratic convention, disagreeing with Douglas' position on popular sovereignty, and ultimately selected incumbent Vice President John C. Breckinridge as their candidate.
A group of former Whigs and Know Nothings formed the Constitutional Union Party and nominated John Bell of Tennessee.
Lincoln and Douglas would compete for votes in the North,
while Bell and Breckinridge primarily found support in the South.
Lincoln had a highly effective campaign team who carefully projected his image as an ideal candidate.
As Michael Martinez says:

Lincoln and his political advisers manipulated his image and background...Sometimes he appeared as a straight-shooting, plain-talking, common-sense-wielding man of the people.
His image as the "Rail Splitter" dates from this era.
His supporters also portrayed him as "Honest Abe," the country fellow who was simply dressed and not especially polished or formal in his manner but who was as honest and trustworthy as his legs were long.
Even Lincoln's tall, gangly frame was used to good advantage during the campaign as many drawings and posters show the candidates sprinting past his vertically challenged rivals.
At other times, Lincoln appeared as a sophisticated, thoughtful, articulate, "presidential" candidate.
Prior to the Republican convention, the Lincoln campaign began cultivating a nationwide youth organization, the Wide Awakes, which it used to generate popular support for Lincoln throughout the country to spearhead large voter registration drives, knowing that new voters and young voters tend to embrace new and young parties.
As Lincoln's ideas of abolishing slavery grew, so did his supporters.
People of the Northern states knew the Southern states would vote against Lincoln because of his ideas of anti-slavery and took action to rally supporters for Lincoln.
As Douglas and the other candidates went through with their campaigns, Lincoln was the only one of them who gave no speeches.
Instead, he monitored the campaign closely and relied on the enthusiasm of the Republican Party.
The party did the leg work that produced majorities across the North, and produced an abundance of campaign posters, leaflets, and newspaper editorials.
There were thousands of Republican speakers who focused first on the party platform, and second on Lincoln's life story, emphasizing his childhood poverty.
The goal was to demonstrate the superior power of "free labor", whereby a common farm boy could work his way to the top by his own efforts.
The Republican Party's production of campaign literature dwarfed the combined opposition; a "Chicago Tribune" writer produced a pamphlet that detailed Lincoln's life, and sold 100,000 to 200,000 copies.
On November 6, 1860, Lincoln was elected the 16th president of the United States, beating Douglas, Breckinridge, and Bell.
He was the first president from the Republican Party.
His victory was entirely due to the strength of his support in the North and West; no ballots were cast for him in 10 of the 15 Southern slave states, and he won only two of 996 counties in all the Southern states.
Lincoln received 1,866,452 votes, Douglas 1,376,957 votes, Breckinridge 849,781 votes, and Bell 588,789 votes.
Turnout was 82.2 percent, with Lincoln winning the free Northern states, as well as California and Oregon.
Douglas won Missouri, and split New Jersey with Lincoln.
Bell won Virginia, Tennessee, and Kentucky, and Breckinridge won the rest of the South.
Although Lincoln won only a plurality of the popular vote, his victory in the electoral college was decisive: Lincoln had 180 and his opponents added together had only 123.
There were fusion tickets in which all of Lincoln's opponents combined to support the same slate of Electors in New York, New Jersey, and Rhode Island, but even if the anti-Lincoln vote had been combined in every state, Lincoln still would have won a majority in the Electoral College.
As Lincoln's election became evident, secessionists made clear their intent to leave the Union before he took office the next March.
On December 20, 1860, South Carolina took the lead by adopting an ordinance of secession; by February 1, 1861, Florida, Mississippi, Alabama, Georgia, Louisiana, and Texas followed.
Six of these states then adopted a constitution and declared themselves to be a sovereign nation, the Confederate States of America.
The upper South and border states (Delaware, Maryland, Virginia, North Carolina, Tennessee, Kentucky, Missouri, and Arkansas) listened to, but initially rejected, the secessionist appeal.
President Buchanan and President-elect Lincoln refused to recognize the Confederacy, declaring secession illegal.
The Confederacy selected Jefferson Davis as its provisional President on February 9, 1861.
There were attempts at compromise.
The Crittenden Compromise would have extended the Missouri Compromise line of 1820, dividing the territories into slave and free, contrary to the Republican Party's free-soil platform.
Lincoln rejected the idea, saying, "I will suffer death before I consent ... to any concession or compromise which looks like buying the privilege to take possession of this government to which we have a constitutional right."
Lincoln, however, did tacitly support the proposed Corwin Amendment to the Constitution, which passed Congress before Lincoln came into office and was then awaiting ratification by the states.
That proposed amendment would have protected slavery in states where it already existed and would have guaranteed that Congress would not interfere with slavery without Southern consent.
A few weeks before the war, Lincoln sent a letter to every governor informing them Congress had passed a joint resolution to amend the Constitution.
Lincoln was open to the possibility of a constitutional convention to make further amendments to the Constitution.
En route to his inauguration by train, Lincoln addressed crowds and legislatures across the North.
The president-elect then evaded possible assassins in Baltimore, who were uncovered by Lincoln's head of security, Allan Pinkerton.
On February 23, 1861, he arrived in disguise in Washington, D.C., which was placed under substantial military guard.
Lincoln directed his inaugural address to the South, proclaiming once again that he had no intention, or inclination, to abolish slavery in the Southern states:

The President ended his address with an appeal to the people of the South: "We are not enemies, but friends.
We must not be enemies ... The mystic chords of memory, stretching from every battlefield, and patriot grave, to every living heart and hearthstone, all over this broad land, will yet swell the chorus of the Union, when again touched, as surely they will be, by the better angels of our nature."
The failure of the Peace Conference of 1861 signaled that legislative compromise was impossible.
By March 1861, no leaders of the insurrection had proposed rejoining the Union on any terms.
Meanwhile, Lincoln and the Republican leadership agreed that the dismantling of the Union could not be tolerated.
Lincoln said as the war was ending:

The commander of Fort Sumter, South Carolina, Major Robert Anderson, sent a request for provisions to Washington, and the execution of Lincoln's order to meet that request was seen by the secessionists as an act of war.
On April 12, 1861, Confederate forces fired on Union troops at Fort Sumter, forcing them to surrender, beginning the war.
Historian Allan Nevins argued that the newly inaugurated Lincoln made three miscalculations: underestimating the gravity of the crisis, exaggerating the strength of Unionist sentiment in the South, and not realizing the Southern Unionists were insisting there be no invasion.
William Tecumseh Sherman talked to Lincoln during inauguration week and was "sadly disappointed" at his failure to realize that "the country was sleeping on a volcano" and that the South was preparing for war.
Historian David Herbert Donald concludes that, "His repeated efforts to avoid collision in the months between inauguration and the firing on Ft.
Sumter showed he adhered to his vow not to be the first to shed fraternal blood.
But he also vowed not to surrender the forts.
The only resolution of these contradictory positions was for the confederates to fire the first shot; they did just that."
On April 15, Lincoln called on all the states to send detachments totaling 75,000 troops to recapture forts, protect Washington, and "preserve the Union", which, in his view, still existed intact despite the actions of the seceding states.
This call forced the states to choose sides.
Virginia declared its secession and was rewarded with the Confederate capital, despite the exposed position of Richmond so close to Union lines.
North Carolina, Tennessee, and Arkansas also voted for secession over the next two months.
Secession sentiment was strong in Missouri and Maryland, but did not prevail; Kentucky tried to be neutral.
The Confederate attack on Fort Sumter rallied Americans north of the Mason-Dixon line to the defense of the American nation.
Historian Allan Nevins says:
States sent Union regiments south in response to Lincoln's call to save the capital and confront the rebellion.
On April 19, mobs in Baltimore, which controlled the rail links, attacked Union troops who were changing trains, and local leaders' groups later burned critical rail bridges to the capital.
The Army responded by arresting local Maryland officials.
Lincoln suspended the writ of "habeas corpus" in areas the army felt it needed to secure for troops to reach Washington.
John Merryman, a Maryland official involved in hindering the U.S.
troop movements, petitioned Supreme Court Chief Justice and Marylander, Roger B. Taney, author of the controversial pro-slavery "Dred Scott" opinion, to issue a writ of "habeas corpus", and in June Taney, acting as a circuit judge and not speaking for the Supreme Court, issued the writ, because in his opinion only Congress could suspend the writ.
Lincoln continued the army policy that the writ was suspended in limited areas despite the Ex parte Merryman ruling.
Before the Civil War, the only money issued by the United States was gold and silver coins, and only such coins ("specie") were legal tender; that is, payment in that form had to be accepted.
Paper currency in the form of banknotes was issued by privately owned banks.
Greenbacks were paper currency (printed in green on the back) issued by the United States during the American Civil War, they were in two forms:

Demand Notes, issued In July 1861, Congress authorized $50,000,000 in Demand Notes.
They bore no interest, but could be redeemed for specie "on demand".
They were not legal tender (before March 1862), but like Treasury Notes could be used to pay customs duties.
Unlike private and state banknotes, Demand Notes were printed on both sides.
The reverse side was printed in green ink, and so the Demand Notes were dubbed "greenbacks".
Initially they were discounted relative to gold, but being fully redeemable in gold were soon at par.
In December 1861, the government had to suspend redemption, and they declined.
Chase authorized paying interest on Demand Notes, which sustained their value.
Importers therefore continued to use Demand Notes in place of gold.
In March 1862, Demand Notes were made legal tender.
As Demand Notes were used to pay duties, they were taken out of circulation.
By mid-1863, about 95% of them were gone.
The other form of greenbacks where the United States Notes issued in 1862–1865.
They were legal tender by law, but were not backed by gold or silver, only the credibility of the U.S.
government.
Could "not" be used to pay customs duties or interest on the public debt.
After the Battle of Fort Sumter, Lincoln realized the importance of taking immediate executive control of the war and forming an overall Union military strategy to put down the rebellion.
Lincoln encountered an unprecedented political and military crisis, and he responded as commander-in-chief, using unprecedented powers.
He expanded his war powers, and imposed a blockade on all the Confederate shipping ports, disbursed funds before appropriation by Congress, and after suspending "habeas corpus", arrested and imprisoned thousands of suspected Confederate sympathizers.
Lincoln was supported by Congress and the northern public for these actions.
In addition, Lincoln had to contend with reinforcing strong Union sympathies in the border slave states and keeping the war from becoming an international conflict.
The war effort was the source of continued disparagement of Lincoln, and dominated his time and attention.
From the start, it was clear that bipartisan support would be essential to success in the war effort, and any manner of compromise alienated factions on both sides of the aisle, such as the appointment of Republicans and Democrats to command positions in the Union Army.
Copperheads criticized Lincoln for refusing to compromise on the slavery issue.
Conversely, the Radical Republicans criticized him for moving too slowly in abolishing slavery.
On August 6, 1861, Lincoln signed the Confiscation Act that authorized judiciary proceedings to confiscate and free slaves who were used to support the Confederate war effort.
In practice, the law had little effect, but it did signal political support for abolishing slavery in the Confederacy.
In late August 1861, General John C. Frémont, the 1856 Republican presidential nominee, issued, without consulting his superiors in Washington, a proclamation of martial law in Missouri.
He declared that any citizen found bearing arms could be court-martialed and shot, and that slaves of persons aiding the rebellion would be freed.
Frémont was already under a cloud with charges of negligence in his command of the Department of the West compounded with allegations of fraud and corruption.
Lincoln overruled Frémont's proclamation.
Lincoln believed that Fremont's emancipation was political, neither militarily necessary nor legal.
After Lincoln acted, Union enlistments from Maryland, Kentucky, and Missouri increased by over 40,000 troops.
In foreign policy, Lincoln's main goal was to stop military aid from countries abroad to the Confederacy.
Lincoln left most diplomatic matters to his Secretary of State, William Seward.
At times Seward was too bellicose, so for balance Lincoln maintained a close working relationship with Senator Charles Sumner, the chairman of the Senate Foreign Relations Committee.
The Trent Affair of late 1861 threatened war with Great Britain.
The U.S.
Navy had illegally intercepted a British mail ship, the "Trent", on the high seas and seized two Confederate envoys; Britain protested vehemently while the U.S.
cheered.
Lincoln ended the crisis by releasing the two diplomats.
Biographer James G. Randall has dissected Lincoln's successful techniques:
Lincoln painstakingly monitored the telegraphic reports coming into the War Department headquarters.
He kept close tabs on all phases of the military effort, consulted with governors, and selected generals based on their past success (as well as their state and party).
In January 1862, after many complaints of inefficiency and profiteering in the War Department, Lincoln replaced Simon Cameron with Edwin Stanton as War Secretary.
Stanton centralized the War Department's activities, auditing and cancelling contracts, saving the federal government $17,000,000.
Stanton was a staunchly Unionist pro-business conservative Democrat who moved toward the Radical Republican faction.
Nevertheless, he worked more often and more closely with Lincoln than any other senior official.
"Stanton and Lincoln virtually conducted the war together," say Thomas and Hyman.
In terms of war strategy, Lincoln articulated two priorities: to ensure that Washington was well-defended, and to conduct an aggressive war effort that would satisfy the demand in the North for prompt, decisive victory; major Northern newspaper editors expected victory within 90 days.
Twice a week, Lincoln would meet with his cabinet in the afternoon, and occasionally Mary Lincoln would force him to take a carriage ride because she was concerned he was working too hard.
Lincoln learned from reading the theoretical book of his chief of staff General Henry Halleck, a disciple of the European strategist Jomini; he began to appreciate the critical need to control strategic points, such as the Mississippi River.
Lincoln saw the importance of Vicksburg and understood the necessity of defeating the enemy's army, rather than simply capturing territory.
After the Union rout at Bull Run, the first major battle of the Civil War, and the retirement of the aged Winfield Scott in late 1861, Lincoln appointed Major General George B. McClellan general-in-chief of all the Union armies.
McClellan, a young West Point graduate, railroad executive, and Pennsylvania Democrat, took several months to plan and attempt his Peninsula Campaign, longer than Lincoln wanted.
The campaign's objective was to capture Richmond by moving the Army of the Potomac by boat to the peninsula and then overland to the Confederate capital.
McClellan's repeated delays frustrated Lincoln and Congress, as did his position that no troops were needed to defend Washington.
Lincoln insisted on holding some of McClellan's troops in defense of the capital; McClellan, who consistently overestimated the strength of Confederate troops, blamed this decision for the ultimate failure of the Peninsula Campaign.
Lincoln removed McClellan as general-in-chief in March 1862, after McClellan's "Harrison's Landing Letter", in which he offered unsolicited political advice to Lincoln urging caution in the war effort.
The office remained empty until July, when Henry Halleck was selected for it.
McClellan's letter incensed Radical Republicans, who successfully pressured Lincoln to appoint John Pope, a Republican, as head of the new Army of Virginia.
Pope complied with Lincoln's strategic desire to move toward Richmond from the north, thus protecting the capital from attack.
However, lacking requested reinforcements from McClellan, now commanding the Army of the Potomac, Pope was soundly defeated at the Second Battle of Bull Run in the summer of 1862, forcing the Army of the Potomac to defend Washington for a second time.
The war also expanded with naval operations in 1862 when the CSS "Virginia", formerly the USS "Merrimack", damaged or destroyed three Union vessels in Norfolk, Virginia, before being engaged and damaged by the USS "Monitor".
Lincoln closely reviewed the dispatches and interrogated naval officers during their clash in the Battle of Hampton Roads.
Despite his dissatisfaction with McClellan's failure to reinforce Pope, Lincoln was desperate, and restored him to command of all forces around Washington, to the dismay of all in his cabinet but Seward.
Two days after McClellan's return to command, General Robert E. Lee's forces crossed the Potomac River into Maryland, leading to the Battle of Antietam in September 1862.
The ensuing Union victory was among the bloodiest in American history, but it enabled Lincoln to announce that he would issue an Emancipation Proclamation in January.
Having composed the Proclamation some time earlier, Lincoln had waited for a military victory to publish it to avoid it being perceived as the product of desperation.
McClellan then resisted the President's demand that he pursue Lee's retreating and exposed army, while his counterpart General Don Carlos Buell likewise refused orders to move the Army of the Ohio against rebel forces in eastern Tennessee.
As a result, Lincoln replaced Buell with William Rosecrans; and, after the 1862 midterm elections, he replaced McClellan with Republican Ambrose Burnside.
Both of these replacements were political moderates and prospectively more supportive of the Commander-in-Chief.
Burnside, against the advice of the president, prematurely launched an offensive across the Rappahannock River and was stunningly defeated by Lee at Fredericksburg in December.
Not only had Burnside been defeated on the battlefield, but his soldiers were disgruntled and undisciplined.
Desertions during 1863 were in the thousands and they increased after Fredericksburg.
Lincoln brought in Joseph Hooker, despite his record of loose talk about the need for a military dictatorship.
The mid-term elections in 1862 brought the Republicans severe losses due to sharp disfavor with the administration over its failure to deliver a speedy end to the war, as well as rising inflation, new high taxes, rumors of corruption, the suspension of "habeas corpus", the military draft law, and fears that freed slaves would undermine the labor market.
The Emancipation Proclamation announced in September gained votes for the Republicans in the rural areas of New England and the upper Midwest, but it lost votes in the cities and the lower Midwest.
While Republicans were discouraged, Democrats were energized and did especially well in Pennsylvania, Ohio, Indiana, and New York.
The Republicans did maintain their majorities in Congress and in the major states, except New York.
The Cincinnati "Gazette" contended that the voters were "depressed by the interminable nature of this war, as so far conducted, and by the rapid exhaustion of the national resources without progress".
In the spring of 1863, Lincoln was optimistic about upcoming military campaigns to the point of thinking the end of the war could be near if a string of victories could be put together; these plans included attacks by Hooker on Lee north of Richmond, Rosecrans on Chattanooga, Grant on Vicksburg, and a naval assault on Charleston.
Hooker was routed by Lee at the Battle of Chancellorsville in May, but continued to command his troops for some weeks.
He ignored Lincoln's order to divide his troops, and possibly force Lee to do the same in Harper's Ferry, and tendered his resignation, which Lincoln accepted.
He was replaced by George Meade, who followed Lee into Pennsylvania for the Gettysburg Campaign, which was a victory for the Union, though Lee's army avoided capture.
At the same time, after initial setbacks, Grant laid siege to Vicksburg and the Union navy attained some success in Charleston harbor.
After the Battle of Gettysburg, Lincoln clearly understood that his military decisions would be more effectively carried out by conveying his orders through his War Secretary or his general-in-chief on to his generals, who resented his civilian interference with their own plans.
Even so, he often continued to give detailed directions to his generals as Commander-in-Chief.
Lincoln understood that the Federal government's power to end slavery was limited by the Constitution, which before 1865, committed the issue to individual states.
He argued before and during his election that the eventual extinction of slavery would result from preventing its expansion into new U.S.
territory.
At the beginning of the war, he also sought to persuade the states to accept compensated emancipation in return for their prohibition of slavery.
Lincoln believed that curtailing slavery in these ways would economically expunge it, as envisioned by the Founding Fathers, under the constitution.
President Lincoln rejected two geographically limited emancipation attempts by Major General John C. Frémont in August 1861 and by Major General David Hunter in May 1862, on the grounds that it was not within their power, and it would upset the border states loyal to the Union.
On June 19, 1862, endorsed by Lincoln, Congress passed an act banning slavery on all federal territory.
In July, the Confiscation Act of 1862 was passed, which set up court procedures that could free the slaves of anyone convicted of aiding the rebellion.
Although Lincoln believed it was not within Congress's power to free the slaves within the states, he approved the bill in deference to the legislature.
He felt such action could only be taken by the Commander-in-Chief using war powers granted to the president by the Constitution, and Lincoln was planning to take that action.
In that month, Lincoln discussed a draft of the Emancipation Proclamation with his cabinet.
In it, he stated that "as a fit and necessary military measure, on January 1, 1863, all persons held as slaves in the Confederate states will thenceforward, and forever, be free".
Privately, Lincoln concluded at this point that the slave base of the Confederacy had to be eliminated.
However, Copperheads argued that emancipation was a stumbling block to peace and reunification.
Republican editor Horace Greeley of the highly influential "New York Tribune" fell for the ploy, and Lincoln refuted it directly in a shrewd letter of August 22, 1862.
Although he said he personally wished all men could be free, Lincoln stated that the primary goal of his actions as the U.S.
president (he used the first person pronoun and explicitly refers to his "official duty") was that of preserving the Union:

The Emancipation Proclamation, issued on September 22, 1862, and put into effect on January 1, 1863, declared free the slaves in 10 states not then under Union control, with exemptions specified for areas already under Union control in two states.
Lincoln spent the next 100 days preparing the army and the nation for emancipation, while Democrats rallied their voters in the 1862 off-year elections by warning of the threat freed slaves posed to northern whites.
Once the abolition of slavery in the rebel states became a military objective, as Union armies advanced south, more slaves were liberated until all three million of them in Confederate territory were freed.
Lincoln's comment on the signing of the Proclamation was: "I never, in my life, felt more certain that I was doing right, than I do in signing this paper."
For some time, Lincoln continued earlier plans to set up colonies for the newly freed slaves.
He commented favorably on colonization in the Emancipation Proclamation, but all attempts at such a massive undertaking failed.
A few days after Emancipation was announced, 13 Republican governors met at the War Governors' Conference; they supported the president's Proclamation, but suggested the removal of General George B. McClellan as commander of the Union Army.
Enlisting former slaves in the military was official government policy after the issuance of the Emancipation Proclamation.
By the spring of 1863, Lincoln was ready to recruit black troops in more than token numbers.
In a letter to Andrew Johnson, the military governor of Tennessee, encouraging him to lead the way in raising black troops, Lincoln wrote, "The bare sight of 50,000 armed and drilled black soldiers on the banks of the Mississippi would end the rebellion at once".
By the end of 1863, at Lincoln's direction, General Lorenzo Thomas had recruited 20 regiments of blacks from the Mississippi Valley.
Frederick Douglass once observed of Lincoln: "In his company, I was never reminded of my humble origin, or of my unpopular color".
With the great Union victory at the Battle of Gettysburg in July 1863, and the defeat of the Copperheads in the Ohio election in the fall, Lincoln maintained a strong base of party support and was in a strong position to redefine the war effort, despite the New York City draft riots.
The stage was set for his address at the Gettysburg battlefield cemetery on November 19, 1863.
Defying Lincoln's prediction that "the world will little note, nor long remember what we say here", the Address became the most quoted speech in American history.
In 272 words, and three minutes, Lincoln asserted the nation was born not in 1789, but in 1776, "conceived in Liberty, and dedicated to the proposition that all men are created equal".
He defined the war as an effort dedicated to these principles of liberty and equality for all.
The emancipation of slaves was now part of the national war effort.
He declared that the deaths of so many brave soldiers would not be in vain, that slavery would end as a result of the losses, and the future of democracy in the world would be assured, that "government of the people, by the people, for the people, shall not perish from the earth".
Lincoln concluded that the Civil War had a profound objective: a new birth of freedom in the nation.
Meade's failure to capture Lee's army as it retreated from Gettysburg, and the continued passivity of the Army of the Potomac, persuaded Lincoln that a change in command was needed.
General Ulysses S. Grant's victories at the Battle of Shiloh and in the Vicksburg campaign impressed Lincoln and made Grant a strong candidate to head the Union Army.
Responding to criticism of Grant after Shiloh, Lincoln had said, "I can't spare this man.
He fights."
With Grant in command, Lincoln felt the Union Army could relentlessly pursue a series of coordinated offensives in multiple theaters, and have a top commander who agreed on the use of black troops.
Nevertheless, Lincoln was concerned that Grant might be considering a candidacy for President in 1864, as McClellan was.
Lincoln arranged for an intermediary to make inquiry into Grant's political intentions, and being assured that he had none, submitted to the Senate Grant's promotion to commander of the Union Army.
He obtained Congress's consent to reinstate for Grant the rank of Lieutenant General, which no officer had held since George Washington.
Grant waged his bloody Overland Campaign in 1864.
This is often characterized as a war of attrition, given high Union losses at battles such as the Battle of the Wilderness and Cold Harbor.
Even though they had the advantage of fighting on the defensive, the Confederate forces had "almost as high a percentage of casualties as the Union forces".
The high casualty figures of the Union alarmed the North; Grant had lost a third of his army, and Lincoln asked what Grant's plans were, to which the general replied, "I propose to fight it out on this line if it takes all summer."
The Confederacy lacked reinforcements, so Lee's army shrank with every costly battle.
Grant's army moved south, crossed the James River, forcing a siege and trench warfare outside Petersburg, Virginia.
Lincoln then made an extended visit to Grant's headquarters at City Point, Virginia.
This allowed the president to confer in person with Grant and William Tecumseh Sherman about the hostilities, as Sherman coincidentally managed a hasty visit to Grant from his position in North Carolina.
Lincoln and the Republican Party mobilized support for the draft throughout the North, and replaced the Union losses.
Lincoln authorized Grant to target the Confederate infrastructure—such as plantations, railroads, and bridges—hoping to destroy the South's morale and weaken its economic ability to continue fighting.
Grant's move to Petersburg resulted in the obstruction of three railroads between Richmond and the South.
This strategy allowed Generals Sherman and Philip Sheridan to destroy plantations and towns in Virginia's Shenandoah Valley.
The damage caused by Sherman's March to the Sea through Georgia in 1864 was limited to a swath, but neither Lincoln nor his commanders saw destruction as the main goal, but rather defeat of the Confederate armies.
Mark E. Neely Jr.
has argued that there was no effort to engage in "total war" against civilians which he believed did take place during World War II.
Confederate general Jubal Early began a series of assaults in the North that threatened the Capital.
During Early's raid on Washington, D.C.
in 1864, Lincoln was watching the combat from an exposed position; Captain Oliver Wendell Holmes shouted at him, "Get down, you damn fool, before you get shot!"
After repeated calls on Grant to defend Washington, Sheridan was appointed and the threat from Early was dispatched.
As Grant continued to wear down Lee's forces, efforts to discuss peace began.
Confederate Vice President Stephens led a group to meet with Lincoln, Seward, and others at Hampton Roads.
Lincoln refused to allow any negotiation with the Confederacy as a coequal; his sole objective was an agreement to end the fighting and the meetings produced no results.
On April 1, 1865, Grant successfully outflanked Lee's forces in the Battle of Five Forks and nearly encircled Petersburg, and the Confederate government evacuated Richmond.
Days later, when that city fell, Lincoln visited the vanquished Confederate capital; as he walked through the city, white Southerners were stone-faced, but freedmen greeted him as a hero.
On April 9, Lee surrendered to Grant at Appomattox and the war was effectively over.
While the war was still being waged, Lincoln faced reelection in 1864.
Lincoln was a master politician, bringing together—and holding together—all the main factions of the Republican Party, and bringing in War Democrats such as Edwin M. Stanton and Andrew Johnson as well.
Lincoln spent many hours a week talking to politicians from across the land and using his patronage powers—greatly expanded over peacetime—to hold the factions of his party together, build support for his own policies, and fend off efforts by Radicals to drop him from the 1864 ticket.
At its 1864 convention, the Republican Party selected Johnson, a War Democrat from the Southern state of Tennessee, as his running mate.
To broaden his coalition to include War Democrats as well as Republicans, Lincoln ran under the label of the new Union Party.
When Grant's 1864 spring campaigns turned into bloody stalemates and Union casualties mounted, the lack of military success wore heavily on the President's re-election prospects, and many Republicans across the country feared that Lincoln would be defeated.
Sharing this fear, Lincoln wrote and signed a pledge that, if he should lose the election, he would still defeat the Confederacy before turning over the White House:

While the Democratic platform followed the "Peace wing" of the party and called the war a "failure", their candidate, General George B. McClellan, supported the war and repudiated the platform.
Lincoln provided Grant with more troops and mobilized his party to renew its support of Grant in the war effort.
Sherman's capture of Atlanta in September and David Farragut's capture of Mobile ended defeatist jitters; the Democratic Party was deeply split, with some leaders and most soldiers openly for Lincoln.
By contrast, the National Union Party was united and energized as Lincoln made emancipation the central issue, and state Republican parties stressed the perfidy of the Copperheads.
On November 8, Lincoln was re-elected in a landslide, carrying all but three states, and receiving 78 percent of the Union soldiers' vote.
On March 4, 1865, Lincoln delivered his second inaugural address.
In it, he deemed the high casualties on both sides to be God's will.
Historian Mark Noll concludes it ranks "among the small handful of semi-sacred texts by which Americans conceive their place in the world".
Lincoln said:

Reconstruction began during the war, as Lincoln and his associates anticipated questions of how to reintegrate the conquered southern states, and how to determine the fates of Confederate leaders and freed slaves.
Shortly after Lee's surrender, a general had asked Lincoln how the defeated Confederates should be treated, and Lincoln replied, "Let 'em up easy."
In keeping with that sentiment, Lincoln led the moderates regarding Reconstruction policy, and was opposed by the Radical Republicans, under Rep.
Thaddeus Stevens, Sen.
Charles Sumner and Sen.
Benjamin Wade, political allies of the president on other issues.
Determined to find a course that would reunite the nation and not alienate the South, Lincoln urged that speedy elections under generous terms be held throughout the war.
His Amnesty Proclamation of December 8, 1863, offered pardons to those who had not held a Confederate civil office, had not mistreated Union prisoners, and would sign an oath of allegiance.
As Southern states were subdued, critical decisions had to be made as to their leadership while their administrations were re-formed.
Of special importance were Tennessee and Arkansas, where Lincoln appointed Generals Andrew Johnson and Frederick Steele as military governors, respectively.
In Louisiana, Lincoln ordered General Nathaniel P. Banks to promote a plan that would restore statehood when 10 percent of the voters agreed to it.
Lincoln's Democratic opponents seized on these appointments to accuse him of using the military to ensure his and the Republicans' political aspirations.
On the other hand, the Radicals denounced his policy as too lenient, and passed their own plan, the Wade-Davis Bill, in 1864.
When Lincoln vetoed the bill, the Radicals retaliated by refusing to seat representatives elected from Louisiana, Arkansas, and Tennessee.
Lincoln's appointments were designed to keep both the moderate and Radical factions in harness.
To fill Chief Justice Taney's seat on the Supreme Court, he named the choice of the Radicals, Salmon P. Chase, who Lincoln believed would uphold the emancipation and paper money policies.
After implementing the Emancipation Proclamation, which did not apply to every state, Lincoln increased pressure on Congress to outlaw slavery throughout the entire nation with a constitutional amendment.
Lincoln declared that such an amendment would "clinch the whole matter".
By December 1863, a proposed constitutional amendment that would outlaw slavery was brought to Congress for passage.
This first attempt at an amendment failed to pass, falling short of the required two-thirds majority on June 15, 1864, in the House of Representatives.
Passage of the proposed amendment became part of the Republican/Unionist platform in the election of 1864.
After a long debate in the House, a second attempt passed Congress on January 31, 1865, and was sent to the state legislatures for ratification.
Upon ratification, it became the Thirteenth Amendment to the United States Constitution on December 6, 1865.
As the war drew to a close, Lincoln's presidential Reconstruction for the South was in flux; having believed the federal government had limited responsibility to the millions of freedmen.
He signed into law Senator Charles Sumner's Freedmen's Bureau bill that set up a temporary federal agency designed to meet the immediate material needs of former slaves.
The law assigned land for a lease of three years with the ability to purchase title for the freedmen.
Lincoln stated that his Louisiana plan did not apply to all states under Reconstruction.
Shortly before his assassination, Lincoln announced he had a new plan for southern Reconstruction.
Discussions with his cabinet revealed Lincoln planned short-term military control over southern states, until readmission under the control of southern Unionists.
Historians agree that it is impossible to predict exactly what Lincoln would have done about Reconstruction if he had lived, but they make projections based on his known policy positions and political acumen.
Lincoln biographers James G. Randall and Richard Current, according to David Lincove, argue that:
Eric Foner argues that:
The successful reunification of the states had consequences for the name of the country.
The term "the United States" has historically been used, sometimes in the plural ("these United States"), and other times in the singular, without any particular grammatical consistency.
The Civil War was a significant force in the eventual dominance of the singular usage by the end of the 19th century.
In recent years, historians such as Harry Jaffa, Herman Belz, John Diggins, Vernon Burton and Eric Foner have stressed Lincoln's redefinition of "republican values".
As early as the 1850s, a time when most political rhetoric focused on the sanctity of the Constitution, Lincoln redirected emphasis to the Declaration of Independence as the foundation of American political values—what he called the "sheet anchor" of republicanism.
The Declaration's emphasis on freedom and equality for all, in contrast to the Constitution's tolerance of slavery, shifted the debate.
As Diggins concludes regarding the highly influential Cooper Union speech of early 1860, "Lincoln presented Americans a theory of history that offers a profound contribution to the theory and destiny of republicanism itself."
His position gained strength because he highlighted the moral basis of republicanism, rather than its legalisms.
Nevertheless, in 1861, Lincoln justified the war in terms of legalisms (the Constitution was a contract, and for one party to get out of a contract all the other parties had to agree), and then in terms of the national duty to guarantee a republican form of government in every state.
Burton (2008) argues that Lincoln's republicanism was taken up by the Freedmen as they were emancipated.
In March 1861, in Lincoln's first inaugural address, he explored the nature of democracy.
He denounced secession as anarchy, and explained that majority rule had to be balanced by constitutional restraints in the American system.
He said "A majority held in restraint by constitutional checks and limitations, and always changing easily with deliberate changes of popular opinions and sentiments, is the only true sovereign of a free people."
Lincoln adhered to the Whig theory of the presidency, which gave Congress primary responsibility for writing the laws while the Executive enforced them.
Lincoln vetoed only four bills passed by Congress; the only important one was the Wade-Davis Bill with its harsh program of Reconstruction.
He signed the Homestead Act in 1862, making millions of acres of government-held land in the West available for purchase at very low cost.
The Morrill Land-Grant Colleges Act, also signed in 1862, provided government grants for agricultural colleges in each state.
The Pacific Railway Acts of 1862 and 1864 granted federal support for the construction of the United States' First Transcontinental Railroad, which was completed in 1869.
The passage of the Homestead Act and the Pacific Railway Acts was made possible by the absence of Southern congressmen and senators who had opposed the measures in the 1850s.
Other important legislation involved two measures to raise revenues for the Federal government: tariffs (a policy with long precedent), and a new Federal income tax.
In 1861, Lincoln signed the second and third Morrill Tariff, the first having become law under James Buchanan.
Also in 1861, Lincoln signed the Revenue Act of 1861, creating the first U.S.
income tax.
This created a flat tax of 3 percent on incomes above $800 ($ in current dollar terms), which was later changed by the Revenue Act of 1862 to a progressive rate structure.
Lincoln also presided over the expansion of the federal government's economic influence in several other areas.
The creation of the system of national banks by the National Banking Act provided a strong financial network in the country.
It also established a national currency.
In 1862, Congress created, with Lincoln's approval, the Department of Agriculture.
In 1862, Lincoln sent a senior general, John Pope, to put down the "Sioux Uprising" in Minnesota.
Presented with 303 execution warrants for convicted Santee Dakota who were accused of killing innocent farmers, Lincoln conducted his own personal review of each of these warrants, eventually approving 39 for execution (one was later reprieved).
President Lincoln had planned to reform federal Indian policy.
In the wake of Grant's casualties in his campaign against Lee, Lincoln had considered yet another executive call for a military draft, but it was never issued.
In response to rumors of one, however, the editors of the "New York World" and the "Journal of Commerce" published a false draft proclamation which created an opportunity for the editors and others employed at the publications to corner the gold market.
Lincoln's reaction was to send the strongest of messages to the media about such behavior; he ordered the military to seize the two papers.
The seizure lasted for two days.
Lincoln is largely responsible for the institution of the Thanksgiving holiday in the United States.
Before Lincoln's presidency, Thanksgiving, while a regional holiday in New England since the 17th century, had been proclaimed by the federal government only sporadically and on irregular dates.
The last such proclamation had been during James Madison's presidency 50 years before.
In 1863, Lincoln declared the final Thursday in November of that year to be a day of Thanksgiving.
In June 1864, Lincoln approved the Yosemite Grant enacted by Congress, which provided unprecedented federal protection for the area now known as Yosemite National Park.
Lincoln's declared philosophy on court nominations was that "we cannot ask a man what he will do, and if we should, and he should answer us, we should despise him for it.
Therefore we must take a man whose opinions are known."
Lincoln made five appointments to the United States Supreme Court.
Noah Haynes Swayne, nominated January 21, 1862, and appointed January 24, 1862, was chosen as an anti-slavery lawyer who was committed to the Union.
Samuel Freeman Miller, nominated and appointed on July 16, 1862, supported Lincoln in the 1860 election and was an avowed abolitionist.
David Davis, Lincoln's campaign manager in 1860, nominated December 1, 1862, and appointed December 8, 1862, had also served as a judge in Lincoln's Illinois court circuit.
Stephen Johnson Field, a previous California Supreme Court justice, was nominated March 6, 1863, and appointed March 10, 1863, and provided geographic balance, as well as political balance to the court as a Democrat.
Finally, Lincoln's Treasury Secretary, Salmon P. Chase, was nominated as Chief Justice, and appointed the same day, on December 6, 1864.
Lincoln believed Chase was an able jurist, would support Reconstruction legislation, and that his appointment united the Republican Party.
Lincoln appointed 32 federal judges, including four Associate Justices and one Chief Justice to the Supreme Court of the United States, and 27 judges to the United States district courts.
Lincoln appointed no judges to the United States circuit courts during his time in office.
West Virginia, admitted to the Union June 20, 1863, contained the former north-westernmost counties of Virginia that seceded from Virginia after that commonwealth declared its secession from the Union.
As a condition for its admission, West Virginia's constitution was required to provide for the gradual abolition of slavery.
Nevada, which became the third State in the far-west of the continent, was admitted as a free state on October 31, 1864.
Abraham Lincoln was assassinated by John Wilkes Booth on Good Friday, April 14, 1865, while attending a play at Ford's Theatre as the American Civil War was drawing to a close.
The assassination occurred five days after the surrender of Robert E. Lee and the Confederate Army of Northern Virginia.
Booth was a well-known actor and a Confederate spy from Maryland; though he never joined the Confederate army, he had contacts with the Confederate secret service.
In 1864, Booth formulated a plan (very similar to one of Thomas N. Conrad previously authorized by the Confederacy) to kidnap Lincoln in exchange for the release of Confederate prisoners.
After attending an April 11, 1865, speech in which Lincoln promoted voting rights for blacks, an incensed Booth changed his plans and became determined to assassinate the president.
Learning that the President and Grant would be attending Ford's Theatre, Booth formulated a plan with co-conspirators to assassinate Lincoln and Grant at the theater, as well as Vice President Johnson and Secretary of State Seward at their homes.
Without his main bodyguard, Ward Hill Lamon, Lincoln left to attend the play "Our American Cousin" on April 14.
At the last minute, Grant decided to go to New Jersey to visit his children instead of attending the play.
Lincoln's bodyguard, John Parker, left Ford's Theater during intermission to drink at the saloon next door.
The now unguarded President sat in his state box in the balcony.
Seizing the opportunity, Booth crept up from behind and at about 10:13 pm, aimed at the back of Lincoln's head and fired at point-blank range, mortally wounding the President.
Major Henry Rathbone momentarily grappled with Booth, but Booth stabbed him and escaped.
After being on the run for 12 days, Booth was tracked down and found on a farm in Virginia, some south of Washington.
After refusing to surrender to Union troops, Booth was killed by Sergeant Boston Corbett on April 26.
Doctor Charles Leale, an Army surgeon, found the President unresponsive, barely breathing and with no detectable pulse.
Having determined that the President had been shot in the head, and not stabbed in the shoulder as originally thought, he made an attempt to clear the blood clot, after which the President began to breathe more naturally.
The dying President was taken across the street to Petersen House.
After remaining in a coma for nine hours, Lincoln died at 7:22 am on April 15.
According to eyewitnesses, his face was fixed in a smile when he expired.
Secretary of War Stanton saluted and said, "Now he belongs to the ages."
Lincoln's flag-enfolded body was then escorted in the rain to the White House by bareheaded Union officers, while the city's church bells rang.
President Johnson was sworn in at 10:00 am, less than 3 hours after Lincoln's death.
The late President lay in state in the East Room, and then in the Capitol Rotunda from April 19 through April 21.
For his final journey with his son Willie, both caskets were transported in the executive coach "United States" and for three weeks the "Lincoln Special" funeral train decorated in black bunting bore Lincoln's remains on a slow circuitous waypoint journey from Washington D.C.
to Springfield, Illinois, stopping at many cities across the North for large-scale memorials attended by hundreds of thousands, as well as many people who gathered in informal trackside tributes with bands, bonfires, and hymn singing or silent reverence with hat in hand as the railway procession slowly passed by.
Poet Walt Whitman composed "When Lilacs Last in the Dooryard Bloom'd" to eulogize Lincoln, one of four poems he wrote about the assassinated president.
Historians have emphasized the widespread shock and sorrow, but also noted that some Lincoln haters cheered when they heard the news.
African-Americans were especially moved; they had lost 'their Moses'.
In a larger sense, the outpouring of grief and anguish was in response to the deaths of so many men in the war that had just ended.
As a young man, Lincoln was a religious skeptic, or, in the words of a biographer, an iconoclast.
Later in life, Lincoln's frequent use of religious imagery and language might have reflected his own personal beliefs or might have been a device to appeal to his audiences, who were mostly evangelical Protestants.
He never joined a church, although he frequently attended with his wife.
However, he was deeply familiar with the Bible, and he both quoted and praised it.
He was private about his beliefs and respected the beliefs of others.
Lincoln never made a clear profession of Christian beliefs.
However, he did believe in an all-powerful God that shaped events and, by 1865, was expressing those beliefs in major speeches.
In the 1840s, Lincoln subscribed to the Doctrine of Necessity, a belief that asserted the human mind was controlled by some higher power.
In the 1850s, Lincoln believed in "providence" in a general way, and rarely used the language or imagery of the evangelicals; he regarded the republicanism of the Founding Fathers with an almost religious reverence.
When he suffered the death of his son Edward, Lincoln more frequently expressed a need to depend on God.
The death of his son Willie in February 1862 may have caused Lincoln to look toward religion for answers and solace.
After Willie's death, Lincoln considered why, from a divine standpoint, the severity of the war was necessary.
He wrote at this time that God "could have either saved or destroyed the Union without a human contest.
Yet the contest began.
And having begun, He could give the final victory to either side any day.
Yet the contest proceeds."
On the day Lincoln was assassinated, he reportedly told his wife he desired to visit the Holy Land.
Several claims abound that Lincoln's health was declining before the assassination.
These are often based on photographs appearing to show weight loss and muscle wasting.
One such claim is that he suffered from a rare genetic disorder, MEN2b, which manifests with a medullary thyroid carcinoma, mucosal neuromas and a Marfanoid appearance.
Others simply claim he had Marfan syndrome, based on his tall appearance with spindly fingers, and the association of possible aortic regurgitation, which can cause bobbing of the head (DeMusset's sign) – based on blurring of Lincoln's head in photographs, which back then had a long exposure time.
Confirmation of this and other diseases could possibly be obtained via DNA analysis of a pillow case stained with Lincoln's blood, currently in possession of the Grand Army of the Republic Museum & Library in Philadelphia, but , the museum has refused to provide a sample for testing.
In surveys of U.S.
scholars ranking presidents conducted since the 1940s, Lincoln is consistently ranked in the top three, often as number one.
A 2004 study found that scholars in the fields of history and politics ranked Lincoln number one, while legal scholars placed him second after Washington.
In presidential ranking polls conducted in the United States since 1948, Lincoln has been rated at the very top in the majority of polls.
Generally, the top three presidents are rated as 1.
Lincoln; 2.
George Washington; and 3.
Franklin D. Roosevelt, although Lincoln and Washington, and Washington and Roosevelt, are occasionally reversed.
President Lincoln's assassination increased his status to the point of making him a national martyr.
Lincoln was viewed by abolitionists as a champion for human liberty.
Republicans linked Lincoln's name to their party.
Many, though not all, in the South considered Lincoln as a man of outstanding ability.
Historians have said he was "a classical liberal" in the 19th century sense.
Allen C. Guelzo states that Lincoln was a
Lincoln became a favorite exemplar for liberal intellectuals across Europe and Latin America and even in Asia.
Schwartz argues that Lincoln's American reputation grew slowly in the late 19th century until the Progressive Era (1900–1920s) when he emerged as one of the most venerated heroes in American history, with even white Southerners in agreement.
The high point came in 1922 with the dedication of the Lincoln Memorial on the National Mall in Washington, D.C.
In the New Deal era liberals honored Lincoln not so much as the self-made man or the great war president, but as the advocate of the common man who they believe would have supported the welfare state.
In the Cold War years, Lincoln's image shifted to emphasize the symbol of freedom who brought hope to those oppressed by communist regimes.
By the 1970s Lincoln had become a hero to political conservatives for his intense nationalism, support for business, his insistence on stopping the spread of human bondage, his acting in terms of Lockean and Burkean principles on behalf of both liberty and tradition, and his devotion to the principles of the Founding Fathers.
As a Whig activist, Lincoln was a spokesman for business interests, favoring high tariffs, banks, internal improvements, and railroads in opposition to the agrarian Democrats.
William C. Harris found that Lincoln's "reverence for the Founding Fathers, the Constitution, the laws under it, and the preservation of the Republic and its institutions undergirded and strengthened his conservatism".
James G. Randall emphasizes his tolerance and especially his moderation "in his preference for orderly progress, his distrust of dangerous agitation, and his reluctance toward ill digested schemes of reform".
Randall concludes that, "he was conservative in his complete avoidance of that type of so-called 'radicalism' which involved abuse of the South, hatred for the slaveholder, thirst for vengeance, partisan plotting, and ungenerous demands that Southern institutions be transformed overnight by outsiders."
By the late 1960s, some African American intellectuals led by Lerone Bennett Jr., rejected Lincoln's role as the Great Emancipator.
Bennett won wide attention when he called Lincoln a white supremacist in 1968.
He noted that Lincoln used ethnic slurs and told jokes that ridiculed blacks.
Bennett argued that Lincoln opposed social equality, and proposed sending freed slaves to another country.
Defenders, such as authors Dirck and Cashin, retorted that he was not as bad as most politicians of his day; and that he was a "moral visionary" who deftly advanced the abolitionist cause, as fast as politically possible.
The emphasis shifted away from Lincoln-the-emancipator to an argument that blacks had freed themselves from slavery, or at least were responsible for pressuring the government on emancipation.
Historian Barry Schwartz wrote in 2009 that Lincoln's image suffered "erosion, fading prestige, benign ridicule" in the late 20th century.
On the other hand, Donald opined in his 1996 biography that Lincoln was distinctly endowed with the personality trait of negative capability, defined by the poet John Keats and attributed to extraordinary leaders who were "content in the midst of uncertainties and doubts, and not compelled toward fact or reason".
In the 21st century, President Barack Obama named Lincoln his favorite president and insisted on using Lincoln's Bible for his swearing in to office at both his inaugurations.
Lincoln has often been portrayed by Hollywood, almost always in a flattering light.
The Union nationalism as envisioned by Lincoln, "helped lead America to the nationalism of Theodore Roosevelt, Woodrow Wilson, and Franklin Delano Roosevelt."
Lincoln's portrait appears on two denominations of United States currency, the penny and the $5 bill.
His likeness also appears on many postage stamps and he has been memorialized in many town, city, and county names, including the capital of Nebraska.
While he is usually portrayed bearded, he first grew a beard in 1860 at the suggestion of 11-year-old Grace Bedell.
The most famous and most visited memorials are Lincoln's sculpture on Mount Rushmore; Lincoln Memorial, Ford's Theatre, and Petersen House (where he died) in Washington, D.C.
; and the Abraham Lincoln Presidential Library and Museum in Springfield, Illinois, not far from Lincoln's home, as well as his tomb.
Barry Schwartz, a sociologist who has examined America's cultural memory, argues that in the 1930s and 1940s, the memory of Abraham Lincoln was practically sacred and provided the nation with "a moral symbol inspiring and guiding American life".
During the Great Depression, he argues, Lincoln served "as a means for seeing the world's disappointments, for making its sufferings not so much explicable as meaningful".
Franklin D. Roosevelt, preparing America for war, used the words of the Civil War president to clarify the threat posed by Germany and Japan.
Americans asked, "What would Lincoln do?"
However, Schwartz also finds that since World War II, Lincoln's symbolic power has lost relevance, and this "fading hero is symptomatic of fading confidence in national greatness".
He suggested that postmodernism and multiculturalism have diluted greatness as a concept.
The United States Navy is named after Lincoln, the second Navy ship to bear his name.
</doc>
<doc id="308" url="https://en.wikipedia.org/wiki?curid=308" title="Aristotle">
Aristotle

Aristotle (; "Aristotélēs", ; 384–322 BC) was an ancient Greek philosopher and scientist born in the city of Stagira, Chalkidiki, in the north of Classical Greece.
Along with Plato, he is considered the "Father of Western Philosophy".
Aristotle provided a complex and harmonious synthesis of the various existing philosophies prior to him, including those of Socrates and Plato, and it was above all from his teachings that the West inherited its fundamental intellectual lexicon, as well as problems and methods of inquiry.
As a result, his philosophy has exerted a unique influence on almost every form of knowledge in the West and it continues to be central to the contemporary philosophical discussion.
Little is known about his life.
His father, Nicomachus, died when Aristotle was a child, and he was brought up by a guardian.
At seventeen or eighteen years of age, he joined Plato's Academy in Athens and remained there until the age of thirty-seven (c.
347 BC).
His writings cover many subjects – including physics, biology, zoology, metaphysics, logic, ethics, aesthetics, poetry, theatre, music, rhetoric, psychology, linguistics, economics, politics and government – and constitute the first comprehensive system of Western philosophy.
Shortly after Plato died, Aristotle left Athens and, at the request of Philip II of Macedon, tutored Alexander the Great beginning in 343 BC.
Teaching Alexander gave Aristotle many opportunities.
He established a library in the Lyceum which helped him to produce many of his hundreds of books, which were papyrus scrolls.
The fact that Aristotle was a pupil of Plato contributed to his former views of Platonism, but, following Plato's death, Aristotle immersed himself in empirical studies and shifted from Platonism to empiricism.
He believed all concepts and knowledge were ultimately based on perception.
Aristotle's views on natural sciences represent the groundwork underlying many of his works.
Aristotle's views on physical science profoundly shaped medieval scholarship.
Their influence extended from Late Antiquity and the Early Middle Ages into the Renaissance, and were not replaced systematically until the Enlightenment and theories such as classical mechanics.
Some of Aristotle's zoological observations, such as on the hectocotyl (reproductive) arm of the octopus, were disbelieved until the 19th century.
His works contain the earliest known formal study of logic, studied by medieval scholars such as Peter Abelard and John Buridan.
Aristotelianism profoundly influenced Islamic thought during the Middle Ages, as well as Christian theology, especially the Neoplatonism of the Early Church and the scholastic tradition of the Catholic Church.
Aristotle was revered among medieval Muslim scholars as "The First Teacher".
His ethics, though always influential, gained renewed interest with the modern advent of virtue ethics.
All aspects of Aristotle's philosophy continue to be the object of academic study.
Though Aristotle wrote many elegant treatises and dialogues for publication, only around a third of his original output has survived, none of it intended for publication.
Aristotle has been depicted by major artists including Raphael and Rembrandt.
Early Modern theories including William Harvey's circulation of the blood and Galileo Galilei's kinematics were developed in reaction to Aristotle's.
In the 19th century, George Boole gave Aristotle's logic a mathematical foundation with his system of algebraic logic.
In the 20th century, Martin Heidegger created a new interpretation of Aristotle's political philosophy, but elsewhere Aristotle was widely criticised, even ridiculed by thinkers such as the philosopher Bertrand Russell and the biologist Peter Medawar.
More recently, Aristotle has again been taken seriously, such as in the thinking of Ayn Rand and Alasdair MacIntyre, while Armand Marie Leroi has reconstructed Aristotle's biology.
The image of Aristotle tutoring the young Alexander remains current, and the "Poetics" continues to play a role in the cinema of the United States.
In general, the details of Aristotle's life are not well-established.
The biographies written in ancient times are often speculative and historians only agree on a few salient points.
Aristotle, whose name means "the best purpose" in Ancient Greek, was born in 384 BC in Stagira, Chalcidice, about 55 km (34 miles) east of modern-day Thessaloniki.
His father Nicomachus was the personal physician to King Amyntas of Macedon.
Both of Aristotle's parents died when he was about thirteen, and Proxenus of Atarneus became his guardian.
Although little information about Aristotle's childhood has survived, he probably spent some time within the Macedonian palace, making his first connections with the Macedonian monarchy.
At the age of seventeen or eighteen, Aristotle moved to Athens to continue his education at Plato's Academy.
He remained there for nearly twenty years before leaving Athens in 348/47 BC.
The traditional story about his departure records that he was disappointed with the Academy's direction after control passed to Plato's nephew Speusippus, although it is possible that he feared the anti-Macedonian sentiments in Athens at that time and left before Plato died.
Aristotle then accompanied Xenocrates to the court of his friend Hermias of Atarneus in Asia Minor.
After the death of Hermias, Aristotle travelled with his pupil Theophrastus to the island of Lesbos, where together they researched the botany and zoology of the island and its sheltered lagoon.
While in Lesbos, Aristotle married Pythias, either Hermias's adoptive daughter or niece.
She bore him a daughter, whom they also named Pythias.
In 343 BC, Aristotle was invited by Philip II of Macedon to become the tutor to his son Alexander.
Aristotle was appointed as the head of the royal academy of Macedon.
During Aristotle's time in the Macedonian court, he gave lessons not only to Alexander, but also to two other future kings: Ptolemy and Cassander.
Aristotle encouraged Alexander toward eastern conquest and Aristotle's own attitude towards Persia was unabashedly ethnocentric.
In one famous example, he counsels Alexander to be "a leader to the Greeks and a despot to the barbarians, to look after the former as after friends and relatives, and to deal with the latter as with beasts or plants".
By 335 BC, Aristotle had returned to Athens, establishing his own school there known as the Lyceum.
Aristotle conducted courses at the school for the next twelve years.
While in Athens, his wife Pythias died and Aristotle became involved with Herpyllis of Stagira, who bore him a son whom he named after his father, Nicomachus.
According to the "Suda", he also had an "erômenos", Palaephatus of Abydus.
This period in Athens, between 335 and 323 BC, is when Aristotle is believed to have composed many of his works.
He wrote many dialogues, of which only fragments have survived.
Those works that have survived are in treatise form and were not, for the most part, intended for widespread publication; they are generally thought to be lecture aids for his students.
His most important treatises include "Physics", "Metaphysics", "Nicomachean Ethics", "Politics", "On the Soul" and "Poetics".
Aristotle studied and made significant contributions to "logic, metaphysics, mathematics, physics, biology, botany, ethics, politics, agriculture, medicine, dance and theatre."
Near the end of his life, Alexander and Aristotle became estranged over Alexander's relationship with Persia and Persians.
A widespread tradition in antiquity suspected Aristotle of playing a role in Alexander's death, but the only evidence of this is an unlikely claim made some six years after the death.
Following Alexander's death, anti-Macedonian sentiment in Athens was rekindled.
In 322 BC, Demophilus and Eurymedon the Hierophant reportedly denounced Aristotle for impiety, prompting him to flee to his mother's family estate in Chalcis, on Euboea, at which occasion he was said to have stated: "I will not allow the Athenians to sin twice against philosophy" – a reference to Athens's trial and execution of Socrates.
He died on Euboea of natural causes later that same year, having named his student Antipater as his chief executor and leaving a will in which he asked to be buried next to his wife.
With the "Prior Analytics", Aristotle is credited with the earliest study of formal logic, and his conception of it was the dominant form of Western logic until 19th-century advances in mathematical logic.
Kant stated in the "Critique of Pure Reason" that with Aristotle logic reached its completion.
What we today call "Aristotelian logic" with its types of syllogism (methods of logical argument), Aristotle himself would have labelled "analytics".
The term "logic" he reserved to mean "dialectics".
Most of Aristotle's work is probably not in its original form, because it was most likely edited by students and later lecturers.
The logical works of Aristotle were compiled into a set of six books called the "Organon" around 40 BC by Andronicus of Rhodes or others among his followers.
The books are:


The order of the books (or the teachings from which they are composed) is not certain, but this list was derived from analysis of Aristotle's writings.
It goes from the basics, the analysis of simple terms in the "Categories," the analysis of propositions and their elementary relations in "On Interpretation", to the study of more complex forms, namely, syllogisms (in the "Analytics") and dialectics (in the "Topics" and "Sophistical Refutations").
The first three treatises form the core of the logical theory "stricto sensu": the grammar of the language of logic and the correct rules of reasoning.
The "Rhetoric" is not conventionally included, but it states that it relies on the "Topics".
Like his teacher Plato, Aristotle's philosophy aims at the universal.
Aristotle's ontology places the universal ("katholou") in particulars ("kath' hekaston"), things in the world, whereas for Plato the universal is a separately existing form which actual things imitate.
This means that Aristotle's epistemology is based on the study of things that exist or happen in the world, and rises to knowledge of the universal, whereas for Plato epistemology begins with knowledge of universal Forms (or ideas) and descends to knowledge of particular imitations of these.
For Aristotle, "form" is still what phenomena are based on, but is "instantiated" in a particular substance.
Aristotle uses induction from examples alongside deduction, whereas Plato relies on deduction from "a priori" principles.
In Aristotle's terminology, "natural philosophy" is a branch of philosophy examining the phenomena of the natural world, and includes fields that would be regarded today as physics, biology and other natural sciences.
Aristotle's work encompassed virtually all facets of intellectual inquiry.
Aristotle makes philosophy in the broad sense coextensive with reasoning, which he also would describe as "science".
Note, however, that his use of the term "science" carries a different meaning than that covered by the term "scientific method".
For Aristotle, "all science ("dianoia") is either practical, poetical or theoretical" ("Metaphysics" 1025b25).
His practical science includes ethics and politics; his poetical science means the study of fine arts including poetry; his theoretical science covers physics, mathematics and metaphysics.
The word "metaphysics" was coined by the first century AD editor who assembled various small selections of Aristotle's works to the treatise we know by the name "Metaphysics".
Aristotle called it "first philosophy", and distinguished it from mathematics and natural science (physics) as the contemplative ("theoretikē") philosophy which is "theological" and studies the divine.
He wrote in his "Metaphysics" (1026a16):

Aristotle examines the concepts of substance ("ousia") and essence ("to ti ên einai", "the what it was to be") in his "Metaphysics" (Book VII), and he concludes that a particular substance is a combination of both matter and form, a philosophical theory called hylomorphism.
In Book VIII, he distinguishes the matter of the substance as the substratum, or the stuff of which it is composed.
For example, the matter of a house is the bricks, stones, timbers etc., or whatever constitutes the "potential" house, while the form of the substance is the "actual" house, namely 'covering for bodies and chattels' or any other differentia that let us define something as a house.
The formula that gives the components is the account of the matter, and the formula that gives the differentia is the account of the form.
With regard to the change ("kinesis") and its causes now, as he defines in his "Physics" and "On Generation and Corruption" 319b–320a, he distinguishes the coming to be from:

The coming to be is a change where nothing persists of which the resultant is a property.
In that particular change he introduces the concept of potentiality ("dynamis") and actuality ("entelecheia") in association with the matter and the form.
Referring to potentiality, this is what a thing is capable of doing, or being acted upon, if the conditions are right and it is not prevented by something else.
For example, the seed of a plant in the soil is potentially ("dynamei") plant, and if is not prevented by something, it will become a plant.
Potentially beings can either 'act' ("poiein") or 'be acted upon' ("paschein"), which can be either innate or learned.
For example, the eyes possess the potentiality of sight (innate – being acted upon), while the capability of playing the flute can be possessed by learning (exercise – acting).
Actuality is the fulfilment of the end of the potentiality.
Because the end ("telos") is the principle of every change, and for the sake of the end exists potentiality, therefore actuality is the end.
Referring then to our previous example, we could say that an actuality is when a plant does one of the activities that plants do.
For that for the sake of which ("to hou heneka") a thing is, is its principle, and the becoming is for the sake of the end; and the actuality is the end, and it is for the sake of this that the potentiality is acquired.
For animals do not see in order that they may have sight, but they have sight that they may see.
In summary, the matter used to make a house has potentiality to be a house and both the activity of building and the form of the final house are actualities, which is also a final cause or end.
Then Aristotle proceeds and concludes that the actuality is prior to potentiality in formula, in time and in substantiality.
With this definition of the particular substance (i.e., matter and form), Aristotle tries to solve the problem of the unity of the beings, for example, "what is it that makes a man one"?
Since, according to Plato there are two Ideas: animal and biped, how then is man a unity?
However, according to Aristotle, the potential being (matter) and the actual one (form) are one and the same.
Plato argued that all things have a universal form, which could be either a property or a relation to other things.
When we look at an apple, for example, we see an apple, and we can also analyse a form of an apple.
In this distinction, there is a particular apple and a universal form of an apple.
Moreover, we can place an apple next to a book, so that we can speak of both the book and apple as being next to each other.
Plato argued that there are some universal forms that are not a part of particular things.
For example, it is possible that there is no particular good in existence, but "good" is still a proper universal form.
Aristotle disagreed with Plato on this point, arguing that all universals are instantiated at some period of time, and that there are no universals that are unattached to existing things.
In addition, Aristotle disagreed with Plato about the location of universals.
Where Plato spoke of the world of forms, a place where all universal forms subsist, Aristotle maintained that universals exist within each thing on which each universal is predicated.
So, according to Aristotle, the form of apple exists within each apple, rather than in the world of the forms.
Aristotle's "natural philosophy" spans a wide range of natural phenomena including those now covered by physics, biology and other natural sciences.
In his "On Generation and Corruption", Aristotle related each of the four elements proposed earlier by Empedocles, Earth, Water, Air, and Fire, to two of the four sensible qualities, hot, cold, wet, and dry.
In the Empedoclean scheme, all matter was made of the four elements, in differing proportions.
Aristotle's scheme added the heavenly Aether, the divine substance of the heavenly spheres, stars and planets.
Aristotle describes two kinds of motion: "violent" or "unnatural motion", such as that of a thrown stone, in the "Physics" (254b10), and "natural motion", such as of a falling object, in "On the Heavens" (300a20).
In violent motion, as soon as the agent stops causing it, the motion stops also; in other words, the natural state of an object is to be at rest, since Aristotle does not address friction.
With this understanding, it can be observed that, as Aristotle stated, heavy objects (on the ground, say) require more force to make them move; and objects pushed with greater force move faster.
This would imply the equation

incorrect in modern physics.
Natural motion depends on the element concerned: the aether naturally moves in a circle around the heavens, while the 4 Empedoclean elements move vertically up (like fire, as is observed) or down (like earth) towards their natural resting places.
In the "Physics" (215a25), Aristotle effectively states a quantitative law, that the speed, v, of a falling body is proportional (say, with constant c) to its weight, W, and inversely proportional to the density, ρ, of the fluid in which it is falling:

Aristotle implies that in a vacuum the speed of fall would become infinite, and concludes from this apparent absurdity that a vacuum is not possible.
Opinions have varied on whether Aristotle intended to state quantitative laws.
Henri Carteron held the "extreme view" that Aristotle's concept of force was basically qualitative, but other authors reject this.
Archimedes corrected Aristotle's theory that bodies move towards their natural resting places; metal boats can float if they displace enough water; floating depends in Archimedes' scheme on the mass and volume of the object, not as Aristotle thought its elementary composition.
Aristotle's writings on motion remained influential until the Early Modern period.
John Philoponus (in the Middle Ages) and Galileo are said to have shown by experiment that Aristotle's claim that a heavier object falls faster than a lighter object is incorrect.
A contrary opinion is given by Carlo Rovelli, who argues that Aristotle's physics of motion is correct within its domain of validity, that of objects in the Earth's gravitational field immersed in a fluid such as air.
In this system, heavy bodies in steady fall indeed travel faster than light ones (whether friction is ignored, or not), and they do fall more slowly in a denser medium.
Newton's "forced" motion corresponds to Aristotle's "violent" motion with its external agent, but Aristotle's assumption that the agent's effect stops immediately it stops acting (e.g., the ball leaves the thrower's hand) has awkward consequences: he has to suppose that surrounding fluid helps to push the ball along to make it continue to rise even though the hand is no longer acting on it, resulting in the Medieval theory of impetus.
Aristotle suggested that the reason for anything coming about can be attributed to four different types of simultaneously active factors.
His term "aitia" is traditionally translated as "cause", but it does not always refer to temporal sequence; it might be better translated as "explanation", but the traditional rendering will be employed here.
Aristotle describes experiments in optics using a camera obscura in "Problems", book 15.
The apparatus consisted of a dark chamber with a small aperture that let light in.
With it, he saw that whatever shape he made the hole, the sun's image always remained circular.
He also noted that increasing the distance between the aperture and the image surface magnified the image.
According to Aristotle, spontaneity and chance are causes of some things, distinguishable from other types of cause such as simple necessity.
Chance as an incidental cause lies in the realm of accidental things, "from what is spontaneous".
There is also more a specific kind of chance, which Aristotle names "luck", that only applies to people's moral choices.
In astronomy, Aristotle refuted Democritus's claim that the Milky Way was made up of "those stars which are shaded by the earth from the sun's rays," pointing out correctly that if "the size of the sun is greater than that of the earth and the distance of the stars from the earth many times greater than that of the sun, then... the sun shines on all the stars and the earth screens none of them."
Aristotle was one of the first people to record any geological observations.
He stated that geological change was too slow to be observed in one person's lifetime.
The geologist Charles Lyell noted that Aristotle described such change, including "lakes that had dried up" and "deserts that had become watered by rivers", giving as examples the growth of the Nile delta since the time of Homer, and "the upheaving of one of the Aeolian islands, previous to a volcanic eruption."'
Aristotle was the first person to study biology systematically, and biology forms a large part of his writings.
He spent two years observing and describing the zoology of Lesbos and the surrounding seas, including in particular the Pyrrha lagoon in the centre of Lesbos.
His data in "History of Animals", "Generation of Animals", "Movement of Animals", and "Parts of Animals" are assembled from his own observations, statements given by people with specialised knowledge such as beekeepers and fishermen, and less accurate accounts provided by travellers from overseas.
His apparent emphasis on animals rather than plants is a historical accident: his works on botany have been lost, but two books on plants by his pupil Theophrastus have survived.
Aristotle reports on the sea-life visible from observation on Lesbos and the catches of fishermen.
He describes the catfish, electric ray, and frogfish in detail, as well as cephalopods such as the octopus and paper nautilus.
His description of the hectocotyl arm of cephalopods, used in sexual reproduction, was widely disbelieved until the 19th century.
He gives accurate descriptions of the four-chambered fore-stomachs of ruminants, and of the ovoviviparous embryological development of the hound shark.
He notes that an animal's structure is well matched to function, so, among birds, the heron, which lives in marshes with soft mud and lives by catching fish, has a long neck and long legs, and a sharp spear-like beak, whereas ducks that swim have short legs and webbed feet.
Darwin, too, noted these sorts of differences between similar kinds of animal, but unlike Aristotle used the data to come to the theory of evolution.
Aristotle's writings can seem to modern readers close to implying evolution, but while Aristotle was aware that new mutations or hybridisations could occur, he saw these as rare accidents.
For Aristotle, accidents, like heat waves in winter, must be considered distinct from natural causes.
He was thus critical of Empedocles's materialist theory of a "survival of the fittest" origin of living things and their organs, and ridiculed the idea that accidents could lead to orderly results.
To put his views into modern terms, he nowhere says that different species can have a common ancestor, or that one kind can change into another, or that kinds can become extinct.
Aristotle did not do experiments in the modern sense.
He used the ancient Greek term "pepeiramenoi" to mean observations, or at most investigative procedures like dissection.
In "Generation of Animals", he finds a fertilised hen's egg of a suitable stage and opens it to see the embryo's heart beating inside.
Instead, he practised a different style of science: systematically gathering data, discovering patterns common to whole groups of animals, and inferring possible causal explanations from these.
This style is common in modern biology when large amounts of data become available in a new field, such as genomics.
It does not result in the same certainty as experimental science, but it sets out testable hypotheses and constructs a narrative explanation of what is observed.
In this sense, Aristotle's biology is scientific.
From the data he collected and documented, Aristotle inferred quite a number of rules relating the life-history features of the live-bearing tetrapods (terrestrial placental mammals) that he studied.
Among these correct predictions are the following.
Brood size decreases with (adult) body mass, so that an elephant has fewer young (usually just one) per brood than a mouse.
Lifespan increases with gestation period, and also with body mass, so that elephants live longer than mice, have a longer period of gestation, and are heavier.
As a final example, fecundity decreases with lifespan, so long-lived kinds like elephants have fewer young in total than short-lived kinds like mice.
Aristotle distinguished about 500 species of animals, arranging these in the "History of Animals" in a graded scale of perfection, a "scala naturae", with man at the top.
His system had eleven grades of animal, from highest potential to lowest, expressed in their form at birth: the highest gave live birth to hot and wet creatures, the lowest laid cold, dry mineral-like eggs.
Animals came above plants, and these in turn were above minerals.
He grouped what the modern zoologist would call vertebrates as the hotter "animals with blood", and below them the colder invertebrates as "animals without blood".
Those with blood were divided into the live-bearing (mammals), and the egg-laying (birds, reptiles, fish).
Those without blood were insects, crustacea (non-shelled – cephalopods, and shelled) and the hard-shelled molluscs (bivalves and gastropods).
He recognised that animals did not exactly fit into a linear scale, and noted various exceptions, such as that sharks had a placenta like the tetrapods.
To a modern biologist, the explanation, not available to Aristotle, is convergent evolution.
He believed that purposive final causes guided all natural processes; this teleological view justified his observed data as an expression of formal design.
Aristotle's psychology, given in his treatise "On the Soul" ("peri psyche"), posits three kinds of soul ("psyches"): the vegetative soul, the sensitive soul, and the rational soul.
Humans have a rational soul.
The human soul incorporates the powers of the other kinds: Like the vegetative soul it can grow and nourish itself; like the sensitive soul it can experience sensations and move locally.
The unique part of the human, rational soul is its ability to receive forms of other things and to compare them using the "nous" (intellect) and "logos" (reason).
For Aristotle, the soul is the form of a living being.
Because all beings are composites of form and matter, the form of living beings is that which endows them with what is specific to living beings, e.g.
the ability to initiate movement (or in the case of plants, growth and chemical transformations, which Aristotle considers types of movement).
In contrast to earlier philosophers, but in accordance with the Egyptians, he placed the rational soul in the heart, rather than the brain.
Notable is Aristotle's division of sensation and thought, which generally differed from the concepts of previous philosophers, with the exception of Alcmaeon.
According to Aristotle in "On the Soul", memory is the ability to hold a perceived experience in the mind and to distinguish between the internal "appearance" and an occurrence in the past.
In other words, a memory is a mental picture (phantasm) that can be recovered.
Aristotle believed an impression is left on a semi-fluid bodily organ that undergoes several changes in order to make a memory.
A memory occurs when stimuli such as sights or sounds are so complex that the nervous system cannot receive all the impressions at once.
These changes are the same as those involved in the operations of sensation, Aristotelian 'common sense', and thinking.
Aristotle uses the term 'memory' for the actual retaining of an experience in the impression that can develop from sensation, and for the intellectual anxiety that comes with the impression because it is formed at a particular time and processing specific contents.
Memory is of the past, prediction is of the future, and sensation is of the present.
Retrieval of impressions cannot be performed suddenly.
A transitional channel is needed and located in our past experiences, both for our previous experience and present experience.
Because Aristotle believes people receive all kinds of sense perceptions and perceive them as impressions, people are continually weaving together new impressions of experiences.
To search for these impressions, people search the memory itself.
Within the memory, if one experience is offered instead of a specific memory, that person will reject this experience until they find what they are looking for.
Recollection occurs when one retrieved experience naturally follows another.
If the chain of "images" is needed, one memory will stimulate the next.
When people recall experiences, they stimulate certain previous experiences until they reach the one that is needed.
Recollection is thus the self-directed activity of retrieving the information stored in a memory impression.
Only humans can remember impressions of intellectual activity, such as numbers and words.
Animals that have perception of time can retrieve memories of their past observations.
Remembering involves only perception of the things remembered and of the time passed.
Aristotle believed the chain of thought, which ends in recollection of certain impressions, was connected systematically in relationships such as similarity, contrast, and contiguity, described in his Laws of Association.
Aristotle believed that past experiences are hidden within the mind.
A force operates to awaken the hidden material to bring up the actual experience.
According to Aristotle, association is the power innate in a mental state, which operates upon the unexpressed remains of former experiences, allowing them to rise and be recalled.
Aristotle describes sleep in "On Sleep and Wakefulness".
Sleep takes place as a result of overuse of the senses or of digestion, so it is vital to the body.
While a person is asleep, the critical activities, which include thinking, sensing, recalling and remembering, do not function as they do during wakefulness.
Since a person cannot sense during sleep they can not have desire, which is the result of sensation.
However, the senses are able to work during sleep, albeit differently, unless they are weary.
Dreams do not involve actually sensing a stimulus.
In dreams, sensation is still involved, but in an altered manner.
Aristotle explains that when a person stares at a moving stimulus such as the waves in a body of water, and then look away, the next thing they look at appears to have a wavelike motion.
When a person perceives a stimulus and the stimulus is no longer the focus of their attention, it leaves an impression.
When the body is awake and the senses are functioning properly, a person constantly encounters new stimuli to sense and so the impressions of previously perceived stimuli are ignored.
However, during sleep the impressions made throughout the day are noticed as there are no new distracting sensory experiences.
So, dreams result from these lasting impressions.
Since impressions are all that are left and not the exact stimuli, dreams do not resemble the actual waking experience.
During sleep, a person is in an altered state of mind.
Aristotle compares a sleeping person to a person who is overtaken by strong feelings toward a stimulus.
For example, a person who has a strong infatuation with someone may begin to think they see that person everywhere because they are so overtaken by their feelings.
Since a person sleeping is in a suggestible state and unable to make judgements, they become easily deceived by what appears in their dreams, like the infatuated person.
This leads the person to believe the dream is real, even when the dreams are absurd in nature.
One component of Aristotle's theory of dreams disagrees with previously held beliefs.
He claimed that dreams are not foretelling and not sent by a divine being.
Aristotle reasoned naturalistically that instances in which dreams do resemble future events are simply coincidences.
Aristotle claimed that a dream is first established by the fact that the person is asleep when they experience it.
If a person had an image appear for a moment after waking up or if they see something in the dark it is not considered a dream because they were awake when it occurred.
Secondly, any sensory experience that is perceived while a person is asleep does not qualify as part of a dream.
For example, if, while a person is sleeping, a door shuts and in their dream they hear a door is shut, this sensory experience is not part of the dream.
Lastly, the images of dreams must be a result of lasting impressions of waking sensory experiences.
Aristotle's practical philosophy covers areas such as ethics, politics, economics, and rhetoric.
Aristotle considered ethics to be a practical rather than theoretical study, i.e., one aimed at becoming good and doing good rather than knowing for its own sake.
He wrote several treatises on ethics, including most notably, the "Nicomachean Ethics".
Aristotle taught that virtue has to do with the proper function ("ergon") of a thing.
An eye is only a good eye in so much as it can see, because the proper function of an eye is sight.
Aristotle reasoned that humans must have a function specific to humans, and that this function must be an activity of the "psuchē" ("soul") in accordance with reason ("logos").
Aristotle identified such an optimum activity (the virtuous mean, between the accompanying vices of excess or deficiency) of the soul as the aim of all human deliberate action, "eudaimonia", generally translated as "happiness" or sometimes "well being".
To have the potential of ever being happy in this way necessarily requires a good character ("ēthikē" "aretē"), often translated as moral or ethical virtue or excellence.
Aristotle taught that to achieve a virtuous and potentially happy character requires a first stage of having the fortune to be habituated not deliberately, but by teachers, and experience, leading to a later stage in which one consciously chooses to do the best things.
When the best people come to live life this way their practical wisdom ("phronesis") and their intellect ("nous") can develop with each other towards the highest possible human virtue, the wisdom of an accomplished theoretical or speculative thinker, or in other words, a philosopher.
In addition to his works on ethics, which address the individual, Aristotle addressed the city in his work titled "Politics".
Aristotle considered the city to be a natural community.
Moreover, he considered the city to be prior in importance to the family which in turn is prior to the individual, "for the whole must of necessity be prior to the part".
He also famously stated that "man is by nature a political animal" and also arguing that humanity's defining factor among others in the animal kingdom is its rationality.
Aristotle conceived of politics as being like an organism rather than like a machine, and as a collection of parts none of which can exist without the others.
Aristotle's conception of the city is organic, and he is considered one of the first to conceive of the city in this manner.
The common modern understanding of a political community as a modern state is quite different from Aristotle's understanding.
Although he was aware of the existence and potential of larger empires, the natural community according to Aristotle was the city ("polis") which functions as a political "community" or "partnership" ("koinōnia").
The aim of the city is not just to avoid injustice or for economic stability, but rather to allow at least some citizens the possibility to live a good life, and to perform beautiful acts: "The political partnership must be regarded, therefore, as being for the sake of noble actions, not for the sake of living together."
This is distinguished from modern approaches, beginning with social contract theory, according to which individuals leave the state of nature because of "fear of violent death" or its "inconveniences."
In "Protrepticus", the character 'Aristotle' states:

Aristotle made substantial contributions to economic thought, especially to thought in the Middle Ages.
In "Politics", Aristotle addresses the city, property, and trade.
His response to criticisms of private property, in Lionel Robbins's view, anticipated later proponents of private property among philosophers and economists, as it related to the overall utility of social arrangements.
Aristotle believed that although communal arrangements may seem beneficial to society, and that although private property is often blamed for social strife, such evils in fact come from human nature.
In "Politics", Aristotle offers one of the earliest accounts of the origin of money.
Money came into use because people became dependent on one another, importing what they needed and exporting the surplus.
For the sake of convenience, people then agreed to deal in something that is intrinsically useful and easily applicable, such as iron or silver.
Aristotle's discussions on retail and interest was a major influence on economic thought in the Middle Ages.
He had a low opinion of retail, believing that contrary to using money to procure things one needs in managing the household, retail trade seeks to make a profit.
It thus uses goods as a means to an end, rather than as an end unto itself.
He believed that retail trade was in this way unnatural.
Similarly, Aristotle considered making a profit through interest unnatural, as it makes a gain out of the money itself, and not from its use.
Aristotle gave a summary of the function of money that was perhaps remarkably precocious for his time.
He wrote that because it is impossible to determine the value of every good through a count of the number of other goods it is worth, the necessity arises of a single universal standard of measurement.
Money thus allows for the association of different goods and makes them "commensurable".
He goes to on state that money is also useful for future exchange, making it a sort of security.
That is, "if we do not want a thing now, we shall be able to get it when we do want it".
Aristotle's "Rhetoric" proposes that a speaker can use three basic kinds of appeals to persuade his audience: "ethos" (an appeal to the speaker's character), "pathos" (an appeal to the audience's emotion), and "logos" (an appeal to logical reasoning).
He also categorises rhetoric into three genres: epideictic (ceremonial speeches dealing with praise or blame), forensic (judicial speeches over guilt or innocence), and deliberative (speeches calling on an audience to make a decision on an issue).
Aristotle also outlines two kinds of rhetorical proofs: "enthymeme" (proof by syllogism) and "paradeigma" (proof by example).
Aristotle writes in his "Poetics" that epic poetry, tragedy, comedy, dithyrambic poetry, painting, sculpture, music, and dance are all fundamentally acts of "mimesis" ("imitation"), each varying in imitation by medium, object, and manner.
He applies the term "mimesis" both as a property of a work of art and also as the product of the artist's intention and contends that the audience's realisation of the "mimesis" is vital to understanding the work itself.
Aristotle states that "mimesis" is a natural instinct of humanity that separates humans from animals and that all human artistry "follows the pattern of nature".
Because of this, Aristotle believed that each of the mimetic arts possesses what Stephen Halliwell calls "highly structured procedures for the achievement of their purposes."
For example, music imitates with the media of rhythm and harmony, whereas dance imitates with rhythm alone, and poetry with language.
The forms also differ in their object of imitation.
Comedy, for instance, is a dramatic imitation of men worse than average; whereas tragedy imitates men slightly better than average.
Lastly, the forms differ in their manner of imitation – through narrative or character, through change or no change, and through drama or no drama.
While it is believed that Aristotle's "Poetics" originally comprised two books – one on comedy and one on tragedy – only the portion that focuses on tragedy has survived.
Aristotle taught that tragedy is composed of six elements: plot-structure, character, style, thought, spectacle, and lyric poetry.
The characters in a tragedy are merely a means of driving the story; and the plot, not the characters, is the chief focus of tragedy.
Tragedy is the imitation of action arousing pity and fear, and is meant to effect the catharsis of those same emotions.
Aristotle concludes "Poetics" with a discussion on which, if either, is superior: epic or tragic mimesis.
He suggests that because tragedy possesses all the attributes of an epic, possibly possesses additional attributes such as spectacle and music, is more unified, and achieves the aim of its mimesis in shorter scope, it can be considered superior to epic.
Aristotle was a keen systematic collector of riddles, folklore, and proverbs; he and his school had a special interest in the riddles of the Delphic Oracle and studied the fables of Aesop.
Aristotle's analysis of procreation describes an active, ensouling masculine element bringing life to an inert, passive female element.
On this ground, proponents of feminist metaphysics have accused Aristotle of misogyny and sexism.
However, Aristotle gave equal weight to women's happiness as he did to men's, and commented in his "Rhetoric" that the things that lead to happiness need to be in women as well as men.
More than 2300 years after his death, Aristotle remains one of the most influential people who ever lived.
He contributed to almost every field of human knowledge then in existence, and he was the founder of many new fields.
According to the philosopher Bryan Magee, "it is doubtful whether any human being has ever known as much as he did".
Among countless other achievements, Aristotle was the founder of formal logic, pioneered the study of zoology, and left every future scientist and philosopher in his debt through his contributions to the scientific method.
Taneli Kukkonen, writing in "The Classical Tradition", observes that his achievement in founding two sciences is unmatched, and his reach in influencing "every branch of intellectual enterprise" including Western ethical and political theory, theology, rhetoric and literary analysis is equally long.
As a result, Kukkonen argues, any analysis of reality today "will almost certainly carry Aristotelian overtones ... evidence of an exceptionally forceful mind."
Jonathan Barnes wrote that "an account of Aristotle's intellectual afterlife would be little less than a history of European thought".
Aristotle's pupil and successor, Theophrastus, wrote the "History of Plants", a pioneering work in botany.
Some of his technical terms remain in use, such as carpel from "carpos", fruit, and pericarp, from "pericarpion", seed chamber.
Theophrastus was much less concerned with formal causes than Aristotle was, instead pragmatically describing how plants functioned.
The immediate influence of Aristotle's work was felt as the Lyceum grew into the Peripatetic school.
Aristotle's notable students included Aristoxenus, Dicaearchus, Demetrius of Phalerum, Eudemos of Rhodes, Harpalus, Hephaestion, Mnason of Phocis, Nicomachus, and Theophrastus.
Aristotle's influence over Alexander the Great is seen in the latter's bringing with him on his expedition a host of zoologists, botanists, and researchers.
He had also learned a great deal about Persian customs and traditions from his teacher.
Although his respect for Aristotle was diminished as his travels made it clear that much of Aristotle's geography was clearly wrong, when the old philosopher released his works to the public, Alexander complained "Thou hast not done well to publish thy acroamatic doctrines; for in what shall I surpass other men if those doctrines wherein I have been trained are to be all men's common property?"
After Theophrastus, the Lyceum failed to produce any original work.
Though interest in Aristotle's ideas survived, they were generally taken unquestioningly.
It is not until the age of Alexandria under the Ptolemies that advances in biology can be again found.
The first medical teacher at Alexandria, Herophilus of Chalcedon, corrected Aristotle, placing intelligence in the brain, and connected the nervous system to motion and sensation.
Herophilus also distinguished between veins and arteries, noting that the latter pulse while the former do not.
Though a few ancient atomists such as Lucretius challenged the teleological viewpoint of Aristotelian ideas about life, teleology (and after the rise of Christianity, natural theology) would remain central to biological thought essentially until the 18th and 19th centuries.
Ernst Mayr states that there was "nothing of any real consequence in biology after Lucretius and Galen until the Renaissance."
Greek Christian scribes played a crucial role in the preservation of Aristotle by copying all the extant Greek language manuscripts of the corpus.
The first Greek Christians to comment extensively on Aristotle were Philoponus, Elias, and David in the sixth century, and Stephen of Alexandria in the early seventh century.
John Philoponus stands out for having attempted a fundamental critique of Aristotle's views on the eternity of the world, movement, and other elements of Aristotelian thought.
Philoponus questioned Aristotle's teaching of physics, noting its flaws and introducing the theory of impetus to explain his observations.
After a hiatus of several centuries, formal commentary by Eustratius and Michael of Ephesus reappeared in the late eleventh and early twelfth centuries, apparently sponsored by Anna Comnena.
Aristotle was one of the most revered Western thinkers in early Islamic theology.
Most of the still extant works of Aristotle, as well as a number of the original Greek commentaries, were translated into Arabic and studied by Muslim philosophers, scientists and scholars.
Averroes, Avicenna and Alpharabius, who wrote on Aristotle in great depth, also influenced Thomas Aquinas and other Western Christian scholastic philosophers.
Alkindus considered Aristotle the outstanding and unique representative of philosophy and Averroes spoke of Aristotle as the "exemplar" for all future philosophers.
Medieval Muslim scholars regularly described Aristotle as the "First Teacher".
The title "teacher" was first given to Aristotle by Muslim scholars, and was later used by Western philosophers (as in the famous poem of Dante) who were influenced by the tradition of Islamic philosophy.
In accordance with the Greek theorists, the Muslims considered Aristotle to be a dogmatic philosopher, the author of a closed system, and believed that Aristotle shared with Plato essential tenets of thought.
Some went so far as to credit Aristotle himself with neo-Platonic metaphysical ideas.
With the loss of the study of ancient Greek in the early medieval Latin West, Aristotle was practically unknown there from c. AD 600 to c.
1100 except through the Latin translation of the "Organon" made by Boethius.
In the twelfth and thirteenth centuries, interest in Aristotle revived and Latin Christians had translations made, both from Arabic translations, such as those by Gerard of Cremona, and from the original Greek, such as those by James of Venice and William of Moerbeke.
After the Scholastic Thomas Aquinas wrote his "Summa Theologica", working from Moerbeke's translations and calling Aristotle "The Philosopher", the demand for Aristotle's writings grew, and the Greek manuscripts returned to the West, stimulating a revival of Aristotelianism in Europe that continued into the Renaissance.
These thinkers blended Aristotelian philosophy with Christianity, bringing the thought of Ancient Greece into the Middle Ages.
Scholars such as Boethius, Peter Abelard, and John Buridan worked on Aristotelian logic.
The medieval English poet Chaucer describes his student as being happy by having

<poem>at his beddes heed
Twenty bookes, clad in blak or reed,
Of aristotle and his philosophie,</poem>

A cautionary medieval tale held that Aristotle advised his pupil Alexander to avoid the king's seductive mistress, Phyllis, but was himself captivated by her, and allowed her to ride him.
Phyllis had secretly told Alexander what to expect, and he witnessed Phyllis proving that a woman's charms could overcome even the greatest philosopher's male intellect.
Artists such as Hans Baldung produced a series of illustrations of the popular theme.
The Italian poet Dante says of Aristotle in "The Divine Comedy":

In the Early Modern period, scientists such as William Harvey in England and Galileo Galilei in Italy reacted against the theories of Aristotle and other classical era thinkers like Galen, establishing new theories based to some degree on observation and experiment.
Harvey demonstrated the circulation of the blood, establishing that the heart functioned as a pump rather than being the seat of the soul and the controller of the body's heat, as Aristotle thought.
Galileo used more doubtful arguments to displace Aristotle's physics, proposing that bodies all fall at the same speed whatever their weight.
The 19th-century German philosopher Friedrich Nietzsche has been said to have taken nearly all of his political philosophy from Aristotle.
Aristotle rigidly separated action from production, and argued for the deserved subservience of some people ("natural slaves"), and the natural superiority (virtue, "arete") of others.
It was Martin Heidegger, not Nietzsche, who elaborated a new interpretation of Aristotle, intended to warrant his deconstruction of scholastic and philosophical tradition.
The English mathematician George Boole fully accepted Aristotle's logic, but decided "to go under, over, and beyond" it with his system of algebraic logic in his 1854 book "The Laws of Thought".
This gives logic a mathematical foundation with equations, enables it to solve equations as well as check validity, and allows it to handle a wider class of problems by expanding propositions of any number of terms, not just two.
During the 20th century, Aristotle's work was widely criticised.
The philosopher Bertrand Russell
argued that "almost every serious intellectual advance has had to begin with an attack on some Aristotelian doctrine".
Russell called Aristotle's ethics "repulsive", and labelled his logic "as definitely antiquated as Ptolemaic astronomy".
Russell stated that these errors made it difficult to do historical justice to Aristotle, until one remembered what an advance he made upon all of his predecessors.
In 1985, the biologist Peter Medawar could still state in "pure seventeenth century" tones that Aristotle had assembled "a strange and generally speaking rather tiresome farrago of hearsay, imperfect observation, wishful thinking and credulity amounting to downright gullibility".
By the start of the 21st century, however, Aristotle was taken more seriously: Kukkonen noted that "In the best 20th-century scholarship Aristotle comes alive as a thinker wrestling with the full weight of the Greek philosophical tradition."
Ayn Rand accredited Aristotle as "the greatest philosopher in history" and cited him as a major influence on her thinking.
More recently, Alasdair MacIntyre has attempted to reform what he calls the Aristotelian tradition in a way that is anti-elitist and capable of disputing the claims of both liberals and Nietzscheans.
Kukkonen observed, too, that "that most enduring of romantic images, Aristotle tutoring the future conqueror Alexander" remained current, as in the 2004 film "Alexander", while the "firm rules" of Aristotle's theory of drama have ensured a role for the "Poetics" in Hollywood.
Biologists continue to be interested in Aristotle's thinking.
Armand Marie Leroi has reconstructed Aristotle's biology, while Niko Tinbergen's four questions, based on Aristotle's four causes, are used to analyse animal behaviour; they examine function, phylogeny, mechanism, and ontogeny.
The works of Aristotle that have survived from antiquity through medieval manuscript transmission are collected in the Corpus Aristotelicum.
These texts, as opposed to Aristotle's lost works, are technical philosophical treatises from within Aristotle's school.
Reference to them is made according to the organisation of Immanuel Bekker's Royal Prussian Academy edition ("Aristotelis Opera edidit Academia Regia Borussica", Berlin, 1831–1870), which in turn is based on ancient classifications of these works.
Aristotle wrote his works on papyrus scrolls, the common writing medium of that era.
His writings are divisible into two groups: the "exoteric", intended for the public, and the "esoteric", for use within the Lyceum school.
Aristotle's "lost" works stray considerably in characterisation from the surviving Aristotelian corpus.
Whereas the lost works appear to have been originally written with a view to subsequent publication, the surviving works mostly resemble lecture notes not intended for publication.
Cicero's description of Aristotle's literary style as "a river of gold" must have applied to the published works, not the surviving notes.
A major question in the history of Aristotle's works is how the exoteric writings were all lost, and how the ones we now possess came to us.
The consensus is that Andronicus of Rhodes collected the esoteric works of Aristotle's school which existed in the form of smaller, separate works, distinguished them from those of Theophrastus and other Peripatetics, edited them, and finally compiled them into the more cohesive, larger works as they are known today.
Aristotle has been depicted by major artists including Lucas Cranach the Elder, Justus van Gent, Raphael, Paolo Veronese, Jusepe de Ribera, Rembrandt, and Francesco Hayez over the centuries.
Among the best-known is Raphael's fresco "The School of Athens", in the Vatican's Apostolic Palace, where the figures of Plato and Aristotle are central to the image, at the architectural vanishing point, reflecting their importance.
Rembrandt's "Aristotle with a Bust of Homer", too, is a celebrated work, showing the knowing philosopher and the blind Homer from an earlier age: as the art critic Jonathan Jones writes, "this painting will remain one of the greatest and most mysterious in the world, ensnaring us in its musty, glowing, pitch-black, terrible knowledge of time."
The Aristotle Mountains in Antarctica are named after Aristotle.
He was the first person known to conjecture, in his book "Meteorology", the existence of a landmass in the southern high-latitude region and called it "Antarctica".
Aristoteles is a crater on the Moon bearing the classical form of Aristotle's name.
The secondary literature on Aristotle is vast.
The following references are only a small selection.
</doc>
<doc id="309" url="https://en.wikipedia.org/wiki?curid=309" title="An American in Paris">
An American in Paris

An American in Paris is a jazz-influenced orchestral piece by the American composer George Gershwin, written in 1928.
Inspired by the time Gershwin had spent in Paris, it evokes the sights and energy of the French capital in the 1920s and is one of his best-known compositions.
Gershwin composed "An American in Paris" on commission from the conductor Walter Damrosch.
He scored the piece for the standard instruments of the symphony orchestra plus celesta, saxophones, and automobile horns.
He brought back some Parisian taxi horns for the New York premiere of the composition, which took place on December 13, 1928, in Carnegie Hall, with Damrosch conducting the New York Philharmonic.
Gershwin completed the orchestration on November 18, less than four weeks before the work's premiere.
Gershwin collaborated on the original program notes with the critic and composer Deems Taylor, noting that: "My purpose here is to portray the impression of an American visitor in Paris as he strolls about the city and listens to various street noises and absorbs the French atmosphere."
When the tone poem moves into the blues, "our American friend ... has succumbed to a spasm of homesickness."
But, "nostalgia is not a fatal disease."
The American visitor "once again is an alert spectator of Parisian life" and "the street noises and French atmosphere are triumphant."
Gershwin was attracted by Maurice Ravel's unusual chords.
Upon Gershwin's request, Ravel accepted him as a student, and Gershwin went on his first trip to Paris in 1926 ready to study.
After his initial student audition with Ravel turned into a sharing of musical theories, Ravel said he couldn't teach him but he would send a letter referring him to Nadia Boulanger.
While the studies were cut short, that 1926 trip resulted in the initial version of An American in Paris written as a 'thank you note' to Gershwin's hosts, Robert and Mabel Shirmer.
Gershwin called it "a rhapsodic ballet"; it is written freely and in a much more modern idiom than his prior works.
Gershwin strongly encouraged Ravel to come to the United States for a tour.
To this end, upon his return to New York, Gershwin joined the efforts of Ravel's friend Robert Schmitz, a pianist Ravel had met during the War, to urge Ravel to tour the U.S.
Schmitz was the head of Pro Musica, promoting Franco-American musical relations, and was able to offer Ravel a $12,000 fee for the tour, an enticement Gershwin knew would be important to Ravel.
Gershwin greeted Ravel in New York in February 1928 at the start of Ravel's U.S.
Tour, and joined Ravel again later in the tour in Los Angeles.
After a lunch together with Chaplin in Beverly Hills, Ravel was persuaded to perform an unscheduled 'house concert' in a friend's music salon, performing among kindred spirits.
Ravel's tour reignited Gershwin's desire to return to Paris which he did in March 1928.
Ravel's high praise of Gershwin in an introductory letter to Boulanger caused Gershwin to seriously consider taking much more time to study abroad in Paris.
Yet after playing for her, she told him she could not teach him.
Nadia Boulanger gave Gershwin basically the same advice she gave all of her accomplished master students: "Don't copy others; be yourself."
In this case, "Why try to be a second rate Ravel when you are already a first rate Gershwin?"
This did not set Gershwin back, as his real intent abroad was to complete a new work based on Paris and perhaps a second rhapsody for piano and orchestra to follow his "Rhapsody in Blue".
Paris at this time hosted many expatriate writers, among them Ezra Pound, W. B. Yeats, Ernest Hemingway; and artist Pablo Picasso.
Gershwin based "An American in Paris" on a melodic fragment called "Very Parisienne", written in 1926 on his first visit to Paris as a gift to his hosts, Robert and Mabel Schirmer.
He described the piece as a "rhapsodic ballet" because it was written freely and is more modern than his previous works.
Gershwin explained in "Musical America", "My purpose here is to portray the impressions of an American visitor in Paris as he strolls about the city, listens to the various street noises, and absorbs the French atmosphere."
The piece is structured into five sections, which culminate in a loose ABA format.
Gershwin's first A episode introduces the two main "walking" themes in the "Allegretto grazioso" and develops a third theme in the "Subito con brio".
The style of this A section is written in the typical French style of composers Claude Debussy and Les Six.
This A section featured duple meter, singsong rhythms, and diatonic melodies with the sounds of oboe, English horn, and taxi horns.
The B section's "Andante ma con ritmo deciso" introduces the American Blues and spasms of homesickness.
The "Allegro" that follows continues to express homesickness in a faster twelve-bar blues.
In the B section, Gershwin uses common time, syncopated rhythms, and bluesy melodies with the sounds of trumpet, saxophone, and snare drum.
"Moderato con grazia" is the last A section that returns to the themes set in A. After recapitulating the "walking" themes, Gershwin overlays the slow blues theme from section B in the final “Grandioso.”

"An American in Paris" is scored for 3 flutes (3rd doubling on piccolo), 2 oboes, English horn, 2 clarinets in B-flat, bass clarinet in B-flat, 2 bassoons, 4 horns in F, 3 trumpets in B-flat, 3 trombones, tuba, timpani, snare drum, bass drum, triangle, wood block, cymbals, low and high tom-toms, xylophone, glockenspiel, celesta, 4 taxi horns labeled as A, B, C and D with circles around them, alto saxophone/soprano saxophone, tenor saxophone/soprano saxophone/alto saxophone, baritone saxophone/soprano saxophone/alto saxophone, and strings.
Although most modern audiences have heard the taxi horns using the notes A, B, C and D, it has recently come to light that Gershwin's intention was to have used the notes A, B, D, and A. It is likely that in labeling the taxi horns as A, B, C and D with circles, he may have been referring to the use of the four different horns and not the notes that they played.
The revised edition by F. Campbell-Watson calls for three saxophones, alto, tenor and baritone.
In this arrangement the soprano and alto doublings have been rewritten to avoid changing instruments.
In 2000, Gershwin specialist Jack Gibbons made his own restoration of the original orchestration of An American in Paris, working directly from Gershwin's original manuscript, including the restoration of Gershwin's soprano saxophone parts removed in F. Campbell-Watson's revision; Gibbons' restored orchestration of An American in Paris was performed at London's Queen Elizabeth Hall on July 9, 2000 by the City of Oxford Orchestra conducted by Levon Parikian

William Daly arranged the score for piano solo which was published by New World Music in 1929.
Gershwin did not particularly like Walter Damrosch's interpretation at the world premiere of "An American in Paris".
He stated that Damrosch's sluggish, dragging tempo caused him to walk out of the hall during a matinee performance of this work.
The audience, according to Edward Cushing, responded with "a demonstration of enthusiasm impressively genuine in contrast to the conventional applause which new music, good and bad, ordinarily arouses."
Critics believed that "An American in Paris" was better crafted than his lukewarm Concerto in F. Some did not think it belonged in a program with classical composers César Franck, Richard Wagner, or Guillaume Lekeu on its premiere.
Gershwin responded to the critics, "It's not a Beethoven Symphony, you know... It's a humorous piece, nothing solemn about it.
It's not intended to draw tears.
If it pleases symphony audiences as a light, jolly piece, a series of impressions musically expressed, it succeeds."
On September 22, 2013, it was announced that a musicological critical edition of the full orchestral score will be eventually released.
The Gershwin family, working in conjunction with the Library of Congress and the University of Michigan, are working to make scores available to the public that represent Gershwin's true intent.
It is unknown if the critical score will include the four minutes of material Gershwin later deleted from the work (such as the restatement of the blues theme after the faster 12 bar blues section), or if the score will document changes in the orchestration during Gershwin's composition process.
The score to "An American in Paris" is currently scheduled to be issued first in a series of scores to be released.
The entire project may take 30 to 40 years to complete, but "An American in Paris" will be an early volume in the series.
Two urtext editions of the work have been published by the German publisher B-Note Music in 2015.
The changes made by Campbell-Watson have been withdrawn in both editions.
In the extended urtext, 120 bars of music have been re-integrated.
Conductor Walter Damrosch had cut them shortly before the first performance.
"An American in Paris" has been frequently recorded.
The first recording was made for RCA Victor in 1929 with Nathaniel Shilkret conducting the RCA Victor Symphony Orchestra, drawn from members of the Philadelphia Orchestra.
Gershwin was on hand to "supervise" the recording; however, Shilkret was reported to be in charge and eventually asked the composer to leave the recording studio.
Then, a little later, Shilkret discovered there was no one to play the brief celesta solo during the slow section, so he hastily asked Gershwin if he might play the solo; Gershwin said he could and so he briefly participated in the actual recording.
This recording is believed to use the taxi horns in the way that Gershwin had intended using the notes A flat, B flat, a higher C and a lower D. The radio broadcast of the September 8, 1937 Hollywood Bowl George Gershwin Memorial Concert, in which "An American in Paris," also conducted by Shilkret, was second on the program, was recorded and was released in 1998 in a two-CD set.
Arthur Fiedler and the Boston Pops Orchestra recorded the work for RCA Victor, including one of the first stereo recordings of the music.
In 1945, Arturo Toscanini conducting the NBC Symphony Orchestra recorded the piece for RCA Victor, one of the few commercial recordings Toscanini made of music by an American composer.
The Seattle Symphony also recorded a version in 1990 of Gershwin's original score, before he made numerous edits resulting in the score as we hear it today.
Harry James released a version of the blues section on his 1953 album "One Night Stand," recorded live at the Aragon Ballroom in Chicago (Columbia GL 522 and CL 522).
In 1951, Metro-Goldwyn-Mayer released the musical film "An American in Paris", featuring Gene Kelly and Leslie Caron.
Winning the 1951 Best Picture Oscar and numerous other awards, the film was directed by Vincente Minnelli, featured many tunes of Gershwin, and concluded with an extensive, elaborate dance sequence built around the "An American in Paris" symphonic poem (arranged for the film by Johnny Green), costing $500,000.
</doc>
<doc id="316" url="https://en.wikipedia.org/wiki?curid=316" title="Academy Award for Best Production Design">
Academy Award for Best Production Design

The Academy Award for Best Production Design recognizes achievement for art direction in film.
The category's original name was Best Art Direction, but was changed to its current name in 2012 for the 85th Academy Awards.
This change resulted from the Art Director's branch of the Academy of Motion Picture Arts and Sciences (AMPAS) being renamed the Designer's branch.
Since 1947, the award is shared with the set decorator(s).
It is awarded to the best interior design in a film.
The films below are listed with their production year (for example, the 2000 Academy Award for Best Art Direction is given to a film from 1999).
In the lists below, the winner of the award for each year is shown first, followed by the other nominees.
</doc>
<doc id="324" url="https://en.wikipedia.org/wiki?curid=324" title="Academy Awards">
Academy Awards

The Academy Awards, also known as the Oscars, are a set of 24 awards for artistic and technical merit in the film industry, given annually by the Academy of Motion Picture Arts and Sciences (AMPAS), to recognize excellence in cinematic achievements as assessed by the Academy's voting membership.
The various category winners are awarded a copy of a golden statuette, officially called the "Academy Award of Merit", although more commonly referred to by its nickname "Oscar".
The award was originally sculpted by George Stanley from a design sketch by Cedric Gibbons.
AMPAS first presented it in 1929 at a private dinner hosted by Douglas Fairbanks in the Hollywood Roosevelt Hotel.
The Academy Awards ceremony was first broadcast on radio in 1930 and televised for the first time in 1953.
It is the oldest worldwide entertainment awards ceremony and is now seen live in more than 200 countries, including being streamed live online.
Its equivalents – the Emmy Awards for television, the Tony Awards for theater, and the Grammy Awards for music – are modeled after the Academy Awards.
The 90th Academy Awards ceremony, honoring the best films of 2017, was held on 4 March 2018, at the Dolby Theatre, in Los Angeles, California.
The ceremony was hosted by Jimmy Kimmel and was broadcast on ABC.
A total of 3,072 Oscars have been awarded from the inception of the award through the 90th.
The first Academy Awards presentation was held on 16 May 1929, at a private dinner function at the Hollywood Roosevelt Hotel with an audience of about 270 people.
The post-awards party was held at the Mayfair Hotel.
The cost of guest tickets for that night's ceremony was $5 ($ in dollars).
Fifteen statuettes were awarded, honoring artists, directors and other participants in the film-making industry of the time, for their works during the 1927–28 period.
The ceremony ran for 15 minutes.
Winners were announced to media three months earlier.
That was changed for the second ceremony in 1930.
Since then, for the rest of the first decade, the results were given to newspapers for publication at 11:00 pm on the night of the awards.
This method was used until an occasion when the "Los Angeles Times" announced the winners before the ceremony began; as a result, the Academy has, since 1941, used a sealed envelope to reveal the name of the winners.
The first Best Actor awarded was Emil Jannings, for his performances in "The Last Command" and "The Way of All Flesh".
He had to return to Europe before the ceremony, so the Academy agreed to give him the prize earlier; this made him the first Academy Award winner in history.
At that time, the winners were recognized for all of their work done in a certain category during the qualifying period; for example, Jannings received the award for two movies in which he starred during that period, and Janet Gaynor later won a single Oscar for performances in three films.
With the fourth ceremony, however, the system changed, and professionals were honored for a specific performance in a single film.
For the first six ceremonies, the eligibility period spanned two calendar years.
At the 29th ceremony, held on 27 March 1957, the Best Foreign Language Film category was introduced.
Until then, foreign-language films had been honored with the Special Achievement Award.
The 74th Academy Awards, held in 2002, presented the first Academy Award for Best Animated Feature.
Since 1973, all Academy Awards ceremonies have ended with the Academy Award for Best Picture.
Traditionally, the previous year's winner for Best Actor and Best Supporting Actor present the awards for Best Actress and Best Supporting Actress, while the previous year's winner for Best Actress and Best Supporting Actress present the awards for Best Actor and Best Supporting Actor.
In addition to the Academy Award of Merit (Oscar award), there are nine honorary (non-competitive) awards presented by the Academy from time to time (except for the Academy Honorary Award, the Technical Achievement Award, and the Student Academy Awards, which are presented annually):

The Academy also awards Nicholl Fellowships in Screenwriting.
The best known award is the Academy Award of Merit, more popularly known as the Oscar statuette.
Made of gold-plated bronze on a black metal base, it is 13.5 in (34.3 cm) tall, weighs 8.5 lb (3.856 kg), and depicts a knight rendered in Art Deco style holding a crusader's sword standing on a reel of film with five spokes.
The five spokes represent the original branches of the Academy: Actors, Writers, Directors, Producers, and Technicians.
The model for the statuette is said to be Mexican actor Emilio "El Indio" Fernández.
Sculptor George Stanley (who also did the Muse Fountain at the Hollywood Bowl) sculpted Cedric Gibbons' design.
The statuettes presented at the initial ceremonies were gold-plated solid bronze.
Within a few years the bronze was abandoned in favor of Britannia metal, a pewter-like alloy which is then plated in copper, nickel silver, and finally, 24-karat gold.
Due to a metal shortage during World War II, Oscars were made of painted plaster for three years.
Following the war, the Academy invited recipients to redeem the plaster figures for gold-plated metal ones.
The only addition to the Oscar since it was created is a minor streamlining of the base.
The original Oscar mold was cast in 1928 at the C.W.
Shumway & Sons Foundry in Batavia, Illinois, which also contributed to casting the molds for the Vince Lombardi Trophy and Emmy Award's statuettes.
From 1983 to 2015, approximately 50 Oscars in a tin alloy with gold plating were made each year in Chicago by Illinois manufacturer R.S.
Owens & Company.
It takes between three and four weeks to manufacture 50 statuettes.
In 2016, the Academy returned to bronze as the core metal of the statuettes, handing manufacturing duties to Walden, New York-based Polich Tallix Fine Art Foundry.
While based on a digital scan of an original 1929 Oscar, the statuettes retain their modern-era dimensions and black pedestal.
Cast in liquid bronze from 3D-printed ceramic molds and polished, they are then electroplated in 24-karat gold by Brooklyn, New York–based Epner Technology.
The time required to produce 50 such statuettes is roughly three months.
R.S.
Owens is expected to continue producing other awards for the Academy and service existing Oscars that need replating.
The origin of the name "Oscar" is disputed.
One biography of Bette Davis, who was a president of the Academy, claims she named the Oscar after her first husband, band leader Harmon Oscar Nelson.
Another claimed origin is the Academy's Executive Secretary, Margaret Herrick, first saw the award in 1931 and made reference to the statuette's reminding her of her "Uncle Oscar" (a nickname for her cousin Oscar Pierce).
Columnist Sidney Skolsky was present during Herrick's naming and seized the name in his byline, "Employees have affectionately dubbed their famous statuette 'Oscar'."
One of the earliest mentions of the term "Oscar" dates to a "Time" magazine article about the 1934 6th Academy Awards.
Walt Disney also thanked the Academy for his Oscar as early as 1932.
The trophy officially received the name "Oscar" in 1939 by the Academy of Motion Picture Arts and Sciences.
To prevent information identifying the Oscar winners from leaking ahead of the ceremony, Oscar statuettes presented at the ceremony have blank baseplates.
Until 2010, winners returned their statuettes to the Academy, and had to wait several weeks to have their names inscribed on their respective Oscars.
Since 2010, winners have had the option of having engraved nameplates applied to their statuettes at an inscription-processing station at the Governor's Ball, a party held immediately after the Oscar ceremony.
The R.S.
Owens company has engraved nameplates made before the ceremony, bearing the name of every potential winner.
The nameplates for the non-winning nominees are later recycled.
Since 1950, the statuettes have been legally encumbered by the requirement that neither winners nor their heirs may sell the statuettes without first offering to sell them back to the Academy for US$1.
If a winner refuses to agree to this stipulation, then the Academy keeps the statuette.
Academy Awards not protected by this agreement have been sold in public auctions and private deals for six-figure sums.
In December 2011, Orson Welles' 1941 Oscar for "Citizen Kane" (Academy Award for Best Original Screenplay) was put up for auction, after his heirs won a 2004 court decision contending that Welles did not sign any agreement to return the statue to the Academy.
On 20 December 2011, it sold in an online auction for US$861,542.
In 1992, Harold Russell needed money for his wife's medical expenses.
In a controversial decision, he consigned his 1946 Oscar for Best Supporting Actor for "The Best Years of Our Lives" to Herman Darvick Autograph Auctions, and on 6 August 1992, in New York City, the Oscar sold to a private collector for $60,500.
Since he won the award before 1950, he was not required to offer it to the Academy first.
Russell defended his decision, saying, "I don't know why anybody would be critical.
My wife's health is much more important than sentimental reasons.
The movie will be here, even if Oscar isn't."
Harold Russell is the only Academy Award-winning actor to ever sell an Oscar.
While the Oscar is owned by the recipient, it is essentially not on the open market.
Michael Todd's grandson tried to sell Todd's Oscar statuette to a movie prop collector in 1989, but the Academy won the legal battle by getting a permanent injunction.
Although some Oscar sales transactions have been successful, some buyers have subsequently returned the statuettes to the Academy, which keeps them in its treasury.
Since 2004, Academy Award nomination results have been announced to the public in mid-January.
Prior to that, the results were announced in early February.
The Academy of Motion Picture Arts and Sciences (AMPAS), a professional honorary organization, maintains a voting membership of over 8,000 .
Academy membership is divided into different branches, with each representing a different discipline in film production.
Actors constitute the largest voting bloc, numbering 1,311 members (22 percent) of the Academy's composition.
Votes have been certified by the auditing firm PricewaterhouseCoopers (and its predecessor Price Waterhouse) for the past 83 annual awards ceremonies.
The firm mails the ballots of eligible nominees to members of the Academy in December to reflect the previous eligible year with a due date sometime in January of the next year, then tabulates the votes in a process that takes thousands of hours.
All AMPAS members must be invited to join by the Board of Governors, on behalf of Academy Branch Executive Committees.
Membership eligibility may be achieved by a competitive nomination or a member may submit a name based on other significant contributions to the field of motion pictures.
New membership proposals are considered annually.
The Academy does not publicly disclose its membership, although as recently as 2007 press releases have announced the names of those who have been invited to join.
The 2007 release also stated that it has just under 6,000 voting members.
While the membership had been growing, stricter policies have kept its size steady since then.
In 2012, the results of a study conducted by the "Los Angeles Times" were published describing the demographic breakdown of approximately 88% of AMPAS' voting membership.
Of the 5,100+ active voters confirmed, 94% were Caucasian, 77% were male, and 54% were found to be over the age of 60.
33% of voting members are former nominees (14%) and winners (19%).
In May 2011, the Academy sent a letter advising its 6,000 or so voting members that an online system for Oscar voting would be implemented in 2013.
According to Rules 2 and 3 of the official Academy Awards Rules, a film must open in the previous calendar year, from midnight at the start of 1 January to midnight at the end of 31 December, in Los Angeles County, California, and play for seven consecutive days, to qualify (except for the Best Foreign Language Film, Best Documentary Feature, and Best Documentary Short Subject).
The Best Foreign Language Film award does not require a U.S.
release.
The Best Documentary Feature award requires week-long releases in "both" Los Angeles County "and" New York City during the previous calendar year.
The Best Documentary Short Subject award has noticeably different eligibility rules from most other competitive awards.
First, the qualifying period for release does not coincide with a calendar year, instead covering a one-year period starting on 1 September and ending on 31 August of the calendar year before the ceremony.
Second, there are multiple methods of qualification.
The main method is a week-long theatrical release in "either" Los Angeles County "or" New York City during the eligibility period.
Films also can qualify by winning specified awards at one of a number of competitive film festivals designated by the Academy.
Finally, a film that is selected as a gold, silver, or bronze medal winner in the Documentary category of the immediately previous Student Academy Awards is also eligible.
For example, the 2009 Best Picture winner, "The Hurt Locker", was actually first released in 2008, but did not qualify for the 2008 awards as it did not play its Oscar-qualifying run in Los Angeles until mid-2009, thus qualifying for the 2009 awards.
Foreign films must include English subtitles, and each country can submit only one film per year.
Rule 2 states that a film must be feature-length, defined as a minimum of 40 minutes, except for short-subject awards, and it must exist either on a 35 mm or 70 mm film print or in 24 frame/s or 48 frame/s progressive scan digital cinema format with a minimum projector resolution of 2048 by 1080 pixels.
Effective with the 90th Academy Awards, presented in 2018, multi-part and limited series will be ineligible for the Best Documentary Feature award.
This followed the win of "", an eight-hour presentation that was screened in a limited release before being broadcast in five parts on ABC and ESPN, in that category in 2017.
The Academy's announcement of the new rule made no direct mention of that film.
Producers must submit an Official Screen Credits online form before the deadline; in case it is not submitted by the defined deadline, the film will be ineligible for Academy Awards in any year.
The form includes the production credits for all related categories.
Then, each form is checked and put in a Reminder List of Eligible Releases.
In late December ballots and copies of the Reminder List of Eligible Releases are mailed to around 6,000 active members.
For most categories, members from each of the branches vote to determine the nominees only in their respective categories (i.e. only directors vote for directors, writers for writers, actors for actors, etc.).
In the special case of Best Picture, all voting members are eligible to select the nominees.
In all major categories, a variant of the single transferable vote is used, with each member casting a ballot with up to five nominees (ten for Best Picture) ranked preferentially.
In certain categories, including Foreign Film, Documentary and Animated Feature, nominees are selected by special screening committees made up of members from all branches.
In most categories the winner is selected from among the nominees by plurality voting of all members.
Since 2009, the Best Picture winner has been chosen by instant runoff voting.
Since 2013, re-weighted range voting has been used to select the nominees for the Best Visual Effects.
Film companies will spend as much as several million dollars on marketing to awards voters for a movie in the running for Best Picture, in attempts to improve chances of receiving Oscars and other movie awards conferred in Oscar season.
The Academy enforces rules to limit overt campaigning by its members so as to try to eliminate excesses and prevent the process from becoming undignified.
It has an awards czar on staff who advises members on allowed practices and levies penalties on offenders.
For example, a producer of the 2009 Best Picture nominee "The Hurt Locker" was disqualified as a producer in the category when he contacted associates urging them to vote for his film and not another that was seen as the front-runner ("The Hurt Locker" eventually won).
The major awards are presented at a live televised ceremony, commonly in late February or early March following the relevant calendar year, and six weeks after the announcement of the nominees.
It is the culmination of the film awards season, which usually begins during November or December of the previous year.
This is an elaborate extravaganza, with the invited guests walking up the red carpet in the creations of the most prominent fashion designers of the day.
Black tie dress is the most common outfit for men, although fashion may dictate not wearing a bow-tie, and musical performers sometimes do not adhere to this.
(The artists who recorded the nominees for Best Original Song quite often perform those songs live at the awards ceremony, and the fact that they are performing is often used to promote the television broadcast.)
The Academy Awards is the only awards show televised live in the United States (excluding Hawaii), Canada, and the United Kingdom, and gathers millions of viewers elsewhere throughout the world.
The Oscars were first televised in 1953 by NBC, which continued to broadcast the event until 1960, when ABC took over, televising the festivities (including the first color broadcast of the event in 1966) through 1970.
NBC regained the rights for five years then ABC resumed broadcast duties in 1976 and its current contract with the Academy runs through 2028.
The Academy has also produced condensed versions of the ceremony for broadcast in international markets (especially those outside of the Americas) in more desirable local timeslots.
The ceremony was broadcast live internationally for the first time via satellite since 1970, but only two South American countries, Chile and Brazil, purchased the rights to air the broadcast.
By that time, the television rights to the Academy Awards had been sold in 50 countries.
A decade later, the rights were already being sold to 60 countries, and by 1984, the TV rights to the Awards were licensed in 76 countries.
The ceremonies were moved up from late March/early April to late February since 2004 to help disrupt and shorten the intense lobbying and ad campaigns associated with Oscar season in the film industry.
Another reason was because of the growing TV ratings success coinciding with the NCAA Basketball Tournament, which would cut into the Academy Awards audience.
(In 1976 and 1977, ABC's regained Oscars were moved from Tuesday to Monday and went directly opposite NBC's NCAA title game.)
The earlier date is also to the advantage of ABC, as it now usually occurs during the highly profitable and important February sweeps period.
Some years, the ceremony is moved into first Sunday of March in order to avoid clash with the Winter Olympic Games.
Another reason for the move to late February and early March is also to avoid the awards ceremony occurring so close to the religious holidays of Passover and Easter, which for decades had been a grievance from members and the general public.
Advertising is somewhat restricted, however, as traditionally no movie studios or competitors of official Academy Award sponsors may advertise during the telecast.
The production of the Academy Awards telecast currently holds the distinction of winning the most Emmys in history, with 47 wins and 195 nominations overall since that award's own launch in 1949.
After many years of being held on Mondays at 9:00 pm Eastern/6:00 p.m Pacific, since the 1999 ceremonies, it was moved to Sundays at 8:30 pm ET/5:30 pm PT.
The reasons given for the move were that more viewers would tune in on Sundays, that Los Angeles rush-hour traffic jams could be avoided, and an earlier start time would allow viewers on the East Coast to go to bed earlier.
For many years the film industry opposed a Sunday broadcast because it would cut into the weekend box office.
In 2010, the Academy contemplated moving the ceremony even further back into January, citing TV viewers' fatigue with the film industry's long awards season.
However, such an accelerated schedule would dramatically decrease the voting period for its members, to the point where some voters would only have time to view the contending films streamed on their computers (as opposed to traditionally receiving the films and ballots in the mail).
Furthermore, a January ceremony on Sunday would clash with National Football League playoff games.
Originally scheduled for 8 April 1968, the 40th Academy Awards ceremony was postponed for two days, because of the assassination of Dr. Martin Luther King, Jr.. On 30 March 1981, the 53rd Academy Awards was postponed for one day, after the shooting of President Ronald Reagan and others in Washington, D.C.
In 1993, an "In Memoriam" segment was introduced, honoring those who had made a significant contribution to cinema who had died in the preceding 12 months, a selection compiled by a small committee of Academy members.
This segment has drawn criticism over the years for the omission of some names.
Criticism was also levied for many years regarding another aspect, with the segment having a "popularity contest" feel as the audience varied their applause to those who had died by the subject's cultural impact; the applause has since been muted during the telecast, and the audience is discouraged from clapping during the segment and giving silent reflection instead.
In terms of broadcast length, the ceremony generally averages three and a half hours.
The first Oscars, in 1929, lasted 15 minutes.
At the other end of the spectrum, the 2002 ceremony lasted four hours and twenty-three minutes.
In 2010, the organizers of the Academy Awards announced winners' acceptance speeches must not run past 45 seconds.
This, according to organizer Bill Mechanic, was to ensure the elimination of what he termed "the single most hated thing on the show" – overly long and embarrassing displays of emotion.
In 2016, in a further effort to streamline speeches, winners' dedications were displayed on an on-screen ticker.
During the 2018 ceremony, host Jimmy Kimmel acknowledged how long the ceremony had become, by announcing that he would give a brand-new jet ski to whoever gave the shortest speech of the night (a reward won by Mark Bridges when accepting his Best Costume Design award for "Phantom Thread").
Although still dominant in ratings, the viewership of the Academy Awards have steadily dropped; the 88th Academy Awards were the lowest-rated in the past eight years (although with increases in male and 18-49 viewership), while the show itself also faced mixed reception.
Following the show, "Variety" reported that ABC was, in negotiating an extension to its contract to broadcast the Oscars, seeking to have more creative control over the broadcast itself.
Currently and nominally, AMPAS is responsible for most aspects of the telecast, including the choice of production staff and hosting, although ABC is allowed to have some input on their decisions.
In August 2016, AMPAS extended its contract with ABC through 2028: the contract neither contains any notable changes, nor gives ABC any further creative control over the telecast.
Historically, the "Oscarcast" has pulled in a bigger haul when box-office hits are favored to win the Best Picture trophy.
More than 57.25 million viewers tuned to the telecast for the 70th Academy Awards in 1998, the year of "Titanic", which generated close to US$600 million at the North American box office pre-Oscars.
The 76th Academy Awards ceremony in which "" (pre-telecast box office earnings of US$368 million) received 11 Awards including Best Picture drew 43.56 million viewers.
The most watched ceremony based on Nielsen ratings to date, however, was the 42nd Academy Awards (Best Picture "Midnight Cowboy") which drew a 43.4% household rating on 7 April 1970.
By contrast, ceremonies honoring films that have not performed well at the box office tend to show weaker ratings.
The 78th Academy Awards which awarded low-budgeted, independent film "Crash" (with a pre-Oscar gross of US$53.4 million) generated an audience of 38.64 million with a household rating of 22.91%.
In 2008, the 80th Academy Awards telecast was watched by 31.76 million viewers on average with an 18.66% household rating, the lowest rated and least watched ceremony at the time, in spite of celebrating 80 years of the Academy Awards.
The Best Picture winner of that particular ceremony was another independently financed film ("No Country for Old Men").
In 1929, the first Academy Awards were presented at a banquet dinner at the Hollywood Roosevelt Hotel.
From 1930 to 1943, the ceremony alternated between two venues: the Ambassador Hotel on Wilshire Boulevard and the Biltmore Hotel in downtown Los Angeles.
Grauman's Chinese Theatre in Hollywood then hosted the awards from 1944 to 1946, followed by the Shrine Auditorium in Los Angeles from 1947 to 1948.
The 21st Academy Awards in 1949 were held at the Academy Award Theatre at what was the Academy's headquarters on Melrose Avenue in Hollywood.
From 1950 to 1960, the awards were presented at Hollywood's Pantages Theatre.
With the advent of television, the awards from 1953 to 1957 took place simultaneously in Hollywood and New York, first at the NBC International Theatre (1953) and then at the NBC Century Theatre, after which the ceremony took place solely in Los Angeles.
The Oscars moved to the Santa Monica Civic Auditorium in Santa Monica, California in 1961.
By 1969, the Academy decided to move the ceremonies back to Los Angeles, this time to the Dorothy Chandler Pavilion at the Los Angeles County Music Center.
In 2002, the Dolby Theatre (previously known as the Kodak Theatre) became the presentation's current venue.
In the first year of the awards, the Best Directing award was split into two categories (Drama and Comedy).
At times, the Best Original Score award has also been split into separate categories (Drama and Comedy/Musical).
From the 1930s through the 1960s, the Art Direction (now Production Design), Cinematography, and Costume Design awards were likewise split into two categories (black-and-white films and color films).
Prior to 2012, the Production Design award was called Art Direction, while the Makeup and Hairstyling award was called Makeup.
In August 2018, the Academy announced that several categories would not be televised live, but rather be recorded during commercial breaks and aired later in the ceremony.
The Board of Governors meets each year and considers new award categories.
To date, the following proposed categories have been rejected:

The Special Academy Awards are voted on by special committees, rather than by the Academy membership as a whole.
They are not always presented on a consistent annual basis.
Due to the positive exposure and prestige of the Academy Awards, studios spend millions of dollars and hire publicists specifically to promote their films during what is typically called the "Oscar season".
This has generated accusations of the Academy Awards being influenced more by marketing than quality.
William Friedkin, an Academy Award-winning film director and former producer of the ceremony, expressed this sentiment at a conference in New York in 2009, describing it as "the greatest promotion scheme that any industry ever devised for itself".
Tim Dirks, editor of AMC's filmsite.org, has written of the Academy Awards,

Typical criticism of the Academy Awards for Best Picture is that among the winners and nominees there is an over-representation of romantic historical epics, biographical dramas, romantic dramedies, and family melodramas, most of which are released in the U.S.
the last three months of the calendar year.
The Oscars have been infamously known for selecting specific genres of movies to be awarded.
This has led to the coining of the term 'Oscar bait', describing such movies.
This has led at times to more specific criticisms that the Academy is disconnected from the audience, e.g., by favoring 'Oscar bait' over audience favorites, or favoring historical melodramas over critically acclaimed movies that depict current life issues.
The Academy Awards have long received criticism over its lack of diversity among the nominees.
This criticism is based on the statistics from every Academy Awards since 1929 which shows us that only 6.4% of academy award nominees have been non-white and since 1991, 11.2% of nominees have been non-white, with the rate of winners being even more polarizing.
The 88th awards ceremony became the target of a boycott, popularized on social media by the #OscarsSoWhite, based on critics' perception that its all-white acting nominee list reflected bias.
In response, the Academy initiated "historic" changes in membership by the year 2020.
Acting prizes in certain years have been criticized for not recognizing superior performances so much as being awarded for personal popularity or presented as a "career honor" to recognize a distinguished nominee's entire body of work.
Some winners critical of the Academy Awards have boycotted the ceremonies and refused to accept their Oscars.
The first to do so was screenwriter Dudley Nichols (Best Writing in 1935 for "The Informer").
Nichols boycotted the 8th Academy Awards ceremony because of conflicts between the Academy and the Writers' Guild.
Nichols eventually accepted the 1935 award three years later, at the 1938 ceremony.
Nichols was nominated for three further Academy Awards during his career.
George C. Scott became the second person to refuse his award (Best Actor in 1970 for "Patton") at the 43rd Academy Awards ceremony.
Scott described it as a "meat parade", saying "I don't want any part of it."
The third person to refuse the award was Marlon Brando, who refused his award (Best Actor for 1972's "The Godfather") citing the film industry's discrimination and mistreatment of Native Americans.
At the 45th Academy Awards ceremony, Brando sent actress and civil rights activist Sacheen Littlefeather to read a 15-page speech detailing his criticisms.
At the 2017 awards ceremony, Warren Beatty and Faye Dunaway mistakenly announced that "La La Land" was the Best Picture award recipient, instead of "Moonlight", the actual winner.
Beatty had been given the wrong envelope and after hesitating during the announcement, handed the envelope to Dunaway, which listed Emma Stone as Best Actress for "La La Land" and led to the confusion.
The proper winner was announced after the acceptance speeches by "La La Land" producers Jordan Horowitz, Marc Platt and Fred Berger.
The following year, Beatty and Dunaway were invited back as announcers for Best Picture winner, which they accomplished without error.
The following events are closely associated with the annual Academy Awards:

It has become a tradition to give out gift bags to the presenters and performers at the Oscars.
In recent years, these gifts have also been extended to award nominees and winners.
The value of each of these gift bags can reach into the tens of thousands of dollars.
In 2014, the value was reported to be as high as US$80,000.
The value has risen to the point where the U.S.
Internal Revenue Service issued a statement regarding the gifts and their taxable status.
Oscar gift bags have included vacation packages to Hawaii and Mexico and Japan, a private dinner party for the recipient and friends at a restaurant, videophones, a four-night stay at a hotel, watches, bracelets, vacation packages, spa treatments, bottles of vodka, maple salad dressing, and weight-loss gummie candy.
Some of the gifts have even had a "risque" element to them; in 2014, the adult products retailer Adam & Eve had a "Secret Room Gifting Suite".
Celebrities visiting the gifting suite included Judith Hoag, Carolyn Hennesy, Kate Linder, Chris Mulkey, Jim O'Heir, and NBA player John Salley.
From 2006 onwards, results are Live+SD, all previous years are Live viewing
The term "Oscar" is a registered trademark of the AMPAS; however, in the Italian language, it is used generically to refer to any award or award ceremony, regardless of which field.
</doc>
<doc id="330" url="https://en.wikipedia.org/wiki?curid=330" title="Actrius">
Actrius

Actresses (Catalan: Actrius) is a 1997 Catalan language Spanish drama film produced and directed by Ventura Pons and based on the award-winning stage play "E.R."
by Josep Maria Benet i Jornet.
The film has no male actors, with all roles played by females.
The film was produced in 1996.
In order to prepare herself to play a role commemorating the life of legendary actress Empar Ribera, young actress (Mercè Pons) interviews three established actresses who had been the Ribera's pupils: the international diva Glòria Marc (Núria Espert), the television star Assumpta Roca (Rosa Maria Sardà), and dubbing director Maria Caminal (Anna Lizaran).
"Actrius" screened in 2001 at the Grauman's Egyptian Theatre in an American Cinematheque retrospective of the works of its director.
The film had first screened at the same location in 1998.
It was also shown at the 1997 Stockholm International Film Festival.
In "Movie - Film - Review", "Daily Mail" staffer Christopher Tookey wrote that though the actresses were "competent in roles that may have some reference to their own careers", the film "is visually unimaginative, never escapes its stage origins, and is almost totally lacking in revelation or surprising incident".
Noting that there were "occasional, refreshing moments of intergenerational bitchiness", they did not "justify comparisons to "All About Eve"", and were "insufficiently different to deserve critical parallels with "Rashomon"".
He also wrote that "The Guardian" called the film a "slow, stuffy chamber-piece", and that "The Evening Standard" stated the film's "best moments exhibit the bitchy tantrums seething beneath the threesome's composed veneers".
MRQE wrote "This cinematic adaptation of a theatrical work is true to the original, but does not stray far from a theatrical rendering of the story."
</doc>
<doc id="332" url="https://en.wikipedia.org/wiki?curid=332" title="Animalia (book)">
Animalia (book)

Animalia is an illustrated children's book by Graeme Base.
It was originally published in 1986, followed by a tenth anniversary edition in 1996, and a 25th anniversary edition in 2012.
Over three million copies have been sold.
A special numbered and signed anniversary edition was also published in 1996, with an embossed gold jacket.
"Animalia" is an alliterative alphabet book and contains twenty-six illustrations, one for each letter of the alphabet.
Each illustration features an animal from the animal kingdom (A is for alligator, B is for butterfly, etc.)
along with a short poem utilizing the letter of the page for many of the words.
The illustrations contain many other objects beginning with that letter that the reader can try to identify.
As an additional challenge, the author has hidden a picture of himself as a child in every picture.
Julia MacRae Books published an "Animalia" colouring book in 2008.
H. N. Abrams also published a wall calendar colouring book version for children the same year.
H. N. Abrams published "The Animalia Wall Frieze", a fold-out over 26 feet in length, in which the author created new riddles for each letter.
The Great American Puzzle Factory created a 300-piece jigsaw puzzle based on the book's cover.
A television series was also created, based on the book, which airs in the United States, Australia, Canada, the United Kingdom, Norway and Venezuela.
It also airs on Minimax for the Czech Republic and Slovakia.
And recently in Greece on the channel ET1.
The Australian Children's Television Foundation released a teaching resource DVD-ROM in 2011 to accompany the TV series with teaching aids for classroom use.
In 2010, The Base Factory and AppBooks released Animalia as an application for iPad and iPhone/iPod Touch.
"Animalia" won the Young Australian's Best Book Award in 1987 for Best Picture Story Book.
The Children's Book Council of Australia designated "Animalia" a 1987 : Honour Book.
Kid's Own Australian Literature Awards named "Animalia" the 1988 Picture Book Winner.
</doc>
<doc id="334" url="https://en.wikipedia.org/wiki?curid=334" title="International Atomic Time">
International Atomic Time

International Atomic Time (TAI, from the French name ) is a high-precision atomic coordinate time standard based on the notional passage of proper time on Earth's geoid.
It is the principal realisation of Terrestrial Time (except for a fixed offset of epoch).
It is also the basis for Coordinated Universal Time (UTC), which is used for civil timekeeping all over the Earth's surface.
, when another leap second was added, TAI is exactly 37 seconds ahead of UTC.
The 37 seconds results from the initial difference of 10 seconds at the start of 1972, plus 27 leap seconds in UTC since 1972.
TAI may be reported using traditional means of specifying days, carried over from non-uniform time standards based on the rotation of the Earth.
Specifically, both Julian Dates and the Gregorian calendar are used.
TAI in this form was synchronised with Universal Time at the beginning of 1958, and the two have drifted apart ever since, due to the changing motion of the Earth.
TAI is a weighted average of the time kept by over 400 atomic clocks in over 50 national laboratories worldwide.
The majority of the clocks involved are caesium clocks; the International System of Units (SI) definition of the second is based on caesium.
The clocks are compared using GPS signals and two-way satellite time and frequency transfer.
Due to the signal averaging TAI is an order of magnitude more stable than its best constituent clock.
The participating institutions each broadcast, in real time, a frequency signal with timecodes, which is their estimate of TAI.
Time codes are usually published in the form of UTC, which differs from TAI by a well-known integer number of seconds.
These time scales are denoted in the form "UTC(NPL)" in the UTC form, where "NPL" in this case identifies the National Physical Laboratory, UK.
The TAI form may be denoted "TAI(NPL)".
The latter is not to be confused with "TA(NPL)", which denotes an independent atomic time scale, not synchronised to TAI or to anything else.
The clocks at different institutions are regularly compared against each other.
The International Bureau of Weights and Measures (BIPM, France), combines these measurements to retrospectively calculate the weighted average that forms the most stable time scale possible.
This combined time scale is published monthly in "Circular T", and is the canonical TAI.
This time scale is expressed in the form of tables of differences UTC − UTC("k") (equivalent to TAI − TAI("k")) for each participating institution "k".
The same circular also gives tables of TAI − TA("k"), for the various unsynchronised atomic time scales.
Errors in publication may be corrected by issuing a revision of the faulty Circular T or by errata in a subsequent Circular T. Aside from this, once published in Circular T, the TAI scale is not revised.
In hindsight it is possible to discover errors in TAI, and to make better estimates of the true proper time scale.
Since the published circulars are definitive, better estimates do not create another version of TAI; it is instead considered to be creating a better realisation of Terrestrial Time (TT).
Early atomic time scales consisted of quartz clocks with frequencies calibrated by a single atomic clock; the atomic clocks were not operated continuously.
Atomic timekeeping services started experimentally in 1955, using the first caesium atomic clock at the National Physical Laboratory, UK (NPL).
It was used as a basis for calibrating the quartz clocks at the Royal Greenwich Observatory and to establish a time scale, called "Greenwich Atomic (GA).
The United States Naval Observatory began the A.1 scale on 13 September 1956, using an Atomichron commercial atomic clock, followed by the NBS-A scale at the National Bureau of Standards, Boulder, Colorado on 9 October 1957.
The International Time Bureau (BIH) began a time scale, T or AM, in July 1955, using both local caesium clocks and comparisons to distant clocks using the phase of VLF radio signals.
The BIH scale, A.1, and NBS-A were defined by an epoch at the beginning of 1958 The procedures used by the BIH evolved, and the name for the time scale changed: "A3" in 1963 and "TA(BIH)" in 1969.
The SI second was defined in terms of the caesium atom in 1967.
From 1971 to 1975 the General Conference on Weights and Measures and the International Committee for Weights and Measures made a series of decisions which designated the BIPM time scale International Atomic Time (TAI).
In the 1970s, it became clear that the clocks participating in TAI were ticking at different rates due to gravitational time dilation, and the combined TAI scale therefore corresponded to an average of the altitudes of the various clocks.
Starting from Julian Date 2443144.5 (1 January 1977 00:00:00), corrections were applied to the output of all participating clocks, so that TAI would correspond to proper time at mean sea level (the geoid).
Because the clocks were, on average, well above sea level, this meant that TAI slowed down, by about one part in a trillion.
The former uncorrected time scale continues to be published, under the name "EAL" ("Echelle Atomique Libre", meaning "Free Atomic Scale").
The instant that the gravitational correction started to be applied serves as the epoch for Barycentric Coordinate Time (TCB), Geocentric Coordinate Time (TCG), and Terrestrial Time (TT), which represent three fundamental time scales in the solar system.
All three of these time scales were defined to read JD 2443144.5003725 (1 January 1977 00:00:32.184) exactly at that instant.
TAI was henceforth a realisation of TT, with the equation TT(TAI) = TAI + 32.184 s.

The continued existence of TAI was questioned in a 2007 letter from the BIPM to the ITU-R which stated "In the case of a redefinition of UTC without leap seconds, the CCTF would consider discussing the possibility of suppressing TAI, as it would remain parallel to the continuous UTC."
UTC is a discontinuous (i.e. regularly adjusted by leap seconds) time scale composed from segments that are linear transformations of atomic time.
From its beginning in 1961 through December 1971 the adjustments were made regularly in fractional leap seconds so that UTC approximated UT2.
Afterwards these adjustments were made only in whole seconds to approximate UT1.
This was a compromise arrangement in order to enable a publicly broadcast time scale; the post-1971 more linear transformation of the BIH's atomic time meant that the time scale would be more stable and easier to synchronize internationally.
The fact that it continues to approximate UT1 means that tasks such as navigation which require a source of Universal Time continue to be well served by the public broadcast of UTC.
</doc>
<doc id="336" url="https://en.wikipedia.org/wiki?curid=336" title="Altruism">
Altruism

Altruism is the principle and moral practice of concern for happiness of other human beings and/or animals, resulting in a quality of life both material and spiritual.
It is a traditional virtue in many cultures and a core aspect of various religious traditions and secular worldviews, though the concept of "others" toward whom concern should be directed can vary among cultures and religions.
In Hindu / Sanathana Dharma (whose philosophy is "inclusive", i.e includes, and welcomes everybody), where the dictum "Vasudaiva Kutumbakam" (the world is ONE FAMILY) is in force, always, "others" means "EVERYONE", unconditionally.
In an extreme case, altruism may become a synonym of selflessness which is the opposite of selfishness.
In a common way of living, it doesn't deny the singular nature of the subject, but realizes the traits of the individual personality in relation to the others, with a true, direct and personal interaction with each of them.
It is focusing both on a single person and the whole community.
In a (not only) Christian practice, it is the law of love direct to the ego and his neighbour.
The word "altruism" was coined by the French philosopher Auguste Comte in French, as "altruisme", for an antonym of egoism.
He derived it from the Italian "altrui", which in turn was derived from Latin "alteri", meaning "other people" or "somebody else".
Altruism in biological observations in field populations of the day organisms is an individual performing an action which is at a cost to themselves (e.g., pleasure and quality of life, time, probability of survival or reproduction), but benefits, either directly or indirectly, another third-party individual, without the expectation of reciprocity or compensation for that action.
Steinberg suggests a definition for altruism in the clinical setting, that is "intentional and voluntary actions that aim to enhance the welfare of another person in the absence of any quid pro quo external rewards".
Altruism can be distinguished from feelings of loyalty, in that whilst the latter is predicated upon social relationships, altruism does not consider relationships.
Much debate exists as to whether ""true"" altruism is possible in human psychology.
The theory of psychological egoism suggests that no act of sharing, helping or sacrificing can be described as truly altruistic, as the actor may receive an intrinsic reward in the form of personal gratification.
The validity of this argument depends on whether intrinsic rewards qualify as "benefits".
The term "altruism" may also refer to an ethical doctrine that claims that individuals are morally obliged to benefit others.
Used in this sense, it is usually contrasted with egoism, which claims individuals are morally obligated to serve themselves first.
The concept has a long history in philosophical and ethical thought.
The term was originally coined in the 19th century by the founding sociologist and philosopher of science, Auguste Comte, and has become a major topic for psychologists (especially evolutionary psychology researchers), evolutionary biologists, and ethologists.
Whilst ideas about altruism from one field can affect the other fields, the different methods and focuses of these fields always lead to different perspectives on altruism.
In simple terms, altruism is caring about the welfare of other people and acting to help them.
Marcel Mauss's book "The Gift" contains a passage called "Note on alms".
This note describes the evolution of the notion of alms (and by extension of altruism) from the notion of sacrifice.
In it, he writes:

Alms are the fruits of a moral notion of the gift and of fortune on the one hand, and of a notion of sacrifice, on the other.
Generosity is an obligation, because Nemesis avenges the poor and the gods for the superabundance of happiness and wealth of certain people who should rid themselves of it.
This is the ancient morality of the gift, which has become a principle of justice.
The gods and the spirits accept that the share of wealth and happiness that has been offered to them and had been hitherto destroyed in useless sacrifices should serve the poor and children.
In the science of ethology (the study of animal behaviour), and more generally in the study of social evolution, altruism refers to behaviour by an individual that increases the fitness of another individual while decreasing the fitness of the actor.
In evolutionary psychology this may be applied to a wide range of human behaviors such as charity, emergency aid, help to coalition partners, tipping, courtship gifts, production of public goods, and environmentalism.
Theories of apparently altruistic behavior were accelerated by the need to produce theories compatible with evolutionary origins.
Two related strands of research on altruism have emerged from traditional evolutionary analyses and from evolutionary game theory a mathematical model and analysis of behavioural strategies.
Some of the proposed mechanisms are:

Such explanations do not imply that humans are always consciously calculating how to increase their inclusive fitness when they are doing altruistic acts.
Instead, evolution has shaped psychological mechanisms, such as emotions, that promote altruistic behaviors.
Every single instance of altruistic behavior need not always increase inclusive fitness; altruistic behaviors would have been selected for if such behaviors on average increased inclusive fitness in the ancestral environment.
This need not imply that on average 50% or more of altruistic acts were beneficial for the altruist in the ancestral environment; if the benefits from helping the right person were very high it would be beneficial to err on the side of caution and usually be altruistic even if in most cases there were no benefits.
The benefits for the altruist may be increased and the costs reduced by being more altruistic towards certain groups.
Research has found that people are more altruistic to kin than to no-kin, to friends than to strangers, to those attractive than to those unattractive, to non-competitors than to competitors, and to members ingroups than to members of outgroup.
The study of altruism was the initial impetus behind George R. Price's development of the Price equation, which is a mathematical equation used to study genetic evolution.
An interesting example of altruism is found in the cellular slime moulds, such as "Dictyostelium mucoroides."
These protists live as individual amoebae until starved, at which point they aggregate and form a multicellular fruiting body in which some cells sacrifice themselves to promote the survival of other cells in the fruiting body.
Selective investment theory proposes that close social bonds, and associated emotional, cognitive, and neurohormonal mechanisms, evolved in order to facilitate long-term, high-cost altruism between those closely depending on one another for survival and reproductive success.
Such cooperative behaviors have sometimes been seen as arguments for left-wing politics such by the Russian zoologist and anarchist Peter Kropotkin in his 1902 book "" and Peter Singer in his book "A Darwinian Left."
Jorge Moll and Jordan Grafman, neuroscientists at the National Institutes of Health and LABS-D'Or Hospital Network (J.M.)
provided the first evidence for the neural bases of altruistic giving in normal healthy volunteers, using functional magnetic resonance imaging.
In their research, published in the Proceedings of the National Academy of Sciences USA in October 2006, they showed that both pure monetary rewards and charitable donations activated the mesolimbic reward pathway, a primitive part of the brain that usually responds to food and sex.
However, when volunteers generously placed the interests of others before their own by making charitable donations, another brain circuit was selectively activated: the subgenual cortex/septal region.
These structures are intimately related to social attachment and bonding in other species.
Altruism, the experiment suggested, was not a superior moral faculty that suppresses basic selfish urges but rather was basic to the brain, hard-wired and pleasurable.
One brain region, the subgenual anterior cingulate cortex/basal forebrain, contributes to learning altruistic behavior, especially in those with trait empathy.
The same study has shown a connection between giving to charity and the promotion of social bonding.
In fact, in an experiment published in March 2007 at the University of Southern California neuroscientist Antonio R. Damasio and his colleagues showed that subjects with damage to the ventromedial prefrontal cortex lack the ability to empathically feel their way to moral answers, and that When confronted with moral dilemmas, these brain-damaged patients coldly came up with "end-justifies-the-means" answers, leading Damasio to conclude that the point was not that they reached immoral conclusions, but that when they were confronted by a difficult issue - in this case as whether to shoot down a passenger plane hijacked by terrorists before it hits a major city - these patients appear to reach decisions without the anguish that afflicts those with normally functioning brains.
According to Adrian Raine, a clinical neuroscientist also at the University of Southern California, one of this study's implications is that society may have to rethink how it judges immoral people: "Psychopaths often feel no empathy or remorse.
Without that awareness, people relying exclusively on reasoning seem to find it harder to sort their way through moral thickets.
Does that mean they should be held to different standards of accountability?"
In another study, in the 1990s, Dr. Bill Harbaugh, a University of Oregon economist, concluded people are motivated to give for reasons of personal prestige and in a similar fMRI scanner test in 2007 with his psychologist colleague Dr. Ulrich Mayr, reached the same conclusions of Jorge Moll and Jordan Grafman about giving to charity, although they were able to divide the study group into two groups: "egoists" and "altruists".
One of their discoveries was that, though rarely, even some of the considered "egoists" sometimes gave more than expected because that would help others, leading to the conclusion that there are other factors in cause in charity, such as a person's environment and values.
The International Encyclopedia of the Social Sciences defines "psychological altruism" as "a motivational state with the goal of increasing another’s welfare."
Psychological altruism is contrasted with "psychological egoism," which refers to the motivation to increase one's own welfare.
There has been some debate on whether or not humans are truly capable of psychological altruism.
Some definitions specify a self-sacrificial nature to altruism and a lack of external rewards for altruistic behaviors.
However, because altruism ultimately benefits the self in many cases, the selflessness of altruistic acts is brought to question.
The social exchange theory postulates that altruism only exists when benefits to the self outweigh costs to the self.
Daniel Batson is a psychologist who examined this question and argues against the social exchange theory.
He identified four major motives for altruism: altruism to ultimately benefit the self (egoism), to ultimately benefit the other person (altruism), to benefit a group (collectivism), or to uphold a moral principle (principlism).
Altruism that ultimately serves selfish gains is thus differentiated from selfless altruism, but the general conclusion has been that empathy-induced altruism can be genuinely selfless.
The "empathy-altruism hypothesis" basically states that psychological altruism does exist and is evoked by the empathic desire to help someone who is suffering.
Feelings of empathic concern are contrasted with feelings of personal distress, which compel people to reduce their own unpleasant emotions.
People with empathic concern help others in distress even when exposure to the situation could be easily avoided, whereas those lacking in empathic concern avoid helping unless it is difficult or impossible to avoid exposure to another's suffering.
Helping behavior is seen in humans at about two years old, when a toddler is capable of understanding subtle emotional cues.
In psychological research on altruism, studies often observe altruism as demonstrated through prosocial behaviors such as helping, comforting, sharing, cooperation, philanthropy, and community service.
Research has found that people are most likely to help if they recognize that a person is in need and feel personal responsibility for reducing the person's distress.
Research also suggests that the number of bystanders witnessing distress or suffering affects the likelihood of helping (the "Bystander effect").
Greater numbers of bystanders decrease individual feelings of responsibility.
However, a witness with a high level of empathic concern is likely to assume personal responsibility entirely regardless of the number of bystanders.
Many studies have observed the effects of volunteerism (as a form of altruism) on happiness and health and have consistently found a strong connection between volunteerism and current and future health and well-being.
In a study of older adults, those who volunteered were higher on life satisfaction and will to live, and lower in depression, anxiety, and somatization.
Volunteerism and helping behavior have not only been shown to improve mental health, but physical health and longevity as well, attributable to the activity and social integration it encourages.
One study examined the physical health of mothers who volunteered over a 30-year period and found that 52% of those who did not belong to a volunteer organization experienced a major illness while only 36% of those who did volunteer experienced one.
A study on adults ages 55+ found that during the four-year study period, people who volunteered for two or more organizations had a 63% lower likelihood of dying.
After controlling for prior health status, it was determined that volunteerism accounted for a 44% reduction in mortality.
Merely being aware of kindness in oneself and others is also associated with greater well-being.
A study that asked participants to count each act of kindness they performed for one week significantly enhanced their subjective happiness.
It is important to note that, while research supports the idea that altruistic acts bring about happiness, it has also been found to work in the opposite direction—that happier people are also kinder.
The relationship between altruistic behavior and happiness is bidirectional.
Studies have found that generosity increases linearly from sad to happy affective states.
Studies have also been careful to note that feeling over-taxed by the needs of others has conversely negative effects on health and happiness.
For example, one study on volunteerism found that feeling overwhelmed by others' demands had an even stronger negative effect on mental health than helping had a positive one (although positive effects were still significant).
Additionally, while generous acts make people feel good about themselves, it is also important for people to appreciate the kindness they receive from others.
Studies suggest that gratitude goes hand-in-hand with kindness and is also very important for our well-being.
A study on the relationship happiness to various character strengths showed that "a conscious focus on gratitude led to reductions in negative affect and increases in optimistic appraisals, positive affect, offering emotional support, sleep quality, and well-being.".
"Sociologists have long been concerned with how to build the good society" ("Altruism, Morality, and Social Solidarity".
American Sociological Association.).
The structure of our societies and how individuals come to exhibit charitable, philanthropic, and other pro-social, altruistic actions for the common good is a largely researched topic within the field.
The American Sociology Association (ASA) acknowledges Public sociology saying, "The intrinsic scientific, policy, and public relevance of this field of investigation in helping to construct 'good societies' is unquestionable" ("Altruism, Morality, and Social Solidarity" ASA).
This type of sociology seeks contributions that aid grassroots and theoretical understandings of what motivates altruism and how it is organized, and promotes an altruistic focus in order to benefit the world and people it studies.
How altruism is framed, organized, carried out, and what motivates it at the group level is an area of focus that sociologists seek to investigate in order to contribute back to the groups it studies and "build the good society".
The motivation of altruism is also the focus of study; some publications link the occurrence of moral outrage to the punishment of perpetrators and compensation of victims.
Pathological altruism is when altruism is taken to an unhealthy extreme, and either harms the altruistic person, or well-intentioned actions cause more harm than good.
The term "pathological altruism" was popularised by the book "Pathological Altruism".
Examples include depression and burnout seen in healthcare professionals, an unhealthy focus on others to the detriment of one's own needs, hoarding of animals, and ineffective philanthropic and social programs that ultimately worsen the situations they are meant to aid.
Most, if not all, of the world's religions promote altruism as a very important moral value.
Buddhism, Christianity, Hinduism, Islam, Jainism, Judaism, and Sikhism, etc., place particular emphasis on altruistic morality.
Altruism figures prominently in Buddhism.
Love and compassion are components of all forms of Buddhism, and are focused on all beings equally: love is the wish that all beings be happy, and compassion is the wish that all beings be free from suffering.
"Many illnesses can be cured by the one medicine of love and compassion.
These qualities are the ultimate source of human happiness, and the need for them lies at the very core of our being" (Dalai Lama).
Still, the notion of altruism is modified in such a world-view, since the belief is that such a practice promotes our own happiness: "The more we care for the happiness of others, the greater our own sense of well-being becomes" (Dalai Lama).
In the context of larger ethical discussions on moral action and judgment, Buddhism is characterized by the belief that negative (unhappy) consequences of our actions derive not from punishment or correction based on moral judgment, but from the law of karma, which functions like a natural law of cause and effect.
A simple illustration of such cause and effect is the case of experiencing the effects of what one causes: if one causes suffering, then as a natural consequence one would experience suffering; if one causes happiness, then as a natural consequence one would experience happiness.
The fundamental principles of Jainism revolve around the concept of altruism, not only for humans but for all sentient beings.
Jainism preaches the view of "Ahimsa" – to live and let live, thereby not harming sentient beings, i.e. uncompromising reverence for all life.
It also considers all living things to be equal.
The first Tirthankara, Rishabhdev, introduced the concept of altruism for all living beings, from extending knowledge and experience to others to donation, giving oneself up for others, non-violence and compassion for all living things.
Jainism prescribes a path of non-violence to progress the soul to this ultimate goal.
A major characteristic of Jain belief is the emphasis on the consequences of not only physical but also mental behaviors.
One's unconquered mind with anger, pride (ego), deceit, greed and uncontrolled sense organs are the powerful enemies of humans.
Anger spoils good relations, pride destroys humility, deceit destroys peace and greed destroys everything.
Jainism recommends conquering anger by forgiveness, pride by humility, deceit by straightforwardness and greed by contentment.
Jains believe that to attain enlightenment and ultimately liberation, one must practice the following ethical principles (major vows) in thought, speech and action.
The degree to which these principles are practiced is different for householders and monks.
They are:
The "great vows" (Mahavrata) are prescribed for monks and "limited vows" (Anuvrata) are prescribed for householders.
The house-holders are encouraged to practice the above-mentioned five vows.
The monks have to observe them very strictly.
With consistent practice, it will be possible to overcome the limitations gradually, accelerating the spiritual progress.
The principle of non-violence seeks to minimize karmas which limit the capabilities of the soul.
Jainism views every soul as worthy of respect because it has the potential to become "Siddha" (God in Jainism).
Because all living beings possess a soul, great care and awareness is essential in one's actions.
Jainism emphasizes the equality of all life, advocating harmlessness towards all, whether the creatures are great or small.
This policy extends even to microscopic organisms.
Jainism acknowledges that every person has different capabilities and capacities to practice and therefore accepts different levels of compliance for ascetics and householders.
Altruism is central to the teachings of Jesus found in the Gospel, especially in the Sermon on the Mount and the Sermon on the Plain.
From biblical to medieval Christian traditions, tensions between self-affirmation and other-regard were sometimes discussed under the heading of "disinterested love", as in the Pauline phrase "love seeks not its own interests."
In his book "Indoctrination and Self-deception," Roderick Hindery tries to shed light on these tensions by contrasting them with impostors of authentic self-affirmation and altruism, by analysis of other-regard within creative individuation of the self, and by contrasting love for the few with love for the many.
Love confirms others in their freedom, shuns propaganda and masks, assures others of its presence, and is ultimately confirmed not by mere declarations from others, but by each person's experience and practice from within.
As in practical arts, the presence and meaning of love becomes validated and grasped not by words and reflections alone, but in the making of the connection.
St Thomas Aquinas interprets 'You should love your neighbour as yourself' as meaning that love for ourselves is the exemplar of love for others.
Considering that "the love with which a man loves himself is the form and root of friendship" and quotes Aristotle that "the origin of friendly relations with others lies in our relations to ourselves," he concluded that though we are not bound to love others more than ourselves, we naturally seek the common good, the good of the whole, more than any private good, the good of a part.
However, he thinks we should love God more than ourselves and our neighbours, and more than our bodily life—since the ultimate purpose of loving our neighbour is to share in eternal beatitude: a more desirable thing than bodily well being.
In coining the word Altruism, as stated above, Comte was probably opposing this Thomistic doctrine, which is present in some theological schools within Catholicism.
Many biblical authors draw a strong connection between love of others and love of God.
1 John 4 states that for one to love God one must love his fellowman, and that hatred of one's fellowman is the same as hatred of God.
Thomas Jay Oord has argued in several books that altruism is but one possible form of love.
An altruistic action is not always a loving action.
Oord defines altruism as acting for the other's good, and he agrees with feminists who note that sometimes love requires acting for one's own good when the other's demands undermine overall well-being.
German philosopher Max Scheler distinguishes two ways in which the strong can help the weak.
One way is a sincere expression of Christian love, "motivated by a powerful feeling of security, strength, and inner salvation, of the invincible fullness of one’s own life and existence".
Another way is merely "one of the many modern substitutes for love, ... nothing but the urge to turn away from oneself and to lose oneself in other people’s business."
At its worst, Scheler says, "love for the small, the poor, the weak, and the oppressed is really disguised hatred, repressed envy, an impulse to detract, etc., directed against the opposite phenomena: wealth, strength, power, largesse."
In Islam, the concept 'ithaar' (إيثار) (altruism) is the notion of 'preferring others to oneself'.
For Sufis, this means devotion to others through complete forgetfulness of one's own concerns, where concern for others is rooted to be a demand made by ALLAH on the human body, considered to be property of ALLAH alone.
The importance lies in sacrifice for the sake of the greater good; Islam considers those practicing Eyaar as abiding by the highest degree of nobility.
This is similar to the notion of chivalry, but unlike that European concept, in i'thar attention is focused on everything in existence.
A constant concern for ALLAH (i.e. God) results in a careful attitude towards people, animals, and other things in this world.
This concept was emphasized by Sufis of Islam like Rabia al-Adawiyya who paid attention to the difference between dedication to ALLAH (i.e. God) and dedication to people.
Thirteenth-century Turkish Sufi poet Yunus Emre explained this philosophy as "Yaratılanı severiz, Yaratandan ötürü" or "We love the creature, because of The Creator."
For many Muslims, i'thar must be practiced as a religious obligation during specific Islamic holidays.
However, i'thar is also still an Islamic ideal to which all Muslims should strive to adhere at all times.
Judaism defines altruism as the desired goal of creation.
The famous Rabbi Abraham Isaac Kook stated that love is the most important attribute in humanity.
This is defined as bestowal, or giving, which is the intention of altruism.
This can be altruism towards humanity that leads to altruism towards the creator or God.
Kabbalah defines God as the force of giving in existence.
Rabbi Moshe Chaim Luzzatto in particular focused on the 'purpose of creation' and how the will of God was to bring creation into perfection and adhesion with this upper force.
Modern Kabbalah developed by Rabbi Yehuda Ashlag, in his writings about the future generation, focuses on how society could achieve an altruistic social framework.
Ashlag proposed that such a framework is the purpose of creation, and everything that happens is to raise humanity to the level of altruism, love for one another.
Ashlag focused on society and its relation to divinity.
Altruism is essential to the Sikh religion.
The central faith in Sikhism is that the greatest deed any one can do is to imbibe and live the godly qualities like love, affection, sacrifice, patience, harmony, truthfulness.
The fifth Nanak, Guru Arjun Dev, sacrificed his life to uphold 22 carats of pure truth, the greatest gift to humanity, the Guru Granth.
The ninth Guru, Tegh Bahadur, sacrificed his head to protect weak and defenseless people against atrocity.
In the late seventeenth century, Guru Gobind Singh Ji (the tenth guru in Sikhism), was in war with the Mughal rulers to protect the people of different faiths when a fellow Sikh, Bhai Kanhaiya, attended the troops of the enemy.
He gave water to both friends and foes who were wounded on the battlefield.
Some of the enemy began to fight again and some Sikh warriors were annoyed by Bhai Kanhaiya as he was helping their enemy.
Sikh soldiers brought Bhai Kanhaiya before Guru Gobind Singh Ji, and complained of his action that they considered counter-productive to their struggle on the battlefield.
"What were you doing, and why?"
asked the Guru.
"I was giving water to the wounded because I saw your face in all of them," replied Bhai Kanhaiya.
The Guru responded, "Then you should also give them ointment to heal their wounds.
You were practicing what you were coached in the house of the Guru."
It was under the tutelage of the Guru that Bhai Kanhaiya subsequently founded a volunteer corps for altruism.
This volunteer corps still to date is engaged in doing good to others and trains new volunteering recruits for doing the same.
In Hinduism Selflessness (Atmatyag), Love (Prema), Kindness (Daya) and Forgiveness (Kshama) are considered as the highest acts of humanity or "Manushyattva".
Giving alms to the beggers or poor people is considered as a divine act or "Punya" and Hindus believe it will free their souls from guilt or "Paapa" and will led them to heaven or "Swarga" in afterlife.
Altruism is also the central act of various Hindu mythology and religious poems and songs.
Swami Vivekananda, the legendary Hindu monk, has said -"Jive prem kare jeijon, Seijon sebiche Iswar" (Whoever loves any living being, is serving god.).
Mass donation of clothes to poor people (Vastraseva), or blood donation camp or mass food donation (Annaseva) for poor people is common in various Hindu religious ceremonies.
Swami Sivananda, an Advaita scholar, reiterates the views in his commentary synthesising Vedanta views on the Brahma Sutras, a Vedantic text.
In his commentary on Chapter 3 of the Brahma Sutras, Sivananda notes that karma is insentient and short-lived, and ceases to exist as soon as a deed is executed.
Hence, karma cannot bestow the fruits of actions at a future date according to one's merit.
Furthermore, one cannot argue that karma generates apurva or punya, which gives fruit.
Since apurva is non-sentient, it cannot act unless moved by an intelligent being such as a god.
It cannot independently bestow reward or punishment.
However the very well known and popular text, the Bhagavad Gita supports the doctrine of karma yoga (achieving oneness with God through action) & "nishkaama karma" or action without expectation / desire for personal gain which can be said to encompass altruism.
Altruistic acts are generally celebrated and very well received in Hindu literature and is central to Hindu morality.
There exists a wide range of philosophical views on humans' obligations or motivations to act altruistically.
Proponents of ethical altruism maintain that individuals are morally obligated to act altruistically.
The opposing view is ethical egoism, which maintains that moral agents should always act in their own self-interest.
Both ethical altruism and ethical egoism contrast with utilitarianism, which maintains that each agent should act in order to maximise the efficacy of their function and the benefit to both themselves and their co-inhabitants.
A related concept in descriptive ethics is psychological egoism, the thesis that humans always act in their own self-interest and that true altruism is impossible.
Rational egoism is the view that rationality consists in acting in one's self-interest (without specifying how this affects one's moral obligations).
The genes OXTR, CD38, COMT, DRD4, DRD5, IGF2, GABRB2 have been found to be candidate genes for altruism.
</doc>
<doc id="339" url="https://en.wikipedia.org/wiki?curid=339" title="Ayn Rand">
Ayn Rand

Ayn Rand (; born Alisa Zinovyevna Rosenbaum;  – March 6, 1982) was a Russian-American writer and philosopher.
She is known for her two best-selling novels, "The Fountainhead" and "Atlas Shrugged", and for developing a philosophical system she named Objectivism.
Educated in Russia, she moved to the United States in 1926.
She had a play produced on Broadway in 1935 and 1936.
After two early novels that were initially unsuccessful, she achieved fame with her 1943 novel, "The Fountainhead".
In 1957, Rand published her best-known work, the novel "Atlas Shrugged".
Afterward, she turned to non-fiction to promote her philosophy, publishing her own periodicals and releasing several collections of essays until her death in 1982.
Rand advocated reason as the only means of acquiring knowledge and rejected faith and religion.
She supported rational and ethical egoism and rejected altruism.
In politics, she condemned the initiation of force as immoral and opposed collectivism and statism as well as anarchism, instead supporting "laissez-faire" capitalism, which she defined as the system based on recognizing individual rights, including property rights.
In art, Rand promoted romantic realism.
She was sharply critical of most philosophers and philosophical traditions known to her, except for Aristotle, Thomas Aquinas and classical liberals.
Literary critics received Rand's fiction with mixed reviews and academia generally ignored or rejected her philosophy, though academic interest has increased in recent decades.
The Objectivist movement attempts to spread her ideas, both to the public and in academic settings.
She has been a significant influence among libertarians and American conservatives.
Rand was born Alisa Zinovyevna Rosenbaum () on February 2, 1905, to a Russian-Jewish bourgeois family living in Saint Petersburg.
She was the eldest of three daughters of Zinovy Zakharovich Rosenbaum and his wife, Anna Borisovna (née Kaplan).
Her father was upwardly mobile and a pharmacist and her mother was socially ambitious and religiously observant.
Rand later said she found school unchallenging and began writing screenplays at the age of eight and novels at the age of ten.
At the prestigious Stoiunina Gymnasium, her closest friend was Vladimir Nabokov's younger sister, Olga.
The two girls shared an intense interest in politics and would engage in debates at the Nabokov mansion: while Olga defended constitutional monarchy, Alisa supported republican ideals.
She was twelve at the time of the February Revolution of 1917, during which she favored Alexander Kerensky over Tsar Nicholas II.
The subsequent October Revolution and the rule of the Bolsheviks under Vladimir Lenin disrupted the life the family had previously enjoyed.
Her father's business was confiscated, and the family fled to the Crimean Peninsula, which was initially under control of the White Army during the Russian Civil War.
While in high school, she realized that she was an atheist and valued reason above any other human virtue.
After graduating from high school in the Crimea in June 1921, she returned with her family to Petrograd (as Saint Petersburg was renamed at that time), where they faced desperate conditions, on occasion nearly starving.
After the Russian Revolution, universities were opened to women, allowing her to be in the first group of women to enroll at Petrograd State University.
At the age of 16, she began her studies in the department of social pedagogy, majoring in history.
At the university she was introduced to the writings of Aristotle and Plato, who would be her greatest influence and counter-influence, respectively.
She also studied the philosophical works of Friedrich Nietzsche.
Able to read French, German and Russian, she also discovered the writers Fyodor Dostoevsky, Victor Hugo, Edmond Rostand, and Friedrich Schiller, who became her perennial favorites.
Along with many other bourgeois students, she was purged from the university shortly before graduating.
After complaints from a group of visiting foreign scientists, however, many of the purged students were allowed to complete their work and graduate, which she did in October 1924.
She then studied for a year at the State Technicum for Screen Arts in Leningrad.
For an assignment she wrote an essay about the Polish actress Pola Negri, which became her first published work.
By this time she had decided her professional surname for writing would be "Rand", possibly because it is graphically similar to a vowelless excerpt of her birth surname in Cyrillic handwriting, and she adopted the first name "Ayn", either from a Finnish name "Aino" or from the Hebrew word ("ayin", meaning "eye").
In late 1925, Rand was granted a visa to visit relatives in Chicago.
She departed on January 17, 1926.
When she arrived in New York City on February 19, 1926, she was so impressed with the skyline of Manhattan that she cried what she later called "tears of splendor".
Intent on staying in the United States to become a screenwriter, she lived for a few months with her relatives, one of whom owned a movie theater and allowed her to watch dozens of films free of charge.
She then left for Hollywood, California.
In Hollywood, a chance meeting with famed director Cecil B. DeMille led to work as an extra in his film "The King of Kings" and a subsequent job as a junior screenwriter.
While working on "The King of Kings", she met an aspiring young actor, Frank O'Connor; the two were married on April 15, 1929.
She became a permanent American resident in July 1929 and an American citizen on March 3, 1931.
Taking various jobs during the 1930s to support her writing, she worked for a time as the head of the costume department at RKO Studios.
She made several attempts to bring her parents and sisters to the United States, but they were unable to acquire permission to emigrate.
Rand's first literary success came with the sale of her screenplay "Red Pawn" to Universal Studios in 1932, although it was never produced.
This was followed by the courtroom drama "Night of January 16th", first produced by E. E. Clive in Hollywood in 1934 and then successfully reopened on Broadway in 1935.
Each night a jury was selected from members of the audience; based on the jury's vote, one of two different endings would be performed.
In 1941, Paramount Pictures produced a movie loosely based on the play.
Rand did not participate in the production and was highly critical of the result.
"Ideal" is a novel and play written in 1934 which were first published in 2015 by her estate.
The heroine is an actress who embodies Randian ideals.
Rand's first published novel, the semi-autobiographical "We the Living", was published in 1936.
Set in Soviet Russia, it focused on the struggle between the individual and the state.
In a 1959 foreword to the novel, Rand stated that "We the Living" "is as near to an autobiography as I will ever write.
It is not an autobiography in the literal, but only in the intellectual sense.
The plot is invented, the background is not ..." Initial sales were slow and the American publisher let it go out of print, although European editions continued to sell.
After the success of her later novels, Rand was able to release a revised version in 1959 that has since sold over three million copies.
In 1942, without Rand's knowledge or permission, the novel was made into a pair of Italian films, "Noi vivi" and "Addio, Kira".
Rediscovered in the 1960s, these films were re-edited into a new version which was approved by Rand and re-released as "We the Living" in 1986.
Her novella "Anthem" was written during a break from the writing of her next major novel, "The Fountainhead".
It presents a vision of a dystopian future world in which totalitarian collectivism has triumphed to such an extent that even the word 'I' has been forgotten and replaced with 'we'.
It was published in England in 1938, but Rand initially could not find an American publisher.
As with "We the Living", Rand's later success allowed her to get a revised version published in 1946, which has sold more than 3.5 million copies.
During the 1940s, Rand became politically active.
She and her husband worked as full-time volunteers for the 1940 presidential campaign of Republican Wendell Willkie.
This work led to Rand's first public speaking experiences; she enjoyed fielding sometimes hostile questions from New York City audiences who had viewed pro-Willkie newsreels.
This activity brought her into contact with other intellectuals sympathetic to free-market capitalism.
She became friends with journalist Henry Hazlitt and his wife, and Hazlitt introduced her to the Austrian School economist Ludwig von Mises.
Despite her philosophical differences with them, Rand strongly endorsed the writings of both men throughout her career, and both of them expressed admiration for her.
Mises once referred to Rand as "the most courageous man in America", a compliment that particularly pleased her because he said "man" instead of "woman".
Rand also became friends with libertarian writer Isabel Paterson.
Rand questioned Paterson about American history and politics long into the night during their many meetings and gave Paterson ideas for her only non-fiction book, "The God of the Machine".
Rand's first major success as a writer came in 1943 with "The Fountainhead", a romantic and philosophical novel that she wrote over a period of seven years.
The novel centers on an uncompromising young architect named Howard Roark and his struggle against what Rand described as "second-handers"—those who attempt to live through others, placing others above themselves.
It was rejected by twelve publishers before finally being accepted by the Bobbs-Merrill Company on the insistence of editor Archibald Ogden, who threatened to quit if his employer did not publish it.
While completing the novel, Rand was prescribed the amphetamine Benzedrine to fight fatigue.
The drug helped her to work long hours to meet her deadline for delivering the novel, but afterwards she was so exhausted that her doctor ordered two weeks' rest.
Her use of the drug for approximately three decades may have contributed to what some of her later associates described as volatile mood swings.
"The Fountainhead" became a worldwide success, bringing Rand fame and financial security.
In 1943, Rand sold the rights for a film version to Warner Bros.
and she returned to Hollywood to write the screenplay.
Finishing her work on that screenplay, she was hired by producer Hal B. Wallis as a screenwriter and script-doctor.
Her work for Wallis included the screenplays for the Oscar-nominated "Love Letters" and "You Came Along".
Rand also worked on other projects, including a planned nonfiction treatment of her philosophy to be called "The Moral Basis of Individualism".
Although the planned book was never completed, a condensed version was published as an essay titled "The Only Path to Tomorrow" in the January 1944 edition of "Reader's Digest" magazine.
Rand extended her involvement with free-market and anti-communist activism while working in Hollywood.
She became involved with the Motion Picture Alliance for the Preservation of American Ideals, a Hollywood anti-Communist group, and wrote articles on the group's behalf.
She also joined the anti-Communist American Writers Association.
A visit by Isabel Paterson to meet with Rand's California associates led to a final falling out between the two when Paterson made comments, which Rand considered rude, to valued political allies.
In 1947, during the Second Red Scare, Rand testified as a "friendly witness" before the United States House Un-American Activities Committee.
Her testimony described the disparity between her personal experiences in the Soviet Union and the portrayal of it in the 1944 film "Song of Russia".
Rand argued that the film grossly misrepresented conditions in the Soviet Union, portraying life there as much better and happier than it actually was.
She wanted to also criticize the lauded 1946 film "The Best Years of Our Lives" for what she interpreted as its negative presentation of the business world, but she was not allowed to testify about it.
When asked after the hearings about her feelings on the effectiveness of the investigations, Rand described the process as "futile".
After several delays, the film version of "The Fountainhead" was released in 1949.
Although it used Rand's screenplay with minimal alterations, she "disliked the movie from beginning to end", and complained about its editing, acting, and other elements.
In the years following the publication of "The Fountainhead", Rand received numerous letters from readers, some of whom the book profoundly influenced.
In 1951, Rand moved from Los Angeles to New York City, where she gathered a group of these admirers around her.
This group (jokingly designated "The Collective") included future Federal Reserve Chairman Alan Greenspan, a young psychology student named Nathan Blumenthal (later Nathaniel Branden) and his wife Barbara and Barbara's cousin Leonard Peikoff.
Initially the group was an informal gathering of friends who met with Rand on weekends at her apartment to discuss philosophy.
She later began allowing them to read the drafts of her new novel, "Atlas Shrugged", as the manuscript pages were written.
In 1954 Rand's close relationship with the younger Nathaniel Branden turned into a romantic affair, with the consent of their spouses.
"Atlas Shrugged", published in 1957, was considered Rand's "magnum opus".
Rand described the theme of the novel as "the role of the mind in man's existence—and, as a corollary, the demonstration of a new moral philosophy: the morality of rational self-interest".
It advocates the core tenets of Rand's philosophy of Objectivism and expresses her concept of human achievement.
The plot involves a dystopian United States in which the most creative industrialists, scientists, and artists respond to a welfare state government by going on strike and retreating to a mountainous hideaway where they build an independent free economy.
The novel's hero and leader of the strike, John Galt, describes the strike as "stopping the motor of the world" by withdrawing the minds of the individuals most contributing to the nation's wealth and achievement.
With this fictional strike, Rand intended to illustrate that without the efforts of the rational and productive, the economy would collapse and society would fall apart.
The novel includes elements of mystery, romance, and science fiction, and it contains an extended exposition of Objectivism in the form of a lengthy monologue delivered by Galt.
Despite many negative reviews, "Atlas Shrugged" became an international bestseller.
In an interview with Mike Wallace, Rand declared herself "the most creative thinker alive".
However, Rand was discouraged and depressed by the reaction of intellectuals to the novel.
"Atlas Shrugged" was Rand's last completed work of fiction; it marked the end of her career as a novelist and the beginning of her role as a popular philosopher.
In 1958, Nathaniel Branden established Nathaniel Branden Lectures, later incorporated as the Nathaniel Branden Institute (NBI), to promote Rand's philosophy.
Collective members gave lectures for NBI and wrote articles for Objectivist periodicals that she edited.
Rand later published some of these articles in book form.
Critics, including some former NBI students and Branden himself, later described the culture of NBI as one of intellectual conformity and excessive reverence for Rand, with some describing NBI or the Objectivist movement itself as a cult or religion.
Rand expressed opinions on a wide range of topics, from literature and music to sexuality and facial hair, and some of her followers mimicked her preferences, wearing clothes to match characters from her novels and buying furniture like hers.
However, some former NBI students believed the extent of these behaviors was exaggerated, and the problem was concentrated among Rand's closest followers in New York.
Rand was unimpressed with many of the NBI students and held them to strict standards, sometimes reacting coldly or angrily to those who disagreed with her.
Throughout the 1960s and 1970s, Rand developed and promoted her Objectivist philosophy through her nonfiction works and by giving talks to students at institutions such as Yale, Princeton, Columbia, Harvard, and the Massachusetts Institute of Technology.
She received an honorary Doctorate of Humane Letters from Lewis & Clark College on 2 October 1963.
She also began delivering annual lectures at the Ford Hall Forum, responding afterward to questions from the audience.
During these speeches and Q&A sessions, she often took controversial stances on political and social issues of the day.
These included supporting abortion rights, opposing the Vietnam War and the military draft (but condemning many draft dodgers as "bums"), supporting Israel in the Yom Kippur War of 1973 against a coalition of Arab nations as "civilized men fighting savages", saying European colonists had the right to develop land taken from American Indians, and calling homosexuality "immoral" and "disgusting", while also advocating the repeal of all laws about it.
She also endorsed several Republican candidates for President of the United States, most strongly Barry Goldwater in 1964, whose candidacy she promoted in several articles for "The Objectivist Newsletter".
In 1964, Nathaniel Branden began an affair with the young actress Patrecia Scott, whom he later married.
Nathaniel and Barbara Branden kept the affair hidden from Rand.
When she learned of it in 1968, though her romantic relationship with Branden had already ended, Rand terminated her relationship with both Brandens, which led to the closure of NBI.
Rand published an article in "The Objectivist" repudiating Nathaniel Branden for dishonesty and other "irrational behavior in his private life".
Branden later apologized in an interview to "every student of Objectivism" for "perpetuating the Ayn Rand mystique" and for "contributing to that dreadful atmosphere of intellectual repressiveness that pervades the Objectivist movement".
In subsequent years, Rand and several more of her closest associates parted company.
Rand underwent surgery for lung cancer in 1974 after decades of heavy smoking.
In 1976, she retired from writing her newsletter and, despite her initial objections, she allowed social worker Evva Pryor, an employee of her attorney, to enroll her in Social Security and Medicare.
During the late 1970s her activities within the Objectivist movement declined, especially after the death of her husband on November 9, 1979.
One of her final projects was work on a never-completed television adaptation of "Atlas Shrugged".
Rand died of heart failure on March 6, 1982, at her home in New York City, and was interred in the Kensico Cemetery, Valhalla, New York.
Rand's funeral was attended by some of her prominent followers, including Alan Greenspan.
A floral arrangement in the shape of a dollar sign was placed near her casket.
In her will, Rand named Leonard Peikoff to inherit her estate.
Rand called her philosophy "Objectivism", describing its essence as "the concept of man as a heroic being, with his own happiness as the moral purpose of his life, with productive achievement as his noblest activity, and reason as his only absolute".
She considered Objectivism a systematic philosophy and laid out positions on metaphysics, epistemology, ethics, political philosophy, and aesthetics.
In metaphysics, Rand supported philosophical realism, and opposed anything she regarded as mysticism or supernaturalism, including all forms of religion.
In epistemology, she considered all knowledge to be based on sense perception, the validity of which she considered axiomatic, and reason, which she described as "the faculty that identifies and integrates the material provided by man's senses".
She rejected all claims of non-perceptual or "a priori" knowledge, including instinct,' 'intuition,' 'revelation,' or any form of 'just knowing.
In her "Introduction to Objectivist Epistemology", Rand presented a theory of concept formation and rejected the analytic–synthetic dichotomy.
In ethics, Rand argued for rational and ethical egoism (rational self-interest), as the guiding moral principle.
She said the individual should "exist for his own sake, neither sacrificing himself to others nor sacrificing others to himself".
She referred to egoism as "the virtue of selfishness" in her book of that title, in which she presented her solution to the is-ought problem by describing a meta-ethical theory that based morality in the needs of "man's survival "qua" man".
She condemned ethical altruism as incompatible with the requirements of human life and happiness, and held that the initiation of force was evil and irrational, writing in "Atlas Shrugged" that "Force and mind are opposites."
Rand's political philosophy emphasized individual rights (including property rights), and she considered "laissez-faire" capitalism the only moral social system because in her view it was the only system based on the protection of those rights.
She opposed statism, which she understood to include theocracy, absolute monarchy, Nazism, fascism, communism, democratic socialism, and dictatorship.
Rand believed that natural rights should be enforced by a constitutionally limited government.
Although her political views are often classified as conservative or libertarian, she preferred the term "radical for capitalism".
She worked with conservatives on political projects, but disagreed with them over issues such as religion and ethics.
She denounced libertarianism, which she associated with anarchism.
She rejected anarchism as a naïve theory based in subjectivism that could only lead to collectivism in practice.
In aesthetics, Rand defined art as a "selective re-creation of reality according to an artist's metaphysical value-judgments".
According to her, art allows philosophical concepts to be presented in a concrete form that can be easily grasped, thereby fulfilling a need of human consciousness.
As a writer, the art form Rand focused on most closely was literature, where she considered romanticism to be the approach that most accurately reflected the existence of human free will.
She described her own approach to literature as "romantic realism".
Rand acknowledged Aristotle as her greatest influence and remarked that in the history of philosophy she could only recommend "three A's"—Aristotle, Aquinas, and Ayn Rand.
In a 1959 interview with Mike Wallace, when asked where her philosophy came from she responded: "Out of my own mind, with the sole acknowledgement of a debt to Aristotle, the only philosopher who ever influenced me.
I devised the rest of my philosophy myself."
However, she also found early inspiration in Friedrich Nietzsche, and scholars have found indications of his influence in early notes from Rand's journals, in passages from the first edition of "We the Living" (which Rand later revised), and in her overall writing style.
However, by the time she wrote "The Fountainhead", Rand had turned against Nietzsche's ideas, and the extent of his influence on her even during her early years is disputed.
Rational egoism was embodied by Russian author Nikolay Chernyshevsky in the 1863 novel "What Is to Be Done?"
and several critics claim that "What Is to Be Done?"
is one of the sources of inspiration for Rand's thought.
For example, the book's main character Lopuhov says "I am not a man to make sacrifices.
And indeed there are no such things.
One acts in the way that one finds most pleasant."
Among the philosophers Rand held in particular disdain was Immanuel Kant, whom she referred to as a "monster", although philosophers George Walsh and Fred Seddon have argued that she misinterpreted Kant and exaggerated their differences.
Rand said her most important contributions to philosophy were her "theory of concepts, [her] ethics, and [her] discovery in politics that evil—the violation of rights—consists of the initiation of force".
She believed epistemology was a foundational branch of philosophy and considered the advocacy of reason to be the single most significant aspect of her philosophy, stating: "I am not "primarily" an advocate of capitalism, but of egoism; and I am not "primarily" an advocate of egoism, but of reason.
If one recognizes the supremacy of reason and applies it consistently, all the rest follows."
During Rand's lifetime, her work evoked both extreme praise and condemnation.
Rand's first novel, "We the Living", was admired by the literary critic H. L. Mencken, her Broadway play "Night of January 16th" was both a critical and popular success, and "The Fountainhead" was hailed by "The New York Times" reviewer Lorine Pruette as "masterful".
Rand's novels were derided by some critics when they were first published as being long and melodramatic.
However, they became bestsellers largely through word of mouth.
The first reviews Rand received were for "Night of January 16th".
Reviews of the production were largely positive, but Rand considered even positive reviews to be embarrassing because of significant changes made to her script by the producer.
Rand believed that her first novel, "We the Living", was not widely reviewed, but Rand scholar Michael S. Berliner says "it was the most reviewed of any of her works", with approximately 125 different reviews being published in more than 200 publications.
Overall these reviews were more positive than the reviews she received for her later work.
Her 1938 novella "Anthem" received little attention from reviewers, both for its first publication in England and for subsequent re-issues.
Rand's first bestseller, "The Fountainhead", received far fewer reviews than "We the Living", and reviewers' opinions were mixed.
Lorine Pruette's positive review in "The New York Times" was one that Rand greatly appreciated.
Pruette called Rand "a writer of great power" who wrote "brilliantly, beautifully and bitterly", and stated that "you will not be able to read this masterful book without thinking through some of the basic concepts of our time".
There were other positive reviews, but Rand dismissed most of them as either not understanding her message or as being from unimportant publications.
Some negative reviews focused on the length of the novel, such as one that called it "a whale of a book" and another that said "anyone who is taken in by it deserves a stern lecture on paper-rationing".
Other negative reviews called the characters unsympathetic and Rand's style "offensively pedestrian".
Rand's 1957 novel "Atlas Shrugged" was widely reviewed and many of the reviews were strongly negative.
In "National Review", conservative author Whittaker Chambers called the book "sophomoric" and "remarkably silly".
He described the tone of the book as "shrillness without reprieve" and accused Rand of supporting a godless system (which he related to that of the Soviets), claiming "From almost any page of "Atlas Shrugged", a voice can be heard, from painful necessity, commanding: 'To a gas chamber—go!.
"Atlas Shrugged" received positive reviews from a few publications, including praise from the noted book reviewer John Chamberlain, but Rand scholar Mimi Reisel Gladstein later wrote that "reviewers seemed to vie with each other in a contest to devise the cleverest put-downs", calling it "execrable claptrap" and "a nightmare"—they also said it was "written out of hate" and showed "remorseless hectoring and prolixity".
Rand's nonfiction received far fewer reviews than her novels had.
The tenor of the criticism for her first nonfiction book, "For the New Intellectual", was similar to that for "Atlas Shrugged", with philosopher Sidney Hook likening her certainty to "the way philosophy is written in the Soviet Union", and author Gore Vidal calling her viewpoint "nearly perfect in its immorality".
Her subsequent books got progressively less attention from reviewers.
On the 100th anniversary of Rand's birth in 2005, Edward Rothstein, writing for "The New York Times", referred to her fictional writing as quaint utopian "retro fantasy" and programmatic neo-Romanticism of the misunderstood artist while criticizing her characters' "isolated rejection of democratic society".
In 2007, book critic Leslie Clark described her fiction as "romance novels with a patina of pseudo-philosophy".
In 2009, "GQ"s critic columnist Tom Carson described her books as "capitalism's version of middlebrow religious novels" such as "" and the "Left Behind" series.
In 1991, a survey conducted for the Library of Congress and the Book-of-the-Month Club asked club members what the most influential book in the respondent's life was.
Rand's "Atlas Shrugged" was the second most popular choice, after the Bible.
Rand's books continue to be widely sold and read, with over 29 million copies sold (with about 10% of that total purchased for free distribution to schools by the Ayn Rand Institute).
In 1998, Modern Library readers voted "Atlas Shrugged" the 20th century's finest work of fiction, followed by "The Fountainhead" in second place, "Anthem" in seventh, and "We the Living" eighth; none of the four appeared on the critics' list.
Although Rand's influence has been greatest in the United States, there has been international interest in her work.
Rand's work continues to be among the top sellers among books in India.
Rand's contemporary admirers included fellow novelists, such as Ira Levin, Kay Nolte Smith and L. Neil Smith; and later writers such as Erika Holzer and Terry Goodkind have been influenced by her.
Other artists who have cited Rand as an important influence on their lives and thought include comic book artist Steve Ditko and musician Neil Peart of Rush.
Rand provided a positive view of business and in response business executives and entrepreneurs have admired and promoted her work.
John Allison of BB&T and Ed Snider of Comcast Spectacor have funded the promotion of Rand's ideas, while Mark Cuban (owner of the Dallas Mavericks) as well as John P. Mackey (CEO of Whole Foods) among others have said they consider Rand crucial to their success.
Rand and her works have been referred to in a variety of media: on television shows including animated sitcoms, live-action comedies, dramas, and game shows, as well as in movies and video games.
She, or a character based on her, figures prominently (in positive and negative lights) in literary and science fiction novels by prominent American authors.
Nick Gillespie, editor in chief of "Reason", has remarked that "Rand's is a tortured immortality, one in which she's as likely to be a punch line as a protagonist..." and that "jibes at Rand as cold and inhuman, run through the popular culture".
Two movies have been made about Rand's life.
A 1997 documentary film, "", was nominated for the Academy Award for Best Documentary Feature.
"The Passion of Ayn Rand", a 1999 television adaptation of the book of the same name, won several awards.
Rand's image also appears on a 1999 U.S.
postage stamp illustrated by artist Nick Gaetano.
Although she rejected the labels "conservative" and "libertarian", Rand has had continuing influence on right-wing politics and libertarianism.
Jim Powell, a senior fellow at the Cato Institute, considers Rand one of the three most important women (along with Rose Wilder Lane and Isabel Paterson) of modern American libertarianism, and David Nolan, one of the founders of the Libertarian Party, stated that "without Ayn Rand, the libertarian movement would not exist".
In his history of the libertarian movement, journalist Brian Doherty described her as "the most influential libertarian of the twentieth century to the public at large" and biographer Jennifer Burns referred to her as "the ultimate gateway drug to life on the right".
Economist and Ayn Rand student George Reisman wrote: "Ayn Rand...in particular, must be cited as providing a philosophical foundation for the case of capitalism, and as being responsible probably more than anyone else for the current spread of pro-capitalist ideas."
She faced intense opposition from William F. Buckley, Jr.
and other contributors for the "National Review" magazine.
They published numerous criticisms in the 1950s and 1960s by Whittaker Chambers, Garry Wills, and M. Stanton Evans.
Nevertheless, her influence among conservatives forced Buckley and other "National Review" contributors to reconsider how traditional notions of virtue and Christianity could be integrated with support for capitalism.
The political figures who cite Rand as an influence are usually conservatives (often members of the Republican Party), despite Rand taking some positions that are atypical for conservatives, such as being pro-choice and an atheist.
A 1987 article in "The New York Times" referred to her as the Reagan administration's "novelist laureate".
Republican Congressmen and conservative pundits have acknowledged her influence on their lives and have recommended her novels.
The financial crisis of 2007–2008 spurred renewed interest in her works, especially "Atlas Shrugged", which some saw as foreshadowing the crisis.
Opinion articles compared real-world events with the plot of the novel.
During this time, signs mentioning Rand and her fictional hero John Galt appeared at Tea Party protests.
There was also increased criticism of her ideas, especially from the political left, with critics blaming the economic crisis on her support of selfishness and free markets, particularly through her influence on Alan Greenspan.
For example, "Mother Jones" remarked that "Rand's particular genius has always been her ability to turn upside down traditional hierarchies and recast the wealthy, the talented, and the powerful as the oppressed" while equating Randian individual well-being with that of the "Volk" according to Goebbels.
Corey Robin of "The Nation" alleged similarities between the "moral syntax of Randianism" and fascism.
During Rand's lifetime, her work received little attention from academic scholars.
When the first academic book about Rand's philosophy appeared in 1971, its author declared writing about Rand "a treacherous undertaking" that could lead to "guilt by association" for taking her seriously.
A few articles about Rand's ideas appeared in academic journals before her death in 1982, many of them in "The Personalist".
One of these was "On the Randian Argument" by libertarian philosopher Robert Nozick, who argued that her meta-ethical argument is unsound and fails to solve the is–ought problem posed by David Hume.
Some responses to Nozick by other academic philosophers were also published in "The Personalist" arguing that Nozick misstated Rand's case.
Academic consideration of Rand as a literary figure during her life was even more limited.
Academic Mimi Gladstein was unable to find any scholarly articles about Rand's novels when she began researching her in 1973, and only three such articles appeared during the rest of the 1970s.
Since Rand's death, interest in her work has gradually increased.
Historian Jennifer Burns has identified "three overlapping waves" of scholarly interest in Rand, the most recent of which is "an explosion of scholarship" since the year 2000.
However, few universities currently include Rand or Objectivism as a philosophical specialty or research area, with many literature and philosophy departments dismissing her as a pop culture phenomenon rather than a subject for serious study.
Gladstein, Harry Binswanger, Allan Gotthelf, John Hospers, Edwin A. Locke, Wallace Matson, Leonard Peikoff, Chris Matthew Sciabarra, and Tara Smith have taught her work in academic institutions.
Sciabarra co-edits the "Journal of Ayn Rand Studies", a nonpartisan peer-reviewed journal dedicated to the study of Rand's philosophical and literary work.
In 1987 Gotthelf, George Walsh and David Kelley co-founded the Ayn Rand Society, a group affiliated with the American Philosophical Association.
In 2012, the University of Pittsburgh Press launched an "Ayn Rand Society Philosophical Studies" series based on the proceedings of the Society.
Smith has written several academic books and papers on Rand's ideas, including "Ayn Rand's Normative Ethics: The Virtuous Egoist", a volume on Rand's ethical theory published by Cambridge University Press.
Rand's ideas have also been made subjects of study at Clemson and Duke universities.
Scholars of English and American literature have largely ignored her work, although attention to her literary work has increased since the 1990s.
Rand scholars Douglas Den Uyl and Douglas B. Rasmussen, while stressing the importance and originality of her thought, describe her style as "literary, hyperbolic and emotional".
Philosopher Jack Wheeler says that despite "the incessant bombast and continuous venting of Randian rage", Rand's ethics are "a most immense achievement, the study of which is vastly more fruitful than any other in contemporary thought".
In the "Literary Encyclopedia" entry for Rand written in 2001, John David Lewis declared that "Rand wrote the most intellectually challenging fiction of her generation".
In a 1999 interview in the "Chronicle of Higher Education", Sciabarra commented, "I know they laugh at Rand", while forecasting a growth of interest in her work in the academic community.
Libertarian philosopher Michael Huemer argues that very few people find Rand's ideas convincing, especially her ethics, which he believes are difficult to interpret and may lack logical coherence.
He attributes the attention she receives to her being a "compelling writer", especially as a novelist.
"Atlas Shrugged" thus outsells Rand's non-fiction works as well as the works of other philosophers of classical liberalism such as Ludwig von Mises, Friedrich Hayek, or Frederic Bastiat.
Political scientist Charles Murray, while praising Rand's literary accomplishments, criticizes her claim that her only "philosophical debt" was to Aristotle, instead asserting that her ideas were derivative of previous thinkers such as John Locke and Friedrich Nietzsche.
Although Rand maintained that Objectivism was an integrated philosophical system, philosopher Robert H. Bass argues that her central ethical ideas are inconsistent and contradictory to her central political ideas.
In 1985, Rand's intellectual heir Leonard Peikoff established the Ayn Rand Institute, a nonprofit organization dedicated to promoting Rand's ideas and works.
In 1990, after an ideological disagreement with Peikoff, philosopher David Kelley founded the Institute for Objectivist Studies, now known as The Atlas Society.
In 2001, historian John McCaskey organized the Anthem Foundation for Objectivist Scholarship, which provides grants for scholarly work on Objectivism in academia.
The charitable foundation of BB&T Corporation has also given grants for teaching Rand's ideas or works.
The University of Texas at Austin, the University of Pittsburgh, and University of North Carolina at Chapel Hill are among the schools that have received grants.
In some cases, these grants have been controversial due to their requiring research or teaching related to Rand.
Novels:

Other fiction:

Non-fiction:





</doc>
<doc id="340" url="https://en.wikipedia.org/wiki?curid=340" title="Alain Connes">
Alain Connes

Alain Connes (; born 1 April 1947) is a French mathematician, currently Professor at the Collège de France, IHÉS, Ohio State University and Vanderbilt University.
He was an Invited Professor at the Conservatoire national des arts et métiers (2000).
Alain Connes studies operator algebras.
In his early work on von Neumann algebras in the 1970s, he succeeded in obtaining the almost complete classification of injective factors.
He also formulated the Connes embedding problem.
Following this, he made contributions in operator K-theory and index theory, which culminated in the Baum–Connes conjecture.
He also introduced cyclic cohomology in the early 1980s as a first step in the study of noncommutative differential geometry.
He was a member of Bourbaki.
Connes has applied his work in areas of mathematics and theoretical physics, including number theory, differential geometry and particle physics.
Connes was awarded the Fields Medal in 1982, the Crafoord Prize in 2001 and the gold medal of the CNRS in 2004.
He was an invited speaker at the ICM in 1974 at Vancouver and in 1986 at Berkeley and a plenary speaker at the ICM in 1978 at Helsinki.
He is a member of the French Academy of Sciences and several foreign academies and societies, including the Danish Academy of Sciences, Norwegian Academy of Sciences, Russian Academy of Sciences, and US National Academy of Sciences.
</doc>
<doc id="344" url="https://en.wikipedia.org/wiki?curid=344" title="Allan Dwan">
Allan Dwan

Allan Dwan (3 April 1885 – 28 December 1981) was a pioneering Canadian-born American motion picture director, producer and screenwriter.
Born Joseph Aloysius Dwan in Toronto, Ontario, Canada, Dwan, was the younger son of commercial traveller of woolen clothing Joseph Michael Dwan (1857–1917) and his wife Mary Jane Dwan, "née" Hunt.
The family moved to the United States when he was seven years old, on 4 December 1892, by ferry from Windsor to Detroit, according to his naturalization petition of August 1939.
His elder brother, Leo Garnet Dwan (1883–1964), became a physician.
At the University of Notre Dame, Allan Dwan studied engineering and began working for a lighting company in Chicago.
However, he had a strong interest in the fledgling motion picture industry and when Essanay Studios offered him the opportunity to become a scriptwriter, he took the job.
At that time, some of the East Coast movie makers began to spend winters in California where the climate allowed them to continue productions requiring warm weather.
Soon, a number of movie companies worked there year-round and, in 1911, Dwan began working part-time in Hollywood.
While still in New York, in 1917 he was the founding president of the East Coast chapter of the Motion Picture Directors Association.
Dwan operated Flying A Studios in La Mesa, California from August 1911 to July 1912.
Flying A was one of the first motion pictures studios in California history.
On 12 August 2011, a plaque was unveiled on the Wolff building at Third Avenue and La Mesa Boulevard commemorating Dwan and the Flying A Studios origins in La Mesa, California.
After making a series of westerns and comedies, Dwan directed fellow Canadian-American Mary Pickford in several very successful movies as well as her husband, Douglas Fairbanks, notably in the acclaimed 1922 "Robin Hood".
Dwan directed Gloria Swanson in eight feature films, and one short film made in the short-lived sound-on-film process Phonofilm.
This short, also featuring Thomas Meighan and Henri de la Falaise, was produced as a joke, for the 26 April 1925 "Lambs' Gambol" for The Lambs, with the film showing Swanson crashing the all-male club.
Following the introduction of the talkies, Dwan directed child-star Shirley Temple in "Heidi" (1937) and "Rebecca of Sunnybrook Farm" (1938).
Dwan helped launch the career of two other successful Hollywood directors, Victor Fleming, who went on to direct "The Wizard of Oz" and "Gone With the Wind", and Marshall Neilan, who became an actor, director, writer and producer.
Over a long career spanning almost 50 years, Dwan directed 125 motion pictures, some of which were highly acclaimed, such as the 1949 box office hit, "Sands of Iwo Jima".
He directed his last movie in 1961.
He died in Los Angeles at the age of ninety-six, and is interred in the San Fernando Mission Cemetery, Mission Hills, California.
Dwan has a star on the Hollywood Walk of Fame at 6263 Hollywood Boulevard.
Daniel Eagan of "Film Journal International" described Dwan as one of the early pioneers of cinema, stating that his style "is so basic as to seem invisible, but he treats his characters with uncommon sympathy and compassion."
Print E-book 



</doc>
<doc id="358" url="https://en.wikipedia.org/wiki?curid=358" title="Algeria">
Algeria

Algeria (; ', familary Algerian Arabic '; ), officially the People's Democratic Republic of Algeria, is a country in the Maghreb (Northwest Africa) on the Mediterranean coast.
The capital and most populous city is Algiers, located in the far north of the country.
With an area of , Algeria is the tenth-largest country in the world, and the largest in Africa.
Algeria is bordered to the northeast by Tunisia, to the east by Libya, to the west by Morocco, to the southwest by the Western Saharan territory, Mauritania, and Mali, to the southeast by Niger, and to the north by the Mediterranean Sea.
The country is a semi-presidential republic consisting of 48 provinces and 1,541 communes (counties).
Ancient Algeria has known many empires and dynasties, including ancient Numidians, Phoenicians, Carthaginians, Romans, Vandals, Byzantines, Umayyads, Abbasids, Idrisid, Aghlabid, Rustamid, Fatimids, Zirid, Hammadids, Almoravids, Almohads, Spaniards, Ottomans and the French colonial empire.
Berbers are the indigenous inhabitants of Algeria.
Algeria is a regional and middle power.
The North African country supplies large amounts of natural gas to Europe, and energy exports are the backbone of the economy.
According to OPEC Algeria has the 16th largest oil reserves in the world and the second largest in Africa, while it has the 9th largest reserves of natural gas.
Sonatrach, the national oil company, is the largest company in Africa.
Algeria has one of the largest militaries in Africa and the largest defence budget on the continent; most of Algeria's weapons are imported from Russia, with whom they are a close ally.
Algeria is a member of the African Union, the Arab League, OPEC, the United Nations and is a founding member of the Arab Maghreb Union.
The country's name derives from the city of Algiers.
The city's name in turn derives from the Arabic ' (, "The Islands"), a truncated form of the older ' (, "Islands of the Mazghanna Tribe"), employed by medieval geographers such as al-Idrisi.
In the region of Ain Hanech (Saïda Province), early remnants (200,000 BC) of hominid occupation in North Africa were found.
Neanderthal tool makers produced hand axes in the Levalloisian and Mousterian styles (43,000 BC) similar to those in the Levant.
Algeria was the site of the highest state of development of Middle Paleolithic Flake tool techniques.
Tools of this era, starting about 30,000 BC, are called Aterian (after the archeological site of Bir el Ater, south of Tebessa).
The earliest blade industries in North Africa are called Iberomaurusian (located mainly in the Oran region).
This industry appears to have spread throughout the coastal regions of the Maghreb between 15,000 and 10,000 BC.
Neolithic civilization (animal domestication and agriculture) developed in the Saharan and Mediterranean Maghreb perhaps as early as 11,000 BC or as late as between 6000 and 2000 BC.
This life, richly depicted in the Tassili n'Ajjer paintings, predominated in Algeria until the classical period.
The mixture of peoples of North Africa coalesced eventually into a distinct native population that came to be called Berbers, who are the indigenous peoples of northern Africa.
From their principal center of power at Carthage, the Carthaginians expanded and established small settlements along the North African coast; by 600 BC, a Phoenician presence existed at Tipasa, east of Cherchell, Hippo Regius (modern Annaba) and Rusicade (modern Skikda).
These settlements served as market towns as well as anchorages.
As Carthaginian power grew, its impact on the indigenous population increased dramatically.
Berber civilization was already at a stage in which agriculture, manufacturing, trade, and political organization supported several states.
Trade links between Carthage and the Berbers in the interior grew, but territorial expansion also resulted in the enslavement or military recruitment of some Berbers and in the extraction of tribute from others.
By the early 4th century BC, Berbers formed the single largest element of the Carthaginian army.
In the Revolt of the Mercenaries, Berber soldiers rebelled from 241 to 238 BC after being unpaid following the defeat of Carthage in the First Punic War.
They succeeded in obtaining control of much of Carthage's North African territory, and they minted coins bearing the name Libyan, used in Greek to describe natives of North Africa.
The Carthaginian state declined because of successive defeats by the Romans in the Punic Wars.
In 146 BC the city of Carthage was destroyed.
As Carthaginian power waned, the influence of Berber leaders in the hinterland grew.
By the 2nd century BC, several large but loosely administered Berber kingdoms had emerged.
Two of them were established in Numidia, behind the coastal areas controlled by Carthage.
West of Numidia lay Mauretania, which extended across the Moulouya River in modern-day Morocco to the Atlantic Ocean.
The high point of Berber civilization, unequaled until the coming of the Almohads and Almoravids more than a millennium later, was reached during the reign of Masinissa in the 2nd century BC.
After Masinissa's death in 148 BC, the Berber kingdoms were divided and reunited several times.
Masinissa's line survived until 24 AD, when the remaining Berber territory was annexed to the Roman Empire.
For several centuries Algeria was ruled by the Romans, who founded many colonies in the region.
Like the rest of North Africa, Algeria was one of the breadbaskets of the empire, exporting cereals and other agricultural products.
Saint Augustine was the bishop of Hippo Regius (modern-day Algeria), located in the Roman province of Africa.
The Germanic Vandals of Geiseric moved into North Africa in 429, and by 435 controlled coastal Numidia.
They did not make any significant settlement on the land, as they were harassed by local tribes.
In fact, by the time the Byzantines arrived Lepcis Magna was abandoned and the Msellata region was occupied by the indigenous Laguatan who had been busy facilitating an Amazigh political, military and cultural revival.
After negligible resistance from the locals, Muslim Arabs of the Umayyad Caliphate conquered Algeria in the mid-7th century and a large number of the indigenous people converted to the newly founded faith of Islam.
After the fall of the Umayyad Caliphate, numerous local dynasties emerged, including the Aghlabids, Almohads, Abdalwadid, Zirids, Rustamids, Hammadids, Almoravids and the Fatimids.
During the Middle Ages, North Africa was home to many great scholars, saints and sovereigns including Judah Ibn Quraysh, the first grammarian to suggest the Afroasiatic language family, the great Sufi masters Sidi Boumediene (Abu Madyan) and Sidi El Houari, and the Emirs Abd Al Mu'min and Yāghmūrasen.
It was during this time that the Fatimids or children of Fatima, daughter of Muhammad, came to the Maghreb.
These "Fatimids" went on to found a long lasting dynasty stretching across the Maghreb, Hejaz and the Levant, boasting a secular inner government, as well as a powerful army and navy, made up primarily of Arabs and Levantines extending from Algeria to their capital state of Cairo.
The Fatimid caliphate began to collapse when its governors the Zirids seceded.
In order to punish them the Fatimids sent the Arab Banu Hilal and Banu Sulaym against them.
The resultant war is recounted in the epic Tāghribāt.
In Al-Tāghrībāt the Amazigh Zirid Hero Khālīfā Al-Zānatī asks daily, for duels, to defeat the Hilalan hero Ābu Zayd al-Hilalī and many other Arab knights in a string of victories.
The Zirids, however, were ultimately defeated ushering in an adoption of Arab customs and culture.
The indigenous Amazigh tribes, however, remained largely independent, and depending on tribe, location and time controlled varying parts of the Maghreb, at times unifying it (as under the Fatimids).
The Fatimid Islamic state, also known as Fatimid Caliphate made an Islamic empire that included North Africa, Sicily, Palestine, Jordan, Lebanon, Syria, Egypt, the Red Sea coast of Africa, Tihamah, Hejaz and Yemen.
Caliphates from Northern Africa traded with the other empires of their time, as well as forming part of a confederated support and trade network with other Islamic states during the Islamic Era.
The Amazighs historically consisted of several tribes.
The two main branches were the Botr and Barnès tribes, who were divided into tribes, and again into sub-tribes.
Each region of the Maghreb contained several tribes (for example, Sanhadja, Houara, Zenata, Masmouda, Kutama, Awarba, and Berghwata).
All these tribes made independent territorial decisions.
Several Amazigh dynasties emerged during the Middle Ages in the Maghreb and other nearby lands.
Ibn Khaldun provides a table summarising the Amazigh dynasties of the Maghreb region, the Zirid, Banu Ifran, Maghrawa, Almoravid, Hammadid, Almohad, Merinid, Abdalwadid, Wattasid, Meknassa and Hafsid dynasties.
In the early 16th century, Spain constructed fortified outposts (presidios) on or near the Algerian coast.
Spain took control of few coastal towns like Mers el Kebir in 1505; Oran in 1509; and Tlemcen, Mostaganem and Ténès in 1510.
In the same year, a few merchants of Algiers ceded one of the rocky islets in their harbour to Spain, which built a fort on it.
The presidios in North Africa turned out to be a costly and largely ineffective military endeavour that did not guarantee access for Spain's merchant fleet.
There reigned in Ifriqiya, current Tunisia, a Berber family, Zirid, somehow recognising the suzerainty of the Fatimid caliph of Cairo.
Probably in 1048, the Zirid ruler or viceroy, el-Mu'izz, decided to end this suzerainty.
The Fatimid state was too weak to attempt a punitive expedition; The Viceroy, el-Mu'izz, also found another means of revenge.
Between the Nile and the Red Sea were living Bedouin tribes expelled from Arabia for their disruption and turbulent influence, both Banu Hilal and Banu Sulaym among others, whose presence disrupted farmers in the Nile Valley since the nomads would often loot.
The then Fatimid vizier devised to relinquish control of the Maghreb and obtained the agreement of his sovereign.
This not only prompted the Bedouins to leave, but the Fatimid treasury even gave them a light expatriation cash allowance.
Whole tribes set off with women, children, ancestors, animals and camping equipment.
Some stopped on the way, especially in Cyrenaica, where they are still one of the essential elements of the settlement but most arrived in Ifriqiya by the Gabes region.
The Zirid ruler tried to stop this rising tide, but each meeting, the last under the walls of Kairouan, his troops were defeated and Arabs remained masters of the field.
The flood was still rising, and in 1057 the Arabs spread on the high plains of Constantine where they gradually choked Qalaa of Banu Hammad, as they had done Kairouan few decades ago.
From there they gradually gained the upper Algiers and Oran plains.
Some were forcibly taken by the Almohads in the second half of the 12th century.
We can say that in the 13th century there were in all of North Africa, with the exception of the main mountain ranges and certain coastal regions remained entirely Berber.
The region of Algeria was partially ruled by Ottomans for three centuries from 1516 to 1830.
In 1516 the Turkish privateer brothers Aruj and Hayreddin Barbarossa, who operated successfully under the Hafsids, moved their base of operations to Algiers.
They succeeded in conquering Jijel and Algiers from the Spaniards but eventually assumed control over the city and the surrounding region, forcing the previous ruler, Abu Hamo Musa III of the "Bani Ziyad" dynasty, to flee.
When Aruj was killed in 1518 during his invasion of Tlemcen, Hayreddin succeeded him as military commander of Algiers.
The Ottoman sultan gave him the title of beylerbey and a contingent of some 2,000 janissaries.
With the aid of this force, Hayreddin conquered the whole area between Constantine and Oran (although the city of Oran remained in Spanish hands until 1791).
The next beylerbey was Hayreddin's son Hasan, who assumed the position in 1544.
Until 1587 the area was governed by officers who served terms with no fixed limits.
Subsequently, with the institution of a regular Ottoman administration, governors with the title of pasha ruled for three-year terms.
The pasha was assisted by janissaries, known in Algeria as the ojaq and led by an agha.
Discontent among the ojaq rose in the mid-1600s because they were not paid regularly, and they repeatedly revolted against the pasha.
As a result, the agha charged the pasha with corruption and incompetence and seized power in 1659.
Plague had repeatedly struck the cities of North Africa.
Algiers lost from 30,000 to 50,000 inhabitants to the plague in 1620–21, and suffered high fatalities in 1654–57, 1665, 1691 and 1740–42.
In 1671, the taifa rebelled, killed the agha, and placed one of its own in power.
The new leader received the title of dey.
After 1689, the right to select the dey passed to the divan, a council of some sixty nobles.
It was at first dominated by the "ojaq"; but by the 18th century, it had become the dey's instrument.
In 1710, the dey persuaded the sultan to recognise him and his successors as regent, replacing the pasha in that role, although Algiers remained a part of the Ottoman Empire.
The dey was in effect a constitutional autocrat.
The dey was elected for a life term, but in the 159 years (1671–1830) that the system survived, fourteen of the twenty-nine deys were assassinated.
Despite usurpation, military coups and occasional mob rule, the day-to-day operation of Ottoman government was remarkably orderly.
Although the regency patronised the tribal chieftains, it never had the unanimous allegiance of the countryside, where heavy taxation frequently provoked unrest.
Autonomous tribal states were tolerated, and the regency's authority was seldom applied in the Kabylie.
The Barbary pirates preyed on Christian and other non-Islamic shipping in the western Mediterranean Sea.
The pirates often took the passengers and crew on the ships and sold them or used them as slaves.
They also did a brisk business in ransoming some of the captives.
According to Robert Davis, from the 16th to 19th century, pirates captured 1 million to 1.25 million Europeans as slaves.
They often made raids, called Razzias, on European coastal towns to capture Christian slaves to sell at slave markets in North Africa and the Ottoman Empire.
In 1544, Hayreddin captured the island of Ischia, taking 4,000 prisoners, and enslaved some 9,000 inhabitants of Lipari, almost the entire population.
In 1551, Turgut Reis enslaved the entire population of the Maltese island of Gozo, between 5,000 and 6,000, sending the captives to Libya.
In 1554, pirates sacked Vieste in southern Italy and took an estimated 7,000 captives as slaves.
In 1558, Barbary corsairs captured the town of Ciutadella (Minorca), destroyed it, slaughtered the inhabitants and took 3,000 survivors as slaves to Istanbul.
Barbary pirates often attacked the Balearic Islands, and in response, the residents built many coastal watchtowers and fortified churches.
The threat was so severe that residents abandoned the island of Formentera.
Between 1609 and 1616, England lost 466 merchant ships to Barbary pirates.
In July 1627 two pirate ships from Algiers sailed as far as Iceland, raiding and capturing slaves.
Two weeks earlier another pirate ship from Salé in Morocco had also raided in Iceland.
Some of the slaves brought to Algiers were later ransomed back to Iceland, but some chose to stay in Algeria.
In 1629 pirate ships from Algeria raided the Faroe Islands.
Barbary raids in the Mediterranean continued to attack Spanish merchant shipping, and as a result, the Spanish navy bombarded Algiers in 1783 and 1784.
In 1792, Spain abandoned Oran, selling it to the Ottoman Empire, and it became the site for a new bey in Algiers, though French influence in the region increased over the 19th century.
In the 19th century, the pirates forged affiliations with Caribbean powers, paying a "licence tax" in exchange for safe harbour of their vessels.
One American slave reported that the Algerians had enslaved 130 American seamen in the Mediterranean and Atlantic from 1785 to 1793.
Piracy on American vessels in the Mediterranean resulted in the United States initiating the First (1801–1805) and Second Barbary Wars (1815).
Following those wars, Algeria was weaker and Europeans, with an Anglo-Dutch fleet commanded by the British Lord Exmouth, attacked Algiers.
After a nine-hour bombardment, they obtained a treaty from the Dey that reaffirmed the conditions imposed by Captain (later Commodore) Stephen Decatur (U.S.
Navy) concerning the demands of tributes.
In addition, the Dey agreed to end the practice of enslaving Christians.
Despite being removed from Algeria in the 19th century, Spain retained a presence in Morocco.
Algeria consistently opposed Spanish fortresses and control in nearby Morocco through the 20th century.
Under the pretext of a slight to their consul, the French invaded and captured Algiers in 1830.
Algerian slave trade and piracy ceased when the French conquered Algiers.
The conquest of Algeria by the French took some time and resulted in considerable bloodshed.
A combination of violence and disease epidemics caused the indigenous Algerian population to decline by nearly one-third from 1830 to 1872.
Historian Ben Kiernan wrote on the French conquest of Algeria: "By 1875, the French conquest was complete.
The war had killed approximately 825,000 indigenous Algerians since 1830."
French losses from 1831–51 were 3,336 killed in action and 92,329 dead in the hospital.
The population of Algeria, which stood at about 2.9 million in 1872, reached nearly 11 million in 1960.
French policy was predicated on "civilising" the country.
During this period, a small but influential French-speaking indigenous elite was formed, made up of Berbers, mostly Kabyles.
As a consequence, French government favored the Kabyles.
About 80% of Indigenous schools were constructed for Kabyles.
From 1848 until independence, France administered the whole Mediterranean region of Algeria as an integral part and "département" of the nation.
One of France's longest-held overseas territories, Algeria became a destination for hundreds of thousands of European immigrants, who became known as "colons" and later, as "Pied-Noirs."
Between 1825 and 1847, 50,000 French people emigrated to Algeria.
These settlers benefited from the French government's confiscation of communal land from tribal peoples, and the application of modern agricultural techniques that increased the amount of arable land.
Many Europeans settled in Oran and Algiers, and by the early 20th century they formed a majority of the population in both cities.
During the late 19th and early 20th century; the European share was almost a fifth of the population.
The French government aimed at making Algeria an assimilated part of France, and this included substantial educational investments especially after 1900.
The indigenous cultural and religious resistance heavily opposed this tendency, but in contrast to the other colonised countries' path in central Asia and Caucasus, Algeria kept its individual skills and a relatively human-capital intensive agriculture.
Gradually, dissatisfaction among the Muslim population, which lacked political and economic status in the colonial system, gave rise to demands for greater political autonomy and eventually independence from France.
In May 1945, the uprising against the occupying French forces was suppressed through what is now known as the Sétif and Guelma massacre.
Tensions between the two population groups came to a head in 1954, when the first violent events of what was later called the Algerian War began.
Historians have estimated that between 30,000 and 150,000 Harkis and their dependents were killed by the Front de Libération Nationale (FLN) or by lynch mobs in Algeria.
The FLN used hit and run attacks in Algeria and France as part of its war, and the French conducted severe reprisals.
The war led to the death of hundreds of thousands of Algerians and hundreds of thousands of injuries.
Historians, like Alistair Horne and Raymond Aron, state that the actual number of Algerian Muslim war dead was far greater than the original FLN and official French estimates but was less than the 1 million deaths claimed by the Algerian government after independence.
Horne estimated Algerian casualties during the span of eight years to be around 700,000.
The war uprooted more than 2 million Algerians.
The war against French rule concluded in 1962, when Algeria gained complete independence following the March 1962 Evian agreements and the July 1962 self-determination referendum.
The number of European "Pied-Noirs" who fled Algeria totaled more than 900,000 between 1962 and 1964.
The exodus to mainland France accelerated after the Oran massacre of 1962, in which hundreds of militants entered European sections of the city, and began attacking civilians.
Algeria's first president was the Front de Libération Nationale (FLN) leader Ahmed Ben Bella.
Morocco's claim to portions of western Algeria led to the Sand War in 1963.
Ben Bella was overthrown in 1965 by Houari Boumédiène, his former ally and defence minister.
Under Ben Bella, the government had become increasingly socialist and authoritarian; Boumédienne continued this trend.
But, he relied much more on the army for his support, and reduced the sole legal party to a symbolic role.
He collectivised agriculture and launched a massive industrialization drive.
Oil extraction facilities were nationalised.
This was especially beneficial to the leadership after the international 1973 oil crisis.
In the 1960s and 1970s under President Houari Boumediene, Algeria pursued a program of industrialization within a state-controlled socialist economy.
Boumediene's successor, Chadli Bendjedid, introduced some liberal economic reforms.
He promoted a policy of Arabisation in Algerian society and public life.
Teachers of Arabic, brought in from other Muslim countries, spread conventional Islamic thought in schools and sowed the seeds of a return to Orthodox Islam.
The Algerian economy became increasingly dependent on oil, leading to hardship when the price collapsed during the 1980s oil glut.
Economic recession caused by the crash in world oil prices resulted in Algerian social unrest during the 1980s; by the end of the decade, Bendjedid introduced a multi-party system.
Political parties developed, such as the Islamic Salvation Front (FIS), a broad coalition of Muslim groups.
In December 1991 the Islamic Salvation Front dominated the first of two rounds of legislative elections.
Fearing the election of an Islamist government, the authorities intervened on 11 January 1992, cancelling the elections.
Bendjedid resigned and a High Council of State was installed to act as Presidency.
It banned the FIS, triggering a civil insurgency between the Front's armed wing, the Armed Islamic Group, and the national armed forces, in which more than 100,000 people are thought to have died.
The Islamist militants conducted a violent campaign of civilian massacres.
At several points in the conflict, the situation in Algeria became a point of international concern, most notably during the crisis surrounding Air France Flight 8969, a hijacking perpetrated by the Armed Islamic Group.
The Armed Islamic Group declared a ceasefire in October 1997.
Algeria held elections in 1999, considered biased by international observers and most opposition groups which were won by President Abdelaziz Bouteflika.
He worked to restore political stability to the country and announced a "Civil Concord" initiative, approved in a referendum, under which many political prisoners were pardoned, and several thousand members of armed groups were granted exemption from prosecution under a limited amnesty, in force until 13 January 2000.
The AIS disbanded and levels of insurgent violence fell rapidly.
The Groupe Salafiste pour la Prédication et le Combat (GSPC), a splinter group of the Group Islamic Army, continued a terrorist campaign against the Government.
Bouteflika was re-elected in the April 2004 presidential election after campaigning on a programme of national reconciliation.
The programme comprised economic, institutional, political and social reform to modernise the country, raise living standards, and tackle the causes of alienation.
It also included a second amnesty initiative, the Charter for Peace and National Reconciliation, which was approved in a referendum in September 2005.
It offered amnesty to most guerrillas and Government security forces.
In November 2008, the Algerian Constitution was amended following a vote in Parliament, removing the two-term limit on Presidential incumbents.
This change enabled Bouteflika to stand for re-election in the 2009 presidential elections, and he was re-elected in April 2009.
During his election campaign and following his re-election, Bouteflika promised to extend the programme of national reconciliation and a $150-billion spending programme to create three million new jobs, the construction of one million new housing units, and to continue public sector and infrastructure modernisation programmes.
A continuing series of protests throughout the country started on 28 December 2010, inspired by similar protests across the Middle East and North Africa.
On 24 February 2011, the government lifted Algeria's 19-year-old state of emergency.
The government enacted legislation dealing with political parties, the electoral code, and the representation of women in elected bodies.
In April 2011, Bouteflika promised further constitutional and political reform.
However, elections are routinely criticized by opposition groups as unfair and international human rights groups say that media censorship and harassment of political opponents continue.
Algeria is the largest country in Africa, and the Mediterranean Basin.
Its southern part includes a significant portion of the Sahara.
To the north, the Tell Atlas form with the Saharan Atlas, further south, two parallel sets of reliefs in approaching eastbound, and between which are inserted vast plains and highlands.
Both Atlas tend to merge in eastern Algeria.
The vast mountain ranges of Aures and Nememcha occupy the entire northeastern Algeria and are delineated by the Tunisian border.
The highest point is Mount Tahat ( m).
Algeria lies mostly between latitudes 19° and 37°N (a small area is north of 37°N and south of 19°N), and longitudes 9°W and 12°E.
Most of the coastal area is hilly, sometimes even mountainous, and there are a few natural harbours.
The area from the coast to the Tell Atlas is fertile.
South of the Tell Atlas is a steppe landscape ending with the Saharan Atlas; farther south, there is the Sahara desert.
The Ahaggar Mountains (), also known as the Hoggar, are a highland region in central Sahara, southern Algeria.
They are located about south of the capital, Algiers, and just east of Tamanghasset.
Algiers, Oran, Constantine, and Annaba are Algeria's main cities.
In this region, midday desert temperatures can be hot year round.
After sunset, however, the clear, dry air permits rapid loss of heat, and the nights are cool to chilly.
Enormous daily ranges in temperature are recorded.
Rainfall is fairly plentiful along the coastal part of the Tell Atlas, ranging from annually, the amount of precipitation increasing from west to east.
Precipitation is heaviest in the northern part of eastern Algeria, where it reaches as much as in some years.
Farther inland, the rainfall is less plentiful.
Algeria also has ergs, or sand dunes, between mountains.
Among these, in the summer time when winds are heavy and gusty, temperatures can get up to .
The varied vegetation of Algeria includes coastal, mountainous and grassy desert-like regions which all support a wide range of wildlife.
Many of the creatures comprising the Algerian wildlife live in close proximity to civilization.
The most commonly seen animals include the wild boars, jackals, and gazelles, although it is not uncommon to spot fennecs (foxes), and jerboas.
Algeria also has a small African leopard and Saharan cheetah population, but these are seldom seen.
A species of deer, the Barbary stag, inhabits the dense humid forests in the north-eastern areas.
A variety of bird species makes the country an attraction for bird watchers.
The forests are inhabited by boars and jackals.
Barbary macaques are the sole native monkey.
Snakes, monitor lizards, and numerous other reptiles can be found living among an array of rodents throughout the semi arid regions of Algeria.
Many animals are now extinct, including the Barbary lions, Atlas bears and crocodiles.
In the north, some of the native flora includes Macchia scrub, olive trees, oaks, cedars and other conifers.
The mountain regions contain large forests of evergreens (Aleppo pine, juniper, and evergreen oak) and some deciduous trees.
Fig, eucalyptus, agave, and various palm trees grow in the warmer areas.
The grape vine is indigenous to the coast.
In the Sahara region, some oases have palm trees.
Acacias with wild olives are the predominant flora in the remainder of the Sahara.
Camels are used extensively; the desert also abounds with venomous and nonvenomous snakes, scorpions, and numerous insects.
Elected politicians are considered to have relatively little sway over Algeria.
Instead, a group of unelected civilian and military "décideurs", known as "le pouvoir" ("the power"), actually rule the country, even deciding who should be president.
The most powerful man may be Mohamed Mediène, head of the military intelligence.
In recent years, many of these generals have died or retired.
After the death of General Larbi Belkheir, Bouteflika put loyalists in key posts, notably at Sonatrach, and secured constitutional amendments that make him re-electable indefinitely.
The head of state is the president of Algeria, who is elected for a five-year term.
The president was formerly limited to two five-year terms, but a constitutional amendment passed by the Parliament on 11 November 2008 removed this limitation.
Algeria has universal suffrage at 18 years of age.
The President is the head of the army, the Council of Ministers and the High Security Council.
He appoints the Prime Minister who is also the head of government.
The Algerian parliament is bicameral; the lower house, the People's National Assembly, has 462 members who are directly elected for five-year terms, while the upper house, the Council of the Nation, has 144 members serving six-year terms, of which 96 members are chosen by local assemblies and 48 are appointed by the president.
According to the constitution, no political association may be formed if it is "based on differences in religion, language, race, gender, profession, or region".
In addition, political campaigns must be exempt from the aforementioned subjects.
Parliamentary elections were last held in May 2012, and were judged to be largely free by international monitors, though local groups alleged fraud and irregularities.
In the elections, the FLN won 221 seats, the military-backed National Rally for Democracy won 70, and the Islamist Green Algeria Alliance won 47.
Algeria is included in the European Union's European Neighbourhood Policy (ENP) which aims at bringing the EU and its neighbours closer.
Giving incentives and rewarding best performers, as well as offering funds in a faster and more flexible manner, are the two main principles underlying the European Neighbourhood Instrument (ENI) that came into force in 2014.
It has a budget of €15.4 billion and provides the bulk of funding through a number of programmes.
In 2009, the French government agreed to compensate victims of nuclear tests in Algeria.
Defense Minister Herve Morin stated that "It's time for our country to be at peace with itself, at peace thanks to a system of compensation and reparations," when presenting the draft law on the payouts.
Algerian officials and activists believe that this is a good first step and hope that this move would encourage broader reparation.
Tensions between Algeria and Morocco in relation to the Western Sahara have been an obstacle to tightening the Arab Maghreb Union, nominally established in 1989, but which has carried little practical weight.
The military of Algeria consists of the People's National Army (ANP), the Algerian National Navy (MRA), and the Algerian Air Force (QJJ), plus the Territorial Air Defence Forces.
It is the direct successor of the National Liberation Army (Armée de Libération Nationale or ALN), the armed wing of the nationalist National Liberation Front which fought French colonial occupation during the Algerian War of Independence (1954–62).
Total military personnel include 147,000 active, 150,000 reserve, and 187,000 paramilitary staff (2008 estimate).
Service in the military is compulsory for men aged 19–30, for a total of 12 months.
The military expenditure was 4.3% of the gross domestic product (GDP) in 2012.
Algeria has the second largest military in North Africa with the largest defence budget in Africa ($10 billion).
In 2007, the Algerian Air Force signed a deal with Russia to purchase 49 MiG-29SMT and 6 MiG-29UBT at an estimated cost of $1.9 billion.
Russia is also building two 636-type diesel submarines for Algeria.
Algeria has been categorized by Freedom House as "not free" since it began publishing such ratings in 1972, with the exception of 1989, 1990, and 1991, when the country was labeled "partly free."
In December 2016, the "Euro-Mediterranean Human Rights Monitor" issued a report regarding violation of media freedom in Algeria.
It clarified that the Algerian government imposed restriction on freedom of the press; expression; and right to peaceful demonstration, protest and assembly as well as intensified censorship of the media and websites.
Due to the fact that the journalists and activists criticize the ruling government, some media organizations' licenses are canceled.
Independent and autonomous trade unions face routine harassment from the government, with many leaders imprisoned and protests suppressed.
In 2016 a number of unions, many of which were involved in the 2010–2012 Algerian Protests, have been deregistered by the government.
Homosexuality is illegal in Algeria.
Public homosexual behavior is punishable by up to two years in prison.
Algeria is divided into 48 provinces ("wilayas"), 553 districts ("daïras") and 1,541 municipalities ("baladiyahs").
Each province, district, and municipality is named after its seat, which is usually the largest city.
The administrative divisions have changed several times since independence.
When introducing new provinces, the numbers of old provinces are kept, hence the non-alphabetical order.
With their official numbers, currently (since 1983) they are

Algeria is classified as an upper middle income country by the World Bank.
Algeria's currency is the dinar (DZD).
The economy remains dominated by the state, a legacy of the country's socialist post-independence development model.
In recent years, the Algerian government has halted the privatization of state-owned industries and imposed restrictions on imports and foreign involvement in its economy.
These restrictions are just started to be lifted off recently although questions about Algeria's slow diversifying economy remains.
Algeria has struggled to develop industries outside hydrocarbons in part because of high costs and an inert state bureaucracy.
The government's efforts to diversify the economy by attracting foreign and domestic investment outside the energy sector have done little to reduce high youth unemployment rates or to address housing shortages.
The country is facing a number of short-term and medium-term problems, including the need to diversify the economy, strengthen political, economic and financial reforms, improve the business climate and reduce inequalities amongst regions.
A wave of economic protests in February and March 2011 prompted the Algerian government to offer more than $23 billion in public grants and retroactive salary and benefit increases.
Public spending has increased by 27% annually during the past 5 years.
The 2010–14 public-investment programme will cost US$286 billion, 40% of which will go to human development.
The Algerian economy grew by 2.6% in 2011, driven by public spending, in particular in the construction and public-works sector, and by growing internal demand.
If hydrocarbons are excluded, growth has been estimated at 4.8%.
Growth of 3% is expected in 2012, rising to 4.2% in 2013.
The rate of inflation was 4% and the budget deficit 3% of GDP.
The current-account surplus is estimated at 9.3% of GDP and at the end of December 2011, official reserves were put at US$182 billion.
Inflation, the lowest in the region, has remained stable at 4% on average between 2003 and 2007.
In 2011 Algeria announced a budgetary surplus of $26.9 billion, 62% increase in comparison to 2010 surplus.
In general, the country exported $73 billion worth of commodities while it imported $46 billion.
Thanks to strong hydrocarbon revenues, Algeria has a cushion of $173 billion in foreign currency reserves and a large hydrocarbon stabilization fund.
In addition, Algeria's external debt is extremely low at about 2% of GDP.
The economy remains very dependent on hydrocarbon wealth, and, despite high foreign exchange reserves (US$178 billion, equivalent to three years of imports), current expenditure growth makes Algeria's budget more vulnerable to the risk of prolonged lower hydrocarbon revenues.
In 2011, the agricultural sector and services recorded growth of 10% and 5.3%, respectively.
About 14% of the labor force are employed in the agricultural sector.
Fiscal policy in 2011 remained expansionist and made it possible to maintain the pace of public investment and to contain the strong demand for jobs and housing.
Algeria has not joined the WTO, despite several years of negotiations.
In March 2006, Russia agreed to erase $4.74 billion of Algeria's Soviet-era debt during a visit by Russian President Vladimir Putin to the country, the first by a Russian leader in half a century.
In return, Algerian President Abdelaziz Bouteflika agreed to buy $7.5 billion worth of combat planes, air-defence systems and other arms from Russia, according to the head of Russia's state arms exporter Rosoboronexport.
Dubai-based conglomerate Emarat Dzayer Group said it had signed a joint venture agreement to develop a $1.6 billion steel factory in Algeria.
Algeria, whose economy is reliant on petroleum, has been an OPEC member since 1969.
Its crude oil production stands at around 1.1 million barrels/day, but it is also a major gas producer and exporter, with important links to Europe.
Hydrocarbons have long been the backbone of the economy, accounting for roughly 60% of budget revenues, 30% of GDP, and over 95% of export earnings.
Algeria has the 10th-largest reserves of natural gas in the world and is the sixth-largest gas exporter.
The U.S.
Energy Information Administration reported that in 2005, Algeria had of proven natural-gas reserves.
It also ranks 16th in oil reserves.
Non-hydrocarbon growth for 2011 was projected at 5%.
To cope with social demands, the authorities raised expenditure, especially on basic food support, employment creation, support for SMEs, and higher salaries.
High hydrocarbon prices have improved the current account and the already large international reserves position.
Income from oil and gas rose in 2011 as a result of continuing high oil prices, though the trend in production volume is downwards.
Production from the oil and gas sector in terms of volume, continues to decline, dropping from 43.2 million tonnes to 32 million tonnes between 2007 and 2011.
Nevertheless, the sector accounted for 98% of the total volume of exports in 2011, against 48% in 1962, and 70% of budgetary receipts, or USD 71.4 billion.
The Algerian national oil company is Sonatrach, which plays a key role in all aspects of the oil and natural gas sectors in Algeria.
All foreign operators must work in partnership with Sonatrach, which usually has majority ownership in production-sharing agreements.
Algeria has invested an estimated 100 billion dinars towards developing research facilities and paying researchers.
This development program is meant to advance alternative energy production, especially solar and wind power.
Algeria is estimated to have the largest solar energy potential in the Mediterranean, so the government has funded the creation of a solar science park in Hassi R'Mel.
Currently, Algeria has 20,000 research professors at various universities and over 780 research labs, with state-set goals to expand to 1,000.
Besides solar energy, areas of research in Algeria include space and satellite telecommunications, nuclear power and medical research.
Despite a decline in total unemployment, youth and women unemployment is high.
Unemployment particularly affects the young, with a jobless rate of 21.5% among the 15–24 age group.
The overall rate of unemployment was 10% in 2011, but remained higher among young people, with a rate of 21.5% for those aged between 15 and 24.
The government strengthened in 2011 the job programmes introduced in 1988, in particular in the framework of the programme to aid those seeking work (Dispositif d'Aide à l'Insertion Professionnelle).
The development of the tourism sector in Algeria had previously been hampered by a lack of facilities, but since 2004 a broad tourism development strategy has been implemented resulting in many hotels of a high modern standard being built.
There are several UNESCO World Heritage Sites in Algeria including Al Qal'a of Beni Hammad, the first capital of the Hammadid empire; Tipasa, a Phoenician and later Roman town; and Djémila and Timgad, both Roman ruins; M'Zab Valley, a limestone valley containing a large urbanized oasis; and the Casbah of Algiers, an important citadel.
The only natural World Heritage Site is the Tassili n'Ajjer, a mountain range.
The Algerian road network is the densest in Africa; its length is estimated at 180,000 km of highways, with more than 3,756 structures and a paving rate of 85%.
This network will be complemented by the East-West Highway, a major infrastructure project currently under construction.
It is a 3-way, highway, linking Annaba in the extreme east to the Tlemcen in the far west.
Algeria is also crossed by the Trans-Sahara Highway, which is now completely paved.
This road is supported by the Algerian government to increase trade between the six countries crossed: Algeria, Mali, Niger, Nigeria, Chad and Tunisia.
In January 2016 Algeria's population was an estimated 40.4 million, who are mainly Arab-Berber ethnically.
At the outset of the 20th century, its population was approximately four million.
About 90% of Algerians live in the northern, coastal area; the inhabitants of the Sahara desert are mainly concentrated in oases, although some 1.5 million remain nomadic or partly nomadic.
28.1% of Algerians are under the age of 15.
Women make up 70% of the country's lawyers and 60% of its judges and also dominate the field of medicine.
Increasingly, women are contributing more to household income than men.
60% of university students are women, according to university researchers.
Between 90,000 and 165,000 Sahrawis from Western Sahara live in the Sahrawi refugee camps, in the western Algerian Sahara desert.
There are also more than 4,000 Palestinian refugees, who are well integrated and have not asked for assistance from the United Nations High Commissioner for Refugees (UNHCR).
In 2009, 35,000 Chinese migrant workers lived in Algeria.
The largest concentration of Algerian migrants outside Algeria is in France, which has reportedly over 1.7 million Algerians of up to the second generation.
Indigenous Berbers as well as Phoenicians, Romans, Byzantines, Arabs, Turks, various Sub-Saharan Africans, and French have contributed to the history of Algeria.
Descendants of Andalusian refugees are also present in the population of Algiers and other cities.
Moreover, Spanish was spoken by these Aragonese and Castillian Morisco descendants deep into the 18th century, and even Catalan was spoken at the same time by Catalan Morisco descendants in the small town of Grish El-Oued.
Despite the dominance of the Berber culture and ethnicity in Algeria, the majority of Algerians identify with an Arabic-based identity, especially after the Arab nationalism rising in the 20th century.
Berbers and Berber-speaking Algerians are divided into many groups with varying languages.
The largest of these are the Kabyles, who live in the Kabylie region east of Algiers, the Chaoui of Northeast Algeria, the Tuaregs in the southern desert and the Shenwa people of North Algeria.
During the colonial period, there was a large (10% in 1960) European population who became known as "Pied-Noirs".
They were primarily of French, Spanish and Italian origin.
Almost all of this population left during the war of independence or immediately after its end.
Berber and Modern Standard Arabic are the official languages.
Algerian Arabic (Darja) is the language used by the majority of the population.
Colloquial Algerian Arabic is heavily infused with borrowings from French and Berber.
Berber has been recognized as a "national language" by the constitutional amendment of 8 May 2002.
Kabyle, the predominant Berber language, is taught and is partially co-official (with a few restrictions) in parts of Kabylie.
In February 2016, the Algerian constitution passed a resolution that would make Berber an official language alongside Arabic.
Although French has no official status, Algeria is the second-largest Francophone country in the world in terms of speakers, and French is widely used in government, media (newspapers, radio, local television), and both the education system (from primary school onwards) and academia due to Algeria's colonial history.
It can be regarded as a lingua franca of Algeria.
In 2008, 11.2 million Algerians could read and write in French.
An Abassa Institute study in April 2000 found that 60% of households could speak and understand French or 18 million in a population of 30 million then.
After an earlier period during which the Algerian government tried to phase out French (which is why it has no official status), in recent decades the government has backtracked and reinforced the study of French and TV programs have reinforced use of the language.
Algeria emerged as a bilingual state after 1962.
Colloquial Algerian Arabic is spoken by about 72% of the population and Berber by 27–30%.
Islam is the predominant religion in Algeria, with its adherents, mostly Sunnis, accounting for 99% of the population according to a 2012 CIA World Factbook estimate, and 97.9% according to Pew Research in 2010.
There are about 150,000 Ibadis in the M'zab Valley in the region of Ghardaia.
Algeria has given the Muslim world a number of prominent thinkers, including Emir Abdelkader, Abdelhamid Ben Badis, Mouloud Kacem Naît Belkacem, Malek Bennabi and Mohamed Arkoun.
Below is a list of the most important Algerian cities:

Modern Algerian literature, split between Arabic, Tamazight and French, has been strongly influenced by the country's recent history.
Famous novelists of the 20th century include Mohammed Dib, Albert Camus, Kateb Yacine and Ahlam Mosteghanemi while Assia Djebar is widely translated.
Among the important novelists of the 1980s were Rachid Mimouni, later vice-president of Amnesty International, and Tahar Djaout, murdered by an Islamist group in 1993 for his secularist views.
Malek Bennabi and Frantz Fanon are noted for their thoughts on decolonization; Augustine of Hippo was born in Tagaste (modern-day Souk Ahras); and Ibn Khaldun, though born in Tunis, wrote the Muqaddima while staying in Algeria.
The works of the Sanusi family in pre-colonial times, and of Emir Abdelkader and Sheikh Ben Badis in colonial times, are widely noted.
The Latin author Apuleius was born in Madaurus (Mdaourouch), in what later became Algeria.
Contemporary Algerian cinema is various in terms of genre, exploring a wider range of themes and issues.
There has been a transition from cinema which focused on the war of independence to films more concerned with the everyday lives of Algerians.
Algerian painters, like or Baya, attempted to revive the prestigious Algerian past prior to French colonization, at the same time that they have contributed to the preservation of the authentic values of Algeria.
In this line, Mohamed Temam, Abdelkhader Houamel have also returned through this art, scenes from the history of the country, the habits and customs of the past and the country life.
Other new artistic currents including the one of M'hamed Issiakhem, Mohammed Khadda and Bachir Yelles, appeared on the scene of Algerian painting, abandoning figurative classical painting to find new pictorial ways, in order to adapt Algerian paintings to the new realities of the country through its struggle and its aspirations.
Mohammed Khadda and M'hamed Issiakhem have been notable in recent years.
The historic roots of Algerian literature go back to the Numidian and Roman African era, when Apuleius wrote "The Golden Ass", the only Latin novel to survive in its entirety.
This period had also known Augustine of Hippo, Nonius Marcellus and Martianus Capella, among many others.
The Middle Ages have known many Arabic writers who revolutionized the Arab world literature, with authors like Ahmad al-Buni, Ibn Manzur and Ibn Khaldoun, who wrote the Muqaddimah while staying in Algeria, and many others.
Albert Camus was an Algerian-born French Pied-Noir author.
In 1957 he was awarded the Nobel Prize in literature.
Today Algeria contains, in its literary landscape, big names having not only marked the Algerian literature, but also the universal literary heritage in Arabic and French.
As a first step, Algerian literature was marked by works whose main concern was the assertion of the Algerian national entity, there is the publication of novels as the "Algerian trilogy" of Mohammed Dib, or even "Nedjma" of Kateb Yacine novel which is often regarded as a monumental and major work.
Other known writers will contribute to the emergence of Algerian literature whom include Mouloud Feraoun, Malek Bennabi, Malek Haddad, Moufdi Zakaria, Abdelhamid Ben Badis, Mohamed Laïd Al-Khalifa, Mouloud Mammeri, Frantz Fanon, and Assia Djebar.
In the aftermath of the independence, several new authors emerged on the Algerian literary scene, they will attempt through their works to expose a number of social problems, among them there are Rachid Boudjedra, Rachid Mimouni, Leila Sebbar, Tahar Djaout and Tahir Wattar.
Currently, a part of Algerian writers tends to be defined in a literature of shocking expression, due to the terrorism that occurred during the 1990s, the other party is defined in a different style of literature who staged an individualistic conception of the human adventure.
Among the most noted recent works, there is the writer, "the swallows of Kabul" and "the attack" of Yasmina Khadra, "the oath of barbarians" of Boualem Sansal, "memory of the flesh" of Ahlam Mosteghanemi and the last novel by Assia Djebar "nowhere in my father's House".
Chaâbi music is a typically Algerian musical genre characterized by specific rhythms and of Qacidate (Popular poems) in Arabic dialect.
The undisputed master of this music is El Hadj M'Hamed El Anka.
The Constantinois Malouf style is saved by musician from whom Mohamed Tahar Fergani is a performer.
Folk music styles include Bedouin music, characterized by the poetic songs based on long kacida (poems); Kabyle music, based on a rich repertoire that is poetry and old tales passed through generations; Shawiya music, a folklore from diverse areas of the Aurès Mountains.
Rahaba music style is unique to the Aures.
Souad Massi is a rising Algerian folk singer.
Other Algerian singers of the diaspora include Manel Filali in Germany and Kenza Farah in France.
Tergui music is sung in Tuareg languages generally, Tinariwen had a worldwide success.
Finally, the staïfi music is born in Sétif and remains a unique style of its kind.
Modern music is available in several facets, Raï music is a style typical of Western Algeria.
Rap, relatively recent style in Algeria, is experiencing significant growth.
The Algerian state's interest in film-industry activities can be seen in the annual budget of DZD 200 million (EUR 1.8) allocated to production, specific measures and an ambitious programme plan implemented by the Ministry of Culture in order to promote national production, renovate the cinema stock and remedy the weak links in distribution and exploitation.
The financial support provided by the state, through the Fund for the Development of the Arts, Techniques and the Film Industry (FDATIC) and the Algerian Agency for Cultural Influence (AARC), plays a key role in the promotion of national production.
Between 2007 and 2013, FDATIC subsidised 98 films (feature films, documentaries and short films).
In mid-2013, AARC had already supported a total of 78 films, including 42 feature films, 6 short films and 30 documentaries.
According to the European Audiovisual Observatory's LUMIERE database, 41 Algerian films were distributed in Europe between 1996 and 2013; 21 films in this repertoire were Algerian-French co-productions.
"Days of Glory" (2006) and "Outside the Law" (2010) recorded the highest number of admissions in the European Union, 3,172,612 and 474,722, respectively.
Algeria won the Palme d'Or for "Chronicle of the Years of Fire" (1975), two Oscars for "Z" (1969), and other awards for "The Battle of Algiers".
Various games have existed in Algeria since antiquity.
In the Aures, people played several games such as El Kherba or El khergueba (chess variant).
Playing cards, checkers and chess games are part of Algerian culture.
Racing (fantasia) and rifle shooting are part of cultural recreation of the Algerians.
The first Algerian and African gold medalist is Boughera El Ouafi in 1928 Olympics of Amsterdam in the Marathon.
The second Algerian Medalist was Alain Mimoun in 1956 Summer Olympics in Melbourne.
Several men and women were champions in athletics in the 1990s including Noureddine Morceli, Hassiba Boulmerka, Nouria Merah-Benida, and Taoufik Makhloufi, all specialized in middle-distance running.
Football is the most popular sport in Algeria.
Several names are engraved in the history of the sport, including Lakhdar Belloumi, Rachid Mekhloufi, Hassen Lalmas, Rabah Madjer, Salah Assad and Djamel Zidane.
The Algeria national football team qualified for the 1982 FIFA World Cup, 1986 FIFA World Cup, 2010 FIFA World Cup and 2014 FIFA World Cup.
In addition, several football clubs have won continental and international trophies as the club ES Sétif or JS Kabylia.
The Algerian Football Federation is an association of Algeria football clubs organizing national competitions and international matches of the selection of Algeria national football team.
Algerian cuisine is rich and diverse.
The country was considered as the "granary of Rome".
It offers a component of dishes and varied dishes, depending on the region and according to the seasons.
The cuisine uses cereals as the main products, since they are always produced with abundance in the country.
There is not a dish where cereals are not present.
Algerian cuisine varies from one region to another, according to seasonal vegetables.
It can be prepared using meat, fish and vegetables.
Among the dishes known, couscous, chorba, Couscous, Rechta, Chakhchoukha, Berkoukes, Shakshouka, Mthewem, Chtitha, Mderbel, Dolma, Brik or Bourek, Garantita, Lham'hlou, etc.
Merguez sausage is widely used in Algeria, but it differs, depending on the region and on the added spices.
Cakes are marketed and can be found in cities either in Algeria, in Europe or North America.
However, traditional cakes are also made at home, following the habits and customs of each family.
Among these cakes, there are Tamina, Baklawa, Chrik, Garn logzelles, Griouech, Kalb el-louz, Makroud, Mbardja, Mchewek, Samsa, Tcharak, Baghrir, Khfaf, Zlabia, Aarayech, Ghroubiya and Mghergchette.
Algerian pastry also contains Tunisian or French cakes.
Marketed and home-made bread products include varieties such as Kessra or Khmira or Harchaya, chopsticks and so-called washers Khoubz dar or Matloue.
Other traditional meals sold often as street food include Mhadjeb, Karantika, Doubara.
(Chakhchokha-Hassoua-T'chicha-Mahjouba and Doubara)are famous in Biskra.
In 2002, Algeria had inadequate numbers of physicians (1.13 per 1,000 people), nurses (2.23 per 1,000 people), and dentists (0.31 per 1,000 people).
Access to "improved water sources" was limited to 92% of the population in urban areas and 80% of the population in the rural areas.
Some 99% of Algerians living in urban areas, but only 82% of those living in rural areas, had access to "improved sanitation".
According to the World Bank, Algeria is making progress toward its goal of "reducing by half the number of people without sustainable access to improved drinking water and basic sanitation by 2015".
Given Algeria's young population, policy favors preventive health care and clinics over hospitals.
In keeping with this policy, the government maintains an immunization program.
However, poor sanitation and unclean water still cause tuberculosis, hepatitis, measles, typhoid fever, cholera and dysentery.
The poor generally receive health care free of charge.
Health records have been maintained in Algeria since 1882 and began adding Muslims living in the South to their Vital record database in 1905 during French rule.
Since the 1970s, in a centralized system that was designed to significantly reduce the rate of illiteracy, the Algerian government introduced a decree by which school attendance became compulsory for all children aged between 6 and 15 years who have the ability to track their learning through the 20 facilities built since independence, now the literacy rate is around 78.7%.
Since 1972, Arabic is used as the language of instruction during the first nine years of schooling.
From the third year, French is taught and it is also the language of instruction for science classes.
The students can also learn English, Italian, Spanish and German.
In 2008, new programs at the elementary appeared, therefore the compulsory schooling does not start at the age of six anymore, but at the age of five.
Apart from the 122 private schools, the Universities of the State are free of charge.
After nine years of primary school, students can go to the high school or to an educational institution.
The school offers two programs: general or technical.
At the end of the third year of secondary school, students pass the exam of the baccalaureate, which allows once it is successful to pursue graduate studies in universities and institutes.
Education is officially compulsory for children between the ages of six and 15.
In 2008, the illiteracy rate for people over 10 was 22.3%, 15.6% for men and 29.0% for women.
The province with the lowest rate of illiteracy was Algiers Province at 11.6%, while the province with the highest rate was Djelfa Province at 35.5%.
Algeria has 26 universities and 67 institutions of higher education, which must accommodate a million Algerians and 80,000 foreign students in 2008.
The University of Algiers, founded in 1879, is the oldest, it offers education in various disciplines (law, medicine, science and letters).
25 of these universities and almost all of the institutions of higher education were founded after the independence of the country.
Even if some of them offer instruction in Arabic like areas of law and the economy, most of the other sectors as science and medicine continue to be provided in French and English.
Among the most important universities, there are the University of Sciences and Technology Houari Boumediene, the University of Mentouri Constantine, and University of Oran Es-Senia.
The University of Abou Bekr Belkaïd in Tlemcen and University of Batna Hadj Lakhdar occupy the 26th and 45th row in Africa.
</doc>
<doc id="359" url="https://en.wikipedia.org/wiki?curid=359" title="List of Atlas Shrugged characters">
List of Atlas Shrugged characters

This is a list of characters in Ayn Rand's novel "Atlas Shrugged."
The following are major characters from the novel.
Dagny Taggart is the protagonist of the novel.
She is Vice-President in Charge of Operations for Taggart Transcontinental, under her brother, James Taggart.
Given James' incompetence, Dagny is responsible for all the workings of the railroad.
Francisco d'Anconia is one of the central characters in "Atlas Shrugged", an owner by inheritance of the world's largest copper mining operation.
He is a childhood friend, and the first love, of Dagny Taggart.
A child prodigy of exceptional talents, Francisco was dubbed the "climax" of the d'Anconia line, an already prestigious family of skilled industrialists.
He was a classmate of John Galt and Ragnar Danneskjöld and student of both Hugh Akston and Robert Stadler.
He began working while still in school, proving that he could have made a fortune without the aid of his family's wealth and power.
Later, Francisco bankrupts the d'Anconia business to put it out of others' reach.
His full name is given as "Francisco Domingo Carlos Andres Sebastián d'Anconia".
John Galt is the primary male hero of "Atlas Shrugged".
He initially appears as an unnamed menial worker for Taggart Transcontinental, who often dines with Eddie Willers in the employees' cafeteria, and leads Eddie to reveal important information about Dagny Taggart and Taggart Transcontinental.
Only Eddie's side of their conversations is given in the novel.
Later in the novel, the reader discovers this worker's true identity.
Before working for Taggart Transcontinental, Galt worked as an engineer for the Twentieth Century Motor Company, where he secretly invented a generator of usable electric energy from ambient static electricity, but abandoned his prototype, and his employment, when dissatisfied by an easily corrupted novel system of payment.
This prototype was found by Dagny Taggart and Hank Rearden.
Galt himself remains concealed throughout much of the novel, in a valley by himself, where he unites the most skillful inventors and business leaders under his leadership.
Much of the book's third division is given to his broadcast speech, which presents the author's philosophy of Objectivism.
Henry (known as "Hank") Rearden is one of the central characters in "Atlas Shrugged".
He owns the most important steel company in the United States, and invents Rearden Metal, an alloy stronger than steel (with similar properties to stainless steel).
He lives in Philadelphia with his wife Lillian, his brother Philip, and his elderly mother.
Rearden represents a type of self-made man or prototypical hero, and illustrates Rand's theory of sex in so far as he accepts the traditional view of sexual congress as a subhuman instinct, but responds sexually to Dagny Taggart.
Edwin "Eddie" Willers is the Special Assistant to the Vice-President in Charge of Operations at Taggart Transcontinental.
His father and grandfather worked for the Taggarts, and himself likewise.
He is completely loyal to Dagny and to Taggart Transcontinental.
Willers does not possess the creative ability of Galt's associates, but matches them in moral courage and is capable of appreciating and making use of their creations.
After Dagny shifts her attention and loyalty to saving the captive Galt, Willers maintains the railroad until its collapse.
One of Galt's first followers, and world-famous as a pirate, who seizes relief ships sent from the United States to the People's States of Europe.
He works to ensure that once those espousing Galt's philosophy are restored to their rightful place in society, they have enough capital to rebuild the world.
Kept in the background for much of the book, Danneskjöld makes a personal appearance to encourage Rearden to persevere in his increasingly difficult situation, and gives him a bar of gold as compensation for the income taxes he has paid over the last several years.
Danneskjöld is married to the actress Kay Ludlow; their relationship is kept hidden from the outside world, which only knows of Ludlow as a retired film star.
Considered a misfit by Galt's other adherents, he views his actions as a means to speed the world along in understanding Galt's perspective.
According to Barbara Branden, who was closely associated with Rand at the time the book was written, there were sections written describing Danneskjöld's adventures at sea, cut from the final published text.
In a 1974 comment at a lecture, Ayn Rand admitted that Danneskjöld's name was a tribute to Victor Hugo's novel, "Hans of Iceland", wherein the hero becomes the first of the Counts of Danneskjöld.
In the published book, Danneskjöld is always seen through the eyes of others (Dagny Taggart or Hank Rearden), except for a brief paragraph in the very last chapter.
The President of Taggart Transcontinental and the book's most important antagonist.
Taggart is an expert influence peddler but incapable of making operational decisions on his own.
He relies on his sister, Dagny Taggart, to actually run the railroad, but nonetheless opposes her in almost every endeavor because of his various anti-capitalist moral and political beliefs.
In a sense, he is the antithesis of Dagny.
This contradiction leads to the recurring absurdity of his life: the desire to overcome those on whom his life depends, and the horror that he will succeed at this.
In the final chapters of the novel, he suffers a complete mental breakdown upon realizing that he can no longer deceive himself in this respect.
The unsupportive wife of Hank Rearden, who dislikes his habits and (secretly at first) seeks to ruin Rearden to prove her own value.
Lillian achieves this, when she passes information to James Taggart about her husband's affair with his sister.
This information is used to persuade Rearden to sign a Gift Certificate which delivers all the property rights of Rearden Metal to others.
Lillian thereafter uses James Taggart for sexual satisfaction, until Hank abandons her.
Ferris is a biologist who works as "co-ordinator" at the State Science Institute.
He uses his position there to deride reason and productive achievement, and publishes a book entitled "Why Do You Think You Think?"
He clashes on several occasions with Hank Rearden, and twice attempts to blackmail Rearden into giving up Rearden Metal.
He is also one of the group of looters who tries to get Rearden to agree to the Steel Unification Plan.
Ferris hosts the demonstration of the Project X weapon, and is the creator of the Ferris Persuader, a torture machine.
When John Galt is captured by the looters, Ferris uses the device on Galt, but it breaks down before extracting the information Ferris wants from Galt.
Ferris represents the group which uses brute force on the heroes to achieve the ends of the looters.
A former professor at Patrick Henry University, and along with colleague Hugh Akston, mentor to Francisco d'Anconia, John Galt and Ragnar Danneskjöld.
He has since become a sell-out, one who had great promise but squandered it for social approval, to the detriment of the free.
He works at the State Science Institute where all his inventions are perverted for use by the military, including the instrument of his demise: Project X (Xylophone).
The character was, in part, modeled on J. Robert Oppenheimer, whom Rand had interviewed for an earlier project, and his part in the creation of nuclear weapons.
To his former student Galt, Stadler represents the epitome of human evil, as the "man who knew better" but chose not to act for the good.
The incompetent and treacherous lobbyist whom Hank Rearden reluctantly employs in Washington, who rises to prominence and authority throughout the novel through trading favours and disloyalty.
In return for betraying Hank by helping broker the Equalization of Opportunity Bill (which, by restricting the number of businesses each person may own to one, forces Hank to divest most of his companies), he is given a senior position at the Bureau of Economic Planning and National Resources.
Later in the novel he becomes its Top Co-ordinator, a position that eventually becomes Economic Dictator of the country.
The following secondary characters also appear in the novel.
</doc>
<doc id="569" url="https://en.wikipedia.org/wiki?curid=569" title="Anthropology">
Anthropology

Anthropology is the study of humans and human behavior and societies in the past and present.
Social anthropology and cultural anthropology study the norms and values of societies.
Linguistic anthropology studies how language affects social life.
Biological or physical anthropology studies the biological development of humans.
Archaeology, which studies past human cultures through investigation of physical evidence, is thought of as a branch of anthropology in the United States and Canada, while in Europe, it is viewed as a discipline in its own right or grouped under other related disciplines, such as history.
The abstract noun "anthropology" is first attested in reference to history.
Its present use first appeared in Renaissance Germany in the works of Magnus Hundt and Otto Casmann.
Their New Latin ' derived from the combining forms of the Greek words "ánthrōpos" (, "human") and "lógos" (, "study").
(Its adjectival form appeared in the works of Aristotle.)
It began to be used in English, possibly via French ', by the early 18th century.
In 1647, the Bartholins, founders of the University of Copenhagen, defined "" as follows:

Anthropology, that is to say the science that treats of man, is divided ordinarily and with reason into Anatomy, which considers the body and the parts, and Psychology, which speaks of the soul.
Sporadic use of the term for some of the subject matter occurred subsequently, such as the use by Étienne Serres in 1839 to describe the natural history, or paleontology, of man, based on comparative anatomy, and the creation of a chair in anthropology and ethnography in 1850 at the National Museum of Natural History (France) by Jean Louis Armand de Quatrefages de Bréau.
Various short-lived organizations of anthropologists had already been formed.
The Société Ethnologique de Paris, the first to use Ethnology, was formed in 1839.
Its members were primarily anti-slavery activists.
When slavery was abolished in France in 1848 the Société was abandoned.
Meanwhile, the Ethnological Society of New York, currently the American Ethnological Society, was founded on its model in 1842, as well as the Ethnological Society of London in 1843, a break-away group of the Aborigines' Protection Society.
These anthropologists of the times were liberal, anti-slavery, and pro-human-rights activists.
They maintained international connections.
Anthropology and many other current fields are the intellectual results of the comparative methods developed in the earlier 19th century.
Theorists in such diverse fields as anatomy, linguistics, and Ethnology, making feature-by-feature comparisons of their subject matters, were beginning to suspect that similarities between animals, languages, and folkways were the result of processes or laws unknown to them then.
For them, the publication of Charles Darwin's "On the Origin of Species" was the epiphany of everything they had begun to suspect.
Darwin himself arrived at his conclusions through comparison of species he had seen in agronomy and in the wild.
Darwin and Wallace unveiled evolution in the late 1850s.
There was an immediate rush to bring it into the social sciences.
Paul Broca in Paris was in the process of breaking away from the Société de biologie to form the first of the explicitly anthropological societies, the Société d'Anthropologie de Paris, meeting for the first time in Paris in 1859.
When he read Darwin, he became an immediate convert to "Transformisme", as the French called evolutionism.
His definition now became "the study of the human group, considered as a whole, in its details, and in relation to the rest of nature".
Broca, being what today would be called a neurosurgeon, had taken an interest in the pathology of speech.
He wanted to localize the difference between man and the other animals, which appeared to reside in speech.
He discovered the speech center of the human brain, today called Broca's area after him.
His interest was mainly in Biological anthropology, but a German philosopher specializing in psychology, Theodor Waitz, took up the theme of general and social anthropology in his six-volume work, entitled "Die Anthropologie der Naturvölker", 1859–1864.
The title was soon translated as "The Anthropology of Primitive Peoples".
The last two volumes were published posthumously.
Waitz defined anthropology as "the science of the nature of man".
By nature he meant matter animated by "the Divine breath"; i.e., he was an animist.
Following Broca's lead, Waitz points out that anthropology is a new field, which would gather material from other fields, but would differ from them in the use of comparative anatomy, physiology, and psychology to differentiate man from "the animals nearest to him".
He stresses that the data of comparison must be empirical, gathered by experimentation.
The history of civilization, as well as ethnology, are to be brought into the comparison.
It is to be presumed fundamentally that the species, man, is a unity, and that "the same laws of thought are applicable to all men".
Waitz was influential among the British ethnologists.
In 1863 the explorer Richard Francis Burton and the speech therapist James Hunt broke away from the Ethnological Society of London to form the Anthropological Society of London, which henceforward would follow the path of the new anthropology rather than just ethnology.
It was the 2nd society dedicated to general anthropology in existence.
Representatives from the French "Société" were present, though not Broca.
In his keynote address, printed in the first volume of its new publication, "The Anthropological Review", Hunt stressed the work of Waitz, adopting his definitions as a standard.
Among the first associates were the young Edward Burnett Tylor, inventor of cultural anthropology, and his brother Alfred Tylor, a geologist.
Previously Edward had referred to himself as an ethnologist; subsequently, an anthropologist.
Similar organizations in other countries followed: The Anthropological Society of Madrid (1865), the American Anthropological Association in 1902, the Anthropological Society of Vienna (1870), the Italian Society of Anthropology and Ethnology (1871), and many others subsequently.
The majority of these were evolutionist.
One notable exception was the Berlin Society for Anthropology, Ethnology, and Prehistory (1869) founded by Rudolph Virchow, known for his vituperative attacks on the evolutionists.
Not religious himself, he insisted that Darwin's conclusions lacked empirical foundation.
During the last three decades of the 19th century, a proliferation of anthropological societies and associations occurred, most independent, most publishing their own journals, and all international in membership and association.
The major theorists belonged to these organizations.
They supported the gradual osmosis of anthropology curricula into the major institutions of higher learning.
By 1898 the American Association for the Advancement of Science was able to report that 48 educational institutions in 13 countries had some curriculum in anthropology.
None of the 75 faculty members were under a department named anthropology.
This meager statistic expanded in the 20th century to comprise anthropology departments in the majority of the world's higher educational institutions, many thousands in number.
Anthropology has diversified from a few major subdivisions to dozens more.
Practical Anthropology, the use of anthropological knowledge and technique to solve specific problems, has arrived; for example, the presence of buried victims might stimulate the use of a forensic archaeologist to recreate the final scene.
The organization has reached global level.
For example, the World Council of Anthropological Associations (WCAA), "a network of national, regional and international associations that aims to promote worldwide communication and cooperation in anthropology", currently contains members from about three dozen nations.
Since the work of Franz Boas and Bronisław Malinowski in the late 19th and early 20th centuries, "social" anthropology in Great Britain and "cultural" anthropology in the US have been distinguished from other social sciences by its emphasis on cross-cultural comparisons, long-term in-depth examination of context, and the importance it places on participant-observation or experiential immersion in the area of research.
Cultural anthropology, in particular, has emphasized cultural relativism, holism, and the use of findings to frame cultural critiques.
This has been particularly prominent in the United States, from Boas' arguments against 19th-century racial ideology, through Margaret Mead's advocacy for gender equality and sexual liberation, to current criticisms of post-colonial oppression and promotion of multiculturalism.
Ethnography is one of its primary research designs as well as the text that is generated from anthropological fieldwork.
In Great Britain and the Commonwealth countries, the British tradition of social anthropology tends to dominate.
In the United States, anthropology has traditionally been divided into the four field approach developed by Franz Boas in the early 20th century: "biological" or "physical" anthropology; "social", "cultural", or "sociocultural" anthropology; and archaeology; plus anthropological linguistics.
These fields frequently overlap but tend to use different methodologies and techniques.
European countries with overseas colonies tended to practice more ethnology (a term coined and defined by Adam F. Kollár in 1783).
It is sometimes referred to as sociocultural anthropology in the parts of the world that were influenced by the European tradition.
Anthropology is a global discipline involving humanities, social sciences and natural sciences.
Anthropology builds upon knowledge from natural sciences, including the discoveries about the origin and evolution of "Homo sapiens", human physical traits, human behavior, the variations among different groups of humans, how the evolutionary past of "Homo sapiens" has influenced its social organization and culture, and from social sciences, including the organization of human social and cultural relations, institutions, social conflicts, etc.
Early anthropology originated in Classical Greece and Persia and studied and tried to understand observable cultural diversity.
As such, anthropology has been central in the development of several new (late 20th century) interdisciplinary fields such as cognitive science, global studies, and various ethnic studies.
According to Clifford Geertz,
Sociocultural anthropology has been heavily influenced by structuralist and postmodern theories, as well as a shift toward the analysis of modern societies.
During the 1970s and 1990s, there was an epistemological shift away from the positivist traditions that had largely informed the discipline.
During this shift, enduring questions about the nature and production of knowledge came to occupy a central place in cultural and social anthropology.
In contrast, archaeology and biological anthropology remained largely positivist.
Due to this difference in epistemology, the four sub-fields of anthropology have lacked cohesion over the last several decades.
Sociocultural anthropology draws together the principle axes of cultural anthropology and social anthropology.
Cultural anthropology is the comparative study of the manifold ways in which people "make sense" of the world around them, while social anthropology is the study of the "relationships" among individuals and groups.
Cultural anthropology is more related to philosophy, literature and the arts (how one's culture affects the experience for self and group, contributing to a more complete understanding of the people's knowledge, customs, and institutions), while social anthropology is more related to sociology and history.
In that, it helps develop an understanding of social structures, typically of others and other populations (such as minorities, subgroups, dissidents, etc.).
There is no hard-and-fast distinction between them, and these categories overlap to a considerable degree.
Inquiry in sociocultural anthropology is guided in part by cultural relativism, the attempt to understand other societies in terms of their own cultural symbols and values.
Accepting other cultures in their own terms moderates reductionism in cross-cultural comparison.
This project is often accommodated in the field of ethnography.
Ethnography can refer to both a methodology and the product of ethnographic research, i.e. an ethnographic monograph.
As a methodology, ethnography is based upon long-term fieldwork within a community or other research site.
Participant observation is one of the foundational methods of social and cultural anthropology.
Ethnology involves the systematic comparison of different cultures.
The process of participant-observation can be especially helpful to understanding a culture from an emic (conceptual, vs. etic, or technical) point of view.
The study of kinship and social organization is a central focus of sociocultural anthropology, as kinship is a human universal.
Sociocultural anthropology also covers economic and political organization, law and conflict resolution, patterns of consumption and exchange, material culture, technology, infrastructure, gender relations, ethnicity, childrearing and socialization, religion, myth, symbols, values, etiquette, worldview, sports, music, nutrition, recreation, games, food, festivals, and language (which is also the object of study in linguistic anthropology).
Comparison across cultures is a key element of method in sociocultural anthropology, including the industrialized (and de-industrialized) West.
Cultures in the Standard Cross-Cultural Sample (SCCS) of world societies are:

Biological anthropology and physical anthropology are synonymous terms to describe anthropological research focused on the study of humans and non-human primates in their biological, evolutionary, and demographic dimensions.
It examines the biological and social factors that have affected the evolution of humans and other primates, and that generate, maintain or change contemporary genetic and physiological variation.
Archaeology is the study of the human past through its material remains.
Artifacts, faunal remains, and human altered landscapes are evidence of the cultural and material lives of past societies.
Archaeologists examine this material remains in order to deduce patterns of past human behavior and cultural practices.
Ethnoarchaeology is a type of archaeology that studies the practices and material remain of living human groups in order to gain a better understanding of the evidence left behind by past human groups, who are presumed to have lived in similar ways.
Linguistic anthropology (not to be confused with anthropological linguistics) seeks to understand the processes of human communications, verbal and non-verbal, variation in language across time and space, the social uses of language, and the relationship between language and culture.
It is the branch of anthropology that brings linguistic methods to bear on anthropological problems, linking the analysis of linguistic forms and processes to the interpretation of sociocultural processes.
Linguistic anthropologists often draw on related fields including sociolinguistics, pragmatics, cognitive linguistics, semiotics, discourse analysis, and narrative analysis.
One of the central problems in the anthropology of art concerns the universality of 'art' as a cultural phenomenon.
Several anthropologists have noted that the Western categories of 'painting', 'sculpture', or 'literature', conceived as independent artistic activities, do not exist, or exist in a significantly different form, in most non-Western contexts.
To surmount this difficulty, anthropologists of art have focused on formal features in objects which, without exclusively being 'artistic', have certain evident 'aesthetic' qualities.
Boas' "Primitive Art", Claude Lévi-Strauss' "The Way of the Masks" (1982) or Geertz's 'Art as Cultural System' (1983) are some examples in this trend to transform the anthropology of 'art' into an anthropology of culturally specific 'aesthetics'.
Media anthropology (also known as the anthropology of media or mass media) emphasizes ethnographic studies as a means of understanding producers, audiences, and other cultural and social aspects of mass media.
The types of ethnographic contexts explored range from contexts of media production (e.g., ethnographies of newsrooms in newspapers, journalists in the field, film production) to contexts of media reception, following audiences in their everyday responses to media.
Other types include cyber anthropology, a relatively new area of internet research, as well as ethnographies of other areas of research which happen to involve media, such as development work, social movements, or health education.
This is in addition to many classic ethnographic contexts, where media such as radio, the press, new media, and television have started to make their presences felt since the early 1990s.
Ethnomusicology is an academic field encompassing various approaches to the study of music (broadly defined), that emphasize its cultural, social, material, cognitive, biological, and other dimensions or contexts instead of or in addition to its isolated sound component or any particular repertoire.
Visual anthropology is concerned, in part, with the study and production of ethnographic photography, film and, since the mid-1990s, new media.
While the term is sometimes used interchangeably with ethnographic film, visual anthropology also encompasses the anthropological study of visual representation, including areas such as performance, museums, art, and the production and reception of mass media.
Visual representations from all cultures, such as sandpaintings, tattoos, sculptures and reliefs, cave paintings, scrimshaw, jewelry, hieroglyphics, paintings, and photographs are included in the focus of visual anthropology.
Economic anthropology attempts to explain human economic behavior in its widest historic, geographic and cultural scope.
It has a complex relationship with the discipline of economics, of which it is highly critical.
Its origins as a sub-field of anthropology begin with the Polish-British founder of anthropology, Bronisław Malinowski, and his French compatriot, Marcel Mauss, on the nature of gift-giving exchange (or reciprocity) as an alternative to market exchange.
Economic Anthropology remains, for the most part, focused upon exchange.
The school of thought derived from Marx and known as Political Economy focuses on production, in contrast.
Economic anthropologists have abandoned the primitivist niche they were relegated to by economists, and have now turned to examine corporations, banks, and the global financial system from an anthropological perspective.
Political economy in anthropology is the application of the theories and methods of Historical Materialism to the traditional concerns of anthropology, including, but not limited to, non-capitalist societies.
Political economy introduced questions of history and colonialism to ahistorical anthropological theories of social structure and culture.
Three main areas of interest rapidly developed.
The first of these areas was concerned with the "pre-capitalist" societies that were subject to evolutionary "tribal" stereotypes.
Sahlin's work on hunter-gatherers as the "original affluent society" did much to dissipate that image.
The second area was concerned with the vast majority of the world's population at the time, the peasantry, many of whom were involved in complex revolutionary wars such as in Vietnam.
The third area was on colonialism, imperialism, and the creation of the capitalist world-system.
More recently, these political economists have more directly addressed issues of industrial (and post-industrial) capitalism around the world.
Applied anthropology refers to the application of the method and theory of anthropology to the analysis and solution of practical problems.
It is a "complex of related, research-based, instrumental methods which produce change or stability in specific cultural systems through the provision of data, initiation of direct action, and/or the formulation of policy".
More simply, applied anthropology is the practical side of anthropological research; it includes researcher involvement and activism within the participating community.
It is closely related to development anthropology (distinct from the more critical anthropology of development).
Anthropology of development tends to view development from a "critical" perspective.
The kind of issues addressed and implications for the approach simply involve pondering why, if a key development goal is to alleviate poverty, is poverty increasing?
Why is there such a gap between plans and outcomes?
Why are those working in development so willing to disregard history and the lessons it might offer?
Why is development so externally driven rather than having an internal basis?
In short, why does so much planned development fail?
"Kinship" can refer both to "the study of" the patterns of social relationships in one or more human cultures, or it can refer to "the patterns of social relationships" themselves.
Over its history, anthropology has developed a number of related concepts and terms, such as "descent", "descent groups", "lineages", "affines", "cognates", and even "fictive kinship".
Broadly, kinship patterns may be considered to include people related both by descent (one's social relations during development), and also relatives by marriage.
Feminist anthropology is a four field approach to anthropology (archeological, biological, cultural, linguistic) that seeks to reduce male bias in research findings, anthropological hiring practices, and the scholarly production of knowledge.
Anthropology engages often with feminists from non-Western traditions, whose perspectives and experiences can differ from those of white European and American feminists.
Historically, such 'peripheral' perspectives have sometimes been marginalized and regarded as less valid or important than knowledge from the western world.
Feminist anthropologists have claimed that their research helps to correct this systematic bias in mainstream feminist theory.
Feminist anthropologists are centrally concerned with the construction of gender across societies.
Feminist anthropology is inclusive of birth anthropology as a specialization.
The first African-American female anthropologist and Caribbeanist is said to be Vera Mae Green who studied ethnic and family relations in the Caribbean as well as the United States, and thereby tried to improve the way black life, experiences, and culture were studied.
Medical anthropology is an interdisciplinary field which studies "human health and disease, health care systems, and biocultural adaptation".
It is believed that William Caudell was the first to discover the field of medical anthropology.
Currently, research in medical anthropology is one of the main growth areas in the field of anthropology as a whole.
It focuses on the following six basic fields:
Other subjects that have become central to medical anthropology worldwide are violence and social suffering (Farmer, 1999, 2003; Beneduce, 2010) as well as other issues that involve physical and psychological harm and suffering that are not a result of illness.
On the other hand, there are fields that intersect with medical anthropology in terms of research methodology and theoretical production, such as "cultural psychiatry" and "transcultural psychiatry" or "ethnopsychiatry".
Nutritional anthropology is a synthetic concept that deals with the interplay between economic systems, nutritional status and food security, and how changes in the former affect the latter.
If economic and environmental changes in a community affect access to food, food security, and dietary health, then this interplay between culture and biology is in turn connected to broader historical and economic trends associated with globalization.
Nutritional status affects overall health status, work performance potential, and the overall potential for economic development (either in terms of human development or traditional western models) for any given group of people.
Psychological anthropology is an interdisciplinary subfield of anthropology that studies the interaction of cultural and mental processes.
This subfield tends to focus on ways in which humans' development and enculturation within a particular cultural group—with its own history, language, practices, and conceptual categories—shape processes of human cognition, emotion, perception, motivation, and mental health.
It also examines how the understanding of cognition, emotion, motivation, and similar psychological processes inform or constrain our models of cultural and social processes.
Cognitive anthropology seeks to explain patterns of shared knowledge, cultural innovation, and transmission over time and space using the methods and theories of the cognitive sciences (especially experimental psychology and evolutionary biology) often through close collaboration with historians, ethnographers, archaeologists, linguists, musicologists and other specialists engaged in the description and interpretation of cultural forms.
Cognitive anthropology is concerned with what people from different groups know and how that implicit knowledge changes the way people perceive and relate to the world around them.
Transpersonal anthropology studies the relationship between altered states of consciousness and culture.
As with transpersonal psychology, the field is much concerned with altered states of consciousness (ASC) and transpersonal experience.
However, the field differs from mainstream transpersonal psychology in taking more cognizance of cross-cultural issues—for instance, the roles of myth, ritual, diet, and texts in evoking and interpreting extraordinary experiences.
Political anthropology concerns the structure of political systems, looked at from the basis of the structure of societies.
Political anthropology developed as a discipline concerned primarily with politics in stateless societies, a new development started from the 1960s, and is still unfolding: anthropologists started increasingly to study more "complex" social settings in which the presence of states, bureaucracies and markets entered both ethnographic accounts and analysis of local phenomena.
The turn towards complex societies meant that political themes were taken up at two main levels.
Firstly, anthropologists continued to study political organization and political phenomena that lay outside the state-regulated sphere (as in patron-client relations or tribal political organization).
Secondly, anthropologists slowly started to develop a disciplinary concern with states and their institutions (and on the relationship between formal and informal political institutions).
An anthropology of the state developed, and it is a most thriving field today.
Geertz' comparative work on "Negara", the Balinese state, is an early, famous example.
Legal anthropology or anthropology of law specializes in "the cross-cultural study of social ordering".
Earlier legal anthropological research often focused more narrowly on conflict management, crime, sanctions, or formal regulation.
More recent applications include issues such as human rights, legal pluralism, and political uprisings.
Public anthropology was created by Robert Borofsky, a professor at Hawaii Pacific University, to "demonstrate the ability of anthropology and anthropologists to effectively address problems beyond the discipline – illuminating larger social issues of our times as well as encouraging broad, public conversations about them with the explicit goal of fostering social change".
Cyborg anthropology originated as a sub-focus group within the American Anthropological Association's annual meeting in 1993.
The sub-group was very closely related to STS and the Society for the Social Studies of Science.
Donna Haraway's 1985 "Cyborg Manifesto" could be considered the founding document of cyborg anthropology by first exploring the philosophical and sociological ramifications of the term.
Cyborg anthropology studies humankind and its relations with the technological systems it has built, specifically modern technological systems that have reflexively shaped notions of what it means to be human beings.
Digital anthropology is the study of the relationship between humans and digital-era technology, and extends to various areas where anthropology and technology intersect.
It is sometimes grouped with sociocultural anthropology, and sometimes considered part of material culture.
The field is new, and thus has a variety of names with a variety of emphases.
These include techno-anthropology, digital ethnography, cyberanthropology, and virtual anthropology.
Ecological anthropology is defined as the "study of cultural adaptations to environments".
The sub-field is also defined as, "the study of relationships between a population of humans and their biophysical environment".
The focus of its research concerns "how cultural beliefs and practices helped human populations adapt to their environments, and how their environment across space and time.
The contemporary perspective of environmental anthropology, and arguably at least the backdrop, if not the focus of most of the ethnographies and cultural fieldworks of today, is political ecology.
Many characterize this new perspective as more informed with culture, politics and power, globalization, localized issues, century anthropology and more.
The focus and data interpretation is often used for arguments for/against or creation of policy, and to prevent corporate exploitation and damage of land.
Often, the observer has become an active part of the struggle either directly (organizing, participation) or indirectly (articles, documentaries, books, ethnographies).
Such is the case with environmental justice advocate Melissa Checker and her relationship with the people of Hyde Park.
Ethnohistory is the study of ethnographic cultures and indigenous customs by examining historical records.
It is also the study of the history of various ethnic groups that may or may not exist today.
Ethnohistory uses both historical and ethnographic data as its foundation.
Its historical methods and materials go beyond the standard use of documents and manuscripts.
Practitioners recognize the utility of such source material as maps, music, paintings, photography, folklore, oral tradition, site exploration, archaeological materials, museum collections, enduring customs, language, and place names.
The anthropology of religion involves the study of religious institutions in relation to other social institutions, and the comparison of religious beliefs and practices across cultures.
Modern anthropology assumes that there is complete continuity between magical thinking and religion, and that every religion is a cultural product, created by the human community that worships it.
Urban anthropology is concerned with issues of urbanization, poverty, and neoliberalism.
Ulf Hannerz quotes a 1960s remark that traditional anthropologists were "a notoriously agoraphobic lot, anti-urban by definition".
Various social processes in the Western World as well as in the "Third World" (the latter being the habitual focus of attention of anthropologists) brought the attention of "specialists in 'other cultures'" closer to their homes.
There are two main approaches to urban anthropology: examining the types of cities or examining the social issues within the cities.
These two methods are overlapping and dependent of each other.
By defining different types of cities, one would use social factors as well as economic and political factors to categorize the cities.
By directly looking at the different social issues, one would also be studying how they affect the dynamic of the city.
Anthrozoology (also known as "human–animal studies") is the study of interaction between living things.
It is an interdisciplinary field that overlaps with a number of other disciplines, including anthropology, ethology, medicine, psychology, veterinary medicine and zoology.
A major focus of anthrozoologic research is the quantifying of the positive effects of human-animal relationships on either party and the study of their interactions.
It includes scholars from a diverse range of fields, including anthropology, sociology, biology, and philosophy.
Biocultural anthropology is the scientific exploration of the relationships between human biology and culture.
Physical anthropologists throughout the first half of the 20th century viewed this relationship from a racial perspective; that is, from the assumption that typological human biological differences lead to cultural differences.
After World War II the emphasis began to shift toward an effort to explore the role culture plays in shaping human biology.
Evolutionary anthropology is the interdisciplinary study of the evolution of human physiology and human behaviour and the relation between hominins and non-hominin primates.
Evolutionary anthropology is based in natural science and social science, combining the human development with socioeconomic factors.
Evolutionary anthropology is concerned with both biological and cultural evolution of humans, past and present.
It is based on a scientific approach, and brings together fields such as archaeology, behavioral ecology, psychology, primatology, and genetics.
It is a dynamic and interdisciplinary field, drawing on many lines of evidence to understand the human experience, past and present.
Forensic anthropology is the application of the science of physical anthropology and human osteology in a legal setting, most often in criminal cases where the victim's remains are in the advanced stages of decomposition.
A forensic anthropologist can assist in the identification of deceased individuals whose remains are decomposed, burned, mutilated or otherwise unrecognizable.
The adjective "forensic" refers to the application of this subfield of science to a court of law.
Paleoanthropology combines the disciplines of paleontology and physical anthropology.
It is the study of ancient humans, as found in fossil hominid evidence such as petrifacted bones and footprints.
Contemporary anthropology is an established science with academic departments at most universities and colleges.
The single largest organization of anthropologists is the American Anthropological Association (AAA), which was founded in 1903.
Its members are anthropologists from around the globe.
In 1989, a group of European and American scholars in the field of anthropology established the European Association of Social Anthropologists (EASA) which serves as a major professional organization for anthropologists working in Europe.
The EASA seeks to advance the status of anthropology in Europe and to increase visibility of marginalized anthropological traditions and thereby contribute to the project of a global anthropology or world anthropology.
Hundreds of other organizations exist in the various sub-fields of anthropology, sometimes divided up by nation or region, and many anthropologists work with collaborators in other disciplines, such as geology, physics, zoology, paleontology, anatomy, music theory, art history, sociology and so on, belonging to professional societies in those disciplines as well.
As the field has matured it has debated and arrived at ethical principles aimed at protecting both the subjects of anthropological research as well as the researchers themselves, and professional societies have generated codes of ethics.
Anthropologists, like other researchers (especially historians and scientists engaged in field research), have over time assisted state policies and projects, especially colonialism.
Some commentators have contended:

As part of their quest for scientific objectivity, present-day anthropologists typically urge cultural relativism, which has an influence on all the sub-fields of anthropology.
This is the notion that cultures should not be judged by another's values or viewpoints, but be examined dispassionately on their own terms.
There should be no notions, in good anthropology, of one culture being better or worse than another culture.
Ethical commitments in anthropology include noticing and documenting genocide, infanticide, racism, mutilation (including circumcision and subincision), and torture.
Topics like racism, slavery, and human sacrifice attract anthropological attention and theories ranging from nutritional deficiencies to genes to acculturation have been proposed, not to mention theories of colonialism and many others as root causes of Man's inhumanity to man.
To illustrate the depth of an anthropological approach, one can take just one of these topics, such as "racism" and find thousands of anthropological references, stretching across all the major and minor sub-fields.
Anthropologists' involvement with the U.S.
government, in particular, has caused bitter controversy within the discipline.
Franz Boas publicly objected to US participation in World War I, and after the war he published a brief expose and condemnation of the participation of several American archaeologists in espionage in Mexico under their cover as scientists.
But by the 1940s, many of Boas' anthropologist contemporaries were active in the allied war effort against the Axis Powers (Nazi Germany, Fascist Italy, and Imperial Japan).
Many served in the armed forces, while others worked in intelligence (for example, Office of Strategic Services and the Office of War Information).
At the same time, David H. Price's work on American anthropology during the Cold War provides detailed accounts of the pursuit and dismissal of several anthropologists from their jobs for communist sympathies.
Attempts to accuse anthropologists of complicity with the CIA and government intelligence activities during the Vietnam War years have turned up surprisingly little.
Many anthropologists (students and teachers) were active in the antiwar movement.
Numerous resolutions condemning the war in all its aspects were passed overwhelmingly at the annual meetings of the American Anthropological Association (AAA).
Professional anthropological bodies often object to the use of anthropology for the benefit of the state.
Their codes of ethics or statements may proscribe anthropologists from giving secret briefings.
The Association of Social Anthropologists of the UK and Commonwealth (ASA) has called certain scholarship ethically dangerous.
The AAA's current 'Statement of Professional Responsibility' clearly states that "in relation with their own government and with host governments ... no secret research, no secret reports or debriefings of any kind should be agreed to or given."
Anthropologists, along with other social scientists, are working with the US military as part of the US Army's strategy in Afghanistan.
The "Christian Science Monitor" reports that "Counterinsurgency efforts focus on better grasping and meeting local needs" in Afghanistan, under the "Human Terrain System" (HTS) program; in addition, HTS teams are working with the US military in Iraq.
In 2009, the American Anthropological Association's Commission on the Engagement of Anthropology with the US Security and Intelligence Communities released its final report concluding, in part, that, "When ethnographic investigation is determined by military missions, not subject to external review, where data collection occurs in the context of war, integrated into the goals of counterinsurgency, and in a potentially coercive environment – all characteristic factors of the HTS concept and its application – it can no longer be considered a legitimate professional exercise of anthropology.
In summary, while we stress that constructive engagement between anthropology and the military is possible, CEAUSSIC suggests that the AAA emphasize the incompatibility of HTS with disciplinary ethics and practice for job seekers and that it further recognize the problem of allowing HTS to define the meaning of "anthropology" within DoD."
Before WWII British 'social anthropology' and American 'cultural anthropology' were still distinct traditions.
After the war, enough British and American anthropologists borrowed ideas and methodological approaches from one another that some began to speak of them collectively as 'sociocultural' anthropology.
There are several characteristics that tend to unite anthropological work.
One of the central characteristics is that anthropology tends to provide a comparatively more holistic account of phenomena and tends to be highly empirical.
The quest for holism leads most anthropologists to study a particular place, problem or phenomenon in detail, using a variety of methods, over a more extensive period than normal in many parts of academia.
In the 1990s and 2000s, calls for clarification of what constitutes a culture, of how an observer knows where his or her own culture ends and another begins, and other crucial topics in writing anthropology were heard.
These dynamic relationships, between what can be observed on the ground, as opposed to what can be observed by compiling many local observations remain fundamental in any kind of anthropology, whether cultural, biological, linguistic or archaeological.
Biological anthropologists are interested in both human variation and in the possibility of human universals (behaviors, ideas or concepts shared by virtually all human cultures).
They use many different methods of study, but modern population genetics, participant observation and other techniques often take anthropologists "into the field," which means traveling to a community in its own setting, to do something called "fieldwork."
On the biological or physical side, human measurements, genetic samples, nutritional data may be gathered and published as articles or monographs.
Along with dividing up their project by theoretical emphasis, anthropologists typically divide the world up into relevant time periods and geographic regions.
Human time on Earth is divided up into relevant cultural traditions based on material, such as the Paleolithic and the Neolithic, of particular use in archaeology.
Further cultural subdivisions according to tool types, such as Olduwan or Mousterian or Levalloisian help archaeologists and other anthropologists in understanding major trends in the human past.
Anthropologists and geographers share approaches to culture regions as well, since mapping cultures is central to both sciences.
By making comparisons across cultural traditions (time-based) and cultural regions (space-based), anthropologists have developed various kinds of comparative method, a central part of their science.
Because anthropology developed from so many different enterprises (see History of anthropology), including but not limited to fossil-hunting, exploring, documentary film-making, paleontology, primatology, antiquity dealings and curatorship, philology, etymology, genetics, regional analysis, ethnology, history, philosophy, and religious studies, it is difficult to characterize the entire field in a brief article, although attempts to write histories of the entire field have been made.
Some authors argue that anthropology originated and developed as the study of "other cultures", both in terms of time (past societies) and space (non-European/non-Western societies).
For example, the classic of urban anthropology, Ulf Hannerz in the introduction to his seminal "Exploring the City: Inquiries Toward an Urban Anthropology" mentions that the "Third World" had habitually received most of attention; anthropologists who traditionally specialized in "other cultures" looked for them far away and started to look "across the tracks" only in late 1960s.
Now there exist many works focusing on peoples and topics very close to the author's "home".
It is also argued that other fields of study, like History and Sociology, on the contrary focus disproportionately on the West.
In France, the study of Western societies has been traditionally left to sociologists, but this is increasingly changing, starting in the 1970s from scholars like Isac Chiva and journals like "Terrain" ("fieldwork"), and developing with the center founded by Marc Augé ("Le Centre d'anthropologie des mondes contemporains", the Anthropological Research Center of Contemporary Societies).
Since the 1980s it has become common for social and cultural anthropologists to set ethnographic research in the North Atlantic region, frequently examining the connections between locations rather than limiting research to a single locale.
There has also been a related shift toward broadening the focus beyond the daily life of ordinary people; increasingly, research is set in settings such as scientific laboratories, social movements, governmental and nongovernmental organizations and businesses.
</doc>
<doc id="572" url="https://en.wikipedia.org/wiki?curid=572" title="Agricultural science">
Agricultural science

Agricultural science is a broad multidisciplinary field of biology that encompasses the parts of exact, natural, economic and social sciences that are used in the practice and understanding of agriculture.
(Veterinary science, but not animal science, is often excluded from the definition.)
The three terms are often confused.
However, they cover different concepts:


Agricultural sciences include research and development on:

Agricultural biotechnology is a specific area of agricultural science involving the use of scientific tools and techniques, including genetic engineering, molecular markers, molecular diagnostics, vaccines, and tissue culture, to modify living organisms: plants, animals, and microorganisms.
One of the most common yield reducers is because of fertilizer not being applied in slightly higher quantities during transition period, the time it takes the soil to rebuild its aggregates and organic matter.
Yields will decrease temporarily because of nitrogen being immobilized in the crop residue, which can take a few months to several years to decompose, depending on the crop's C to N ratio and the local environment.
In the 18th century, Johann Friedrich Mayer conducted experiments on the use of gypsum (hydrated calcium sulphate) as a fertilizer.
In 1843, John Lawes and Henry Gilbert began a set of long-term field experiments at Rothamsted Research Station in England; some of them are still running.
In the United States, a scientific revolution in agriculture began with the Hatch Act of 1887, which used the term "agricultural science".
The Hatch Act was driven by farmers' interest in knowing the constituents of early artificial fertilizer.
The Smith-Hughes Act of 1917 shifted agricultural education back to its vocational roots, but the scientific foundation had been built.
After 1906, public expenditures on agricultural research in the US exceeded private expenditures for the next 44 years.
</doc>
<doc id="573" url="https://en.wikipedia.org/wiki?curid=573" title="Alchemy">
Alchemy

Alchemy (from Arabic "al-kīmiyā") is a philosophical and protoscientific tradition practiced throughout Europe, Africa, and Asia.
It aims to purify, mature, and perfect certain objects.
Common aims were chrysopoeia, the transmutation of "base metals" (e.g., lead) into "noble metals" (particularly gold); the creation of an elixir of immortality; the creation of panaceas able to cure any disease; and the development of an alkahest, a universal solvent.
The perfection of the human body and soul was thought to permit or result from the alchemical magnum opus and, in the Hellenistic and western tradition, the achievement of gnosis.
In Europe, the creation of a philosopher's stone was variously connected with all of these projects.
In English, the term is often limited to descriptions of European alchemy, but similar practices existed in the Far East, the Indian subcontinent, and the Muslim world.
In Europe, following the 12th-century Renaissance produced by the translation of Islamic works on science and the Recovery of Aristotle, alchemists played a significant role in early modern science (particularly chemistry and medicine).
Islamic and European alchemists developed a structure of basic laboratory techniques, theory, terminology, and experimental method, some of which are still in use today.
However, they continued antiquity's belief in four elements and guarded their work in secrecy including cyphers and cryptic symbolism.
Their work was guided by Hermetic principles related to magic, mythology, and religion.
Modern discussions of alchemy are generally split into an examination of its exoteric practical applications and its esoteric spiritual aspects, despite the arguments of scholars like Holmyard and von Franz that they should be understood as complementary.
The former is pursued by historians of the physical sciences who examine the subject in terms of early chemistry, medicine, and charlatanism, and the philosophical and religious contexts in which these events occurred.
The latter interests historians of esotericism, psychologists, and some philosophers and spiritualists.
The subject has also made an ongoing impact on literature and the arts.
Despite this split, which von Franz believes has existed since the Western traditions' origin in a mix of Greek philosophy that was mixed with Egyptian and Mesopotamian technology, numerous sources have stressed an integration of esoteric and exoteric approaches to alchemy as far back as Pseudo-Democritus's first-century  "On Physical and Mystical Matters" ().
The word alchemy comes from Old French "alquemie", "alkimie", used in Medieval Latin as "alchymia".
This name was itself brought from the Arabic word "al-kīmiyā’" ( or ) composed of two parts: the Late Greek term "khēmeía" (χημεία), "khēmía" (χημία), meaning 'to fuse or cast a metal', and the Arabic definite article "al-" (), meaning 'The'.
Together this association can be interpreted as 'the process of transmutation by which to fuse or reunite with the divine or original form'.
Its roots can be traced to the Egyptian name "kēme" (hieroglyphic 𓆎𓅓𓏏𓊖 "khmi" ), meaning ‘black earth’ which refers to the fertile and auriferous soil of the Nile valley, as opposed to red desert sand.
According to the Egyptologist Wallis Budge, the Arabic word "al-kīmiyaʾ" actually means "the Egyptian [science]", borrowing from the Coptic word for "Egypt", "kēme" (or its equivalent in the Mediaeval Bohairic dialect of Coptic, "khēme").
This Coptic word derives from Demotic "kmỉ", itself from ancient Egyptian "kmt".
The ancient Egyptian word referred to both the country and the colour "black" (Egypt was the "Black Land", by contrast with the "Red Land", the surrounding desert); so this etymology could also explain the nickname "Egyptian black arts".
However, according to Mahn, this theory may be an example of folk etymology.
Assuming an Egyptian origin, chemistry is defined as follows:

Thus, according to Budge and others, chemistry derives from an Egyptian word "khemein" or "khēmia", "preparation of black powder", ultimately derived from the name "khem", Egypt.
A decree of Diocletian, written about 300 AD in Greek, speaks against "the ancient writings of the Egyptians, which treat of the "khēmia" transmutation of gold and silver".
The Medieval Latin form was influenced by Greek "chymeia" (χυμεία) meaning ‘mixture’ and referring to pharmaceutical chemistry.
Alchemy is several philosophical traditions spanning some four millennia and three continents.
These traditions' general penchant for cryptic and symbolic language makes it hard to trace their mutual influences and "genetic" relationships.
One can distinguish at least three major strands, which appear to be largely independent, at least in their earlier stages: Chinese alchemy, centered in China and its zone of cultural influence; Indian alchemy, centered on the Indian subcontinent; and Western alchemy, which occurred around the Mediterranean and whose center has shifted over the millennia from Greco-Roman Egypt, to the Islamic world, and finally medieval Europe.
Chinese alchemy was closely connected to Taoism and Indian alchemy with the Dharmic faiths, whereas Western alchemy developed its own philosophical system that was largely independent of, but influenced by, various Western religions.
It is still an open question whether these three strands share a common origin, or to what extent they influenced each other.
The start of Western alchemy may generally be traced to ancient and Hellenistic Egypt, where the city of Alexandria was a center of alchemical knowledge, and retained its pre-eminence through most of the Greek and Roman periods.
Here, elements of technology, religion, mythology, and Hellenistic philosophy, each with their own much longer histories, combined to form the earliest known records of alchemy in the West.
Zosimos of Panopolis wrote the oldest known books on alchemy, while Mary the Jewess is credited as being the first non-fictitious Western alchemist.
They wrote in Greek and lived in Egypt under Roman rule.
Mythology – Zosimos of Panopolis asserted that alchemy dated back to Pharaonic Egypt where it was the domain of the priestly class, though there is little to no evidence for his assertion.
Alchemical writers used Classical figures from Greek, Roman, and Egyptian mythology to illuminate their works and allegorize alchemical transmutation.
These included the pantheon of gods related to the Classical planets, Isis, Osiris, Jason, and many others.
The central figure in the mythology of alchemy is Hermes Trismegistus (or Thrice-Great Hermes).
His name is derived from the god Thoth and his Greek counterpart Hermes.
Hermes and his caduceus or serpent-staff, were among alchemy's principal symbols.
According to Clement of Alexandria, he wrote what were called the "forty-two books of Hermes", covering all fields of knowledge.
The "Hermetica" of Thrice-Great Hermes is generally understood to form the basis for Western alchemical philosophy and practice, called the hermetic philosophy by its early practitioners.
These writings were collected in the first centuries of the common era.
Technology – The dawn of Western alchemy is sometimes associated with that of metallurgy, extending back to 3500 .
Many writings were lost when the emperor Diocletian ordered the burning of alchemical books after suppressing a revolt in Alexandria ( 292).
Few original Egyptian documents on alchemy have survived, most notable among them the Stockholm papyrus and the Leyden papyrus X. Dating from  300–500, they contained recipes for dyeing and making artificial gemstones, cleaning and fabricating pearls, and manufacturing of imitation gold and silver.
These writings lack the mystical, philosophical elements of alchemy, but do contain the works of Bolus of Mendes (or Pseudo-Democritus), which aligned these recipes with theoretical knowledge of astrology and the classical elements.
Between the time of Bolus and Zosimos, the change took place that transformed this metallurgy into a Hermetic art.
Philosophy – Alexandria acted as a melting pot for philosophies of Pythagoreanism, Platonism, Stoicism and Gnosticism which formed the origin of alchemy's character.
An important example of alchemy's roots in Greek philosophy, originated by Empedocles and developed by Aristotle, was that all things in the universe were formed from only four elements: earth, air, water, and fire.
According to Aristotle, each element had a sphere to which it belonged and to which it would return if left undisturbed.
The four elements of the Greek were mostly qualitative aspects of matter, not quantitative, as our modern elements are; "...True alchemy never regarded earth, air, water, and fire as corporeal or chemical substances in the present-day sense of the word.
The four elements are simply the primary, and most general, qualities by means of which the amorphous and purely quantitative substance of all bodies first reveals itself in differentiated form."
Later alchemists extensively developed the mystical aspects of this concept.
Alchemy coexisted alongside emerging Christianity.
Lactantius believed Hermes Trismegistus had prophesied its birth.
St Augustine later affirmed this in the 4th & 5th centuries, but also condemned Trismegistus for idolatry.
Examples of Pagan, Christian, and Jewish alchemists can be found during this period.
Most of the Greco-Roman alchemists preceding Zosimos are known only by pseudonyms, such as Moses, Isis, Cleopatra, Democritus, and Ostanes.
Others authors such as Komarios, and Chymes, we only know through fragments of text.
After  400, Greek alchemical writers occupied themselves solely in commenting on the works of these predecessors.
By the middle of the 7th century alchemy was almost an entirely mystical discipline.
It was at that time that Khalid Ibn Yazid sparked its migration from Alexandria to the Islamic world, facilitating the translation and preservation of Greek alchemical texts in the 8th and 9th centuries.
The Vedas describe a connection between eternal life and gold.
The use of Mercury for alchemy is first documented in the 3rd- or 4th-century "Arthashastra".
Buddhist texts from the 2nd to 5th centuries mention the transmutation of base metals to gold.
Greek alchemy may have been introduced to Ancient India through the invasions of Alexander the Great in 325 , and kingdoms that were culturally influenced by the Greeks like Gandhāra, although hard evidence for this is lacking.
The 11th-century Persian chemist and physician Abū Rayhān Bīrūnī, who visited Gujarat as part of the court of Mahmud of Ghazni, reported that they

The goals of alchemy in India included the creation of a divine body (Sanskrit "divya-deham") and immortality while still embodied (Sanskrit "jīvan-mukti").
Sanskrit alchemical texts include much material on the manipulation of mercury and sulphur, that are homologized with the semen of the god Śiva and the menstrual blood of the goddess Devī.
Some early alchemical writings seem to have their origins in the Kaula tantric schools associated to the teachings of the personality of Matsyendranath.
Other early writings are found in the Jaina medical treatise "Kalyāṇakārakam" of Ugrāditya, written in South India in the early 9th century.
Two famous early Indian alchemical authors were Nāgārjuna Siddha and Nityanātha Siddha.
Nāgārjuna Siddha was a Buddhist monk.
His book, "Rasendramangalam", is an example of Indian alchemy and medicine.
Nityanātha Siddha wrote "Rasaratnākara", also a highly influential work.
In Sanskrit, "rasa" translates to "mercury", and Nāgārjuna Siddha was said to have developed a method of converting mercury into gold.
Reliable scholarship on Indian alchemy has been advanced in a major way by the publication of "The Alchemical Body" by David Gordon White.
Trustworthy scholarship on Indian alchemy must now take the findings of this work into account.
An important modern bibliography on Indian alchemical studies has also been provided by David Gordon White at Oxford Bibliographies Online.
The contents of 39 Sanskrit alchemical treatises have been analysed in detail in G. Jan Meulenbeld's "History of Indian Medical Literature".
The discussion of these works in HIML gives a summary of the contents of each work, their special features, and where possible the evidence concerning their dating.
Chapter 13 of HIML, "Various works on rasaśāstra and ratnaśāstra" (or "Various works on alchemy and gems") gives brief details of a further 655 (six hundred and fifty-five) treatises.
In some cases Meulenbeld gives notes on the contents and authorship of these works; in other cases references are made only to the unpublished manuscripts of these titles.
A great deal remains to be discovered about Indian alchemical literature.
The content of the Sanskrit alchemical corpus has not yet (2014) been adequately integrated into the wider general history of alchemy.
After the fall of the Roman Empire, the focus of alchemical development moved to the Islamic World.
Much more is known about Islamic alchemy because it was better documented: indeed, most of the earlier writings that have come down through the years were preserved as Arabic translations.
The word "alchemy" itself was derived from the Arabic word "al-kīmiyā’" (الكيمياء).
The early Islamic world was a melting pot for alchemy.
Platonic and Aristotelian thought, which had already been somewhat appropriated into hermetical science, continued to be assimilated during the late 7th and early 8th centuries through Syriac translations and scholarship.
In the late 8th century, Jābir ibn Hayyān (Latinized as "Geber" or "Geberus") introduced a new approach to alchemy, based on scientific methodology and controlled experimentation in the laboratory, in contrast to the ancient Greek and Egyptian alchemists whose works were often allegorical and unintelligible, with very little concern for laboratory work.
Jabir is thus "considered by many to be the father of chemistry", albeit others reserve that title for Robert Boyle or Antoine Lavoisier.
The science historian, Paul Kraus, wrote:

Jabir himself clearly recognized and proclaimed the importance of experimentation:

Early Islamic chemists such as Jabir Ibn Hayyan, Al-Kindi ("Alkindus") and Muhammad ibn Zakarīya Rāzi ("Rasis" or "Rhazes") contributed a number of key chemical discoveries, such as the muriatic (hydrochloric acid), sulfuric and nitric acids, and more.
The discovery that aqua regia, a mixture of nitric and hydrochloric acids, could dissolve the noblest metal, gold, was to fuel the imagination of alchemists for the next millennium.
Islamic philosophers also made great contributions to alchemical hermeticism.
The most influential author in this regard was arguably Jabir.
Jabir's ultimate goal was "Takwin", the artificial creation of life in the alchemical laboratory, up to, and including, human life.
He analyzed each Aristotelian element in terms of four basic qualities of "hotness", "coldness", "dryness", and "moistness".
According to Jabir, in each metal two of these qualities were interior and two were exterior.
For example, lead was externally cold and dry, while gold was hot and moist.
Thus, Jabir theorized, by rearranging the qualities of one metal, a different metal would result.
By this reasoning, the search for the philosopher's stone was introduced to Western alchemy.
Jabir developed an elaborate numerology whereby the root letters of a substance's name in Arabic, when treated with various transformations, held correspondences to the element's physical properties.
The elemental system used in medieval alchemy also originated with Jabir.
His original system consisted of seven elements, which included the five classical elements (aether, air, earth, fire, and water) in addition to two chemical elements representing the metals: sulphur, "the stone which burns", which characterized the principle of combustibility, and mercury, which contained the idealized principle of metallic properties.
Shortly thereafter, this evolved into eight elements, with the Arabic concept of the three metallic principles: sulphur giving flammability or combustion, mercury giving volatility and stability, and salt giving solidity.
The atomic theory of corpuscularianism, where all physical bodies possess an inner and outer layer of minute particles or corpuscles, also has its origins in the work of Jabir.
From the 9th to 14th centuries, alchemical theories faced criticism from a variety of practical Muslim chemists, including Alkindus, Abū al-Rayhān al-Bīrūnī, Avicenna and Ibn Khaldun.
In particular, they wrote refutations against the idea of the transmutation of metals.
Whereas European alchemy eventually centered on the transmutation of base metals into noble metals, Chinese alchemy had a more obvious connection to medicine.
The philosopher's stone of European alchemists can be compared to the Grand Elixir of Immortality sought by Chinese alchemists.
However, in the hermetic view, these two goals were not unconnected, and the philosopher's stone was often equated with the universal panacea; therefore, the two traditions may have had more in common than initially appears.
Black powder may have been an important invention of Chinese alchemists.
As previously stated above, Chinese alchemy was more related to medicine.
It is said that the Chinese invented gunpowder while trying to find a potion for eternal life.
Described in 9th-century texts and used in fireworks in China by the 10th century, it was used in cannons by 1290.
From China, the use of gunpowder spread to Japan, the Mongols, the Muslim world, and Europe.
Gunpowder was used by the Mongols against the Hungarians in 1241, and in Europe by the 14th century.
Chinese alchemy was closely connected to Taoist forms of traditional Chinese medicine, such as Acupuncture and Moxibustion, and to martial arts such as Tai Chi Chuan and Kung Fu (although some Tai Chi schools believe that their art derives from the philosophical or hygienic branches of Taoism, not Alchemical).
In fact, in the early Song dynasty, followers of this Taoist idea (chiefly the elite and upper class) would ingest mercuric sulfide, which, though tolerable in low levels, led many to suicide.
Thinking that this consequential death would lead to freedom and access to the Taoist heavens, the ensuing deaths encouraged people to eschew this method of alchemy in favor of external sources (the aforementioned Tai Chi Chuan, mastering of the qi, etc.)
The introduction of alchemy to Latin Europe may be dated to 11 February 1144, with the completion of Robert of Chester's translation of the Arabic "Book of the Composition of Alchemy".
Although European craftsmen and technicians preexisted, Robert notes in his preface that alchemy was unknown in Latin Europe at the time of his writing.
The translation of Arabic texts concerning numerous disciplines including alchemy flourished in 12th-century Toledo, Spain, through contributors like Gerard of Cremona and Adelard of Bath.
Translations of the time included the Turba Philosophorum, and the works of Avicenna and al-Razi.
These brought with them many new words to the European vocabulary for which there was no previous Latin equivalent.
Alcohol, carboy, elixir, and athanor are examples.
Meanwhile, theologian contemporaries of the translators made strides towards the reconciliation of faith and experimental rationalism, thereby priming Europe for the influx of alchemical thought.
The 11th-century St Anselm put forth the opinion that faith and rationalism were compatible and encouraged rationalism in a Christian context.
In the early 12th century, Peter Abelard followed Anselm's work, laying down the foundation for acceptance of Aristotelian thought before the first works of Aristotle had reached the West.
In the early 13th century, Robert Grosseteste used Abelard's methods of analysis and added the use of observation, experimentation, and conclusions when conducting scientific investigations.
Grosseteste also did much work to reconcile Platonic and Aristotelian thinking.
Through much of the 12th and 13th centuries, alchemical knowledge in Europe remained centered on translations, and new Latin contributions were not made.
The efforts of the translators were succeeded by that of the encyclopaedists.
In the 13th century, Albertus Magnus and Roger Bacon were the most notable of these, their work summarizing and explaining the newly imported alchemical knowledge in Aristotelian terms.
Albertus Magnus, a Dominican monk, is known to have written works such as the "Book of Minerals" where he observed and commented on the operations and theories of alchemical authorities like Hermes and Democritus and unnamed alchemists of his time.
Albertus critically compared these to the writings of Aristotle and Avicenna, where they concerned the transmutation of metals.
From the time shortly after his death through to the 15th century, more than 28 alchemical tracts were misattributed to him, a common practice giving rise to his reputation as an accomplished alchemist.
Likewise, alchemical texts have been attributed to Albert's student Thomas Aquinas.
Roger Bacon, a Franciscan monk who wrote on a wide variety of topics including optics, comparative linguistics, and medicine, composed his "Great Work" () for as part of a project towards rebuilding the medieval university curriculum to include the new learning of his time.
While alchemy was not more important to him than other sciences and he did not produce allegorical works on the topic, he did consider it and astrology to be important parts of both natural philosophy and theology and his contributions advanced alchemy's connections to soteriology and Christian theology.
Bacon's writings integrated morality, salvation, alchemy, and the prolongation of life.
His correspondence with Clement highlighted this, noting the importance of alchemy to the papacy.
Like the Greeks before him, Bacon acknowledged the division of alchemy into practical and theoretical spheres.
He noted that the theoretical lay outside the scope of Aristotle, the natural philosophers, and all Latin writers of his time.
The practical, however, confirmed the theoretical thought experiment, and Bacon advocated its uses in natural science and medicine.
In later European legend, however, Bacon became an archmage.
In particular, along with Albertus Magnus, he was credited with the forging of a brazen head capable of answering its owner's questions.
Soon after Bacon, the influential work of Pseudo-Geber (sometimes identified as Paul of Taranto) appeared.
His "Summa Perfectionis" remained a staple summary of alchemical practice and theory through the medieval and renaissance periods.
It was notable for its inclusion of practical chemical operations alongside sulphur-mercury theory, and the unusual clarity with which they were described.
By the end of the 13th century, alchemy had developed into a fairly structured system of belief.
Adepts believed in the macrocosm-microcosm theories of Hermes, that is to say, they believed that processes that affect minerals and other substances could have an effect on the human body (for example, if one could learn the secret of purifying gold, one could use the technique to purify the human soul).
They believed in the four elements and the four qualities as described above, and they had a strong tradition of cloaking their written ideas in a labyrinth of coded jargon set with traps to mislead the uninitiated.
Finally, the alchemists practiced their art: they actively experimented with chemicals and made observations and theories about how the universe operated.
Their entire philosophy revolved around their belief that man's soul was divided within himself after the fall of Adam.
By purifying the two parts of man's soul, man could be reunited with God.
In the 14th century, alchemy became more accessible to Europeans outside the confines of Latin speaking churchmen and scholars.
Alchemical discourse shifted from scholarly philosophical debate to an exposed social commentary on the alchemists themselves.
Dante, Piers Plowman, and Chaucer all painted unflattering pictures of alchemists as thieves and liars.
Pope John XXII's 1317 edict, "Spondent quas non exhibent" forbade the false promises of transmutation made by pseudo-alchemists.
In 1403, Henry IV of England banned the practice of multiplying metals (although it was possible to buy a licence to attempt to make gold alchemically, and a number were granted by Henry VI and Edward IV).
These critiques and regulations centered more around pseudo-alchemical charlatanism than the actual study of alchemy, which continued with an increasingly Christian tone.
The 14th century saw the Christian imagery of death and resurrection employed in the alchemical texts of Petrus Bonus, John of Rupescissa, and in works written in the name of Raymond Lull and Arnold of Villanova.
Nicolas Flamel is a well-known alchemist, but a good example of pseudepigraphy, the practice of giving your works the name of someone else, usually more famous.
Although the historical Flamel existed, the writings and legends assigned to him only appeared in 1612.
Flamel was not a religious scholar as were many of his predecessors, and his entire interest in the subject revolved around the pursuit of the philosopher's stone.
His work spends a great deal of time describing the processes and reactions, but never actually gives the formula for carrying out the transmutations.
Most of 'his' work was aimed at gathering alchemical knowledge that had existed before him, especially as regarded the philosopher's stone.
Through the 14th and 15th centuries, alchemists were much like Flamel: they concentrated on looking for the philosophers' stone.
Bernard Trevisan and George Ripley made similar contributions.
Their cryptic allusions and symbolism led to wide variations in interpretation of the art.
During the Renaissance, Hermetic and Platonic foundations were restored to European alchemy.
The dawn of medical, pharmaceutical, occult, and entrepreneurial branches of alchemy followed.
In the late 15th century, Marsilo Ficino translated the Corpus Hermeticum and the works of Plato into Latin.
These were previously unavailable to Europeans who for the first time had a full picture of the alchemical theory that Bacon had declared absent.
Renaissance Humanism and Renaissance Neoplatonism guided alchemists away from physics to refocus on mankind as the alchemical vessel.
Esoteric systems developed that blended alchemy into a broader occult Hermeticism, fusing it with magic, astrology, and Christian cabala.
A key figure in this development was German Heinrich Cornelius Agrippa (1486–1535), who received his Hermetic education in Italy in the schools of the humanists.
In his "De Occulta Philosophia", he attempted to merge Kabbalah, Hermeticism, and alchemy.
He was instrumental in spreading this new blend of Hermeticism outside the borders of Italy.
Philippus Aureolus Paracelsus, (Theophrastus Bombastus von Hohenheim, 1493–1541) cast alchemy into a new form, rejecting some of Agrippa's occultism and moving away from chrysopoeia.
Paracelsus pioneered the use of chemicals and minerals in medicine and wrote, "Many have said of Alchemy, that it is for the making of gold and silver.
For me such is not the aim, but to consider only what virtue and power may lie in medicines."
His hermetical views were that sickness and health in the body relied on the harmony of man the microcosm and Nature the macrocosm.
He took an approach different from those before him, using this analogy not in the manner of soul-purification but in the manner that humans must have certain balances of minerals in their bodies, and that certain illnesses of the body had chemical remedies that could cure them.
Paracelsian practical alchemy, especially herbal medicine and plant remedies has since been named spagyric (a synonym for alchemy from the Greek words meaning "to separate" and "to join together", based on the Latin alchemic maxim: "solve et coagula").
Iatrochemistry also refers to the pharmaceutical applications of alchemy championed by Paracelsus.
John Dee (13 July 1527 – December, 1608) followed Agrippa's occult tradition.
Although better known for angel summoning, divination, and his role as astrologer, cryptographer, and consultant to Queen Elizabeth I, Dee's alchemical "Monas Hieroglyphica", written in 1564 was his most popular and influential work.
His writing portrayed alchemy as a sort of terrestrial astronomy in line with the Hermetic axiom "As above so below".
During the 17th century, a short-lived "supernatural" interpretation of alchemy became popular, including support by fellows of the Royal Society: Robert Boyle and Elias Ashmole.
Proponents of the supernatural interpretation of alchemy believed that the philosopher's stone might be used to summon and communicate with angels.
Entrepreneurial opportunities were common for the alchemists of Renaissance Europe.
Alchemists were contracted by the elite for practical purposes related to mining, medical services, and the production of chemicals, medicines, metals, and gemstones.
Rudolf II, Holy Roman Emperor, in the late 16th century, famously received and sponsored various alchemists at his court in Prague, including Dee and his associate Edward Kelley.
King James IV of Scotland, Julius, Duke of Brunswick-Lüneburg, Henry V, Duke of Brunswick-Lüneburg, Augustus, Elector of Saxony, Julius Echter von Mespelbrunn, and Maurice, Landgrave of Hesse-Kassel all contracted alchemists.
John's son Arthur Dee worked as a court physician to Michael I of Russia and Charles I of England but also compiled the alchemical book "Fasciculus Chemicus".
Although most of these appointments were legitimate, the trend of pseudo-alchemical fraud continued through the Renaissance.
"Betrüger" would use sleight of hand, or claims of secret knowledge to make money or secure patronage.
Legitimate mystical and medical alchemists such as Michael Maier and Heinrich Khunrath wrote about fraudulent transmutations, distinguishing themselves from the con artists.
False alchemists were sometimes prosecuted for fraud.
The terms "chemia" and "alchemia" were used as synonyms in the early modern period, and the differences between alchemy, chemistry and small-scale assaying and metallurgy were not as neat as in the present day.
There were important overlaps between practitioners, and trying to classify them into alchemists, chemists and craftsmen is anachronistic.
For example, Tycho Brahe (1546–1601), an alchemist better known for his astronomical and astrological investigations, had a laboratory built at his Uraniborg observatory/research institute.
Michael Sendivogius ("Michał Sędziwój", 1566–1636), a Polish alchemist, philosopher, medical doctor and pioneer of chemistry wrote mystical works but is also credited with distilling oxygen in a lab sometime around 1600.
Sendivogious taught his technique to Cornelius Drebbel who, in 1621, applied this in a submarine.
Isaac Newton devoted considerably more of his writing to the study of alchemy (see Isaac Newton's occult studies) than he did to either optics or physics.
Other early modern alchemists who were eminent in their other studies include Robert Boyle, and Jan Baptist van Helmont.
Their Hermeticism complemented rather than precluded their practical achievements in medicine and science.
The decline of European alchemy was brought about by the rise of modern science with its emphasis on rigorous quantitative experimentation and its disdain for "ancient wisdom".
Although the seeds of these events were planted as early as the 17th century, alchemy still flourished for some two hundred years, and in fact may have reached its peak in the 18th century.
As late as 1781 James Price claimed to have produced a powder that could transmute mercury into silver or gold.
Early modern European alchemy continued to exhibit a diversity of theories, practices, and purposes: "Scholastic and anti-Aristotelian, Paracelsian and anti-Paracelsian, Hermetic, Neoplatonic, mechanistic, vitalistic, and more—plus virtually every combination and compromise thereof."
Robert Boyle (1627–1691) pioneered the scientific method in chemical investigations.
He assumed nothing in his experiments and compiled every piece of relevant data.
Boyle would note the place in which the experiment was carried out, the wind characteristics, the position of the Sun and Moon, and the barometer reading, all just in case they proved to be relevant.
This approach eventually led to the founding of modern chemistry in the 18th and 19th centuries, based on revolutionary discoveries of Lavoisier and John Dalton.
Beginning around 1720, a rigid distinction was drawn between "alchemy" and "chemistry" for the first time.
By the 1740s, "alchemy" was now restricted to the realm of gold making, leading to the popular belief that alchemists were charlatans, and the tradition itself nothing more than a fraud.
In order to protect the developing science of modern chemistry from the negative censure of which alchemy was being subjected, academic writers during the scientific Enlightenment attempted, for the sake of survival, to divorce and separate the "new" chemistry from the "old" practices of alchemy.
This move was mostly successful, and the consequences of this continued into the 19th and 20th centuries, and even to the present day.
During the occult revival of the early 19th century, alchemy received new attention as an occult science.
The esoteric or occultist school, which arose during the 19th century, held (and continues to hold) the view that the substances and operations mentioned in alchemical literature are to be interpreted in a spiritual sense, and it downplays the role of the alchemy as a practical tradition or protoscience.
This interpretation further forwarded the view that alchemy is an art primarily concerned with spiritual enlightenment or illumination, as opposed to the physical manipulation of apparatus and chemicals, and claims that the obscure language of the alchemical texts were an allegorical guise for spiritual, moral or mystical processes.
In the 19th-century revival of alchemy, the two most seminal figures were Mary Anne Atwood and Ethan Allen Hitchcock, who independently published similar works regarding spiritual alchemy.
Both forwarded a completely esoteric view of alchemy, as Atwood claimed: "No modern art or chemistry, notwithstanding all its surreptitious claims, has any thing in common with Alchemy."
Atwood's work influenced subsequent authors of the occult revival including Eliphas Levi, Arthur Edward Waite, and Rudolf Steiner.
Hitchcock, in his "Remarks Upon Alchymists" (1855) attempted to make a case for his spiritual interpretation with his claim that the alchemists wrote about a spiritual discipline under a materialistic guise in order to avoid accusations of blasphemy from the church and state.
In 1845, Baron Carl Reichenbach, published his studies on Odic force, a concept with some similarities to alchemy, but his research did not enter the mainstream of scientific discussion.
Several women appear in the earliest history of alchemy.
Michael Maier names Mary the Jewess, Cleopatra the Alchemist, Medera, and Taphnutia as the four women who knew how to make the philosopher's stone.
Zosimos' sister Theosebia (later known as Euthica the Arab) and Isis the Prophetess also played a role in early alchemical texts.
The first alchemist whose name we know is said to have been Mary the Jewess (c.
200 A.D.).
Early sources claim that Mary (or Maria) devised a number of improvements to alchemical equipment and tools as well as novel techniques in chemistry.
Her best known advances were in heating and distillation processes.
The laboratory water-bath, known eponymously (especially in France) as the bain-marie, is said to have been invented or at least improved by her.
Essentially a double-boiler, it was (and is) used in chemistry for processes that require gentle heating.
The tribikos (a modified distillation apparatus) and the kerotakis (a more intricate apparatus used especially for sublimations) are two other advancements in the process of distillation that are credited to her.
The occasional claim that Mary was the first to discover hydrochloric acid is not accepted by most authorities.
Although we have no writing from Mary herself, she is known from the early-fourth-century writings of Zosimos of Panopolis.
Due to the proliferation of pseudepigrapha and anonymous works, it is difficult to know which of the alchemists were actually women.
After the Greco-Roman period, women's names appear less frequently in the alchemical literature.
Women vacate the history of alchemy during the medieval and renaissance periods, aside from the fictitious account of Perenelle Flamel.
Mary Anne Atwood's "A Suggestive Inquiry into the Hermetic Mystery" (1850) marks their return during the nineteenth-century occult revival.
The history of alchemy has become a significant and recognized subject of academic study.
As the language of the alchemists is analyzed, historians are becoming more aware of the intellectual connections between that discipline and other facets of Western cultural history, such as the evolution of science and philosophy, the sociology and psychology of the intellectual communities, kabbalism, spiritualism, Rosicrucianism, and other mystic movements.
Institutions involved in this research include The Chymistry of Isaac Newton project at Indiana University, the University of Exeter Centre for the Study of Esotericism (EXESESO), the European Society for the Study of Western Esotericism (ESSWE), and the University of Amsterdam's Sub-department for the History of Hermetic Philosophy and Related Currents.
A large collection of books on alchemy is kept in the Bibliotheca Philosophica Hermetica in Amsterdam.
A recipe found in a mid-19th-century kabbalah based book features step by step instructions on turning copper into gold.
The author attributed this recipe to an ancient manuscript he located.
Journals which publish regularly on the topic of Alchemy include 'Ambix', published by the Society for the History of Alchemy and Chemistry, and 'Isis', published by The History of Science Society.
Western alchemical theory corresponds to the worldview of late antiquity in which it was born.
Concepts were imported from Neoplatonism and earlier Greek cosmology.
As such, the Classical elements appear in alchemical writings, as do the seven Classical planets and the corresponding seven metals of antiquity.
Similarly, the gods of the Roman pantheon who are associated with these luminaries are discussed in alchemical literature.
The concepts of prima materia and anima mundi are central to the theory of the philosopher's stone.
In the eyes of a variety of esoteric and Hermetic practitioners, alchemy is fundamentally spiritual.
Transmutation of lead into gold is presented as an analogy for personal transmutation, purification, and perfection.
The writings attributed to Hermes Trismegistus are a primary source of alchemical theory.
He is named "alchemy's founder and chief patron, authority, inspiration and guide".
Early alchemists, such as Zosimos of Panopolis (c. AD 300), highlight the spiritual nature of the alchemical quest, symbolic of a religious regeneration of the human soul.
This approach continued in the Middle Ages, as metaphysical aspects, substances, physical states, and material processes were used as metaphors for spiritual entities, spiritual states, and, ultimately, transformation.
In this sense, the literal meanings of 'Alchemical Formulas' were a blind, hiding their true spiritual philosophy.
Practitioners and patrons such as Melchior Cibinensis and Pope Innocent VIII existed within the ranks of the church, while Martin Luther applauded alchemy for its consistency with Christian teachings.
Both the transmutation of common metals into gold and the universal panacea symbolized evolution from an imperfect, diseased, corruptible, and ephemeral state toward a perfect, healthy, incorruptible, and everlasting state, so the philosopher's stone then represented a mystic key that would make this evolution possible.
Applied to the alchemist himself, the twin goal symbolized his evolution from ignorance to enlightenment, and the stone represented a hidden spiritual truth or power that would lead to that goal.
In texts that are written according to this view, the cryptic alchemical symbols, diagrams, and textual imagery of late alchemical works typically contain multiple layers of meanings, allegories, and references to other equally cryptic works; and must be laboriously decoded to discover their true meaning.
In his 1766 "Alchemical Catechism", Théodore Henri de Tschudi denotes that the usage of the metals was merely symbolic:

The Great Work of Alchemy is often described as a series of four stages represented by colors.
Due to the complexity and obscurity of alchemical literature, and the 18th-century disappearance of remaining alchemical practitioners into the area of chemistry; the general understanding of alchemy has been strongly influenced by several distinct and radically different interpretations.
Those focusing on the exoteric, such as historians of science Lawrence M. Principe and William R. Newman, have interpreted the 'decknamen' (or code words) of alchemy as physical substances.
These scholars have reconstructed physicochemical experiments that they say are described in medieval and early modern texts.
At the opposite end of the spectrum, focusing on the esoteric, scholars, such as George Calian and Anna Marie Roos, who question the reading of Principe and Newman, interpret these same decknamen as spiritual, religious, or psychological concepts.
Today new interpretations of alchemy are still perpetuated, sometimes merging in concepts from New Age or radical environmentalism movements.
Groups like the Rosicrucians and Freemasons have a continued interest in alchemy and its symbolism.
Since the Victorian revival of alchemy, "occultists reinterpreted alchemy as a spiritual practice, involving the self-transformation of the practitioner and only incidentally or not at all the transformation of laboratory substances.
", which has contributed to a merger of magic and alchemy in popular thought.
Traditional medicine can use the concept of the transmutation of natural substances, using pharmacological or a combination of pharmacological and spiritual techniques.
In Ayurveda, the samskaras are claimed to transform heavy metals and toxic herbs in a way that removes their toxicity.
These processes are actively used to the present day.
Spagyrists of the 20th century, Albert Richard Riedel and Jean Dubuis, merged Paracelsian alchemy with occultism, teaching laboratory pharmaceutical methods.
The schools they founded, "Les Philosophes de la Nature" and "The Paracelsus Research Society", popularized modern spagyrics including the manufacture of herbal tinctures and products.
The courses, books, organizations, and conferences generated by their students continue to influence popular applications of alchemy as a New Age medicinal practice.
Alchemical symbolism has been important in depth and analytical psychology and was revived and popularized from near extinction by the Swiss psychologist Carl Gustav Jung.
Initially confounded and at odds with alchemy and its images, after being given a copy of the translation of "The Secret of the Golden Flower", a Chinese alchemical text, by his friend Richard Wilhelm, Jung discovered a direct correlation or parallels between the symbolic images in the alchemical drawings and the inner, symbolic images coming up in dreams, visions or imaginations during the psychic processes of transformation occurring in his patients.
A process, which he called "process of individuation".
He regarded the alchemical images as symbols expressing aspects of this "process of individuation" of which the creation of the gold or lapis within were symbols for its origin and goal.
Together with his alchemical "mystica soror", Jungian Swiss analyst Marie-Louise von Franz, Jung began collecting all the old alchemical texts available, compiled a lexicon of key phrases with cross-references and pored over them.
The volumes of work he wrote brought new light into understanding the art of transubstantiation and renewed alchemy's popularity as a symbolic process of coming into wholeness as a human being where opposites brought into contact and inner and outer, spirit and matter are reunited in the "hieros gamos" or divine marriage.
His writings are influential in psychology and for persons who have an interest in understanding the importance of dreams, symbols and the unconscious archetypal forces (archetypes) that influence all of life.
Both von Franz and Jung have contributed greatly to the subject and work of alchemy and its continued presence in psychology as well as contemporary culture.
Jung wrote volumes on alchemy and his magnum opus is Volume 14 of his Collected Works, "Mysterium Conuinctionis."
Alchemy has had a long-standing relationship with art, seen both in alchemical texts and in mainstream entertainment.
"Literary alchemy" appears throughout the history of English literature from Shakespeare to J. K. Rowling, and also the popular Japanese manga Full Metal Alchemist.
Here, characters or plot structure follow an alchemical magnum opus.
In the 14th century, Chaucer began a trend of alchemical satire that can still be seen in recent fantasy works like those of Terry Pratchett.
Visual artists had a similar relationship with alchemy.
While some of them used alchemy as a source of satire, others worked with the alchemists themselves or integrated alchemical thought or symbols in their work.
Music was also present in the works of alchemists and continues to influence popular performers.
In the last hundred years, alchemists have been portrayed in a magical and spagyric role in fantasy fiction, film, television, novels, comics and video games.
</doc>
<doc id="579" url="https://en.wikipedia.org/wiki?curid=579" title="Alien">
Alien

Alien primarily refers to:

Alien(s), or The Alien(s) may also refer to:













</doc>
<doc id="580" url="https://en.wikipedia.org/wiki?curid=580" title="Astronomer">
Astronomer

An astronomer is a scientist in the field of astronomy who focuses their studies on a specific question or field outside the scope of Earth.
They observe astronomical objects such as stars, planets, moons, comets, and galaxies – in either observational (by analyzing the data) or theoretical astronomy.
Examples of topics or fields astronomers study include planetary science, solar astronomy, the origin or evolution of stars, or the formation of galaxies.
Related but distinct subjects like physical cosmology, which studies the Universe as a whole.
Astronomers usually fall under either of two main types: observational and theoretical.
Observational astronomers make direct observations of celestial objects and analyze the data.
In contrast, theoretical astronomers create and investigate models of things that cannot be observed.
Because it takes millions to billions of years for a system of stars or a galaxy to complete a life cycle, astronomers must observe snapshots of different systems at unique points in their evolution to determine how they form, evolve, and die.
They use these data to create models or simulations to theorize how different celestial objects work.
Further subcategories under these two main branches of astronomy include planetary astronomy, galactic astronomy, or physical cosmology.
Historically, astronomy was more concerned with the classification and description of phenomena in the sky, while astrophysics attempted to explain these phenomena and the differences between them using physical laws.
Today, that distinction has mostly disappeared and the terms "astronomer" and "astrophysicist" are interchangeable.
Professional astronomers are highly educated individuals who typically have a Ph.D.
in physics or astronomy and are employed by research institutions or universities.
They spend the majority of their time working on research, although they quite often have other duties such as teaching, building instruments, or aiding in the operation of an observatory.
The number of professional astronomers in the United States is actually quite small.
The American Astronomical Society, which is the major organization of professional astronomers in North America, has approximately 7,000 members.
This number includes scientists from other fields such as physics, geology, and engineering, whose research interests are closely related to astronomy.
The International Astronomical Union comprises almost 10,145 members from 70 different countries who are involved in astronomical research at the Ph.D.
level and beyond.
Contrary to the classical image of an old astronomer peering through a telescope through the dark hours of the night, it is far more common to use a charge-coupled device (CCD) camera to record a long, deep exposure, allowing a more sensitive image to be created because the light is added over time.
Before CCDs, photographic plates were a common method of observation.
Modern astronomers spend relatively little time at telescopes usually just a few weeks per year.
Analysis of observed phenomena, along with making predictions as to the causes of what they observe, takes the majority of observational astronomers' time.
Astronomers who serve as faculty spend much of their time teaching undergraduate and graduate classes.
Most universities also have outreach programs including public telescope time and sometimes planetariums as a public service to encourage interest in the field.
Those who become astronomers usually have a broad background in maths, sciences and computing in high school.
Taking courses that teach how to research, write and present papers are also invaluable.
In college/university most astronomers get a Ph.D.
in astronomy or physics.
While there is a relatively low number of professional astronomers, the field is popular among amateurs.
Most cities have amateur astronomy clubs that meet on a regular basis and often host star parties.
The Astronomical Society of the Pacific is the largest general astronomical society in the world, comprising both professional and amateur astronomers as well as educators from 70 different nations.
Like any hobby, most people who think of themselves as amateur astronomers may devote a few hours a month to stargazing and reading the latest developments in research.
However, amateurs span the range from so-called "armchair astronomers" to the very ambitious, who own science-grade telescopes and instruments with which they are able to make their own discoveries and assist professional astronomers in research.
</doc>
<doc id="586" url="https://en.wikipedia.org/wiki?curid=586" title="ASCII">
ASCII

ASCII ( ), abbreviated from American Standard Code for Information Interchange, is a character encoding standard for electronic communication.
ASCII codes represent text in computers, telecommunications equipment, and other devices.
Most modern character-encoding schemes are based on ASCII, although they support many additional characters.
ASCII is the traditional name for the encoding system; the Internet Assigned Numbers Authority (IANA) prefers the updated name US-ASCII, which clarifies that this system was developed in the US and based on the typographical symbols predominantly in use there.
ASCII is one of the IEEE milestones.
ASCII was developed from telegraph code.
Its first commercial use was as a seven-bit teleprinter code promoted by Bell data services.
Work on the ASCII standard began on October 6, 1960, with the first meeting of the American Standards Association's (ASA) (now the American National Standards Institute or ANSI) X3.2 subcommittee.
The first edition of the standard was published in 1963, underwent a major revision during 1967, and experienced its most recent update during 1986.
Compared to earlier telegraph codes, the proposed Bell code and ASCII were both ordered for more convenient sorting (i.e., alphabetization) of lists, and added features for devices other than teleprinters.
Originally based on the English alphabet, ASCII encodes 128 specified characters into seven-bit integers as shown by the ASCII chart above.
Ninety-five of the encoded characters are printable: these include the digits "0" to "9", lowercase letters "a" to "z", uppercase letters "A" to "Z", and punctuation symbols.
In addition, the original ASCII specification included 33 non-printing control codes which originated with Teletype machines; most of these are now obsolete, although a few are still commonly used, such as the carriage return, line feed and tab codes.
For example, lowercase "i" would be represented in the ASCII encoding by binary 1101001 = hexadecimal 69 ("i" is the ninth letter) = decimal 105.
The American Standard Code for Information Interchange (ASCII) was developed under the auspices of a committee of the American Standards Association (ASA), called the X3 committee, by its X3.2 (later X3L2) subcommittee, and later by that subcommittee's X3.2.4 working group (now INCITS).
The ASA became the United States of America Standards Institute (USASI) and ultimately the American National Standards Institute (ANSI).
With the other special characters and control codes filled in, ASCII was published as ASA X3.4-1963, leaving 28 code positions without any assigned meaning, reserved for future standardization, and one unassigned control code.
There was some debate at the time whether there should be more control characters rather than the lowercase alphabet.
The indecision did not last long: during May 1963 the CCITT Working Party on the New Telegraph Alphabet proposed to assign lowercase characters to "sticks" 6 and 7, and International Organization for Standardization TC 97 SC 2 voted during October to incorporate the change into its draft standard.
The X3.2.4 task group voted its approval for the change to ASCII at its May 1963 meeting.
Locating the lowercase letters in "sticks" 6 and 7 caused the characters to differ in bit pattern from the upper case by a single bit, which simplified case-insensitive character matching and the construction of keyboards and printers.
The X3 committee made other changes, including other new characters (the brace and vertical bar characters), renaming some control characters (SOM became start of header (SOH)) and moving or removing others (RU was removed).
ASCII was subsequently updated as USAS X3.4-1967, then USAS X3.4-1968, ANSI X3.4-1977, and finally, ANSI X3.4-1986.
Revisions of the ASCII standard:


In the X3.15 standard, the X3 committee also addressed how ASCII should be transmitted (least significant bit first), and how it should be recorded on perforated tape.
They proposed a 9-track standard for magnetic tape, and attempted to deal with some punched card formats.
The X3.2 subcommittee designed ASCII based on the earlier teleprinter encoding systems.
Like other character encodings, ASCII specifies a correspondence between digital bit patterns and character symbols (i.e. graphemes and control characters).
This allows digital devices to communicate with each other and to process, store, and communicate character-oriented information such as written language.
Before ASCII was developed, the encodings in use included 26 alphabetic characters, 10 numerical digits, and from 11 to 25 special graphic symbols.
To include all these, and control characters compatible with the Comité Consultatif International Téléphonique et Télégraphique (CCITT) International Telegraph Alphabet No.
2 (ITA2) standard of 1924, FIELDATA (1956), and early EBCDIC (1963), more than 64 codes were required for ASCII.
ITA2 were in turn based on the 5-bit telegraph code Émile Baudot invented in 1870 and patented in 1874.
The committee debated the possibility of a shift function (like in ITA2), which would allow more than 64 codes to be represented by a six-bit code.
In a shifted code, some character codes determine choices between options for the following character codes.
It allows compact encoding, but is less reliable for data transmission, as an error in transmitting the shift code typically makes a long part of the transmission unreadable.
The standards committee decided against shifting, and so ASCII required at least a seven-bit code.
The committee considered an eight-bit code, since eight bits (octets) would allow two four-bit patterns to efficiently encode two digits with binary-coded decimal.
However, it would require all data transmission to send eight bits when seven could suffice.
The committee voted to use a seven-bit code to minimize costs associated with data transmission.
Since perforated tape at the time could record eight bits in one position, it also allowed for a parity bit for error checking if desired.
Eight-bit machines (with octets as the native data type) that did not use parity checking typically set the eighth bit to 0.
In some printers, the high bit was used to enable Italics printing.
The code itself was patterned so that most control codes were together and all graphic codes were together, for ease of identification.
The first two so called "ASCII sticks" (32 positions) were reserved for control characters.
The "space" character had to come before graphics to make sorting easier, so it became position 20; for the same reason, many special signs commonly used as separators were placed before digits.
The committee decided it was important to support uppercase 64-character alphabets, and chose to pattern ASCII so it could be reduced easily to a usable 64-character set of graphic codes, as was done in the DEC SIXBIT code (1963).
Lowercase letters were therefore not interleaved with uppercase.
To keep options available for lowercase letters and other graphics, the special and numeric codes were arranged before the letters, and the letter "A" was placed in position 41 to match the draft of the corresponding British standard.
The digits 0–9 are prefixed with 011, but the remaining 4 bits correspond to their respective values in binary, making conversion with binary-coded decimal straightforward.
Many of the non-alphanumeric characters were positioned to correspond to their shifted position on typewriters; an important subtlety is that these were based on "mechanical" typewriters, not "electric" typewriters.
Mechanical typewriters followed the standard set by the Remington No.
2 (1878), the first typewriter with a shift key, and the shifted values of codice_1 were codice_2 early typewriters omitted "0" and "1", using "O" (capital letter "o") and "l" (lowercase letter "L") instead, but codice_3 and codice_4 pairs became standard once 0 and 1 became common.
Thus, in ASCII codice_5 were placed in the second stick, positions 1–5, corresponding to the digits 1–5 in the adjacent stick.
The parentheses could not correspond to "9" and "0", however, because the place corresponding to "0" was taken by the space character.
This was accommodated by removing codice_6 (underscore) from "6" and shifting the remaining characters, which corresponded to many European typewriters that placed the parentheses with "8" and "9".
This discrepancy from typewriters led to bit-paired keyboards, notably the Teletype Model 33, which used the left-shifted layout corresponding to ASCII, not to traditional mechanical typewriters.
Electric typewriters, notably the IBM Selectric (1961), used a somewhat different layout that has become standard on computers following the IBM PC (1981), especially Model M (1984) and thus shift values for symbols on modern keyboards do not correspond as closely to the ASCII table as earlier keyboards did.
The codice_7 pair also dates to the No.
2, and the codice_8 pairs were used on some keyboards (others, including the No.
2, did not shift codice_9 (comma) or codice_10 (full stop) so they could be used in uppercase without unshifting).
However, ASCII split the codice_11 pair (dating to No.
2), and rearranged mathematical symbols (varied conventions, commonly codice_12) to codice_13.
Some common characters were not included, notably codice_14, while codice_15 were included as diacritics for international use, and codice_16 for mathematical use, together with the simple line characters codice_17 (in addition to common codice_18).
The "@" symbol was not used in continental Europe and the committee expected it would be replaced by an accented "À" in the French variation, so the "@" was placed in position 40, right before the letter A.

The control codes felt essential for data transmission were the start of message (SOM), end of address (EOA), end of message (EOM), end of transmission (EOT), "who are you?"
(WRU), "are you?"
(RU), a reserved device control (DC0), synchronous idle (SYNC), and acknowledge (ACK).
These were positioned to maximize the Hamming distance between their bit patterns.
ASCII-code order is also called "ASCIIbetical" order.
Collation of data is sometimes done in this order rather than "standard" alphabetical order (collating sequence).
The main deviations in ASCII order are:

An intermediate order converts uppercase letters to lowercase before comparing ASCII values.
ASCII reserves the first 32 codes (numbers 0–31 decimal) for control characters: codes originally intended not to represent printable information, but rather to control devices (such as printers) that make use of ASCII, or to provide meta-information about data streams such as those stored on magnetic tape.
For example, character 10 represents the "line feed" function (which causes a printer to advance its paper), and character 8 represents "backspace".
refers to control characters that do not include carriage return, line feed or white space as non-whitespace control characters.
Except for the control characters that prescribe elementary line-oriented formatting, ASCII does not define any mechanism for describing the structure or appearance of text within a document.
Other schemes, such as markup languages, address page and document layout and formatting.
The original ASCII standard used only short descriptive phrases for each control character.
The ambiguity this caused was sometimes intentional, for example where a character would be used slightly differently on a terminal link than on a data stream, and sometimes accidental, for example with the meaning of "delete".
Probably the most influential single device on the interpretation of these characters was the Teletype Model 33 ASR, which was a printing terminal with an available paper tape reader/punch option.
Paper tape was a very popular medium for long-term program storage until the 1980s, less costly and in some ways less fragile than magnetic tape.
In particular, the Teletype Model 33 machine assignments for codes 17 (Control-Q, DC1, also known as XON), 19 (Control-S, DC3, also known as XOFF), and 127 (Delete) became de facto standards.
The Model 33 was also notable for taking the description of Control-G (code 7, BEL, meaning audibly alert the operator) literally, as the unit contained an actual bell which it rang when it received a BEL character.
Because the keytop for the O key also showed a left-arrow symbol (from ASCII-1963, which had this character instead of underscore), a noncompliant use of code 15 (Control-O, Shift In) interpreted as "delete previous character" was also adopted by many early timesharing systems but eventually became neglected.
When a Teletype 33 ASR equipped with the automatic paper tape reader received a Control-S (XOFF, an abbreviation for transmit off), it caused the tape reader to stop; receiving Control-Q (XON, "transmit on") caused the tape reader to resume.
This technique became adopted by several early computer operating systems as a "handshaking" signal warning a sender to stop transmission because of impending overflow; it persists to this day in many systems as a manual output control technique.
On some systems Control-S retains its meaning but Control-Q is replaced by a second Control-S to resume output.
The 33 ASR also could be configured to employ Control-R (DC2) and Control-T (DC4) to start and stop the tape punch; on some units equipped with this function, the corresponding control character lettering on the keycap above the letter was TAPE and TAPE respectively.
The Teletype could not move the head backwards, so it did not put a key on the keyboard to send a BS (backspace).
Instead there was a key marked "rubout" that sent code 127 (DEL).
The purpose of this key was to erase mistakes in a hand-typed paper tape: the operator had to push a button on the tape punch to back it up, then type the rubout, which punched all holes and replaced the mistake with a character that was intended to be ignored.
Teletypes were commonly used for the less-expensive computers from Digital Equipment Corporation, so these systems had to use the available key and thus the DEL code to erase the previous character.
Because of this, DEC video terminals (by default) sent the DEL code for the key marked "Backspace" while the key marked "Delete" sent an escape sequence, while many other terminals sent BS for the Backspace key.
The Unix terminal driver could only use one code to back up, this could be set to BS "or" DEL, but not both, resulting in a very long period of annoyance where you had to correct it depending on what terminal you were using (modern shells using readline understand both codes).
The assumption that no key sent a BS caused Control+H to be used for other purposes, such as a "help" command in Emacs.
Many more of the control codes have been given meanings quite different from their original ones.
The "escape" character (ESC, code 27), for example, was intended originally to allow sending other control characters as literals instead of invoking their meaning.
This is the same meaning of "escape" encountered in URL encodings, C language strings, and other systems where certain characters have a reserved meaning.
Over time this meaning has been co-opted and has eventually been changed.
In modern use, an ESC sent to the terminal usually indicates the start of a command sequence usually in the form of a so-called "ANSI escape code" (or, more properly, a "Control Sequence Introducer") from ECMA-48 (1972) and its successors, beginning with ESC followed by a "<nowiki>[</nowiki>" (left-bracket) character.
An ESC sent from the terminal is most often used as an out-of-band character used to terminate an operation, as in the TECO and vi text editors.
In graphical user interface (GUI) and windowing systems, ESC generally causes an application to abort its current operation or to exit (terminate) altogether.
The inherent ambiguity of many control characters, combined with their historical usage, created problems when transferring "plain text" files between systems.
The best example of this is the newline problem on various operating systems.
Teletype machines required that a line of text be terminated with both "Carriage Return" (which moves the printhead to the beginning of the line) and "Line Feed" (which advances the paper one line without moving the printhead).
The name "Carriage Return" comes from the fact that on a manual typewriter the carriage holding the paper moved while the position where the typebars struck the ribbon remained stationary.
The entire carriage had to be pushed (returned) to the right in order to position the left margin of the paper for the next line.
DEC operating systems (OS/8, RT-11, RSX-11, RSTS, TOPS-10, etc.)
used both characters to mark the end of a line so that the console device (originally Teletype machines) would work.
By the time so-called "glass TTYs" (later called CRTs or terminals) came along, the convention was so well established that backward compatibility necessitated continuing the convention.
When Gary Kildall created CP/M he was inspired by some command line interface conventions used in DEC's RT-11.
Until the introduction of PC DOS in 1981, IBM had no hand in this because their 1970s operating systems used EBCDIC instead of ASCII and they were oriented toward punch-card input and line printer output on which the concept of carriage return was meaningless.
IBM's PC DOS (also marketed as MS-DOS by Microsoft) inherited the convention by virtue of being loosely based on CP/M, and Windows inherited it from MS-DOS.
Unfortunately, requiring two characters to mark the end of a line introduces unnecessary complexity and questions as to how to interpret each character when encountered alone.
To simplify matters plain text data streams, including files, on Multics used line feed (LF) alone as a line terminator.
Unix and Unix-like systems, and Amiga systems, adopted this convention from Multics.
The original Macintosh OS, Apple DOS, and ProDOS, on the other hand, used carriage return (CR) alone as a line terminator; however, since Apple replaced these operating systems with the Unix-based macOS operating system, they now use line feed (LF) as well.
The Radio Shack TRS-80 also used a lone CR to terminate lines.
Computers attached to the ARPANET included machines running operating systems such as TOPS-10 and TENEX using CR-LF line endings, machines running operating systems such as Multics using LF line endings, and machines running operating systems such as OS/360 that represented lines as a character count followed by the characters of the line and that used EBCDIC rather than ASCII.
The Telnet protocol defined an ASCII "Network Virtual Terminal" (NVT), so that connections between hosts with different line-ending conventions and character sets could be supported by transmitting a standard text format over the network.
Telnet used ASCII along with CR-LF line endings, and software using other conventions would translate between the local conventions and the NVT.
The File Transfer Protocol adopted the Telnet protocol, including use of the Network Virtual Terminal, for use when transmitting commands and transferring data in the default ASCII mode.
This adds complexity to implementations of those protocols, and to other network protocols, such as those used for E-mail and the World Wide Web, on systems not using the NVT's CR-LF line-ending convention.
The PDP-6 monitor, and its PDP-10 successor TOPS-10, used Control-Z (SUB) as an end-of-file indication for input from a terminal.
Some operating systems such as CP/M tracked file length only in units of disk blocks and used Control-Z to mark the end of the actual text in the file.
For these reasons, EOF, or end-of-file, was used colloquially and conventionally as a three-letter acronym for Control-Z instead of SUBstitute.
The end-of-text code (ETX), also known as Control-C, was inappropriate for a variety of reasons, while using Z as the control code to end a file is analogous to it ending the alphabet and serves as a very convenient mnemonic aid.
A historically common and still prevalent convention uses the ETX code convention to interrupt and halt a program via an input data stream, usually from a keyboard.
In C library and Unix conventions, the null character is used to terminate text strings; such null-terminated strings can be known in abbreviation as ASCIZ or ASCIIZ, where here Z stands for "zero".
Other representations might be used by specialist equipment, for example ISO 2047 graphics or hexadecimal numbers.
Codes 20 to 7E, known as the printable characters, represent letters, digits, punctuation marks, and a few miscellaneous symbols.
There are 95 printable characters in total.
Code 20, the "space" character, denotes the space between words, as produced by the space bar of a keyboard.
Since the space character is considered an invisible graphic (rather than a control character) it is listed in the table below instead of in the previous section.
Code 7F corresponds to the non-printable "delete" (DEL) control character and is therefore omitted from this chart; it is covered in the previous section's chart.
Earlier versions of ASCII used the up arrow instead of the caret (5E) and the left arrow instead of the underscore (5F).
]]
ASCII was first used commercially during 1963 as a seven-bit teleprinter code for American Telephone & Telegraph's TWX (TeletypeWriter eXchange) network.
TWX originally used the earlier five-bit ITA2, which was also used by the competing Telex teleprinter system.
Bob Bemer introduced features such as the escape sequence.
His British colleague Hugh McGregor Ross helped to popularize this work according to Bemer, "so much so that the code that was to become ASCII was first called the "Bemer-Ross Code" in Europe".
Because of his extensive work on ASCII, Bemer has been called "the father of ASCII".
On March 11, 1968, U.S.
President Lyndon B. Johnson mandated that all computers purchased by the United States federal government support ASCII, stating:
I have also approved recommendations of the Secretary of Commerce regarding standards for recording the Standard Code for Information Interchange on magnetic tapes and paper tapes when they are used in computer operations.
All computers and related equipment configurations brought into the Federal Government inventory on and after July 1, 1969, must have the capability to use the Standard Code for Information Interchange and the formats prescribed by the magnetic tape and paper tape standards when these media are used.
ASCII was the most common character encoding on the World Wide Web until December 2007, when UTF-8 encoding surpassed it; UTF-8 is backward compatible with ASCII.
As computer technology spread throughout the world, different standards bodies and corporations developed many variations of ASCII to facilitate the expression of non-English languages that used Roman-based alphabets.
One could class some of these variations as "ASCII extensions", although some misuse that term to represent all variants, including those that do not preserve ASCII's character-map in the 7-bit range.
Furthermore, the ASCII extensions have also been mislabelled as ASCII.
From early in its development, ASCII was intended to be just one of several national variants of an international character code standard.
Other international standards bodies have ratified character encodings such as ISO 646 (1967) that are identical or nearly identical to ASCII, with extensions for characters outside the English alphabet and symbols used outside the United States, such as the symbol for the United Kingdom's pound sterling (£).
Almost every country needed an adapted version of ASCII, since ASCII suited the needs of only the US and a few other countries.
For example, Canada had its own version that supported French characters.
Many other countries developed variants of ASCII to include non-English letters (e.g.
é, ñ, ß, Ł), currency symbols (e.g.
£, ¥), etc.
See also YUSCII (Yugoslavia).
It would share most characters in common, but assign other locally useful characters to several code points reserved for "national use".
However, the four years that elapsed between the publication of ASCII-1963 and ISO's first acceptance of an international recommendation during 1967 caused ASCII's choices for the national use characters to seem to be de facto standards for the world, causing confusion and incompatibility once other countries did begin to make their own assignments to these code points.
ISO/IEC 646, like ASCII, is a 7-bit character set.
It does not make any additional codes available, so the same code points encoded different characters in different countries.
Escape codes were defined to indicate which national variant applied to a piece of text, but they were rarely used, so it was often impossible to know what variant to work with and, therefore, which character a code represented, and in general, text-processing systems could cope with only one variant anyway.
Because the bracket and brace characters of ASCII were assigned to "national use" code points that were used for accented letters in other national variants of ISO/IEC 646, a German, French, or Swedish, etc.
programmer using their national variant of ISO/IEC 646, rather than ASCII, had to write, and thus read, something such as

instead of

C trigraphs were created to solve this problem for ANSI C, although their late introduction and inconsistent implementation in compilers limited their use.
Many programmers kept their computers on US-ASCII, so plain-text in Swedish, German etc.
(for example, in e-mail or Usenet) contained "{, }" and similar variants in the middle of words, something those programmers got used to.
For example, a Swedish programmer mailing another programmer asking if they should go for lunch, could get "N{ jag har sm|rg}sar" as the answer, which should be "Nä jag har smörgåsar" meaning "No I've got sandwiches".
Eventually, as 8-, 16- and 32-bit (and later 64-bit) computers began to replace 12-, 18- and 36-bit computers as the norm, it became common to use an 8-bit byte to store each character in memory, providing an opportunity for extended, 8-bit relatives of ASCII.
In most cases these developed as true extensions of ASCII, leaving the original character-mapping intact, but adding additional character definitions after the first 128 (i.e., 7-bit) characters.
Encodings include ISCII (India), VISCII (Vietnam).
Although these encodings are sometimes referred to as ASCII, true ASCII is defined strictly only by the ANSI standard.
Most early home computer systems developed their own 8-bit character sets containing line-drawing and game glyphs, and often filled in some or all of the control characters from 0 to 31 with more graphics.
Kaypro CP/M computers used the "upper" 128 characters for the Greek alphabet.
The PETSCII code Commodore International used for their 8-bit systems is probably unique among post-1970 codes in being based on ASCII-1963, instead of the more common ASCII-1967, such as found on the ZX Spectrum computer.
Atari 8-bit computers and Galaksija computers also used ASCII variants.
The IBM PC defined code page 437, which replaced the control characters with graphic symbols such as smiley faces, and mapped additional graphic characters to the upper 128 positions.
Operating systems such as DOS supported these code pages, and manufacturers of IBM PCs supported them in hardware.
Digital Equipment Corporation developed the Multinational Character Set (DEC-MCS) for use in the popular VT220 terminal as one of the first extensions designed more for international languages than for block graphics.
The Macintosh defined Mac OS Roman and Postscript also defined a set, both of these contained both international letters and typographic punctuation marks instead of graphics, more like modern character sets.
The ISO/IEC 8859 standard (derived from the DEC-MCS) finally provided a standard that most systems copied (at least as accurately as they copied ASCII, but with many substitutions).
A popular further extension designed by Microsoft, Windows-1252 (often mislabeled as ISO-8859-1), added the typographic punctuation marks needed for traditional text printing.
ISO-8859-1, Windows-1252, and the original 7-bit ASCII were the most common character encodings until 2008 when UTF-8 became more common.
ISO/IEC 4873 introduced 32 additional control codes defined in the 80–9F hexadecimal range, as part of extending the 7-bit ASCII encoding to become an 8-bit system.
Unicode and the ISO/IEC 10646 Universal Character Set (UCS) have a much wider array of characters and their various encoding forms have begun to supplant ISO/IEC 8859 and ASCII rapidly in many environments.
While ASCII is limited to 128 characters, Unicode and the UCS support more characters by separating the concepts of unique identification (using natural numbers called "code points") and encoding (to 8-, 16- or 32-bit binary formats, called UTF-8, UTF-16 and UTF-32).
ASCII was incorporated into the Unicode (1991) character set as the first 128 symbols, so the 7-bit ASCII characters have the same numeric codes in both sets.
This allows UTF-8 to be backward compatible with 7-bit ASCII, as a UTF-8 file containing only ASCII characters is identical to an ASCII file containing the same sequence of characters.
Even more importantly, forward compatibility is ensured as software that recognizes only 7-bit ASCII characters as special and does not alter bytes with the highest bit set (as is often done to support 8-bit ASCII extensions such as ISO-8859-1) will preserve UTF-8 data unchanged.
</doc>
<doc id="590" url="https://en.wikipedia.org/wiki?curid=590" title="Austin (disambiguation)">
Austin (disambiguation)

Austin is the capital of Texas in the United States.
Austin may also refer to:













</doc>
<doc id="593" url="https://en.wikipedia.org/wiki?curid=593" title="Animation">
Animation

Animation is a method in which pictures are manipulated to appear as moving images.
In traditional animation, images are drawn or painted by hand on transparent celluloid sheets to be photographed and exhibited on film.
Today most animations are made with computer-generated imagery (CGI).
Computer animation can be very detailed 3D animation, while 2D computer animation can be used for stylistic reasons, low bandwidth or faster real-time renderings.
Other common animation methods apply a stop motion technique to two and three-dimensional objects like paper cutouts, puppets or clay figures.
Commonly the effect of animation is achieved by a rapid succession of sequential images that minimally differ from each other.
The illusion—as in motion pictures in general—is thought to rely on the phi phenomenon and beta movement, but the exact causes are still uncertain.
Analog mechanical animation media that rely on the rapid display of sequential images include the phénakisticope, zoetrope, flip book, praxinoscope and film.
Television and video are popular electronic animation media that originally were analog and now operate digitally.
For display on the computer, techniques like animated GIF and Flash animation were developed.
Apart from short films, feature films, animated gifs and other media dedicated to the display moving images, animation is also heavily used for video games, motion graphics and special effects.
The physical movement of image parts through simple mechanics in for instance the moving images in magic lantern shows can also be considered animation.
Mechanical animation of actual robotic devices is known as animatronics.
Animators are artists who specialize in creating animation.
The word "animation" stems from the Latin "animationem" (nominative "animatio"), noun of action from past participle stem of "animare", meaning "the action of imparting life".
The primary meaning of the English word is "liveliness" and has been in use much longer than the meaning of "moving image medium".
The history of animation started long before the development of cinematography.
Humans have probably attempted to depict motion as far back as the paleolithic period.
Shadow play and the magic lantern offered popular shows with moving images as the result of manipulation by hand and/or some minor mechanics.
A 5,200-year old pottery bowl discovered in Shahr-e Sukhteh, Iran, has five sequential images painted around it that seem to show phases of a goat leaping up to nip at a tree.
In 1833, the phenakistiscope introduced the stroboscopic principle of modern animation, which would also provide the basis for the zoetrope (1866), the flip book (1868), the praxinoscope (1877) and cinematography.
Charles-Émile Reynaud further developed his projection praxinoscope into the Théâtre Optique with transparent hand-painted colorful pictures in a long perforated strip wound between two spools, patented in December 1888.
From 28 October 1892 to March 1900 Reynaud gave over 12,800 shows to a total of over 500.000 visitors at the Musée Grévin in Paris.
His "Pantomimes Lumineuses" series of animated films each contained 300 to 700 frames that were manipulated back and forth to last 10 to 15 minutes per film.
Piano music, song and some dialogue were performed live, while some sound effects were synchronized with an electromagnet.
When film became a common medium some manufacturers of optical toys adapted small magic lanterns into toy film projectors for short loops of film.
By 1902, they were producing many chromolithography film loops, usually by tracing live-action film footage (much like the later rotoscoping technique).
Some early filmmakers, including J. Stuart Blackton, Arthur Melbourne-Cooper, Segundo de Chomón and Edwin S. Porter experimented with stop-motion animation, possibly since around 1899.
Blackton's "The Haunted Hotel" (1907) was the first huge success that baffled audiences with objects apparently moving by themselves and inspired other filmmakers to try the technique for themselves.
J. Stuart Blackton also experimented with animation drawn on blackboards and some cutout animation in "Humorous Phases of Funny Faces" (1906).
In 1908, Émile Cohl's "Fantasmagorie" was released with a white-on-black chalkline look created with negative prints from black ink drawings on white paper.
The film largely consists of a stick figure moving about and encountering all kinds of morphing objects, including a wine bottle that transforms into a flower.
Inspired by Émile Cohl's stop-motion film "Les allumettes animées [Animated Matches]" (1908), Ladislas Starevich started making his influential puppet animations in 1910.
Winsor McCay's "Little Nemo" (1911) showcased very detailed drawings.
His "Gertie the Dinosaur" (1914) was an also an early example of character development in drawn animation.
During the 1910s, the production of animated short films typically referred to as "cartoons", became an industry of its own and cartoon shorts were produced for showing in movie theaters.
The most successful producer at the time was John Randolph Bray, who, along with animator Earl Hurd, patented the cel animation process that dominated the animation industry for the rest of the decade.
"El Apóstol" (Spanish: "The Apostle") was a 1917 Argentine animated film utilizing cutout animation, and the world's first animated feature film.
Unfortunately, a fire that destroyed producer Federico Valle's film studio incinerated the only known copy of "El Apóstol", and it is now considered a lost film.
The earliest extant feature-length animated film is The Adventures of Prince Achmed (1926) made by director Lotte Reiniger and her collaborators Carl Koch and Berthold Bartosch.
In 1932, the first short animated film created entirely with Technicolor (using red/green/blue photographic filters and three strips of film) was Walt Disney's "Flowers and Trees", directed by Burt Gillett.
But, the first feature film that was done with this technique, apart from the movie The Vanities Fair (1935), by Rouben Mamoulian, was "Snow White and the Seven Dwarfs", also by Walt Disney.
In 1958, Hanna-Barbera released "The Huckleberry Hound Show", the first half hour television program to feature only in animation.
Terrytoons released "Tom Terrific" that same year.
Television significantly decreased public attention to the animated shorts being shown in theaters.
Computer animation has become popular since "Toy Story" (1995), the first feature-length animated film completely made using this technique.
In 2008, the animation market was worth US$68.4 billion.
Animation as an art and industry continues to thrive as of the mid-2010s because well-made animated projects can find audiences across borders and in all four quadrants.
Animated feature-length films returned the highest gross margins (around 52%) of all film genres in the 2004–2013 timeframe.
Traditional animation (also called cel animation or hand-drawn animation) was the process used for most animated films of the 20th century.
The individual frames of a traditionally animated film are photographs of drawings, first drawn on paper.
To create the illusion of movement, each drawing differs slightly from the one before it.
The animators' drawings are traced or photocopied onto transparent acetate sheets called cels, which are filled in with paints in assigned colors or tones on the side opposite the line drawings.
The completed character cels are photographed one-by-one against a painted background by a rostrum camera onto motion picture film.
The traditional cel animation process became obsolete by the beginning of the 21st century.
Today, animators' drawings and the backgrounds are either scanned into or drawn directly into a computer system.
Various software programs are used to color the drawings and simulate camera movement and effects.
The final animated piece is output to one of several delivery media, including traditional 35 mm film and newer media with digital video.
The "look" of traditional cel animation is still preserved, and the character animators' work has remained essentially the same over the past 70 years.
Some animation producers have used the term "tradigital" (a play on the words "traditional" and "digital") to describe cel animation that uses significant computer technology.
Examples of traditionally animated feature films include "Pinocchio" (United States, 1940), "Animal Farm" (United Kingdom, 1954), "Lucky and Zorba" (Italy, 1998), and "The Illusionist" (British-French, 2010).
Traditionally animated films produced with the aid of computer technology include "The Lion King" (US, 1994), "The Prince of Egypt" (US, 1998), "Akira" (Japan, 1988), "Spirited Away" (Japan, 2001), "The Triplets of Belleville" (France, 2003), and "The Secret of Kells" (Irish-French-Belgian, 2009).
Full animation refers to the process of producing high-quality traditionally animated films that regularly use detailed drawings and plausible movement, having a smooth animation.
Fully animated films can be made in a variety of styles, from more realistically animated works like those produced by the Walt Disney studio ("The Little Mermaid", "Beauty and the Beast", "Aladdin", "The Lion King") to the more 'cartoon' styles of the Warner Bros.
animation studio.
Many of the Disney animated features are examples of full animation, as are non-Disney works, "The Secret of NIMH" (US, 1982), "The Iron Giant" (US, 1999), and "Nocturna" (Spain, 2007).
Fully animated films are animated at 24 frames per second, with a combination of animation on ones and twos, meaning that drawings can be held for one frame out of 24 or two frames out of 24.
Limited animation involves the use of less detailed or more stylized drawings and methods of movement usually a choppy or "skippy" movement animation.
Limited animation uses fewer drawings per second, thereby limiting the fluidity of the animation.
This is a more economic technique.
Pioneered by the artists at the American studio United Productions of America, limited animation can be used as a method of stylized artistic expression, as in "Gerald McBoing-Boing" (US, 1951), "Yellow Submarine" (UK, 1968), and certain anime produced in Japan.
Its primary use, however, has been in producing cost-effective animated content for media for television (the work of Hanna-Barbera, Filmation, and other TV animation studios) and later the Internet (web cartoons).
Rotoscoping is a technique patented by Max Fleischer in 1917 where animators trace live-action movement, frame by frame.
The source film can be directly copied from actors' outlines into animated drawings, as in "The Lord of the Rings" (US, 1978), or used in a stylized and expressive manner, as in "Waking Life" (US, 2001) and "A Scanner Darkly" (US, 2006).
Some other examples are "Fire and Ice" (US, 1983), "Heavy Metal" (1981), and "Aku no Hana" (2013).
Live-action/animation is a technique combining hand-drawn characters into live action shots or live action actors into animated shots.
One of the earlier uses was in Koko the Clown when Koko was drawn over live action footage.
Other examples include "Allegro Non Troppo" (Italy, 1976), "Who Framed Roger Rabbit" (US, 1988), "Space Jam" (US, 1996) and "Osmosis Jones" (US, 2001).
Stop-motion animation is used to describe animation created by physically manipulating real-world objects and photographing them one frame of film at a time to create the illusion of movement.
There are many different types of stop-motion animation, usually named after the medium used to create the animation.
Computer software is widely available to create this type of animation; traditional stop motion animation is usually less expensive but more time-consuming to produce than current computer animation.
Computer animation encompasses a variety of techniques, the unifying factor being that the animation is created digitally on a computer.
2D animation techniques tend to focus on image manipulation while 3D techniques usually build virtual worlds in which characters and objects move and interact.
3D animation can create images that seem real to the viewer.
2D animation figures are created or edited on the computer using 2D bitmap graphics and 2D vector graphics.
This includes automated computerized versions of traditional animation techniques, interpolated morphing, onion skinning and interpolated rotoscoping.
2D animation has many applications, including analog computer animation, Flash animation, and PowerPoint animation.
Cinemagraphs are still photographs in the form of an animated GIF file of which part is animated.
Final line advection animation is a technique used in 2D animation, to give artists and animators more influence and control over the final product as everything is done within the same department.
Speaking about using this approach in "Paperman", John Kahrs said that "Our animators can change things, actually erase away the CG underlayer if they want, and change the profile of the arm."
3D animation is digitally modeled and manipulated by an animator.
The animator usually starts by creating a 3D polygon mesh to manipulate.
A mesh typically includes many vertices that are connected by edges and faces, which give the visual appearance of form to a 3D object or 3D environment.
Sometimes, the mesh is given an internal digital skeletal structure called an armature that can be used to control the mesh by weighting the vertices.
This process is called rigging and can be used in conjunction with keyframes to create movement.
Other techniques can be applied, mathematical functions (e.g., gravity, particle simulations), simulated fur or hair, and effects, fire and water simulations.
These techniques fall under the category of 3D dynamics.
An animator is an artist who creates a visual sequence (or audio-visual if added sound) of multiple sequential images that generate the illusion of movement, that is, an animation.
Animations are currently in many areas of technology and video, such as cinema, television, video games or the internet.
Generally, these works require the collaboration of several animators.
The methods to create these images depend on the animator and style that one wants to achieve (with images generated by computer, manually ...).
Animators can be divided into animators of characters (artists who are specialized in the movements, dialogue and acting of the characters) and animators of special effects (for example vehicles, machinery or natural phenomena such as water, snow, rain).
The creation of non-trivial animation works (i.e., longer than a few seconds) has developed as a form of filmmaking, with certain unique aspects.
Traits common to both live-action and animated feature-length films are labor-intensity and high production costs.
The most important difference is that once a film is in the production phase, the marginal cost of one more shot is higher for animated films than live-action films.
It is relatively easy for a director to ask for one more take during principal photography of a live-action film, but every take on an animated film must be manually rendered by animators (although the task of rendering slightly different takes has been made less tedious by modern computer animation).
It is pointless for a studio to pay the salaries of dozens of animators to spend weeks creating a visually dazzling five-minute scene if that scene fails to effectively advance the plot of the film.
Thus, animation studios starting with Disney began the practice in the 1930s of maintaining story departments where storyboard artists develop every single scene through storyboards, then handing the film over to the animators only after the production team is satisfied that all the scenes make sense as a whole.
While live-action films are now also storyboarded, they enjoy more latitude to depart from storyboards (i.e., real-time improvisation).
Another problem unique to animation is the requirement to maintain a film's consistency from start to finish, even as films have grown longer and teams have grown larger.
Animators, like all artists, necessarily have individual styles, but must subordinate their individuality in a consistent way to whatever style is employed on a particular film.
Since the early 1980s, teams of about 500 to 600 people, of whom 50 to 70 are animators, typically have created feature-length animated films.
It is relatively easy for two or three artists to match their styles; synchronizing those of dozens of artists is more difficult.
This problem is usually solved by having a separate group of visual development artists develop an overall look and palette for each film before animation begins.
Character designers on the visual development team draw model sheets to show how each character should look like with different facial expressions, posed in different positions, and viewed from different angles.
On traditionally animated projects, maquettes were often sculpted to further help the animators see how characters would look from different angles.
Unlike live-action films, animated films were traditionally developed beyond the synopsis stage through the storyboard format; the storyboard artists would then receive credit for writing the film.
In the early 1960s, animation studios began hiring professional screenwriters to write screenplays (while also continuing to use story departments) and screenplays had become commonplace for animated films by the late 1980s.
Criticism of animation has been common in media and cinema since its inception.
With its popularity, a large amount of criticism has arisen, especially animated feature-length films.
Many concerns of cultural representation, psychological effects on children have been brought up around the animation industry, which has remained rather politically unchanged and stagnant since its inception into mainstream culture.
As with any other form of media, animation too has instituted awards for excellence in the field.
The original awards for animation were presented by the Academy of Motion Picture Arts and Sciences for animated shorts from the year 1932, during the 5th Academy Awards function.
The first winner of the Academy Award was the short "Flowers and Trees", a production by Walt Disney Productions.
The Academy Award for a feature-length animated motion picture was only instituted for the year 2001, and awarded during the 74th Academy Awards in 2002.
It was won by the film "Shrek", produced by DreamWorks and Pacific Data Images.
Disney/Pixar have produced the most films either to win or be nominated for the award.
The list of both awards can be obtained here:

Several other countries have instituted an award for best animated feature film as part of their national film awards: Africa Movie Academy Award for Best Animation (since 2008), BAFTA Award for Best Animated Film (since 2006), César Award for Best Animated Film (since 2011), Golden Rooster Award for Best Animation (since 1981), Goya Award for Best Animated Film (since 1989), Japan Academy Prize for Animation of the Year (since 2007), National Film Award for Best Animated Film (since 2006).
Also since 2007, the Asia Pacific Screen Award for Best Animated Feature Film has been awarded at the Asia Pacific Screen Awards.
Since 2009, the European Film Awards have awarded the European Film Award for Best Animated Film.
The Annie Award is another award presented for excellence in the field of animation.
Unlike the Academy Awards, the Annie Awards are only received for achievements in the field of animation and not for any other field of technical and artistic endeavor.
They were re-organized in 1992 to create a new field for Best Animated feature.
The 1990s winners were dominated by Walt Disney, however, newer studios, led by Pixar & DreamWorks, have now begun to consistently vie for this award.
The list of awardees is as follows: 





</doc>
<doc id="594" url="https://en.wikipedia.org/wiki?curid=594" title="Apollo">
Apollo

Apollo (Attic, Ionic, and Homeric Greek: , "Apollōn" ( ) is one of the most important and complex of the Olympian deities in classical Greek and Roman religion and Greek and Roman mythology.
The national divinity of the Greeks, Apollo has been variously recognized as a god of music, truth and prophecy, healing, the sun and light, plague, poetry, and more.
Apollo is the son of Zeus and Leto, and has a twin sister, the chaste huntress Artemis.
Seen as the most beautiful god and the ideal of the "kouros" (a beardless, athletic youth), Apollo is considered to be the most Greek of all gods.
Apollo is known in Greek-influenced Etruscan mythology as "Apulu".
As the patron of Delphi ("Pythian Apollo"), Apollo was an oracular god—the prophetic deity of the Delphic Oracle.
Medicine and healing are associated with Apollo, whether through the god himself or mediated through his son Asclepius, yet Apollo was also seen as a god who could bring ill-health and deadly plague.
As an archer who never missed, Apollo carried a golden bow (silver bow, sometimes) and a quiver of golden arrows.
As the inventor of archery and taught the same to the people.
His arrows could inflict harm by causing sudden deaths or deadly plague.
As the leader of the Muses ("Apollon Musegetes") and director of their choir, Apollo functions as the patron god of music and poetry.
He is the inventor of string-music.
The Cithara and the lyre are also his inventions.
The lyre is a common attribute of Apollo.
Hymns sung to Apollo were called paeans.
Apollo favors and delights in the foundation of towns and the establishment of civil constitution.
Hence is associated with dominion over colonists.
Additionally, he is the god of foreigners, the protector of fugitives and refugees.
Apollo is the giver and interpreter of laws.
He presides over the divine law and custom along with Zeus, Demeter and Themis.
As the protector of young, Apollo ("kourotrophos") is concerned with the health of children.
He presides over their education and brings them out of their adolescence.
Due to this belief, boys in Ancient Greece, upon reaching their adulthood, cut their hair and dedicated it to Apollo.
He is the patron of herdsmen and protector of herds and flocks.
He is believed to cause abundance in the milk produced by cattle, and is also connected with their fertility.
Apollo is the god who affords help and wards off evil.
He delivered men from the epidemics.
Various epithets call him the "averter of evil".
As an agricultural deity, Apollo protects the crops from diseases, especially the rust in corns and grains.
He is also the controller and destroyer of pests that infect plants and plant harvests.
In Hellenistic times, especially during the 5th century BCE, as "Apollo Helios" he became identified among Greeks with Helios, Titan god of the sun.
In Latin texts, however, there was no conflation of Apollo with Sol among the classical Latin poets until 1st century AD.
Apollo and Helios/Sol remained separate beings in literary and mythological texts until the 5th century CE.
Apollo (Attic, Ionic, and Homeric Greek: , "Apollōn" ( ); Doric: , "Apellōn"; Arcadocypriot: , "Apeilōn"; Aeolic: , "Aploun"; )

The name "Apollo"—unlike the related older name "Paean"—is generally not found in the Linear B (Mycenean Greek) texts, although there is a possible attestation in the lacunose form "]pe-rjo-[" (Linear B: ]-[) on the KN E 842 tablet.
The etymology of the name is uncertain.
The spelling ( in Classical Attic) had almost superseded all other forms by the beginning of the common era, but the Doric form, "Apellon" (), is more archaic, as it is derived from an earlier .
It probably is a cognate to the Doric month "Apellaios" (), and the offerings apellaia () at the initiation of the young men during the family-festival apellai ().
According to some scholars, the words are derived from the Doric word "apella" (), which originally meant "wall," "fence for animals" and later "assembly within the limits of the square."
Apella () is the name of the popular assembly in Sparta, corresponding to the "ecclesia" ().
R. S. P. Beekes rejected the connection of the theonym with the noun "apellai" and suggested a Pre-Greek proto-form *"Apalun".
Several instances of popular etymology are attested from ancient authors.
Thus, the Greeks most often associated Apollo's name with the Greek verb ("apollymi"), "to destroy".
Plato in "Cratylus" connects the name with ("apolysis"), "redemption", with ("apolousis"), "purification", and with ("[h]aploun"), "simple", in particular in reference to the Thessalian form of the name, , and finally with ("aeiballon"), "ever-shooting".
Hesychius connects the name Apollo with the Doric ("apella"), which means "assembly", so that Apollo would be the god of political life, and he also gives the explanation ("sekos"), "fold", in which case Apollo would be the god of flocks and herds.
In the ancient Macedonian language ("pella") means "stone," and some toponyms may be derived from this word: (Pella, the capital of ancient Macedonia) and ("Pellēnē"/"Pallene").
A number of non-Greek etymologies have been suggested for the name, The Hittite form "Apaliunas" ("") is attested in the Manapa-Tarhunta letter, perhaps related to Hurrian (and certainly the Etruscan) "Aplu", a god of plague, in turn likely from Akkadian "Aplu Enlil" meaning simply "the son of Enlil", a title that was given to the god Nergal, who was linked to Shamash, Babylonian god of the sun.
The role of Apollo as god of plague is evident in the invocation of Apollo Smintheus ("mouse Apollo") by Chryses, the Trojan priest of Apollo, with the purpose of sending a plague against the Greeks (the reasoning behind a god of the plague becoming a god of healing is apotropaic, meaning that the god responsible for bringing the plague must be appeased in order to remove the plague).
The Hittite testimony reflects an early form "", which may also be surmised from comparison of Cypriot with Doric .
The name of the Lydian god "Qλdãns" /kʷʎðãns/ may reflect an earlier /kʷalyán-/ before palatalization, syncope, and the pre-Lydian sound change *y ">" d. Note the labiovelar in place of the labial /p/ found in pre-Doric "Ἀπέλjων" and Hittite "Apaliunas".
A Luwian etymology suggested for "Apaliunas" makes Apollo "The One of Entrapment", perhaps in the sense of "Hunter".
Apollo's chief epithet was Phoebus ( ; , "Phoibos" ), literally "bright".
It was very commonly used by both the Greeks and Romans for Apollo's role as the god of light.
Like other Greek deities, he had a number of others applied to him, reflecting the variety of roles, duties, and aspects ascribed to the god.
However, while Apollo has a great number of appellations in Greek myth, only a few occur in Latin literature.
Apollo's birthplace was Mount Cynthus on the island of Delos.
Delphi and Actium were his primary places of worship.
Apollo was worshipped throughout the Roman Empire.
In the traditionally Celtic lands he was most often seen as a healing and sun god.
He was often equated with Celtic gods of similar character.
The cult centers of Apollo in Greece, Delphi and Delos, date from the 8th century BCE.
The Delos sanctuary was primarily dedicated to Artemis, Apollo's twin sister.
At Delphi, Apollo was venerated as the slayer of Pytho.
For the Greeks, Apollo was all the Gods in one and through the centuries he acquired different functions which could originate from different gods.
In archaic Greece he was the prophet, the oracular god who in older times was connected with "healing".
In classical Greece he was the god of light and of music, but in popular religion he had a strong function to keep away evil.
Walter Burkert discerned three components in the prehistory of Apollo worship, which he termed "a Dorian-northwest Greek component, a Cretan-Minoan component, and a Syro-Hittite component."
From his eastern origin Apollo brought the art of inspection of "symbols and omina" (σημεία και τέρατα : "semeia kai terata"), and of the observation of the omens of the days.
The inspiration oracular-cult was probably introduced from Anatolia.
The ritualism belonged to Apollo from the beginning.
The Greeks created the legalism, the supervision of the orders of the gods, and the demand for moderation and harmony.
Apollo became the god of shining youth, ideal beauty, fine arts, philosophy, moderation, spiritual-life, the protector of music, divine law and perceptible order.
The improvement of the old Anatolian god, and his elevation to an intellectual sphere, may be considered an achievement of the Greek people.
The function of Apollo as a "healer" is connected with Paean (), the physician of the Gods in the "Iliad", who seems to come from a more primitive religion.
Paeοn is probably connected with the Mycenean "pa-ja-wo-ne" (Linear B: ), but this is not certain.
He did not have a separate cult, but he was the personification of the holy magic-song sung by the magicians that was supposed to cure disease.
Later the Greeks knew the original meaning of the relevant song "paean" ().
The magicians were also called "seer-doctors" (), and they used an ecstatic prophetic art which was used exactly by the god Apollo at the oracles.
In the "Iliad", Apollo is the healer under the gods, but he is also the bringer of disease and death with his arrows, similar to the function of the Vedic god of disease Rudra.
He sends a plague () to the Achaeans.
The god who sends a disease can also prevent it; therefore, when it stops, they make a purifying ceremony and offer him a hecatomb to ward off evil.
When the oath of his priest appeases, they pray and with a song they call their own god, the "Paean".
Some common epithets of Apollo as a healer are "paion" (, literally "healer" or "helper") "epikourios" (, "help"), "oulios" (, "healed wound", also a "scar" ) and "loimios" (, "plague").
In classical times, his strong function in popular religion was to keep away evil, and was therefore called "apotropaios" (, "divert", "deter", "avert") and "alexikakos" (from v.
+ n. , "defend from evil").
In later writers, the word, usually spelled "Paean", becomes a mere epithet of Apollo in his capacity as a god of healing.
Homer illustrated Paeon the god, and the song both of apotropaic thanksgiving or triumph.
Such songs were originally addressed to Apollo, and afterwards to other gods: to Dionysus, to Apollo Helios, to Apollo's son Asclepius the healer.
About the 4th century BCE, the paean became merely a formula of adulation; its object was either to implore protection against disease and misfortune, or to offer thanks after such protection had been rendered.
It was in this way that Apollo had become recognised as the god of music.
Apollo's role as the slayer of the Python led to his association with battle and victory; hence it became the Roman custom for a paean to be sung by an army on the march and before entering into battle, when a fleet left the harbour, and also after a victory had been won.
The connection with the Dorians and their initiation festival "apellai" is reinforced by the month "Apellaios" in northwest Greek calendars.
The family-festival was dedicated to Apollo (Doric: ).
"Apellaios" is the month of these rites, and Apellon is the "megistos kouros" (the great Kouros).
However it can explain only the Doric type of the name, which is connected with the Ancient Macedonian word "pella" (Pella), "stone".
Stones played an important part in the cult of the god, especially in the oracular shrine of Delphi (Omphalos).
The "Homeric hymn" represents Apollo as a Northern intruder.
His arrival must have occurred during the "Dark Ages" that followed the destruction of the Mycenaean civilization, and his conflict with Gaia (Mother Earth) was represented by the legend of his slaying her daughter the serpent Python.
The earth deity had power over the ghostly world, and it is believed that she was the deity behind the oracle.
The older tales mentioned two dragons who were perhaps intentionally conflated.
A female dragon named Delphyne (, "womb"), and a male serpent Typhon (, "to smoke"), the adversary of Zeus in the Titanomachy, who the narrators confused with Python.
Python was the good daemon (ἀγαθὸς δαίμων) of the temple as it appears in Minoan religion, but she was represented as a dragon, as often happens in Northern European folklore as well as in the East.
Apollo and his sister Artemis can bring death with their arrows.
The conception that diseases and death come from invisible shots sent by supernatural beings, or magicians is common in Germanic and Norse mythology.
In Greek mythology Artemis was the leader (, "hegemon") of the nymphs, who had similar functions with the Nordic Elves.
The "elf-shot" originally indicated disease or death attributed to the elves, but it was later attested denoting stone arrow-heads which were used by witches to harm people, and also for healing rituals.
The Vedic Rudra has some similar functions with Apollo.
The terrible god is called "The Archer", and the bow is also an attribute of Shiva.
Rudra could bring diseases with his arrows, but he was able to free people of them, and his alternative Shiba is a healer physician god.
However the Indo-European component of Apollo does not explain his strong relation with omens, exorcisms, and with the oracular cult.
It seems an oracular cult existed in Delphi from the Mycenaean age.
In historical times, the priests of Delphi were called Labryaden, "the double-axe men", which indicates Minoan origin.
The double-axe, labrys, was the holy symbol of the Cretan labyrinth.
The Homeric hymn adds that Apollo appeared as a dolphin and carried Cretan priests to Delphi, where they evidently transferred their religious practices.
"Apollo Delphinios" or "Delphidios" was a sea-god especially worshiped in Crete and in the islands.
Apollo's sister Artemis, who was the Greek goddess of hunting, is identified with Britomartis (Diktynna), the Minoan "Mistress of the animals".
In her earliest depictions she is accompanied by the "Mister of the animals", a male god of hunting who had the bow as his attribute.
His original name is unknown, but it seems that he was absorbed by the more popular Apollo, who stood by the virgin "Mistress of the Animals", becoming her brother.
The old oracles in Delphi seem to be connected with a local tradition of the priesthood, and there is not clear evidence that a kind of inspiration-prophecy existed in the temple.
This led some scholars to the conclusion that Pythia carried on the rituals in a consistent procedure through many centuries, according to the local tradition.
In that regard, the mythical seeress Sibyl of Anatolian origin, with her ecstatic art, looks unrelated to the oracle itself.
However, the Greek tradition is referring to the existence of vapours and chewing of laurel-leaves, which seem to be confirmed by recent studies.
Plato describes the priestesses of Delphi and Dodona as frenzied women, obsessed by "mania" (, "frenzy"), a Greek word he connected with "mantis" (, "prophet").
Frenzied women like Sibyls from whose lips the god speaks are recorded in the Near East as Mari in the second millennium BC.
Although Crete had contacts with Mari from 2000 BC, there is no evidence that the ecstatic prophetic art existed during the Minoan and Mycenean ages.
It is more probable that this art was introduced later from Anatolia and regenerated an existing oracular cult that was local to Delphi and dormant in several areas of Greece.
A non-Greek origin of Apollo has long been assumed in scholarship.
The name of Apollo's mother Leto has Lydian origin, and she was worshipped on the coasts of Asia Minor.
The inspiration oracular cult was probably introduced into Greece from Anatolia, which is the origin of Sibyl, and where existed some of the oldest oracular shrines.
Omens, symbols, purifications, and exorcisms appear in old Assyro-Babylonian texts, and these rituals were spread into the empire of the Hittites.
In a Hittite text is mentioned that the king invited a Babylonian priestess for a certain "purification".
A similar story is mentioned by Plutarch.
He writes that the Cretan seer Epimenides purified Athens after the pollution brought by the Alcmeonidae, and that the seer's expertise in sacrifices and reform of funeral practices were of great help to Solon in his reform of the Athenian state.
The story indicates that Epimenides was probably heir to the shamanic religions of Asia, and proves, together with the Homeric hymn, that Crete had a resisting religion up to historical times.
It seems that these rituals were dormant in Greece, and they were reinforced when the Greeks migrated to Anatolia.
Homer pictures Apollo on the side of the Trojans, fighting against the Achaeans, during the Trojan War.
He is pictured as a terrible god, less trusted by the Greeks than other gods.
The god seems to be related to "Appaliunas", a tutelary god of Wilusa (Troy) in Asia Minor, but the word is not complete.
The stones found in front of the gates of Homeric Troy were the symbols of Apollo.
A western Anatolian origin may also be bolstered by references to the parallel worship of "Artimus" (Artemis) and "Qλdãns", whose name may be cognate with the Hittite and Doric forms, in surviving Lydian texts"."
However, recent scholars have cast doubt on the identification of "Qλdãns" with Apollo.
The Greeks gave to him the name "agyieus" as the protector god of public places and houses who wards off evil, and his symbol was a tapered stone or column.
However, while usually Greek festivals were celebrated at the full moon, all the feasts of Apollo were celebrated at the seventh day of the month, and the emphasis given to that day ("sibutu") indicates a Babylonian origin.
The Late Bronze Age (from 1700 to 1200 BCE) Hittite and Hurrian "Aplu" was a god of plague, invoked during plague years.
Here we have an apotropaic situation, where a god originally bringing the plague was invoked to end it.
Aplu, meaning "the son of", was a title given to the god Nergal, who was linked to the Babylonian god of the sun Shamash.
Homer interprets Apollo as a terrible god () who brings death and disease with his arrows, but who can also heal, possessing a magic art that separates him from the other Greek gods.
In "Iliad", his priest prays to "Apollo Smintheus", the mouse god who retains an older agricultural function as the protector from field rats.
All these functions, including the function of the healer-god Paean, who seems to have Mycenean origin, are fused in the cult of Apollo.
Unusually among the Olympic deities, Apollo had two cult sites that had widespread influence: Delos and Delphi.
In cult practice, Delian Apollo and Pythian Apollo (the Apollo of Delphi) were so distinct that they might both have shrines in the same locality.
Apollo's cult was already fully established when written sources commenced, about 650 BCE.
Apollo became extremely important to the Greek world as an oracular deity in the archaic period, and the frequency of theophoric names such as "Apollodorus" or "Apollonios" and cities named "Apollonia" testify to his popularity.
Oracular sanctuaries to Apollo were established in other sites.
In the 2nd and 3rd century CE, those at Didyma and Clarus pronounced the so-called "theological oracles", in which Apollo confirms that all deities are aspects or servants of an all-encompassing, highest deity.
"In the 3rd century, Apollo fell silent.
Julian the Apostate (359–361) tried to revive the Delphic oracle, but failed."
Apollo had a famous oracle in Delphi, and other notable ones in Clarus and Branchidae.
His oracular shrine in Abae in Phocis, where he bore the toponymic epithet "Abaeus" (, "Apollon Abaios"), was important enough to be consulted by Croesus.
His oracular shrines include:

Oracles were also given by sons of Apollo.
Many temples were dedicated to Apollo in Greece and the Greek colonies.
They show the spread of the cult of Apollo and the evolution of the Greek architecture, which was mostly based on the rightness of form and on mathematical relations.
Some of the earliest temples, especially in Crete, do not belong to any Greek order.
It seems that the first peripteral temples were rectangular wooden structures.
The different wooden elements were considered divine, and their forms were preserved in the marble or stone elements of the temples of Doric order.
The Greeks used standard types because they believed that the world of objects was a series of typical forms which could be represented in several instances.
The temples should be canonic, and the architects were trying to achieve this esthetic perfection.
From the earliest times there were certain rules strictly observed in rectangular peripteral and prostyle buildings.
The first buildings were built narrowly in order to hold the roof, and when the dimensions changed some mathematical relations became necessary in order to keep the original forms.
This probably influenced the theory of numbers of Pythagoras, who believed that behind the appearance of things there was the permanent principle of mathematics.
The Doric order dominated during the 6th and the 5th century BC but there was a mathematical problem regarding the position of the triglyphs, which couldn't be solved without changing the original forms.
The order was almost abandoned for the Ionic order, but the Ionic capital also posed an insoluble problem at the corner of a temple.
Both orders were abandoned for the Corinthian order gradually during the Hellenistic age and under Rome.
The most important temples are:






Apollo appears often in the myths, plays and hymns.
As Zeus' favorite son, Apollo had direct access to the mind of Zeus and was willing to reveal this knowledge to humans.
A divinity beyond human comprehension, he appears both as a beneficial and a wrathful god.
When Zeus' wife Hera discovered that Leto was impregnanted by Zeus, she banned Leto from giving birth on "terra firma".
In her wanderings, Leto sought shelter on many lands, only to be rejected by them.
Finally, Apollo, still in Leto's womb, spoke to his mother and told her about the newly created floating island of Delos, which was neither mainland nor a real island.
Leto, when welcomed by Delos, gave birth there, clinging to a palm tree.
It is also stated that Hera kidnapped Eileithyia, the goddess of childbirth, to prevent Leto from going into labor.
The other gods tricked Hera into letting her go by offering her a necklace of amber 9 yards or 8.2 meters long.
When Apollo was born clutching a golden sword, the swans circled Delos seven times and the nymphs sang in delight.
Soon after he was born, he was washed clean by the goddesses and was covered in white garment, with golden bands fastened around him.
Since Leto was unable to feed the new born, Themis, the goddess of divine law, fed him the nectar, or ambrosia.
Upon tasting the divine food, Apollo broke free of the bands fastened onto him and declared that he would be the master of lyre and archery, and interpret the will of Zeus to humankind.
Apollo was a precocious child.
Even before he was born, in anger he had foretold the fate of Niobe and the death of Python during Leto's wanderings.Leto was accepted by the people of Delos and she promised them that her son would be always favorable towards the city.
Afterwards, Zeus secured Delos to the bottom of the ocean.
This island later became sacred to Apollo.
Apollo was born on the seventh day (, "hebdomagenes") of the month Thargelion —according to Delian tradition—or of the month Bysios—according to Delphian tradition.
The seventh and twentieth, the days of the new and full moon, were ever afterwards held sacred to him.
Mythographers agree that Artemis was born first and subsequently assisted with the birth of Apollo, or that Artemis was born on the island of Ortygia and that she helped Leto cross the sea to Delos the next day to give birth to Apollo.
As a child, Apollo is said to have built a foundation and an altar on Delos using the horns of the goats that his sister Artemis hunted.
Since he learnt the art of building when young, he later became "Archegetes", the founder of towns and god who guided men to build new cities.
In his young years when Zeus had ignored him, Apollo spent his time herding cows.
There, he was reared by Thriae, the bee nymphs, who trained him and enhanced his prophetic skills.During this time, Apollo also invented the lyre, and along with Artemis, the art of archery.
He then taught to the humans the art of healing and archery.
Phoebe, his grandmother, gave the oracular shrine of Delphi to Apollo as a birthday gift.
Themis inspired him to be the oracular voice of Delphi thereon.
Later, with his bow and arrows that he had received from Hephaestus, Apollo went in search of the chthonic dragon Python, which lived in Delphi beside the Castalian Spring, and was a terror to the people.
Hera had sent this serpent to hunt the pregnant Leto to her death across the world.
To avenge the trouble given to his mother, Apollo killed Python in the sacred cave at Delphi with his arrows.
Even though he had freed the people from a great bane, Python was a child of Gaia.
Zeus, not wanting to incur her anger, decided to punished Apollo.
Zeus banned Apollo from Olympus, and told him to purify himself.
Exiled, Apollo had to serve as a slave for nine years.
He also travelled to Crete, where Carmanor performed purification rites on him.
Further, Zeus ordered him to go to the Vale of Tempe and bath in waters of Peneus.
Apollo later established the Pythian games to appropriate Gaia.
Only after this, he was given his place on Olympus.
Henceforth, Apollo became the god who cleansed himself from the sin of murder and, made men aware of their guilt and purified them.
Zeus, for his son's integrity, gave Apollo the seat next to him on his right side.
He also gifted to Apollo a golden tripod, bow and arrows, a golden chariot drawn by swans and the land of Delphi.
From then, like his sister Athena, Apollo became the executive of Zeus.
Soon after his return, Apollo needed to recruit people to Delphi.
So, when he spotted a ship sailing from Crete, he sprang aboard in the form of a dolphin.
The crew was awed into submission and followed a course that led the ship to Delphi.
There Apollo revealed himself as a god.
Initiating them to his service, he instructed them to keep righteousness in their hearts.
The Pythia was Apollo's high priestess and his mouthpiece through whom he gave prophecies.
Pythia is arguably the constant favorite of Apollo among the mortals.
Hera once again sent another giant, Tityos to rape Leto.
This time Apollo shot him with his arrows and attacked him with his golden sword.
According to variations, Artemis aided in protecting their mother by attacking with her arrows.
After the battle Zeus finally relented his aid and hurled Tityos down to Tartarus.
There, he was pegged to the rock floor, covering an area of , where a pair of vultures feasted daily on his liver.
Once Apollo, along with Athena and Poseidon, participated in Hera's scheme to hold Zeus captive and demand a better rule from him.
This was unsuccessful and Zeus, feeling beatryed, sent Apollo and Poseidon to serve as slaves under the Trojan king Laomedon.
According to varied version, both gods went to test Laomedon.
Apollo guarded the cattle of Laomedon in the valleys of mount Ida, while Poseidon built the walls of Troy.
There, Apollo had a lover named Ourea, and sired a son Ileus by her.
Later, Apollo was also compelled by Laomedon to build the walls.
Apollo obeyed, and by playing on his lyre, he built the walls of Troy.
However, the king refused to give them the wages he had promised.
Angered, Apollo sent a pestilence to the city.
To deliver the city from it, Laomedon had to sacrifice his daughter Hesione (who would later be saved by Heracles).
Apollo sided with the Trojans during the Trojan war, a war waged by the Greeks against the Trojans.
During the war, Agamemnon, a Greek hero captured Chryseis, the daughter of Apollo's priest Chryses.
Angered, Apollo shot arrows infected with the plague into the Greek encampment.
He demanded to return the girl, and the Achaeans (Greeks) complied, indirectly causing the "anger of Achilles", which is the theme of the "Iliad".
Receiving the aegis from Zeus, Apollo entered the battlefield, causing great terror to the enemy with his war cry, pushing them back and destroying many of them.
He is described as "the rouser of armies", because he rallied the Trojan army when they were falling apart.
When Zeus allowed the other gods to get involved in the war, Apollo was provoked by Poseidon to a duel.
However, Apollo declined to fight him, saying that he wouldn't fight his uncle for the sake of mortals.
When Diomedes, the greek hero, injured Aeneas, a Trojan ally, Aphrodite tried to rescue him but Diomedes injured her as well.
Apollo then enveloped Aeneas in a cloud to protect him.
He repelled the attacks Diomedes made on him and gave the hero a stern warning to abstain himself from attacking a god.
Aeneas was then taken to Pergamos, a sacred spot in Troy, where he was healed.
After the death of Sarpedon, a son of Zeus, Apollo rescued the corpse from the battlefield as per his father's wish and cleaned it.
He then gave it to Sleep (Hypnos) and Death (Thanatos).
Apollo had also once convinced Athena to stop the war for that day, so that the warriors can relieve themselves for a while.
The Trojan hero Hector was favored by Apollo, who, according to some, was the god's own son by Hecuba.
When he got injured, Apollo healed him and encouraged him to take up the arms.
During a duel with Achilles, when Hector was about to lose, Apollo hid Hector in a cloud of mist to save him.
At last, after Hector's fated death, Apollo protected his corpse from Achilles' attempt to mutilate it by creating a magical cloud over the corpse.
The Greek warrior Patroclus tried to get into the fort of Troy and was stopped by Apollo.
Encouraging Hector to attack Patroclus, Apollo stripped the armour of Patroclus and broke his weapons.
Patroclus was eventually killed by Hector.
Apollo held anger towards Achilles throughout the war.
The reason for this was the murder of his son Tenes before the war began, and brutal assassination of his another son Troilus in his own temple, both by Achilles.
Not only did Apollo save Hector from Achilles, he also tricked Achilles by disguising himself as a Trojan warrior and driving him away from the gates.
He foiled Achilles' attempt to mutilate Hector's dead body.
Finally, Apollo caused Achilles' death by guiding an arrow shot by Paris into Achilles' heel.
In some versions, Apollo himself killed Achilles by taking the disguise of Paris.
Apollo helped many Trojan warriors, including Agenor, Polydamas, Glaucus in the battlefield.
Though he greatly favored the Trojans, Apollo was bound to follow the orders of Zeus and served his father loyally during the war.
When Zeus struck down Apollo's son Asclepius with a lightning bolt for resurrecting the dead, Apollo in revenge killed the Cyclopes, who had fashioned the bolt for Zeus.
Apollo would have been banished to Tartarus forever for this, but his mother Leto intervened, and reminding Zeus of their old love, pleaded him not to kill their son.
Zeus obliged and sentenced Apollo to one year of hard labor.
During this time he served as herdsman for King Admetus of Pherae in Thessaly.
His mere presence is said to have made the cows give birth to twins.
Apollo shared a romantic relationship with Admetus during his stay.
Admetus treated Apollo well, and, in return, the god conferred great benefits on Admetus.
Out of love and gratitude, Apollo helped Admetus win Alcestis, the daughter of King Pelias and later convinced, or tricked the Fates to let Admetus live past his time.
The love between Apollo and Admetus was a favorite topic of Roman poets like Ovid and Servius.
The fate of Niobe was prophesied by Apollo while he was still in Leto's womb.
Niobe was the queen of Thebes and wife of Amphion.
She displayed hubris when she boasted of her superiority to Leto because she had fourteen children (Niobids), seven male and seven female, while Leto had only two.
She further mocked Apollo's effeminate appearance and Artemis' manly appearance.
Leto, insulted by this, told her children to punish Niobe.
Accordingly, Apollo killed Niobe's sons, and Artemis her daughters.
Apollo and Artemis used poisoned arrows to kill them, though according to some versions of the myth, among the Niobids, Chloris and her brother Amyclas were not killed because they prayed to Leto.
Amphion, at the sight of his dead sons, either killed himself or was killed by Apollo after swearing revenge.
A devastated Niobe fled to Mount Sipylos in Asia Minor and turned into stone as she wept.
Her tears formed the river Achelous.
Zeus had turned all the people of Thebes to stone and so no one buried the Niobids until the ninth day after their death, when the gods themselves entombed them.
As a child, Apollo built an altar made of goat horns which was considered as one of the wonders of the world.
In the first Olympic games, Apollo defeated Ares and became the victor in wrestling.
He outran Hermes in the race and won first place.
Apollo killed Aloadae, the twin giants, when they attempted to storm Mt.
Olympus.
During the gigantomachy, Apollo killed the giant Ephialtes and with Zeus, he killed Porphyrion, the king of giants.
When Odysseus, with the help of Athena, attacked the Bryges (backed by Ares), he lost.
This caused Athena and Ares to enter into a direct duel.
Their fight continued until Apollo intervened between the war siblings and resolved the conflict.
When Heracles tried to steal the Delphic tripod to start his own Oracle, he was stopped by Apollo.
A duel ensued between Apollo and Heracles where Athena supported the latter.
Soon, Zeus intervened to stop the fight and punished Heracles for his act.
When Phorbas, a robber, had seized the roads to Delphi and was harassing the pilgrims, Apollo defeated and killed him in a boxing match.
Apollo rescued Hemithea and Parthenos, sisters of Rhoeo from their drunk father and turned them into goddesses.
Apollo rescued several dryads from drowning during the war against Indians waged by Dionysus.
When the Argonauts were facing a terrible storm, Jason prayed to his patron, Apollo, to help them.
Apollo used his his bow and golden arrow to shed light upon the island Anafi, where the Argonauts soon took shelter.
Apollo helped the Greek hero, Diomedes, to escape from a great tempest.
As a token of gratitude, Diomedes built a temple in honor of "Apollo Epibaterius", "Apollo the embarker".
Periphas, a noble king, was honoured to the same extent as Zeus by mortals.
Due to this Zeus wished to destroy him.
But Apollo requested his father not to do so, since Periphas was a virtuous man.
Zeus agreed and metamorphosed Periphas into an eagle and made the eagle his companion.
Apollo spoke to Zeus regarding Prometheus, the titan who was punished by Zeus for stealing fire.
Apollo, with tears in his eyes, pleaded Zeus to release the kind Titan.
Zeus, moved by Apollo's words and the tears of Artemis and Leto, sent Heracles to free Prometheus.
Chiron, the abandoned centaur was fostered by Apollo who instructed him in medicine, prophecy, archery and more.
Chiron's calm nature and wisdom, in contrast to rest of the centaurs, is attributed to the education Apollo gave him.
Apollo adopted Carnus, the abandoned son of Zeus and Europa.
He fostered him with the help of his mother Leto and later educated the child.
Apollo personally readied his son Aristaeus for the Indian war by providing him with bow, arrows and a shield.
Later on, he also rescued Aristaeus from drowning in a river during the war.
Anius, Apollo's son by Rhoeo, was abandoned by his mother.
Apollo brought him up and educated him.
Anius later became the priest of Apollo and the king of Delos.
In a similar fashion, he had educated his sons Idmon and Iamus (by taking him to Olympia).
Upon the death of his son Idmon, Apollo commanded the Megarians and Boetians to build a town around the tomb of the hero, and to honor him for his bravery and sacrifice.
Apollo saved a shepherd (name unknown) from death in a large deep cave, by the means of vultures.
To thank him, the shepherd built Apollo a temple under the name Vulturius.
Apollo guided Aphrodite to his sanctuary when she was grief-stricken with Adonis' death.
He helped her free herself from the heartbreak.
Apollo divides months into summer and winter.
He rides on the back of a swan to the land of the Hyperboreans during the winter months, and the absence of warmth in winters is due to his departure.
During his absence, Delphi was under the care of Dionysus, and no prophecies were given during winters.
Apollo's music is soulful and enchanting.
His music would deliver people from their pain, and hence, like Dionysus, he is also called the liberator.
Apollo is often seen as the companion of the Muses and as "Musagetes", he leads them into dance while he sang.
He is found delighting the immortal gods with his songs and music on the lyre.
Apollo and the Muses are often seen on Parnassus, which is one of their favorite spots.
Apollo was always invited to play music on weddings of the gods, like the marriage of Eros and Psyche, Peleus and Thetis.
The invention of lyre is attributed either to Hermes or to Apollo himself.
Distinctions have been made that Hermes invented lyre made of tortoise shell, where as the lyre Apollo invented was a regular lyre.
Myths tell that the infant Hermes stole a number of Apollo's cows and took them to a cave in the woods near Pylos, covering their tracks.
In the cave, he found a tortoise and killed it, then removed the insides.
He used one of the cow's intestines and the tortoise shell and made his lyre.
Upon discovering the theft, Apollo confronted Hermes and asked him to return his cattle.
When Hermes acted innocent, Apollo took the matter to Zeus.
Zeus, having seen the events, sided with Apollo, and ordered Hermes to return the cattle.
Hermes then began to play music on the lyre he had invented.
Apollo, a god of music, fell in love with the instrument and offered to allow exchange of the cattle for the lyre.
Hence, Apollo then became a master of the lyre.
According to other versions, Apollo had invented the lyre himself, whose strings he tore in repent to the excess punishment he had given to Marsyas.
Hermes' lyre, therefore, is rather a reinvention.
Apollo participated in musical contests when challenged by others.
He was the victor in all the contests, but usually punished his opponents severely for their hubris.
Once Pan had the audacity to compare his music with that of Apollo and to challenge Apollo, the god of music.
The mountain-god Tmolus was chosen to umpire.
Pan blew on his pipes, and with his rustic melody gave great satisfaction to himself and his faithful follower, Midas, who happened to be present.
Then Apollo struck the strings of his lyre.
It was so beautiful that Tmolus at once awarded the victory to Apollo, and everyone were pleased with the judgement.
Only Midas dissented and questioned the justice of the award.
Apollo would not suffer such a depraved pair of ears any longer, and caused them to become the ears of a donkey.
Marsyas was a satyr who was punished by Apollo for his hubris

He had found an aulos on the ground, tossed away after being invented by Athena because it made her cheeks puffy.
Athena had also placed a curse upon the instrument, that whoever would pick it up would be severely punished.
When Marsyas played the flute, everyone became frenzied with joy.
This led Marsyas to think that he was better than Apollo, and he challenged the god to a musical contest.
The contest was judged by the Muses.
The contestants agreed to the rule that the victor can do anything with the loser.
After they each performed, both were deemed equal until Apollo decreed they play and sing at the same time.
Marsyas argued against this, saying that Apollo would have an advantage.
But Apollo presented the counterpoint that since Marsyas played the flute, which needed air blown from the throat, it was same as singing.
The judges agreed with Apollo.
Apollo played his lyre and sang at the same time, mesmerising the audience.
Marsyas could not do this, as he only knew how to use the flute and not singing.
Apollo was declared the winner because of this.
According to some, Marsyas played his flute out of tune at one point and accepted his defeat.
Out of shame, he assigned to himself the punishment of being skinned for a wine sack.
Another variation is that Apollo played his instrument (the lyre) upside down.
Marsyas could not do this with his instrument (the flute), and so Apollo hung him from a tree to flay him alive.
Apollo flayed Marsyas alive in a cave near Celaenae in Phrygia for his hubris to challenge a god.
He then nailed Marsyas' shaggy skin to a nearby pine-tree.
Marsyas' blood turned into the river Marsyas.
But, as an act of repent and purification for killing Marsyas, he tore the strings of his lyre.
Staying away from music for a long time, he isolated himself and wandered with Cybele till he reached Hyperborea, his mother's native.
Cinyras was a ruler of Cyprus, who was a friend of Agamemnon.
Cinyras promised to assist Agamemnon in the Trojan war, but did not keep his promise.
Agamemnon cursed Cinyras.
He invoked Apollo and asked the god to avenge the broken promise.
Apollo then had a lyre-playing contest with Cinyras, and defeated him.
Either Cinyras committed suicide when he lost, or was killed by Apollo.
Love affairs ascribed to Apollo are a late development in Greek mythology.
Their vivid anecdotal qualities have made some of them favorites of painters since the Renaissance, the result being that they stand out more prominently in the modern imagination.
Daphne was a nymph whose parentage varies.
She scorned Apollo's adavnces and ran away from him.
When Apollo chased her in order to persuade her, she changed herself into a laurel tree.
According to other versions, she cried for help during the chase, and Gaea helped her by taking her in and placing a laurel tree in her place.
According to Roman poet Ovid, the chase was brought about by Cupid, who hit Apollo with golden arrow of love and Daphne with leaden arrow of hatred.
The myth explains the origin of the laurel and connection of Apollo with the laurel and it's leaves, which his priestess employed at Delphi.
The leaves became the symbol of victory and laurel wreaths were given to the victors of the Pythian games.
Apollo is said to have been the lover of all nine Muses, and not being able to choose one of them, decided to remain unwed.
He fathered the Corybantes by the Muse Thalia, Orpheus by Calliope, Linus of Thrace by Calliope or Urania and Hymenaios(Hymen) by either Terpsichore or Clio or Calliope.
Cyrene, was a Thessalian princess whom Apollo loved.
In her honor, he built the city Cyrene and made her its ruler.
She was later granted longevity by Apollo who turned her into a nymph.
The couple had two sons, Aristaeus, and Idmon.
Evadne was a nymph daughter of Poseidon and a lover of Apollo.
She bore him a son, Iamos.
During the time of the childbirth, Apollo sent Eileithyia, the goddess of childbirth to assist her.
Rhoeo, a princess of the island of Naxos was loved by Apollo.
Out of affection for her, Apollo turned her sisters into goddesses.
On the island Delos she bore Apollo a son named Anius.
Not wanting to have the child, she entrusted the infant to Apollo and left.
Apollo raised and educated the child on his own.
Ourea, a daughter of Poseidon, fell in love with Apollo when he and Poseidon were serving the Trojan king Laomedon.
They both united on the day the walls of Troy were built.
She bore to Apollo a son, whom Apollo named Ileus, after the city of his birth, Ilion (Troy).
Ileus was very dear to Apollo.
Thero, daughter of Phylas, a maiden as beautiful as the moonbeams, was loved by the radiant Apollo, and she loved him in return.
By their union, she became mother of Chaeron, who was famed as "the tamer of horses".
He later built the city Chaeronea.
Hyrie or Thyrie was the mother of Cycnus.
Apollo turned both the mother and son into swans when they jumped into a lake and tried to kill themselves.
Hecuba was the wife of King Priam of Troy, and Apollo had a son with her named Troilus.
An oracle prophesied that Troy would not be defeated as long as Troilus reached the age of twenty alive.
He was ambushed and killed by Achilleus, and Apollo avenged his death by killing Achilles.
Coronis, was daughter of Phlegyas, King of the Lapiths.
While pregnant with Asclepius, Coronis fell in love with Ischys, son of Elatus and slept with him.
When Apollo found out about her infidelity through his prophetic powers, he sent his sister, Artemis, to kill Coronis.
Apollo rescued the baby by cutting open Koronis' belly and gave it to the centaur Chiron to raise.
In Euripides' play "Ion", Apollo fathered Ion by Creusa, wife of Xuthus.
He used his powers to conceal her pregnancy from her father.
Later, when Creusa left Ion to die in the wild, Apollo asked Hermes to save the child and bring him to the oracle at Delphi, where he was raised by a priestess.
Hyacinth or Hyacinthus was Apollo's favorite lover.
He was a Spartan prince, beautiful and athletic.
The pair was practicing throwing the discus when the discus thrown by Apollo was blown off course by the jealous Zephyrus and struck Hyacinthus in the head, killing him instantly.
Apollo is said to be filled with grief: out of Hyacinthus' blood, Apollo created a flower named after him as a memorial to his death, and his tears stained the flower petals with the interjection , meaning "alas".
He was later resurrected and taken to heaven.
The festival Hyacinthia was a national celebration of Sparta, which commemorated the death and rebirth of Hyacinthus.
Another male lover was Cyparissus, a descendant of Heracles.
Apollo gave him a tame deer as a companion but Cyparissus accidentally killed it with a javelin as it lay asleep in the undergrowth.
Cyparissus was so saddened by it's death that he asked Apollo to let his tears fall forever.
Apollo granted the request by turning him into the Cypress named after him, which was said to be a sad tree because the sap forms droplets like tears on the trunk.
Admetus, the king of Pherae, was Apollo's another dear lover.
During his exile, which lasted either for one year or nine years, Apollo served Admetus as a herdsman.
Developing a passion for the king there, he herded and fed the cattle, and caused the cows to give birth to twin calves.
He would make cheese and serve it to Admetus and was often seen being domestic, causing embarrassment to his family.
When Admetus wanted to marry princess Alcestis, Apollo provided a chariot pulled by a lion and a boar he had tamed.
This satisfied Alcestis' father and he let Admetus marry his daughter.
Further, Apollo saved the king from Artemis' wrath and also convinced the Moirai to postpone Admetus' death once.
Branchus, a shepherd, one day came across Apollo in the woods.
Captivated by the god's beauty, he kissed Apollo.
Apollo requited his affections and wanting to reward him, bestowed prophetic skills on him.
His descendants, the Branchides, were an influential clan of prophets.
Other male lovers of Apollo include:


Apollo sired many children, from mortal women, nymphs as well as the goddesses.
His children grew up to be physicians, musicians, poets, seers or archers.
Many of his sons founded new cities and became kings.
They were all usually very beautiful.
Asclepius is the most famous son of Apollo.
Apollo brought the child into the world by performing cesarean.
His skills as a physician surpassed that of Apollo's.
Zeus killed him for bringing back the dead, but later upon Apollo's request, he was resurrected as a god.
Aristaeus, the son of Apollo and Cyrene, was placed under the care of Chiron after his birth.
He became the god of beekeeping, cheese making, animal husbandry and more.
He was ultimately given immortality for the benefits he bestowed upon the humanity.
The Corybantes were spear-clashing, dancing demigods.
They were seven sons of Apollo and the nymph Rhetia or the Muse Thalia.
Apollo's children who became musicians and bards include Orpheus, Linus, Ialemus, Hymenaeus, Philammon, and Eleuther.
Apis, Idmon, Iamus, Tenerus, Mopsus and others were gifted seers.
Anius, Pythaeus and Ismenus lived as high priests.
Most of them were trained by Apollo himself.
Delphos, Dryops, Miletos, Tenes, Epidaurus, Ceos, Lycoras, Syrus, Pisus, Marathus, Acraepheus, Cicon, Chaeron and many other sons of Apollo, under the guidance of his words, founded eponymous cities.
Apollo fathered 3 daughters, Nete, Mese, Hypate, who formed a minor group of Muses, the "Musa Apollonides".
They were worshipped at Apollo's shrine in Delphi and are named after the highest, middle and lowest strings of his lyre.
His other daughters are Phemonoe (the poetess and seer), Eriopis (known for her lovely hair), Pamphile (silk weaver), Phoebe and Hilyra (the wives of Dioscuri), Parthenos (turned into a constellation upon her death) and by some accounts, Scylla.
Additionally, Apollo fostered and educated Chiron, the centaur who later became the greatest teacher and educated many demigods, including Apollo's sons.
Apollo also fostered Carnus, the son of Zeus and Europa.
Marpessa was kidnapped by Idas but was loved by Apollo as well.
Zeus made her choose between them, and she chose Idas on the grounds that Apollo, being immortal, would tire of her when she grew old.
Sinope, a nymph, was approached by the amorous Apollo.
She made him promise that he would grant to her whatever she would ask for, and then cleverly asked him to let her stay a virgin.
Apollo kept his promise and went back.
Bolina was admired by Apollo but she refused him and jumped into the sea.
To avoid her death, Apollo turned her into a nymph and let her go.
Castalia was a nymph whom Apollo loved.
She fled from him and dove into the spring at Delphi, at the base of Mt.
Parnassos, which was then named after her.
Water from this spring was sacred; it was used to clean the Delphian temples and inspire the priestesses.
Cassandra, was a daughter of Hecuba and Priam.
Apollo wished to court her.
Cassandra promised to return his love on one condition - he should give her the power to see the future.
Apollo fulfilled her wish, but she went back on her word and rejected him soon after.
Angered that she broke her promise, Apollo cursed her that even though she would see the future, no one would ever believe her prophecies.
Hestia, the goddess of hearth, rejected Apollo and Poseidon's marriage proposal and swore that she would always stay unmarried.
Artemis as the sister of Apollo, is "thea apollousa", that is, she as a female divinity represented the same idea that Apollo did as a male divinity.
In the pre-hellenic period, their relationship was described as the one between husband and wife, and there seems to have been a tradition which actually described Artemis as the wife of Apollo.
However, this relationship was never sexual but spiritual, which is why they both are seen being unmarried in the Hellenic period.
Artemis, like her brother, is armed with a bow and arrows.
She is the cause of sudden deaths of women.
She also is the protector of the young, especially girls.
Though she has nothing to do with oracles, music or poetry, she sometimes led the female chorus on Olympus while Apollo sang.
The laurel was sacred to both.
"Artemis Daphnaia" had her temple among the Lacedemonians, at a place called Hypsoi.
"Apollo Daphnephoros" had a temple in Eretria, a "place where the citizens are to take the oaths.
When Apollo was regarded as identical with the sun or Helios, Artemis was naturally regarded as Selene or the moon.
In later Greek times when Apollo was seen as the god of sun, she became the goddess of moon.
She also acts sometimes in conjunction with her brother.
She killed Coronis when the latter insulted her brother's affections.
In the Trojan war, like Apollo, Artemis sided with the Trojans.
They both were fiercely protective of their mother.
In Aeschylus' "Oresteia" trilogy, Clytemnestra kills her husband, King Agamemnon because he had sacrificed their daughter Iphigenia to proceed forward with the Trojan war.
Apollo gives an order through the Oracle at Delphi that Agamemnon's son, Orestes, is to kill Clytemnestra and Aegisthus, her lover.
Orestes and Pylades carry out the revenge, and consequently Orestes is pursued by the Erinyes or Furies (female personifications of vengeance).
Apollo and the Furies argue about whether the matricide was justified; Apollo holds that the bond of marriage is sacred and Orestes was avenging his father, whereas the Erinyes say that the bond of blood between mother and son is more meaningful than the bond of marriage.
They invade his temple, and he drives them away.
He says that the matter should be brought before Athena.
Apollo promises to protect Orestes, as Orestes has become Apollo's supplicant.
Apollo advocates Orestes at the trial, and ultimately Athena rules in favor of Apollo.
The Roman worship of Apollo was adopted from the Greeks.
As a quintessentially Greek god, Apollo had no direct Roman equivalent, although later Roman poets often referred to him as Phoebus.
There was a tradition that the Delphic oracle was consulted as early as the period of the kings of Rome during the reign of Tarquinius Superbus.
On the occasion of a pestilence in the 430s BCE, Apollo's first temple at Rome was established in the Flaminian fields, replacing an older cult site there known as the "Apollinare".
During the Second Punic War in 212 BCE, the "Ludi Apollinares" ("Apollonian Games") were instituted in his honor, on the instructions of a prophecy attributed to one Marcius.
In the time of Augustus, who considered himself under the special protection of Apollo and was even said to be his son, his worship developed and he became one of the chief gods of Rome.
After the battle of Actium, which was fought near a sanctuary of Apollo, Augustus enlarged Apollo's temple, dedicated a portion of the spoils to him, and instituted quinquennial games in his honour.
He also erected a new temple to the god on the Palatine hill.
Sacrifices and prayers on the Palatine to Apollo and Diana formed the culmination of the Secular Games, held in 17 BCE to celebrate the dawn of a new era.
The chief Apollonian festival was the Pythian Games held every four years at Delphi and was one of the four great Panhellenic Games.
Also of major importance was the Delia held every four years on Delos.
Athenian annual festivals included the Boedromia, Metageitnia, Pyanepsia, and Thargelia.
Spartan annual festivals were the Carneia and the Hyacinthia.
Thebes every nine years held the Daphnephoria.
Apollo's most common attributes were the bow and arrow.
Other attributes of his included the kithara (an advanced version of the common lyre), the plectrum and the sword.
Another common emblem was the sacrificial tripod, representing his prophetic powers.
The Pythian Games were held in Apollo's honor every four years at Delphi.
The bay laurel plant was used in expiatory sacrifices and in making the crown of victory at these games.
The palm tree was also sacred to Apollo because he had been born under one in Delos.
Animals sacred to Apollo included wolves, dolphins, roe deer, swans, cicadas (symbolizing music and song), hawks, ravens, crows, snakes (referencing Apollo's function as the god of prophecy), mice and griffins, mythical eagle–lion hybrids of Eastern origin.
As god of colonization, Apollo gave oracular guidance on colonies, especially during the height of colonization, 750–550 BCE.
According to Greek tradition, he helped Cretan or Arcadian colonists found the city of Troy.
However, this story may reflect a cultural influence which had the reverse direction: Hittite cuneiform texts mention a Minor Asian god called "Appaliunas" or "Apalunas" in connection with the city of Wilusa attested in Hittite inscriptions, which is now generally regarded as being identical with the Greek Ilion by most scholars.
In this interpretation, Apollo's title of "Lykegenes" can simply be read as "born in Lycia", which effectively severs the god's supposed link with wolves (possibly a folk etymology).
In literary contexts, Apollo represents harmony, order, and reason—characteristics contrasted with those of Dionysus, god of wine, who represents ecstasy and disorder.
The contrast between the roles of these gods is reflected in the adjectives Apollonian and Dionysian.
However, the Greeks thought of the two qualities as complementary: the two gods are brothers, and when Apollo at winter left for Hyperborea, he would leave the Delphic oracle to Dionysus.
This contrast appears to be shown on the two sides of the Borghese Vase.
Apollo is often associated with the Golden Mean.
This is the Greek ideal of moderation and a virtue that opposes gluttony.
Apollo is a common theme in Greek and Roman art and also in the art of the Renaissance.
The earliest Greek word for a statue is "delight" (, "agalma"), and the sculptors tried to create forms which would inspire such guiding vision.
Greek art puts into Apollo the highest degree of power and beauty that can be imagined.
The sculptors derived this from observations on human beings, but they also embodied in concrete form, issues beyond the reach of ordinary thought.
The naked bodies of the statues are associated with the cult of the body that was essentially a religious activity.
The muscular frames and limbs combined with slim waists indicate the Greek desire for health, and the physical capacity which was necessary in the hard Greek environment.
The statues of Apollo embody beauty, balance and inspire awe before the beauty of the world.
The evolution of the Greek sculpture can be observed in his depictions from the almost static formal Kouros type in early archaic period, to the representation of motion in a relative harmonious whole in late archaic period.
In classical Greece the emphasis is not given to the illusive imaginative reality represented by the ideal forms, but to the analogies and the interaction of the members in the whole, a method created by Polykleitos.
Finally Praxiteles seems to be released from any art and religious conformities, and his masterpieces are a mixture of naturalism with stylization.
The evolution of the Greek art seems to go parallel with the Greek philosophical conceptions, which changed from the natural-philosophy of Thales to the metaphysical theory of Pythagoras.
Thales searched for a simple material-form directly perceptible by the senses, behind the appearances of things, and his theory is also related to the older animism.
This was paralleled in sculpture by the absolute representation of vigorous life, through unnaturally simplified forms.
Pythagoras believed that behind the appearance of things, there was the permanent principle of mathematics, and that the forms were based on a transcendental mathematical relation.
The forms on earth, are imperfect imitations (, "eikones", "images") of the celestial world of numbers.
His ideas had a great influence on post-Archaic art.
The Greek architects and sculptors were always trying to find the mathematical relation, that would lead to the esthetic perfection.
(canon).
In classical Greece, Anaxagoras asserted that a divine reason (mind) gave order to the seeds of the universe, and Plato extended the Greek belief of "ideal forms" to his metaphysical theory of "forms" ("ideai", "ideas").
The forms on earth are imperfect duplicates of the intellectual celestial ideas.
The Greek words "oida" (, "(I) know") and "eidos" (, "species"), a thing seen, have the same root as the word "idea" (), a thing ἰδείν to see.
indicating how the Greek mind moved from the gift of the senses, to the principles beyond the senses.
The artists in Plato's time moved away from his theories and art tends to be a mixture of naturalism with stylization.
The Greek sculptors considered the senses more important, and the proportions were used to unite the sensible with the intellectual.
Kouros ("male youth") is the modern term given to those representations of standing male youths which first appear in the archaic period in Greece.
This type served certain religious needs and was first proposed for what was previously thought to be depictions of "Apollo".
The first statues are certainly still and formal.
The formality of their stance seems to be related with the Egyptian precedent, but it was accepted for a good reason.
The sculptors had a clear idea of what a young man is, and embodied the archaic smile of good manners, the firm and springy step, the balance of the body, dignity, and youthful happiness.
When they tried to depict the most abiding qualities of men, it was because men had common roots with the unchanging gods.
The adoption of a standard recognizable type for a long time, is probably because nature gives preference in survival of a type which has long be adopted by the climatic conditions, and also due to the general Greek belief that nature expresses itself in "ideal forms" that can be imagined and represented.
These forms expressed immortality.
Apollo was the immortal god of "ideal balance and order".
His shrine in Delphi, that he shared in winter with Dionysius had the inscriptions: (gnōthi seautón="know thyself") and ("mēdén ágan", "nothing in excess"), and (eggýa pára d'atē, "make a pledge and mischief is nigh").
In the first large-scale depictions during the early archaic period (640–580 BC), the artists tried to draw one's attention to look into the interior of the face and the body which were not represented as lifeless masses, but as being full of life.
The Greeks maintained, until late in their civilization, an almost animistic idea that the statues are in some sense alive.
This embodies the belief that the image was somehow the god or man himself.
A fine example is the statue of the "Sacred Gate Kouros" which was found at the cemetery of Dipylon in Athens (Dipylon Kouros).
The statue is the "thing in itself", and his slender face with the deep eyes express an intellectual eternity.
According to the Greek tradition the Dipylon master was named Daedalus, and in his statues the limbs were freed from the body, giving the impression that the statues could move.
It is considered that he created also the "New York kouros", which is the oldest fully preserved statue of "Kouros" type, and seems to be the incarnation of the god himself.
The animistic idea as the representation of the imaginative reality, is sanctified in the Homeric poems and in Greek myths, in stories of the god Hephaestus (Phaistos) and the mythic Daedalus (the builder of the labyrinth) that made images which moved of their own accord.
This kind of art goes back to the Minoan period, when its main theme was the representation of motion in a specific moment.
These free-standing statues were usually marble, but also the form rendered in limestone, bronze, ivory and terracotta.
The earliest examples of life-sized statues of Apollo, may be two figures from the Ionic sanctuary on the island of Delos.
Such statues were found across the Greek speaking world, the preponderance of these were found at the sanctuaries of Apollo with more than one hundred from the sanctuary of "Apollo Ptoios", Boeotia alone.
The last stage in the development of the "Kouros type" is the late archaic period (520–485 BC), in which the Greek sculpture attained a full knowledge of human anatomy and used to create a relative harmonious whole.
Ranking from the very few bronzes survived to us is the masterpiece bronze Piraeus Apollo.
It was found in Piraeus, the harbour of Athens.
The statue originally held the bow in its left hand, and a cup of pouring libation in its right hand.
It probably comes from north-eastern Peloponnesus.
The emphasis is given in anatomy, and it is one of the first attempts to represent a kind of motion, and beauty relative to proportions, which appear mostly in post-Archaic art.
The statue throws some light on an artistic centre which, with an independently developed harder, simpler and heavier style, restricts Ionian influence in Athens.
Finally, this is the germ from which the art of Polykleitos was to grow two or three generations later.
At the beginning of the Classical period, it was considered that beauty in visible things as in everything else, consisted of symmetry and proportions.
The artists tried also to represent motion in a specific moment (Myron), which may be considered as the reappearance of the dormant Minoan element.
Anatomy and geometry are fused in one, and each does something to the other.
The Greek sculptors tried to clarify it by looking for mathematical proportions, just as they sought some reality behind appearances.
Polykleitos in his "Canon" wrote that beauty consists in the proportion not of the elements (materials), but of the parts, that is the interrelation of parts with one another and with the whole.
It seems that he was influenced by the theories of Pythagoras.
The famous "Apollo of Mantua" and its variants are early forms of the Apollo Citharoedus statue type, in which the god holds the cithara in his left arm.
The type is represented by neo-Attic Imperial Roman copies of the late 1st or early 2nd century, modelled upon a supposed Greek bronze original made in the second quarter of the 5th century BCE, in a style similar to works of Polykleitos but more archaic.
The Apollo held the "cythara" against his extended left arm, of which in the Louvre example, a fragment of one twisting scrolling horn upright remains against his biceps.
Though the proportions were always important in Greek art, the appeal of the Greek sculptures eludes any explanation by proportion alone.
The statues of Apollo were thought to incarnate his living presence, and these representations of illusive imaginative reality had deep roots in the Minoan period, and in the beliefs of the first Greek speaking people who entered the region during the bronze-age.
Just as the Greeks saw the mountains, forests, sea and rivers as inhabited by concrete beings, so nature in all of its manifestations possesses clear form, and the form of a work of art.
Spiritual life is incorporated in matter, when it is given artistic form.
Just as in the arts the Greeks sought some reality behind appearances, so in mathematics they sought permanent principles which could be applied wherever the conditions were the same.
Artists and sculptors tried to find this ideal order in relation with mathematics, but they believed that this ideal order revealed itself not so much to the dispassionate intellect, as to the whole sentient self.
Things as we see them, and as they really are, are one, that each stresses the nature of the other in a single unity.
In the archaic pediments and friezes of the temples, the artists had a problem to fit a group of figures into an isosceles triangle with acute angles at the base.
The Siphnian Treasury in Delphi was one of the first Greek buildings utilizing the solution to put the dominating form in the middle, and to complete the descending scale of height with other figures sitting or kneeling.
The pediment shows the story of Heracles stealing Apollo's tripod that was strongly associated with his oracular inspiration.
Their two figures hold the centre.
In the pediment of the temple of Zeus in Olympia, the single figure of Apollo is dominating the scene.
These representations rely on presenting scenes directly to the eye for their own visible sake.
They care for the schematic arrangements of bodies in space, but only as parts in a larger whole.
While each scene has its own character and completeness it must fit into the general sequence to which it belongs.
In these archaic pediments the sculptors use empty intervals, to suggest a passage to and from a busy battlefield.
The artists seem to have been dominated by geometrical pattern and order, and this was improved when classical art brought a greater freedom and economy.
Apollo as a handsome beardless young man, is often depicted with a kithara (as Apollo Citharoedus) or bow in his hand, or reclining on a tree (the Apollo Lykeios and Apollo Sauroctonos types).
The Apollo Belvedere is a marble sculpture that was rediscovered in the late 15th century; for centuries it epitomized the ideals of Classical Antiquity for Europeans, from the Renaissance through the 19th century.
The marble is a Hellenistic or Roman copy of a bronze original by the Greek sculptor Leochares, made between 350 and 325 BCE.
The life-size so-called "Adonis" found in 1780 on the site of a "villa suburbana" near the Via Labicana in the Roman suburb of Centocelle is identified as an Apollo by modern scholars.
In the late 2nd century CE floor mosaic from El Djem, Roman "Thysdrus", he is identifiable as Apollo Helios by his effulgent halo, though now even a god's divine nakedness is concealed by his cloak, a mark of increasing conventions of modesty in the later Empire.
Another haloed Apollo in mosaic, from Hadrumentum, is in the museum at Sousse.
The conventions of this representation, head tilted, lips slightly parted, large-eyed, curling hair cut in locks grazing the neck, were developed in the 3rd century BCE to depict Alexander the Great.
Some time after this mosaic was executed, the earliest depictions of Christ would also be beardless and haloed.
Apollo has often featured in postclassical art and literature.
Percy Bysshe Shelley composed a "Hymn of Apollo" (1820), and the god's instruction of the Muses formed the subject of Igor Stravinsky's "Apollon musagète" (1927–1928).
In 1978, the Canadian band Rush released an album with songs "Apollo: Bringer of Wisdom"/"Dionysus: Bringer of Love".
In discussion of the arts, a distinction is sometimes made between the Apollonian and Dionysian impulses where the former is concerned with imposing intellectual order and the latter with chaotic creativity.
Friedrich Nietzsche argued that a fusion of the two was most desirable.
Carl Jung's Apollo archetype represents what he saw as the disposition in people to over-intellectualise and maintain emotional distance.
Charles Handy, in "Gods of Management" (1978) uses Greek gods as a metaphor to portray various types of organisational culture.
Apollo represents a 'role' culture where order, reason, and bureaucracy prevail.
In spaceflight, the NASA program for landing astronauts on the Moon was named Apollo.
</doc>
<doc id="595" url="https://en.wikipedia.org/wiki?curid=595" title="Andre Agassi">
Andre Agassi

Andre Kirk Agassi ( ; born April 29, 1970) is an American retired professional tennis player and former world No.
1 who was one of the sport's most dominant players from the early 1990s to the mid-2000s.
Generally considered by critics and fellow players to be one of the greatest tennis players of all time, Agassi has been called the greatest service returner ever to play the game and was described by the BBC upon his retirement as "perhaps the biggest worldwide star in the sport's history".
As a result, he is credited for helping to revive the popularity of tennis during the 1990s.
In singles tennis, Agassi is an eight-time Grand Slam champion and a 1996 Olympic gold medalist, as well as being a runner-up in seven other Grand Slam tournaments.
During the Open Era, Agassi was the first male player to win four Australian Open titles, a record that was later surpassed by Novak Djokovic when he won his fifth title in 2015, and then by Roger Federer in 2017.
Agassi is one of five male singles players to achieve the Career Grand Slam in the Open Era and one of eight in history, the first of two to achieve the Career Golden Slam (Career Grand Slam and Olympic Gold Medal, the other being Rafael Nadal), and the only man to win the Career Golden Slam and the ATP Tour World Championships: a distinction dubbed as a "Career Super Slam" by "Sports Illustrated".
Agassi was the first male player to win all four Grand Slam tournaments on three different surfaces (hard, clay and grass), and the last American male to win both the French Open (in 1999) and the Australian Open (in 2003).
He also won 17 ATP Masters Series titles and was part of the winning Davis Cup teams in 1990, 1992 and 1995.
Agassi reached the world No.
1 ranking for the first time in 1995 but was troubled by personal issues during the mid-to-late 1990s and sank to No.
141 in 1997, prompting many to believe that his career was over.
Agassi returned to No.
1 in 1999 and enjoyed the most successful run of his career over the next four years.
During his 20-plus year tour career, Agassi was known by the nickname "The Punisher".
After suffering from sciatica caused by two bulging discs in his back, a spondylolisthesis (vertebral displacement) and a bone spur that interfered with the nerve, Agassi retired from professional tennis on September 3, 2006, after losing in the third round of the US Open to Benjamin Becker.
He is the founder of the Andre Agassi Charitable Foundation, which has raised over $60 million for at-risk children in Southern Nevada.
In 2001, the Foundation opened the Andre Agassi College Preparatory Academy in Las Vegas, a K-12 public charter school for at-risk children.
He has been married to fellow tennis player Steffi Graf since 2001.
Andre Agassi was born in Las Vegas, Nevada to Emmanuel "Mike" Agassi, a former Olympic boxer from Iran and Elizabeth "Betty" Agassi (née Dudley).
His father claims to have Armenian and Assyrian heritage.
One of his ancestors changed his surname from Aghassian to Agassi to avoid persecution.
Andre Agassi's mother, Betty, is a breast cancer survivor.
He has three older siblings – Rita (last wife to Pancho Gonzales), Philip and Tami.
Andre was given the middle name Kirk after Kirk Kerkorian, an Armenian American billionaire.
Agassi, a waiter at Tropicana Las Vegas, met Kerkorian in 1963.
Agassi at the age of 12 (with his good friend and doubles partner Roddy Parks) won the 1982 National Indoor Boys 14s Doubles Championship in Chicago.
Agassi describes more of his memorable experiences and juvenile pranks with Roddy in his book "Open".
At the age of 13, Agassi was sent to Nick Bollettieri's Tennis Academy in Florida.
He was meant to stay for only three months because that was all his father could afford.
After thirty minutes of watching Agassi play, Bollettieri called Mike and said: "Take your check back.
He's here for free," claiming that Agassi had more natural talent than anyone else he had seen.
Agassi dropped out of school in the ninth grade.
Agassi turned professional at the age of 16 and competed in his first tournament at La Quinta, California.
He won his first match against John Austin, but then lost his second match to Mats Wilander.
By the end of 1986, Agassi was ranked No.
91.
He won his first top-level singles title in 1987 at the Sul American Open in Itaparica and ended the year ranked No.
25.
He won six additional tournaments in 1988 (Memphis, U.S.
Men's Clay Court Championships, Forest Hills WCT, Stuttgart Outdoor, Volvo International and Livingston Open), and, by December of that year, he had surpassed US$1 million in career prize money after playing in just 43 tournaments—the fastest anyone in history had reached that level.
During 1988, he also set the open-era record for most consecutive victories by a male teenager (a record that stood for 17 years until Rafael Nadal broke it in 2005).
His year-end ranking was No.
3, behind second-ranked Ivan Lendl and top-ranked Mats Wilander.
Both the Association of Tennis Professionals and "Tennis" magazine named Agassi the Most Improved Player of the Year for 1988.
In addition to not playing the Australian Open (which later became his best Grand Slam event) for the first eight years of his career, Agassi chose not to play at Wimbledon from 1988 through 1990 and publicly stated that he did not wish to play there because of the event's traditionalism, particularly its "predominantly white" dress code to which players at the event are required to conform.
Strong performances on the tour meant that Agassi was quickly tipped as a future Grand Slam champion.
While still a teenager, he reached the semifinals of both the French Open and the US Open in 1988 and made the US Open semifinals in 1989.
He began the 1990s with a series of near-misses.
He reached his first Grand Slam final in 1990 at the French Open, where he was favored before losing in four sets to Andrés Gómez, which he later attributed in his book to worrying about his wig falling off during the match.
He reached his second Grand Slam final of the year at the US Open, defeating defending champion Boris Becker in the semifinals.
His opponent in the final was Pete Sampras; a year earlier, Agassi had crushed Sampras, after which time he told his coach that he felt bad for Sampras because he was never going to make it as a pro.
Agassi lost the US Open final to Sampras in three sets.
The rivalry between these two American players became the dominant rivalry in tennis over the rest of the decade.
Agassi ended 1990 on high note as he helped the United States win its first Davis Cup in 8 years and won his only Tennis Masters Cup, beating reigning Wimbledon champion Stefan Edberg in the final.
In 1991, Agassi reached his second consecutive French Open final, where he faced fellow Bollettieri Academy alumnus Jim Courier.
Courier emerged the victor in a five-set final.
Agassi decided to play at Wimbledon in 1991, leading to weeks of speculation in the media about the clothes he would wear.
He eventually emerged for the first round in a completely white outfit.
He reached the quarterfinals on that occasion, losing in five sets to David Wheaton.
Agassi's Grand Slam tournament breakthrough came at Wimbledon, not at the French Open or the US Open, where he had previously enjoyed success.
In 1992, he defeated Goran Ivanišević in a five-set final.
Along the way, Agassi overcame two former Wimbledon champions: Boris Becker and John McEnroe.
No other baseliner would triumph at Wimbledon until Lleyton Hewitt ten years later.
Agassi was named the BBC Overseas Sports Personality of the Year in 1992.
Agassi once again played on the United States' Davis Cup winning team in 1992.
It was their second Davis cup title in three years.
In 1993, Agassi won the only doubles title of his career, at the Cincinnati Masters, partnered with Petr Korda.
He missed much of the early part of that year due to injuries.
Although he made the quarterfinals in his Wimbledon title defense, he lost to eventual champion and No.
1 Pete Sampras in five sets.
Agassi lost in the first round at the US Open to Thomas Enqvist and required wrist surgery late in the year.
With new coach Brad Gilbert on board, Agassi began to employ more of a tactical, consistent approach, which fueled his resurgence.
He started slowly in 1994, losing in the first week at the French Open and Wimbledon.
Nevertheless, he emerged during the hard-court season, winning the Canadian Open.
His comeback culminated at the 1994 US Open with a five-set fourth-round victory against compatriot Michael Chang.
He then became the first man to capture the US Open as an unseeded player, beating Michael Stich in the final.
Along the way, he beat 5 seeded players.
In 1995, Agassi shaved his balding head, breaking with his old "image is everything" style.
He competed in the 1995 Australian Open (his first appearance at the event) and won, beating Sampras in a four-set final.
Agassi and Sampras met in five tournament finals in 1995, all on hardcourt, with Agassi winning three.
Agassi won three Masters Series events in 1995 (Cincinnati, Key Biscayne, and the Canadian Open) and seven titles total.
He compiled a career-best 26-match winning streak during the summer hard-court circuit, with the last victory being in an intense late night four-set semifinal of the US Open against Boris Becker.
The streak ended the next day when Agassi lost the final to Sampras.
Agassi reached the world No.
1 ranking for the first time in April 1995.
He held that ranking until November, for a total of 30 weeks.
Agassi skipped most of the fall indoor season which allowed Sampras to surpass him and finish ranked No.
1 at the year-end ranking.
In terms of win/loss record, 1995 was Agassi's best year.
He won 73 and lost 9 matches, and was also once again a key player on the United States' Davis Cup winning team—the third and final Davis Cup title of his career.
1996 was a less successful year for Agassi, as he failed to reach any Grand Slam final.
He suffered two early-round losses at the hands of compatriots Chris Woodruff and Doug Flach at the French Open and Wimbledon, respectively, and lost to Chang in straight sets in the Australian and US Open semifinals.
At the time, Agassi blamed the Australian Open loss on the windy conditions, but later said in his biography that he had lost the match on purpose, as he did not want to play Boris Becker, whom he would have faced in that final.
The high point for Agassi was winning the men's singles gold medal at the Olympic Games in Atlanta, beating Sergi Bruguera of Spain in the final.
Agassi also successfully defended his singles titles in Cincinnati and Key Biscayne.
1997 was the low point of Agassi's career.
His wrist injury resurfaced, and he played only 24 matches during the year.
He later confessed that he started using crystal methamphetamine at that time, allegedly on the urging of a friend.
He failed an ATP drug test, but wrote a letter claiming the same friend had spiked a drink.
The ATP dropped the failed drug test as a warning.
In his autobiography, Agassi admitted that the letter was a lie.
He quit the drug soon after.
At this time Agassi was also in a failing marriage with actress Brooke Shields and had lost interest in the game.
He won no top-level titles, and his ranking sank to No.
141 on November 10, 1997, prompting many to believe that his run as one of the sport's premier competitors was over and he would never again win any significant championships.
In 1998, Agassi began a rigorous conditioning program and worked his way back up the rankings by playing in Challenger Series tournaments, a circuit for pro players ranked outside the world's top 50.
After returning to top physical and mental shape, Agassi recorded the most successful period of his tennis career and also played classic matches in that period against Pete Sampras and Patrick Rafter.
In 1998, Agassi won five titles and leapt from No.
110 to No.
6, the highest jump into the top 10 made by any player during a calendar year.
At Wimbledon, he had an early loss in the second round to Tommy Haas.
He won five titles in ten finals and was runner-up at the Masters Series tournament in Key Biscayne, losing to Marcelo Ríos, who became No.
1 as a result.
At the year end he was awarded the ATP Most Improved Player of the Year for the second time in his career (the first being 10 years earlier in 1988).
Agassi entered the history books in 1999 when he came back from two sets to love down to beat Andrei Medvedev in a five-set French Open final, becoming, at the time, only the fifth male player (joining Rod Laver, Fred Perry, Roy Emerson and Don Budge—these have since been joined by Roger Federer, Rafael Nadal, and Novak Djokovic) to win all four Grand Slam singles titles during his career.
Only Laver, Agassi, Federer, Nadal and Djokovic have achieved this feat during the open era.
This win also made him the first (of only four, the next being Federer, Nadal and Djokovic respectively) male player in history to have won all four Grand Slam titles on three different surfaces (clay, grass and hard courts).
Agassi also became the only male player to win the Career Super Slam, consisting of all four Grand Slam tournaments plus an Olympic gold medal in singles and a Year-End Championship.
Agassi followed his 1999 French Open victory by reaching the Wimbledon final, where he lost to Sampras in straight sets.
He rebounded from his Wimbledon defeat by winning the US Open, beating Todd Martin in five sets (rallying from a two sets to one deficit) in the final.
Overall during the year Agassi won 5 titles including two majors and the ATP Masters Series in Paris, where he beat Marat Safin.
Agassi ended 1999 as the No.
1, ending Sampras's record of six consecutive year-ending top rankings (1993–98).
This was the only time Agassi ended the year at No.
1.
He began the next year by capturing his second Australian Open title, beating Sampras in a five-set semifinal and Yevgeny Kafelnikov in a four-set final.
He was the first male player to have reached four consecutive Grand Slam finals since Rod Laver achieved the Grand Slam in 1969.
At the time, Agassi was also only the fourth player since Laver to be the reigning champion of three of four Grand Slam events, missing only the Wimbledon title.. 2000 also saw Agassi reach the semifinals at Wimbledon, where he lost in five sets to Rafter in a match considered by many to be one of the best ever at Wimbledon.
At the inaugural Tennis Masters Cup in Lisbon, Agassi reached the final after defeating Marat Safin in the semifinals to end the Russian's hopes to become the youngest No.
1 in the history of tennis.
Agassi then lost to Gustavo Kuerten in the final, allowing Kuerten to be crowned year-end No.
1.
Agassi opened 2001 by successfully defending his Australian Open title with a straight-sets final win over Arnaud Clément.
En route, he beat a cramping Rafter in five sets in front of a sell-out crowd in what turned out to be the Aussie's last Australian Open.
At Wimbledon, they met again in the semifinals, where Agassi lost another close match to Rafter, 8–6 in the fifth set.
In the quarterfinals at the US Open, Agassi lost a 3-hour, 33 minute epic match with Sampras, 7–6, 6–7, 6–7, 6–7, with no breaks of serve during the 52-game match.
Despite the setback, Agassi finished 2001 ranked No.
3, becoming the only male tennis player to finish a year ranked in the top 3 in three different decades (1980s, 1990s, 2000s).
He also was the oldest player (age 31) to finish in the top three since 32-year-old Connors finished at No.
2 in 1984.
2002 opened with disappointment for Agassi, as injury forced him to skip the Australian Open, where he was a two-time defending champion.
Agassi recovered from the injury and later that year defended his Key Biscayne title beating then rising Roger Federer in a four-set final.
The last duel between Agassi and Sampras came in the final of the US Open, which Sampras won in four sets and left Sampras with a 20–14 edge in their 34 career meetings.
The match was the last of Sampras's career.
Agassi's US Open finish, along with his Masters Series victories in Key Biscayne, Rome and Madrid, helped him finish 2002 as the oldest year-end No.
2 at 32 years and 8 months.
In 2003, Agassi won the eighth (and final) Grand Slam title of his career at the Australian Open, where he beat Rainer Schüttler in straight sets in the final.
In March, he won his sixth career and third consecutive Key Biscayne title, in the process surpassing his wife, Steffi Graf, who was a five-time winner of the event.
The final was his 18th straight win in that tournament, which broke the previous record of 17 set by Sampras from 1993–95.
(Agassi's winning streak continued to 20 after winning his first two matches at the 2004 edition of that tournament before bowing to Agustín Calleri.)
With the victory, Agassi became the youngest (19 years old) and oldest (32) winner of the Key Biscayne tournament.
On April 28, 2003, he recaptured the No.
1 ranking after a quarterfinal victory over Xavier Malisse at the Queen's Club Championships to become the oldest top-ranked male player since the ATP rankings began at 33 years and 13 days.
The record was later surpassed by Roger Federer in 2018.
He had held the No.
1 ranking for two weeks, when Lleyton Hewitt took it back on May 12, 2003.
Agassi then recaptured the No.
1 ranking once again on June 16, 2003, which he held for 12 weeks until September 7, 2003.
During his career, Agassi held the No.
1 ranking for a total of 101 weeks.
Agassi's ranking slipped when injuries forced him to withdraw from many events.
He did manage to reach the US Open semifinals, where he lost to Juan Carlos Ferrero and surrendered his No.
1 ranking to Ferrero.
At the year-end Tennis Masters Cup, Agassi lost in the final to Federer and finished the year ranked No.
4.
At age 33, he was the oldest player to rank in the top 5 since Connors, at age 35, was No.
4 in 1987.
In 2004, Agassi began the year with a five-set loss in the semifinals of the Australian Open to Marat Safin; the loss ended Agassi's 26-match winning streak at the event, a record that still stands.
He won the Masters series event in Cincinnati to bring his career total to 59 top-level singles titles and a record 17 ATP Masters Series titles, having already won seven of the nine ATP Masters tournament—all except the tournaments in Monte Carlo and Hamburg.
At 34, he became the second-oldest singles champion in Cincinnati tournament history (the tournament began in 1899), surpassed only by Ken Rosewall, who won the title in 1970 at age 35.
He finished the year ranked No.
8, the oldest player to finish in the top 10 since the 36-year-old Connors was No.
7 in 1988.
Agassi also became only the sixth male player during the open era to reach 800 career wins with his first-round victory over Alex Bogomolov in Countrywide Classic in Los Angeles.
Agassi's 2005 began with a quarterfinal loss to Federer at the Australian Open.
Agassi had several other deep runs at tournaments, but had to withdraw from several events due to injury.
He lost to Jarkko Nieminen in the first round of the French Open.
He won his fourth title in Los Angeles and reached the final of the Rogers Cup, before falling to No.
2 Rafael Nadal.
Agassi's 2005 was defined by an improbable run to the US Open final.
After beating Răzvan Sabău and Ivo Karlović in straight sets and Tomáš Berdych in four sets, Agassi won three consecutive five-set matches to advance to the final.
The most notable of these matches was his quarterfinal victory over James Blake, where he rallied from two sets down to win in the fifth set tie-breaker.
His other five-set victims were Xavier Malisse in the fourth round and Robby Ginepri in the semifinals.
In the final, Agassi faced Federer, who was seeking his second consecutive US Open title and his sixth Grand Slam title in two years.
Federer defeated Agassi in four sets.
Agassi finished 2005 ranked No.
7, his 16th time in the year-end top-10 rankings, which tied Connors for the most times ranked in the top 10 at year's end.
Agassi had a poor start to 2006.
He was still recovering from an ankle injury and also suffering from back and leg pain and lack of match play.
Agassi withdrew from the Australian Open because of the ankle injury, and his back injury and other pains forced him to withdraw from several other events, eventually skipping the entire clay-court season including the French Open.
This caused his ranking to drop out of the top 10 for the last time.
Agassi returned for the grass-court season, playing a tune-up, and then Wimbledon.
He was defeated in the third round by world No.
2 (and eventual runner-up) Rafael Nadal.
Against conventions, Agassi, the losing player, was interviewed on court after the match.
At Wimbledon, Agassi announced his plans to retire following the US Open.
Agassi played only two events during the summer hard-court season with his best result being a quarterfinal loss at the Countrywide Classic in Los Angeles to Fernando González of Chile, which resulted in him being unseeded at the US Open.
Agassi had a short, but dramatic, run in his final US Open.
Because of extreme back pain, Agassi was forced to receive anti-inflammatory injections after every match.
After a tough four-set win against Andrei Pavel, Agassi faced eighth-seeded Marcos Baghdatis in the second round who had earlier advanced to the 2006 Australian Open final and Wimbledon semifinals.
Agassi won in five tough sets as the younger Baghdatis succumbed to muscle cramping in the final set.
In his last match, Agassi fell to 112th-ranked big-serving Benjamin Becker of Germany in four sets.
Agassi received a four-minute standing ovation from the crowd after the match and delivered a retirement speech.
The rivalry has been considered the greatest of the generation of players competing in the 90's, as Sampras and Agassi were the most successful players of that decade.
They also had very contrasting playing styles, with Sampras being considered the greatest server while Agassi the greatest serve returner of the game.
Agassi and Sampras met 34 times on the tour level with Agassi trailing 14–20.
The 1990 US Open was their first meeting in a Grand Slam tournament final.
Agassi was favored as he was ranked No.
4 at the time, compared to the No.
12 ranking of Sampras and because Agassi had defeated Sampras in their only previously completed match.
Agassi however lost the final to Sampras in straight sets.
Their next meeting in a Grand Slam was at the 1992 French Open, where they met in the quarterfinals.
Although Sampras was ranked higher, Agassi came out winning in straight sets.
They met again on a Grand Slam level at the quarterfinals of Wimbledon in 1993, where Agassi was the defending champion and Sampras was the newly minted world No.
1.
Agassi dug out from a two-nothing hole, levelling the match at 2 sets apiece, however Sampras prevailed in five sets, and went on to win his first Wimbledon championship.
With both Sampras and Agassi participating, the U.S.
won the Davis Cup in 1995.
The year should be considered the peak of the rivalry as together they won 3 out of 4 major titles, meeting each other twice in the finals, and were occupying top two spots in the rankings for the whole year.
They met 5 times during the year, all in the title matches, including the Australian Open, the Newsweek Champions Cup (now Indian Wells), the Lipton International Players Championships (now Miami Open), the Canadian Open, and the US Open.
Agassi won three of the finals, including the Australian Open, however Sampras took the US Open title, ending Agassi's 26-match winning streak.
After Agassi had taken most of the fall season off, Sampras took over the No.
1 ranking for the end of the season.
In the following 3 years, while Sampras continued winning Grand Slam titles every season, Agassi slumped in the rankings and struggled in major competitions.
The next time Sampras and Agassi met in a Grand Slam final was at Wimbledon in 1999, where Sampras won in straight sets.
For both, it was considered a career rejuvenation, as Sampras had suffered a string of disappointments in the previous year while Agassi was regaining his status as a top-ranked player after winning the French Open.
Sampras forfeited the No.
1 ranking to Agassi when injury forced him to withdraw from that year's US Open, which Agassi went on to win.
They faced each other twice in the season-ending ATP Tour World Championships, with Sampras losing the round-robin match, but winning the final.
In 2000's they met three more times on the Grand Slam level offering three memorable contests.
In 2000, the top-ranked Agassi defeated No.
3 Sampras in the semifinals of the Australian Open in five sets, which was an important win for Agassi who lost 4 of the previous 5 matches against Sampras.
In arguably their most memorable match ever, Sampras defeated Agassi in the 2001 US Open quarterfinals in four sets.
There were no breaks of serve during the entire match.
Reruns of the match are frequently featured on television, especially during US Open rain delays, and the match is considered one of the best in history because of the level of play presented by both players.
Their last meeting was the final of the 2002 US Open, which was their third meeting in a US Open final, but first since 1995.
The match was also notable because they had defeated several up-and-coming players en route to the final.
Sampras had defeated No.
3 Tommy Haas in the fourth round and future No.
1 Andy Roddick in the quarterfinals, while Agassi had defeated No.
1 and defending champion Lleyton Hewitt in the semifinals.
Sampras defeated Agassi in four sets.
This was the final ATP tour singles match of Sampras's career.
Michael Chang was the opponent Agassi faced the most frequently from all the players other than Sampras.
They met 22 times on the tour level with Agassi leading 15-7.
Chang, unlike most of Agassi's big rivals, had a playing style similar to his.
Both players preferred to dominate the game from the baseline with Chang being more defensive-minded.
The outcome was that most of their meetings were built on long and entertaining rallies.
The rivalry began late in the 1980s with both players being considered the prodigies of the next great generation of American tennis players, despite both having a foreign descent.
Agassi won first four matches including a straight set victory in the round 16 of the 1988 US Open and defeating Chang, the defending champion, in the 1990 French Open in a four-set quarterfinal.
Arguably their best match took place in the round 16 of the 1994 US Open.
While both players presented high quality shot-making, the momentum changed from set to set with Agassi eventually prevailing with a five-set victory.
It turned out to be the toughest contest on his way to his first US Open title.
Their next two Grand Slam meetings came in 1996, with Chang recording easy straight set victories in semifinals of both the Australian Open and the US Open.
Years after, Agassi shockingly admitted in his book, that he had lost the first of the matches on purpose as he did not want to face Boris Becker, who was awaiting the winner in the final.
Agassi won the final four of their matches, with the last being in 2003 at the Miami Open with Chang being clearly after his prime.
Boris Becker and Agassi played 14 times with Agassi leading 10–4.
Becker dominated the rivalry early on, winning their first three matches in 1988 and 1989 before Agassi turned the rivalry in 1990, and won 10 of their last 11 matches.
They first played at Indian Wells in 1988, with Becker prevailing.
Their most notable match was the 1989 Davis Cup semifinal match, which Becker won in five sets after losing the first two in tiebreaks.
Agassi, considered a baseliner with a playing style not suiting grass, shocked Becker, a three-time champion, in a five set quarterfinal at Wimbledon in 1992 on his way to his first Grand Slam title.
The intensity of the rivalry peaked in 1995.
Becker won that year's Wimbledon semifinal after being down a set and two breaks, to eventually win in four sets.
In a highly anticipated rematch in the US Open semifinal, this time it was Agassi who came out victorious in four tight sets.
Their final match was played at Hong Kong in 1999, which Agassi won in three sets.
Agassi and Pat Rafter played fifteen times with Agassi leading 10–5.
The rivalry has been considered special and delivered memorable encounters, because of the players' contrasting styles of play, with Rafter using traditional serve & volley methods against Agassi's variety of return of serves and passing shots as main weapons.
Agassi led 8–2 on hard courts, but Rafter surprisingly won their sole match on clay at the 1999 Rome Masters.
They played four matches at Wimbledon with both winning two matches each.
Agassi won the first two in 1993 and 1999, while Rafter took their 2000 and 2001 encounters, both the gruelling 5-setters often being presented on the lists of best matches ever played.
Agassi also won both their meetings at the Australian Open, in 1995 and 2001, on his way to the title in both occasions.
Rafter however took their only US Open encounter in 1997 and went on to win the title.
Agassi and Roger Federer played 11 times, and Federer led their head-to-head series 8–3.
With the retirement of Sampras, the rivalry against 11 years younger Federer, who was another great server like Sampras, became Agassi's main rivalry for the final years of his career.
Agassi won their first three matches, but then went on to lose eight consecutive ones.
They first met in just the third tournament of Federer's career at the 1998 Swiss Indoors in Federer's hometown, with Agassi prevailing over the 17-year-old.
Agassi also defeated Federer at the 2001 US Open and the finals of the Miami Open in 2002.
Federer began to turn the tide at the Masters Cup in 2003, when he defeated Agassi in both the round robin and the final.
They played a memorable quarterfinal match at the 2004 US Open that spanned over two windy days, with Federer eventually prevailing in five sets.
At the 2005 Dubai Championships, Federer and Agassi attracted worldwide headlines with a publicity stunt that saw the two tennis legends play on a helipad almost 220 meters above sea level at the hotel Burj al-Arab.
Their final duel took place in the final of the 2005 US Open.
In the historic clash of generations, Federer was victorious in four sets in front of a pro-Agassi crowd.
The match marked the last final appearance of Agassi's career.
Agassi earned more than $30 million in prize-money during his career, sixth only to Djokovic, Federer, Nadal, Sampras and Murray to date (May 2018).
He also earned more than $25 million a year through endorsements during his career, fourth in all sports at the time.
Since retiring after the 2006 US Open, Agassi has participated in a series of charity tournaments and continues his work with his own charity.
On September 5, 2007, he was a surprise guest commentator for the Andy Roddick/Roger Federer US Open quarterfinal.
He played an exhibition match at Wimbledon, teaming with his wife, Steffi Graf, to play with Tim Henman and Kim Clijsters.
He played World Team Tennis for the Philadelphia Freedoms in the summer of 2009.
At the 2009 French Open, Agassi was on hand to present Roger Federer, who completed his Career Grand Slam by winning the tournament and joined Agassi as one of six men to complete the Career Grand Slam, with the trophy.
Also in 2009 Agassi played at the Outback Champions Series event for the first time.
He played the Cancer Treatment Centers of America Tennis Championships at Surprise, Arizona, where he reached the final before bowing to eventual champion Todd Martin.
He also announced that he will not be playing the tour on a full-time basis, and played the tournament as a favor to long-time friend Jim Courier.
Agassi returned to the tour renamed for the PowerShares Series in 2011 and participated in a total of seven events while winning two.
Agassi beat Courier in the final of the Staples Champions Cup in Boston and later defeated Sampras at the CTCA Championships at his hometown Las Vegas.
In 2012, Agassi took part in five tournaments, winning three of those.
In November, at first he won BILT Champions Showdown in San Jose, beating John McEnroe in the final.
The following day, he defended his title of the CTCA Championships, while defeating Courier in the decisive match.
In the series season finale, he beat Michael Chang for the Acura Champions Cup.
The series and Agassi came back to action in 2014.
Agassi won both tournaments he participated in.
At the Camden Wealth Advisors Cup's final in Houston, Agassi beat James Blake for a rematch of their 2005 US Open quarterfinal.
He defeated Blake again in Portland to win the title of the Cancer Treatment Centers of America Championships.
In 2015, Agassi took part in just one event of the PowerShares Series, losing to Mark Philippoussis in the final of the Champions Shootout.
The following year he took part in two events, at first losing to Blake in Chicago, and the next day defeating Mardy Fish, but losing to Roddick in Charleston.
In 2009, in Macau Agassi and Sampras met for the first time on court since the 2002 US Open final.
Sampras won the exhibition in three sets.
The rivalry between the former champions headlined sports media again in March 2010 after the two participated in the "Hit for Haiti" charity event organized to raise money for the victims of the earthquake.
Partnered with Roger Federer and Rafael Nadal, the old rivals began making jokes on each other which ended up with Sampras intentionally striking a serve at Agassi's body.
After the event Agassi admitted that he had crossed the line with his jokes and publicly apologized to Sampras.
Agassi and Sampras met again one year later for an exhibition match at Madison Square Garden in New York in front of 19 000 spectators as Sampras defeated Agassi in two sets.
On March 3, 2014, Agassi and Sampras squared off for an exhibition in London for the annual World Tennis Day.
This time it was Agassi who came out on top in two straight sets.
He returned to the tour in May 2017 in the position of coach to Novak Djokovic for the French Open.
Agassi announced the end of the partnership on March 31, 2018, stating that there were too many disagreements in the relationship.
Early in his career, Agassi would look to end points quickly by playing first-strike tennis, typically by inducing a weak return with a deep, hard shot, and then playing a winner at an extreme angle.
His groundstrokes, return of serve, baseline game, anticipation, and eye–hand coordination were always among the best in the game.
On the rare occasion that he charged the net, Agassi liked to take the ball in the air and hit a swinging volley for a winner.
His favored groundstroke was his flat, accurate two-handed backhand, hit well cross-court but especially down the line.
His forehand was nearly as strong, especially his inside-out to the ad court.
Agassi's strength was in dictating play from the back of the court.
While he was growing up, his father and Nick Bollettieri trained him in this way.
When in control of a point, Agassi would often pass up an opportunity to attempt a winner and hit a conservative shot to minimize his errors, and to make his opponent run more.
This change to more methodical, less aggressive baseline play was largely initiated by his longtime coach, Brad Gilbert, in their first year together in 1994.
Gilbert encouraged Agassi to wear out opponents with his deep, flat groundstrokes and to use his fitness to win attrition wars, and noted Agassi's two-handed backhand down the line as his very best shot.
A signature play later in his career was a change up drop shot to the deuce court after deep penetrating groundstrokes.
This would often be followed by a passing shot or lob if the opponent was fast enough to retrieve it.
Agassi was raised on hardcourts, but found much of his early major-tournament success on the red clay of Roland Garros, reaching two consecutive finals there early in his career.
Despite grass being his worst surface, his first major win was at the slick grass of Wimbledon in 1992, a tournament that he professed to hating at the time.
His strongest surface over the course of his career, was indeed hardcourt, where he won six of his eight majors.
Agassi established a limited liability company named Andre Agassi Ventures (formerly named Agassi Enterprises).
Agassi, along with five athlete partners (including Wayne Gretzky, Joe Montana, Shaquille O'Neal, Ken Griffey, Jr., and Monica Seles) opened a chain of sports-themed restaurant named Official All Star Café in April 1996.
The restaurant closed down in 2001.
In 1999, he paid $1 million for a 10 percent stake in Nevada First Bank and made a $10 million profit when it was sold to Western Alliance Bancorp in 2006.
In 2002, he joined the Tennis Channel to promote the channel to consumers and cable and satellite industry, and made an equity investment in the network.
After meeting chef Michael Mina at one of his restaurants in San Francisco, Agassi partnered with him in 2002 to start Mina Group Inc. and opened 18 concept restaurants in San Francisco, San Jose, Dana Point, Atlantic City and Las Vegas.
Agassi was an equity investor of a group that acquired Golden Nugget Las Vegas and Golden Nugget Laughlin from MGM Mirage for $215 million in 2004.
One year later, the group sold the hotel-casino to Landry's, Inc. for $163 million in cash and $182 million in assumed debt.
In 2007, he sat on the board of Meadows Bank, an independent bank in Nevada.
He has invested in start-up companies backed by Allen & Company.
Agassi and Graf formed a company called Agassi Graf Holdings.
They invested in PURE, a nightclub at Caesars Palace, which opened in 2004, and sold it to Angel Management Group in 2010.
In August 2006, Agassi and Graf developed a joint venture with high-end furniture maker Kreiss Enterprises.
They launched a furniture line called Agassi Graf Collection.
In September, Agassi and Graf, through their company Agassi Graf Development LLC, along with Bayview Financial LP, finalized an agreement to develop a condominium hotel, Fairmont Tamarack, at Tamarack Resort in Donnelly, Idaho.
Due to difficult market conditions and delays, they withdrew from the project in 2009.
The group still owns three small chunks of land.
In September, they collaborated with Steve Case's Exclusive Resorts to co-develop luxury resorts and design Agassi-Graf Tennis and Fitness Centers.
They also invested in online ticket reseller viagogo in 2009 and both serve as board members and advisors of the company.
In October 2012, Village Roadshow and investors including Agassi and Graf announced plans to build new water park called Wet'n'Wild Las Vegas in Las Vegas.
Village Roadshow has a 51% stake in the park while Agassi, Graf, and other private investors hold the remaining 49%.
The park opened in May 2013.
IMG managed Agassi from the time he turned pro in 1986 through January 2000 before switching to SFX Sports Group.
His business manager, lawyer and agent was childhood friend Perry Rogers, but they have been estranged since 2008.
In 2009, he and Graf signed with CAA.
Agassi used Prince Graphite rackets early in his career.
He signed a $7 million endorsement contract with Belgian tennis racquet makers Donnay.
He later switched to Head Ti Radical racket and Head's LiquidMetal Radical racket, having signed a multimillion-dollar endorsement deal with Head in 1993.
He renewed his contract in 1999 and in November 2003, he signed a lifetime agreement with Head.
He also endorses Penn tennis balls.
On July 25, 2005, Agassi left Nike after 17 years and signed an endorsement deal with Adidas.
A major reason for Agassi leaving Nike was because Nike refused to donate to Agassi's charities, and Adidas was more than happy to do so.
On May 13, 2013, Agassi rejoined Nike.
Agassi was sponsored by DuPont, Ebel, Mountain Dew in 1993, Mazda in 1997, Kia Motors in 2002, American Express and Deutsche Bank in 2003.
In 1990, he appeared in a television commercial for Canon Inc., promoting the Canon EOS Rebel camera.
Between 1999 and 2000, he signed a multimillion-dollar, multiyear endorsement deal with Schick and became the worldwide spokesman for the company.
Agassi signed a multiyear contract with Twinlab and promoted the company's nutritional supplements.
In mid-2003, he was named the spokesman of Aramis Life, a fragrance by Aramis and signed a five-year deal with the company.
In March 2004, he signed a ten-year agreement worth $1.5 million a year with 24 Hour Fitness, which will open five Andre Agassi fitness centers by year-end.
Prior to the 2012 Australian Open, Agassi and Australian winemaker Jacobs Creek announced a three-year partnership and created the Open Film Series to "[share] personal stories about the life defining moments that shaped his character on and off the court."
In 2007, watchmaker Longines named Agassi as their brand ambassador.
Agassi and his mother appeared in a Got Milk?
advertisement in 2002.
Agassi has appeared in many advertisements and television commercials with Graf.
They both endorsed Deutsche Telekom in 2002, Genworth Financial and Canon Inc. in 2004, LVMH in 2007, and Nintendo Wii and Wii Fit U and Longines in 2013.
In the early 1990s Agassi dated American entertainer Barbra Streisand.
He wrote about the relationship in his 2009 autobiography, "We agree that we're good for each other, and so what if she's twenty-eight years older?
We're sympatico, and the public outcry only adds spice to our connection.
It makes our friendship feel forbidden, taboo – another piece of my overall rebellion.
Dating Barbra Streisand is like wearing Hot Lava."
He was married to Brooke Shields from 1997 to 1999.
He married Steffi Graf on October 22, 2001 at their Las Vegas home; the only witnesses were their mothers.
They have two children: son Jaden Gil (born 2001) and daughter Jaz Elle (born 2003).
Agassi has said that he and Graf are not pushing their children toward becoming tennis players.
The Graf-Agassi family resides in Summerlin, a community in the Las Vegas Valley.
Graf's mother and brother, Michael, with his four children also live there.
Long-time trainer Gil Reyes has been called one of Agassi's closest friends; some have described him as being a "father figure" to Agassi.
In 2012, Agassi and Reyes introduced their own line of fitness equipment, BILT By Agassi and Reyes.
In December 2008, Agassi's childhood friend and former business manager, Perry Rogers, sued Graf for $50,000 in management fees he claimed that she owed him.
Agassi's autobiography, "Open: An Autobiography," (written with assistance from J. R. Moehringer), was published in November 2009.
In it, Agassi admitted that he used and tested positive for methamphetamine in 1997.
In response to this revelation, Roger Federer declared himself shocked and disappointed, while Marat Safin argued that Agassi should return his prize money and be stripped of his titles.
In an interview with CBS, Agassi justified himself and asked for understanding, saying that, "It was a period in my life where I needed help."
Agassi said that he had always hated tennis during his career because of the constant pressure it exerted on him.
He also said he wore a hairpiece earlier in his career and thought Pete Sampras was "robotic".
The book reached No.
1 on the "New York Times" Best Seller list and received favorable reviews.
It won the Autobiography category of the 2010 British Sports Book Awards.
In 2017, Agassi appeared in the documentary film "Love Means Zero", which highlighted the troubled relationship between his coach Nick Bollettieri and him.
Agassi has donated more than $100,000 to Democratic candidates, and $2000 to Republicans.
On September 1, 2010, when he appeared on daily WNYC public radio program "The Brian Lehrer Show," he stated that he is a registered Independent.
Agassi founded the Andre Agassi Charitable Association in 1994, which assists Las Vegas' young people.
He was awarded the ATP Arthur Ashe Humanitarian award in 1995 for his efforts to help disadvantaged youth.
He is regularly cited as the most charitable and socially involved player in professional tennis.
It has also been claimed that he may be the most charitable athlete of his generation.
Agassi's charities help in assisting children reach their athletic potential.
His Boys & Girls Club sees 2,000 children throughout the year and boasts a world-class junior tennis team.
It also has a basketball program (the Agassi Stars) and a rigorous system that encourages a mix of academics and athletics.
In 2001, Agassi opened the Andre Agassi College Preparatory Academy in Las Vegas, a tuition-free charter school for at-risk children in the area.
He personally donated $35 million to the school.
In 2009, the graduating class had 100 percent graduation rate and expected a 100 percent college acceptance rate.
Among other child-related programs that Agassi supports through his Andre Agassi Charitable Foundation is Clark County's only residential facility for abused and neglected children, Child Haven.
In 1997, Agassi donated funding to Child Haven for a six-room classroom building now named the Agassi Center for Education.
His foundation also provided $720,000 to assist in the building of the Andre Agassi Cottage for Medically Fragile Children.
This 20-bed facility opened in December 2001, and accommodates developmentally delayed or handicapped children and children quarantined for infectious diseases.
In 2007, along with several other athletes, Agassi founded the charity Athletes for Hope, which helps professional athletes get involved in charitable causes and aims to inspire all people to volunteer and support their communities.
He created the Canyon-Agassi Charter School Facilities Fund, now known as the Turner-Agassi Charter School Facilities Fund.
The Fund is an investment initiative for social change, focusing on the "nationwide effort to move charters from stopgap buildings into permanent campuses."
In September 2013, the Andre Agassi Foundation for Education formed a partnership with V20 Foods to launch Box Budd!es, a line of kids' healthy snacks.
All proceeds go to the Foundation.
In February 2014, Agassi remodeled the vacant University of Phoenix building as a new school called the Doral Academy West through the Canyon-Agassi Charter School Facilities Fund.
Doral Academy opened in August 2014.
The Fund purchased a 4.6-acre plot in Henderson, Nevada to house the Somerset Academy of Las Vegas, which will relocate from its campus inside a church.
By winning the 1999 French Open, Agassi completed a men's singles Career Grand Slam.
He is the 5th of 8 male players in history (after Budge, Perry, Laver, Emerson and before Federer, Nadal and Djokovic) to achieve this.
</doc>
<doc id="597" url="https://en.wikipedia.org/wiki?curid=597" title="Austroasiatic languages">
Austroasiatic languages

The Austroasiatic languages, formerly known as Mon–Khmer, are a large language family of Mainland Southeast Asia, also scattered throughout India, Bangladesh, Nepal and the southern border of China, with around 117 million speakers.
The name "Austroasiatic" comes from a combination of the Latin words for "South" and "Asia", hence "South Asia".
Of these languages, only Vietnamese, Khmer, and Mon have a long-established recorded history, and only Vietnamese and Khmer have official status as modern national languages (in Vietnam and Cambodia, respectively).
In Myanmar, the Wa language is the de facto official language of Wa State.
Santali is recognized as a regional language of India.
The rest of the languages are spoken by minority groups and have no official status.
"Ethnologue" identifies 168 Austroasiatic languages.
These form thirteen established families (plus perhaps Shompen, which is poorly attested, as a fourteenth), which have traditionally been grouped into two, as Mon–Khmer and Munda.
However, one recent classification posits three groups (Munda, Nuclear Mon-Khmer and Khasi–Khmuic) while another has abandoned Mon–Khmer as a taxon altogether, making it synonymous with the larger family.
Austroasiatic languages have a disjunct distribution across India, Bangladesh, Nepal and Southeast Asia, separated by regions where other languages are spoken.
They appear to be the extant autochthonous languages of Southeast Asia (if Andaman islands are not included), with the neighboring Indo-Aryan, Kra–Dai, Dravidian, Austronesian, and Sino-Tibetan languages being the result of later migrations.
A 2015 made analysis using the Automated Similarity Judgment Program resulted in Japanese being grouped with the Ainu and the Austroasiatic languages.
Regarding word structure, Austroasiatic languages are well known for having an iambic "sesquisyllabic" pattern, with basic nouns and verbs consisting of an initial, unstressed, reduced minor syllable followed by a stressed, full syllable.
This reduction of presyllables has led to a variety among modern languages of phonological shapes of the same original Proto-Austroasiatic prefixes, such as the causative prefix, ranging from CVC syllables to consonant clusters to single consonants.
As for word formation, most Austroasiatic languages have a variety of derivational prefixes, many have infixes, but suffixes are almost completely non-existent in most branches except Munda, and a few specialized exceptions in other Austroasiatic branches.
The Austroasiatic languages are further characterized as having unusually large vowel inventories and employing some sort of register contrast, either between modal (normal) voice and breathy (lax) voice or between modal voice and creaky voice.
Languages in the Pearic branch and some in the Vietic branch can have a three- or even four-way voicing contrast.
However, some Austroasiatic languages have lost the register contrast by evolving more diphthongs or in a few cases, such as Vietnamese, tonogenesis.
Vietnamese has been so heavily influenced by Chinese that its original Austroasiatic phonological quality is obscured and now resembles that of South Chinese languages, whereas Khmer, which had more influence from Sanskrit, has retained a more typically Austroasiatic structure.
Much work has been done on the reconstruction of Proto-Mon–Khmer in Harry L. Shorto's "Mon–Khmer Comparative Dictionary".
Little work has been done on the Munda languages, which are not well documented.
With their demotion from a primary branch, Proto-Mon–Khmer becomes synonymous with Proto-Austroasiatic.
Paul Sidwell (2005) reconstructs the consonant inventory of Proto-Mon–Khmer as follows:

This is identical to earlier reconstructions except for .
is better preserved in the Katuic languages, which Sidwell has specialized in.
Sidwell (2011) suggests that the likely homeland of Austroasiatic is the middle Mekong, in the area of the Bahnaric and Katuic languages (approximately where modern Laos, Thailand, and Cambodia come together), and that the family is not as old as frequently assumed, dating to perhaps 2000 BCE.
Peiros (2011) criticized Sidwell's theory heavily and calls it a bunch of contradictions.
He show with his analysis that the homeland of Austroasiatic is somewhere near the Yangtze.
He suggests the Sichuan Basin as likely homeland of proto-Austroasiatic before they migrated to other parts of central and southern China and than into Southeast Asia.
He further suggests that the family must be as old as proto-Austronesian and proto-Sinotibetan or even older.
Georg van Driem (2011) proposes that the homeland of Austroasiatic is somewhere in southern China.
He suggests that the region around the Pearl River (China) is the likely homeland of the Austroasiatic languages and people.
He further suggests, based on genetic studies, that the migration of Kra–Dai people from Taiwan replaced the original Austroasiatic language but the effect on the people was only minor.
Local Austroasiatic speakers adopted Kra-Dai languages and partially their culture.
The linguists Sagar (2011) and Bellwood (2013) support the theory of an origin of Austroasiatic along the Yangtze river in southern China.
A genetic and linguistic research in 2015 about ancient people in East Asia suggest an origin and homeland of Austroasiatic in today southern China or even further north.
Linguists traditionally recognize two primary divisions of Austroasiatic: the Mon–Khmer languages of Southeast Asia, Northeast India and the Nicobar Islands, and the Munda languages of East and Central India and parts of Bangladesh, parts of Nepal.
However, no evidence for this classification has ever been published.
Each of the families that is written in boldface type below is accepted as a valid clade.
By contrast, the relationships "between" these families within Austroasiatic are debated.
In addition to the traditional classification, two recent proposals are given, neither of which accepts traditional "Mon–Khmer" as a valid unit.
However, little of the data used for competing classifications has ever been published, and therefore cannot be evaluated by peer review.
In addition, there are suggestions that additional branches of Austroasiatic might be preserved in substrata of Acehnese in Sumatra (Diffloth), the Chamic languages of Vietnam, and the Land Dayak languages of Borneo (Adelaar 1995).
Diffloth's widely cited original classification, now abandoned by Diffloth himself, is used in "Encyclopædia Britannica" and—except for the breakup of Southern Mon–Khmer—in "Ethnologue."
Peiros is a lexicostatistic classification, based on percentages of shared vocabulary.
This means that languages can appear to be more distantly related than they actually are due to language contact.
Indeed, when Sidwell (2009) replicated Peiros's study with languages known well enough to account for loans, he did not find the internal (branching) structure below.
Diffloth compares reconstructions of various clades, and attempts to classify them based on shared innovations, though like other classifications the evidence has not been published.
As a schematic, we have:

Or in more detail,




This family tree is consistent with recent studies of migration of Y-Chromosomal haplogroup O2a1-M95.
However, the dates obtained from by Zhivotovsky method DNA studies are several times older than that given by linguists.
The route map of the people with haplogroup O2a1-M95, speaking this language can be seen in this link.
Other geneticists criticise the Zhivotovsky method.
Roger Blench (2009) also proposes that there might have been other primary branches of Austroasiatic that are now extinct, based on substrate evidence in modern-day languages.
Other languages with proposed Austroasiatic substrata are:

John Peterson (2017) suggests that "pre-Munda" languages may have once dominated the eastern Indo-Gangetic Plain, and were then absorbed by Indo-Aryan languages at an early date as Indo-Aryan spread east.
Peterson notes that eastern Indo-Aryan languages display many morphosyntactic features similar to those of Munda languages, while western Indo-Aryan languages do not.
Paul Sidwell (2009), in a lexicostatistical comparison of 36 languages which are well known enough to exclude loan words, finds little evidence for internal branching, though he did find an area of increased contact between the Bahnaric and Katuic languages, such that languages of all branches apart from the geographically distant Munda and Nicobarese show greater similarity to Bahnaric and Katuic the closer they are to those branches, without any noticeable innovations common to Bahnaric and Katuic.
He therefore takes the conservative view that the thirteen branches of Austroasiatic should be treated as equidistant on current evidence.
Sidwell & Blench (2011) discuss this proposal in more detail, and note that there is good evidence for a Khasi–Palaungic node, which could also possibly be closely related to Khmuic.
If this would the case, Sidwell & Blench suggest that Khasic may have been an early offshoot of Palaungic that had spread westward.
Sidwell & Blench (2011) suggest Shompen as an additional branch, and believe that a Vieto-Katuic connection is worth investigating.
In general, however, the family is thought to have diversified too quickly for a deeply nested structure to have developed, since Proto-Austroasiatic speakers are believed by Sidwell to have radiated out from the central Mekong river valley relatively quickly.
Subsequently, Sidwell (2015a: 179) proposed that Nicobarese subgroups with Aslian, just as how Khasian and Palaungic subgroup with each other.
A subsequent computational phylogenetic analysis of the Austroasiatic language family by Sidwell (2015b) suggests that Austroasiatic branches may have a loosely nested structure rather than a completely rake-like structure, with an east-west division (consisting of Munda, Khasic, Palaungic, and Khmuic forming a western group as opposed to all of the other branches) occurring possibly as early as 7,000 years before present.
Integrating computational phylogenetic linguistics with recent archaeological findings, Paul Sidwell (2015c) further expanded his Mekong riverine hypothesis by proposing that Austroasiatic had ultimately expanded into Indochina from the Lingnan area of southern China, with the subsequent Mekong riverine dispersal taking place after the initial arrival of Neolithic farmers from southern China.
Sidwell (2015c) tentatively suggests that Austroasiatic may have begun to split up 5,000 years B.P.
during the Neolithic transition era of mainland Southeast Asia, with all the major branches of Austroasiatic formed by 4,000 B.P.
Austroasiatic would have had two possible dispersal routes from the western periphery of the Pearl River watershed of Lingnan, which would have been either a coastal route down the coast of Vietnam, or downstream through the Mekong River via Yunnan.
Both the reconstructed lexicon of Proto-Austroasiatic and the archaeological record clearly show that early Austroasiatic speakers around 4,000 B.P.
cultivated rice and millet, kept livestock such as dogs, pigs, and chickens, and thrived mostly in estuarine rather than coastal environments.
At 4,500 B.P., this "Neolithic package" suddenly arrived in Indochina from the Lingnan area without cereal grains and displaced the earlier pre-Neolithic hunter-gatherer cultures, with grain husks found in northern Indochina by 4,100 B.P.
and in southern Indochina by 3,800 B.P.
However, Sidwell (2015c) found that iron is not reconstructable in Proto-Austroasiatic, since each Austroasiatic branch has different terms for iron that had been borrowed relatively lately from Tai, Chinese, Tibetan, Malay, and other languages.
During the Iron Age about 2,500 B.P., relatively young Austroasiatic branches in Indochina such as Vietic, Katuic, Pearic, and Khmer were formed, while the more internally diverse Bahnaric branch (dating to about 3,000 B.P.)
underwent more extensive internal diversification.
By the Iron Age, all of the Austroasiatic branches were more or less in their present-day locations, with most of the diversification within Austroasiatic taking place during the Iron Age.
Paul Sidwell (2018) considers the Austroasiatic language family to have rapidly diversified around 4,000 years B.P.
during the arrival of rice agriculture in Indochina, but notes that the origin of Proto-Austroasiatic itself is older than that date.
The lexicon of Proto-Austroasiatic can be divided into an early and late stratum.
The early stratum consists of basic lexicon including body parts, animal names, natural features, and pronouns, while the names of cultural items (agriculture terms and words for cultural artifacts, which are reconstructable in Proto-Austroasiatic) form part of the later stratum.
Roger Blench (2017) suggests that vocabulary related to aquatic subsistence strategies (such as boats, waterways, river fauna, and fish capture techniques), can be reconstructed for Proto-Austroasiatic.
Blench (2017) finds widespread Austroasiatic roots for 'river, valley', 'boat', 'fish', 'catfish sp.
', 'eel', 'prawn', 'shrimp' (Central Austroasiatic), 'crab', 'tortoise', 'turtle', 'otter', 'crocodile', 'heron, fishing bird', and 'fish trap'.
Archaeological evidence for the presence of agriculture in northern Indochina (northern Vietnam, Laos, and other nearby areas) dates back to only about 4,000 years B.P.
(2,000 B.C.
), with agriculture ultimately being introduced from further up to the north in the Yangtze valley where it has been dated to 6,000 B.P.
Hence, this points to a relatively late riverine dispersal of Austroasiatic as compared to Sino-Tibetan, whose speakers had a distinct non-riverine culture.
In addition to living an aquatic-based lifestyle, early Austroasiatic speakers would have also had access to livestock, crops, and newer types of watercraft.
As early Austroasiatic speakers dispersed rapidly via waterways, they would have encountered speakers of older language families who were already settled in the area, such as Sino-Tibetan.
Other than Latin-based alphabets, many Austroasiatic languages are written with the Khmer, Thai, Lao, and Burmese alphabets.
Vietnamese divergently had an indigenous script based on Chinese logographic writing.
This has since been supplanted by the Latin alphabet in the 20th century.
The following are examples of past-used alphabets or current alphabets of Austroasiatic languages.
According to Chaubey et al., "Austro-Asiatic speakers in India today are derived from dispersal from Southeast Asia, followed by extensive sex-specific admixture with local Indian populations."
According to Riccio et al., the Munda people are likely descended from Austroasiatic migrants from southeast Asia.
According to Zhang et al., Austroasiatic migrations from southeast Asia into India took place after the last Glacial maximum, circa 10,000 years ago.
Arunkumar et al.
suggest Austroasiatic migrations from southeast Asia occurred into northeast India 5.2 ± 0.6 kya and into East India 4.3 ± 0.2 kya.
</doc>
<doc id="599" url="https://en.wikipedia.org/wiki?curid=599" title="Afroasiatic languages">
Afroasiatic languages

Afroasiatic (Afro-Asiatic), also known as Afrasian and in older sources as Hamito-Semitic (Chamito-Semitic) or Semito-Hamitic, is a large language family of about 300 languages.
It includes languages spoken predominantly in West Asia, North Africa, the Horn of Africa and parts of the Sahel.
Afroasiatic languages have over 495 million native speakers, the fourth largest number of any language family (after Indo-European, Sino-Tibetan and Niger–Congo).
The phylum has six branches: Berber, Chadic, Cushitic, Egyptian, Omotic and Semitic.
By far the most widely spoken Afroasiatic language or dialect continuum is Arabic.
A de facto group of distinct language varieties within the Semitic branch, the languages that evolved from Proto-Arabic have around 313 million native speakers, concentrated primarily in West Asia, North Africa.
Other widely spoken Afroasiatic languages include:
In addition to languages spoken today, Afroasiatic includes several important ancient languages, such as Ancient Egyptian, which forms a distinct branch of the family, and Akkadian, Biblical Hebrew and Old Aramaic, all of which are from the Semitic branch.
The original homeland of the Afroasiatic family, and when the parent language (i.e. Proto-Afroasiatic) was spoken, are yet to be agreed upon by historical linguists.
Proposed locations include North Africa, the Horn of Africa, the Eastern Sahara and the Levant (see below).
During the early 1800s, linguists grouped the Berber, Cushitic and Egyptian languages within a "Hamitic" phylum, in acknowledgement of these languages' genetic relation with each other and with those in the Semitic phylum.
The terms "Hamitic" and "Semitic" were etymologically derived from the Book of Genesis, which describes various Biblical tribes descended from Ham and Shem, two sons of Noah.
By the 1860s, the main constituent elements within the broader Afroasiatic family had been worked out.
Friedrich Müller introduced the name "Hamito-Semitic" for the entire family in his "Grundriss der Sprachwissenschaft" (1876).
Maurice Delafosse (1914) later coined the term "Afroasiatic" (often now spelled "Afro-Asiatic").
However, it did not come into general use until Joseph Greenberg (1950) formally proposed its adoption.
In doing so, Greenberg sought to emphasize the fact that Afroasiatic spanned the continents of both Africa and Asia.
Individual scholars have also called the family "Erythraean" (Tucker 1966) and "Lisramic" (Hodge 1972).
In lieu of "Hamito-Semitic", the Russian linguist Igor Diakonoff later suggested the term "Afrasian", meaning "half African, half Asiatic", in reference to the geographic distribution of the family's constituent languages.
The term "Hamito-Semitic" remains in use in the academic traditions of some European countries.
Scholars generally treat the Afroasiatic language family as including the following branches:

Although there is general agreement on these six families, linguists who study Afroasiatic raise some points of disagreement, in particular:

In the 9th century, the Hebrew grammarian Judah ibn Quraysh of Tiaret in Algeria was the first to link two branches of Afroasiatic together; he perceived a relationship between Berber and Semitic.
He knew of Semitic through his study of Arabic, Hebrew, and Aramaic.
In the course of the 19th century, Europeans also began suggesting such relationships.
In 1844, Theodor Benfey suggested a language family consisting of Semitic, Berber, and Cushitic (calling the latter "Ethiopic").
In the same year, T.N.
Newman suggested a relationship between Semitic and Hausa, but this would long remain a topic of dispute and uncertainty.
Friedrich Müller named the traditional Hamito-Semitic family in 1876 in his "Grundriss der Sprachwissenschaft" ("Outline of Linguistics"), and defined it as consisting of a Semitic group plus a "Hamitic" group containing Egyptian, Berber, and Cushitic; he excluded the Chadic group.
It was the Egyptologist Karl Richard Lepsius (1810–1884) who restricted Hamitic to the non-Semitic languages in Africa, which are characterized by a grammatical gender system.
This "Hamitic language group" was proposed to unite various, mainly North-African, languages, including the Ancient Egyptian language, the Berber languages, the Cushitic languages, the Beja language, and the Chadic languages.
Unlike Müller, Lepsius considered that Hausa and Nama were part of the Hamitic group.
These classifications relied in part on non-linguistic anthropological and racial arguments.
Both authors used the skin-color, mode of subsistence, and other characteristics of native speakers as part of their arguments that particular languages should be grouped together.
In 1912, Carl Meinhof published "Die Sprachen der Hamiten" ("The Languages of the Hamites"), in which he expanded Lepsius's model, adding the Fula, Maasai, Bari, Nandi, Sandawe and Hadza languages to the Hamitic group.
Meinhof's model was widely supported into the 1940s.
Meinhof's system of classification of the Hamitic languages was based on a belief that "speakers of Hamitic became largely coterminous with cattle herding peoples with essentially Caucasian origins, intrinsically different from and superior to the 'Negroes of Africa'."
However, in the case of the so-called Nilo-Hamitic languages (a concept he introduced), it was based on the typological feature of gender and a "fallacious theory of language mixture."
Meinhof did this although earlier work by scholars such as Lepsius and Johnston had substantiated that the languages which he would later dub "Nilo-Hamitic" were in fact Nilotic languages, with numerous similarities in vocabulary to other Nilotic languages.
Leo Reinisch (1909) had already proposed linking Cushitic and Chadic, while urging their more distant affinity with Egyptian and Semitic.
However, his suggestion found little acceptance.
Marcel Cohen (1924) rejected the idea of a distinct "Hamitic" subgroup, and included Hausa (a Chadic language) in his comparative Hamito-Semitic vocabulary.
Finally, Joseph Greenberg's 1950 work led to the widespread rejection of "Hamitic" as a language category by linguists.
Greenberg refuted Meinhof's linguistic theories, and rejected the use of racial and social evidence.
In dismissing the notion of a separate "Nilo-Hamitic" language category in particular, Greenberg was "returning to a view widely held a half century earlier."
He consequently rejoined Meinhof's so-called Nilo-Hamitic languages with their appropriate Nilotic siblings.
He also added (and sub-classified) the Chadic languages, and proposed the new name Afroasiatic for the family.
Almost all scholars have accepted this classification as the new and continued consensus.
Greenberg's model was fully developed in his book "The Languages of Africa" (1963), in which he reassigned most of Meinhof's additions to Hamitic to other language families, notably Nilo-Saharan.
Following Isaac Schapera and rejecting Meinhof, he classified the Khoekhoe language as a member of the Khoisan languages, a grouping that has since proven inaccurate and excessively motivated on the presence of click sounds.
To Khoisan he also added the Tanzanian Hadza and Sandawe, though this view has been discredited as linguists working on these languages consider them to be linguistic isolates.
Despite this, Greenberg's classification remains a starting point for modern work of many languages spoken in Africa, and the Hamitic category (and its extension to Nilo-Hamitic) has no part in this.
Since the three traditional branches of the Hamitic languages (Berber, Cushitic and Egyptian) have not been shown to form an exclusive (monophyletic) phylogenetic unit of their own, separate from other Afroasiatic languages, linguists no longer use the term in this sense.
Each of these branches is instead now regarded as an independent subgroup of the larger Afroasiatic family.
In 1969, Harold Fleming proposed that what had previously been known as Western Cushitic is an independent branch of Afroasiatic, suggesting for it the new name Omotic.
This proposal and name have met with widespread acceptance.
Several scholars, including Harold Fleming and Robert Hetzron, have since questioned the traditional inclusion of Beja in Cushitic.
"Glottolog" does not accept that the inclusion or even unity of Omotic has been established, nor that of Ongota or the unclassified Kujarge.
It therefore splits off the following groups as small families: South Omotic, Mao, Dizoid, Gonga–Gimojan (North Omotic apart from the preceding), Ongota, Kujarge.
Little agreement exists on the subgrouping of the five or six branches of Afroasiatic: Semitic, Egyptian, Berber, Chadic, Cushitic, and Omotic.
However, Christopher Ehret (1979), Harold Fleming (1981), and Joseph Greenberg (1981) all agree that the Omotic branch split from the rest first.
Otherwise:

Afroasiatic is one of the four major language families spoken in Africa identified by Joseph Greenberg in his book "The Languages of Africa" (1963).
It is one of the few whose speech area is transcontinental, with languages from Afroasiatic's Semitic branch also spoken in the Middle East and Europe.
There are no generally accepted relations between Afroasiatic and any other language family.
However, several proposals grouping Afroasiatic with one or more other language families have been made.
The best-known of these are the following:

The earliest written evidence of an Afroasiatic language is an Ancient Egyptian inscription dated to c.
3400 BC (5,400 years ago).
Symbols on Gerzean (Naqada II) pottery resembling Egyptian hieroglyphs date back to c.
4000 BC, suggesting an earlier possible dating.
This gives us a minimum date for the age of Afroasiatic.
However, Ancient Egyptian is highly divergent from Proto-Afroasiatic (Trombetti 1905: 1–2), and considerable time must have elapsed in between them.
Estimates of the date at which the Proto-Afroasiatic language was spoken vary widely.
They fall within a range between approximately 7,500 BC (9,500 years ago), and approximately 16,000 BC (18,000 years ago).
According to Igor M. Diakonoff (1988: 33n), Proto-Afroasiatic was spoken c.
10,000 BC.
Christopher Ehret (2002: 35–36) asserts that Proto-Afroasiatic was spoken c.
11,000 BC at the latest, and possibly as early as c.
16,000 BC.
These dates are older than those associated with other proto-languages.
The term Afroasiatic Urheimat ("Urheimat" meaning "original homeland" in German) refers to the hypothetical place where Proto-Afroasiatic language speakers lived in a single linguistic community, or complex of communities, before this original language dispersed geographically and divided into distinct languages.
Afroasiatic languages are today primarily spoken in West Asia, North Africa, the Horn of Africa, and parts of the Sahel.
Their distribution seems to have been influenced by the Sahara pump operating over the last 10,000 years.
There is no agreement when or where the original homeland of this language family existed.
The main theories of Urheimat are the Levant, the Eastern Sahara, North Africa and the Horn of Africa.
H. Ekkehard Wolff proposes that Proto-Afroasiatic arose in the Fertile Crescent between 15,000 and 9,000 years BC during the Neolithic revolution, then migrated to Africa around 8,000 BC to develop into the Egyptian, Chadic, Omotic, Cushitic and Berber branches.
Widespread (though not universal) features of the Afroasiatic languages include:

One of the most remarkable shared features among the Afroasiatic languages is the prefixing verb conjugation (see the table at the start of this section), with a distinctive pattern of prefixes beginning with /ʔ t n y/, and in particular a pattern whereby third-singular masculine /y-/ is opposed to third-singular feminine and second-singular /t-/.
According to Ehret (1996), tonal languages appear in the Omotic and Chadic branches of Afroasiatic, as well as in certain Cushitic languages.
The Semitic, Berber and Egyptian branches generally do not use tones phonemically.
The following are some examples of Afroasiatic cognates, including ten pronouns, three nouns, and three verbs.
There are two etymological dictionaries of Afroasiatic, one by Christopher Ehret, and one by Vladimir Orel and Olga Stolbova.
The two dictionaries disagree on almost everything.
The following table contains the thirty roots or so (out of thousands) that represent a fragile consensus of present research:

Some of the main sources for Afroasiatic etymologies include:





</doc>
<doc id="600" url="https://en.wikipedia.org/wiki?curid=600" title="Andorra">
Andorra

Andorra (; ), officially the Principality of Andorra (), also called the Principality of the Valleys of Andorra (), is a sovereign landlocked microstate on the Iberian Peninsula, in the eastern Pyrenees, bordered by France in the north and Spain in the south.
Believed to have been created by Charlemagne, Andorra was ruled by the Count of Urgell until 988 when it was transferred to the Diocese of Urgell, and the present principality was formed by a charter in 1278.
It is known as a principality as it is a diarchy headed by two Co-Princes: the Catholic Bishop of Urgell in Spain and the President of the Republic of France.
Andorra is the sixth-smallest nation in Europe, having an area of and a population of approximately .
The Andorrans are a Romance ethnic group of originally Catalan descent.
Andorra is the 16th-smallest country in the world by land and 11th-smallest country by population.
Its capital Andorra la Vella is the highest capital city in Europe, at an elevation of above sea level.
The official language is Catalan, although Spanish, Portuguese, and French are also commonly spoken.
Andorra's tourism services an estimated 10.2 million visitors annually.
It is not a member of the European Union, but the euro is its official currency.
It has been a member of the United Nations since 1993.
In 2013, the people of Andorra had the highest life expectancy in the world at 81 years, according to the Global Burden of Disease Study.
The origin of the word "Andorra" is unknown, although several hypotheses have been formulated.
The oldest derivation of the word "Andorra" is from the Greek historian Polybius ("The Histories" III, 35, 1) who describes the "Andosins", an Iberian Pre-Roman tribe, as historically located in the valleys of Andorra and facing the Carthaginian army in its passage through the Pyrenees during the Punic Wars.
The word "Andosini" or "Andosins" (Ἀνδοσίνοι) may derive from the Basque "handia" whose meaning is "big" or "giant".
The Andorran toponymy shows evidence of Basque language in the area.
Another theory suggests that the word "Andorra" may derive from the old word "Anorra" that contains the Basque word "ur" (water).
Another theory suggests that "Andorra" may derive from Arabic "al-durra", meaning "The forest" (الدرة).
When the Moors colonized the Iberian Peninsula, the valleys of the Pyrenees were covered by large tracts of forest, and other regions and towns, also administered by Muslims, received this designation.
Other theories suggest that the term derives from the Navarro-Aragonese "andurrial", which means "land covered with bushes" or "scrubland".
The folk etymology holds that Charlemagne had named the region as a reference to the Biblical Canaanite valley of "Endor" or "Andor" (where the Midianites had been defeated), a name also bestowed by his heir and son Louis le Debonnaire after defeating the Moors in the "wild valleys of Hell".
"La Balma de la Margineda", found by archaeologists at Sant Julia de Loria, was first settled in 9,500 BC as a passing place between the two sides of the Pyrenees.
The seasonal camp was perfectly located for hunting and fishing by the groups of hunter-gatherers from Ariege and Segre.
During the Neolithic Age a group of people moved to the Valley of Madriu (nowadays Natural Parc located in Escaldes-Engordany declared UNESCO World Heritage Site) as a permanent camp in 6640 BC.
The population of the valley grew cereals, raised domestic livestock and developed a commercial trade with people from the Segre and Occitania.
Other archaeological deposits include the "Tombs of Segudet" (Ordino) and "Feixa del Moro" (Sant Julia de Loria) both dated in 490–4300 BC as an example of the Urn culture in Andorra.
The model of small settlements begin to evolved as a complex urbanism during the Bronze Age.
Metallurgical items of iron, ancient coins and relicaries can be found in the ancient sanctuaries scattered around the country.
The sanctuary of "Roc de les Bruixes" (Stone of the Witches) is maybe the most important archeological complex of this age in Andorra, located in the parish of Canillo, about the rituals of funerals, ancient scripture and engraved stone murals.
The inhabitants of the valleys were traditionally associated with the Iberians and historically located in Andorra as the Iberian tribe "Andosins" or "Andosini" (Ἀνδοσίνους) during the 7th and 2nd centuries BC.
Influenced by Aquitanias, Basque and Iberian languages the locals developed some current toponyms.
Early writings and documents relating this group of people goes back to the second century BC by the Greek writer Polybius in his "Histories" during the Punic Wars.
Some of the most significant remains of this era are the Castle of the "Roc d'Enclar" (part of the early Marca Hispanica), "l'Anxiu" in Les Escaldes and "Roc de L'Oral" in Encamp.
The presence of Roman influence is recorded from the 2nd century BC to the 5th century AD.
The places found with more Roman presence are in "Camp Vermell" (Red Field) in Sant Julia de Loria and in some places in Encamp as well as in the "Roc d'Enclar".
People continued trading, mainly with wine and cereals, with the Roman cities of Urgellet (nowaday La Seu d'Urgell) and all across Segre through the "Via Romana Strata Ceretana" (also known as "Strata Confluetana").
After the fall of the Roman Empire Andorra came under the influence of the Visigoths, not remotely from the Kingdom of Toledo, but locally from the Diocese of Urgell.
The Visigoths remained in the valleys for 200 years, during which time Christianity spread.
When the Muslim Empire and its conquest of the Iberian Peninsula replaced the ruling Visigoths, Andorra was sheltered from these invaders by the Franks.
Tradition holds that "Charles the Great" (Charlemagne) granted a charter to the Andorran people for a contingent of five thousand soldiers under the command of "Marc Almugaver", in return for fighting against the Moors near Porté-Puymorens (Cerdanya).
Andorra remained part of the Marca Hispanica of the Frankish Empire being part of the territory ruled by the Count of Urgell and eventually by the bishop of the Diocese of Urgell.
Also tradition holds that it was guaranteed by the son of Charlemagne, Louis the Pious, writing the "Carta de Poblament" or a local municipal charter circa 805.
In 988, Borrell II, Count of Urgell, gave the Andorran valleys to the Diocese of Urgell in exchange for land in Cerdanya.
Since then the Bishop of Urgell, based in Seu d'Urgell, has been Co-prince of Andorra.
The first document that mentions "Andorra" as a territory is the "Acta de Consagració i Dotació de la Catedral de la Seu d'Urgell" (Deed of Consecration and Endowment of the Cathedral of La Seu d'Urgell).
The old document dated from 839 depicts the six old parishes of the Andorran valleys and therefore the administrative division of the country.
Before 1095, Andorra did not have any type of military protection and the Bishop of Urgell, who knew that the Count of Urgell wanted to reclaim the Andorran valleys, asked the Lord of Caboet for help and protection.
In 1095 the Lord of Caboet and the Bishop of Urgell signed under oath a declaration of their co-sovereignty over Andorra.
Arnalda, daughter of Arnau of Caboet, married the Viscount of Castellbò and both became Viscounts of Castellbò and Cerdanya.
Years later their daughter, Ermessenda, married Roger Bernat II, the French Count of Foix.
They became Roger Bernat II and Ermessenda I, Counts of Foix, Viscounts of Castellbò and Cerdanya, and co-sovereigns of Andorra (shared with the Bishop of Urgell).
In the 13th century, a military dispute arose between the Bishop of Urgell and the Count of Foix as aftermath of the Cathar Crusade.
The conflict was resolved in 1278 with the mediation of the king of Aragon, Pere II between the Bishop and the Count, by the signing of the first paréage which provided that Andorra's sovereignty be shared between the count of Foix (whose title would ultimately transfer to the French head of state) and the Bishop of Urgell, in Catalonia.
This gave the principality its territory and political form.
A second paréage was signed in 1288 after a dispute when the Count of Foix ordered the construction of a castle in "Roc d'Enclar".
The document was ratified by the noble notary Jaume Orig of Puigcerdà and the construction of military structures in the country was prohibited.
In 1364 the political organization of the country named the figure of the syndic (now spokesman and president of the parliament) as representative of the Andorrans to their co-princes making possible the creation of local departments ("comuns", "quarts" and "veïnats").
After being ratified by the Bishop Francesc Tovia and the Count Jean I, the "Consell de la Terra" or Consell General de les Valls (General Council of the Valleys) was founded in 1419, the second oldest parliament in Europe.
The syndic Andreu d'Alàs and the General Council organized the creation of the Justice Courts ("La Cort de Justicia") in 1433 with the Co-Princes and the collection of taxes like "foc i lloc" (literally "fire and site", a national tax active since then).
Although we can find remains of ecclesiastical works dating before the 9th century ("Sant Vicenç d'Enclar" or Església de Santa Coloma), Andorra developed exquisite Romanesque Art during the 9th through 14th centuries, as much in the construction of churches, bridges, religious murals and statues of the Virgin and Child (being the most important the Our Lady of Meritxell).
Nowadays, the Romanesque buildings that form part of Andorra's cultural heritage stand out in a remarkable way, with an emphasis on Església de Sant Esteve, Sant Joan de Caselles, Església de Sant Miquel d'Engolasters, Sant Martí de la Cortinada and the medieval bridges of Margineda and Escalls among many others.
While the Catalan Pyrenees were embryonic of the Catalan language at the end of the 11th century Andorra was influenced by the appearance of that language where it was adopted by proximity and influence even decades before it was expanded by the rest of the Kingdom of Aragon.
The local population based its economy during the Middle Ages in the livestock and agriculture, as well as in furs and weavers.
Later, at the end of the 11th century, the first foundries of iron began to appear in Northern Parishes like Ordino, much appreciated by the master artisans who developed the art of the forges, an important economic activity in the country from the 15th century.
In 1601 the Tribunal de Corts (High Court of Justice) was created as a result of Huguenot rebellions from France, Inquisition courts coming from Spain and indigenous witchcraft experienced in the country due to the Reformation and Counter-Reformation.
With the passage of time, the co-title to Andorra passed to the kings of Navarre.
After Henry of Navarre became King Henry IV of France, he issued an edict in 1607, that established the head of the French state and the Bishop of Urgell as Co-Princes of Andorra.
During 1617 communal councils form the "sometent" (popular militia or army) to deal with the rise of "bandolerisme" (brigandage) and the Consell de la Terra was defined and structured in terms of its composition, organization and competences current today .
Andorra continued with the same economic system that it had during the 12th-14th centuries with a large production of metallurgy ("fargues", a system similar to "Farga catalana") and with the introduction of tobacco circa 1692 and import trade.
The fair of Andorra la Vella was ratified by the co-princes in 1371 and 1448 being the most important annual national festival commercially ever since.
The country had a unique and experienced guild of weavers, "Confraria de Paraires i Teixidors", located in Escaldes-Engordany founded in 1604 taking advantage of the thermal waters of the area.
By that time the country was characterized by the social system of "prohoms" (wealthy society) and "casalers" (rest of the population with smaller economic acquisition), deriving from the tradition of "pubilla" and "hereu".
Three centuries after its foundation the Consell de la Terra located its headquarters and the Tribunal de Corts in Casa de la Vall in 1702.
The manor house built in 1580 served as a noble fortress of the Busquets family.
Inside the parliament was placed the "Closet of the six keys" ("Armari de les sis claus") representative of each Andorran parish and where the Andorran constitution and other documents and laws were kept later on.
In both Guerra dels Segadors and Guerra de Sucesión Española conflicts, the Andorran people (although with the statement neutral country) supported the Catalans who saw their rights reduced in 1716.
The reaction was the promotion of Catalan writings in Andorra, with cultural works such as the "Book of Privileges" ("Llibre de Privilegis de 1674"), "Manual Digest" (1748) by Antoni Fiter i Rossell or the "Polità andorrà" (1763) by Antoni Puig.
After the French Revolution, in 1809, Napoleon I reestablished the Co-Principate and deleted the French medieval tithe.
However, in 1812–13, the First French Empire annexed Catalonia during the Peninsular War ("Guerra del francés").
They divided it into four départements, with Andorra being made part of the district of Puigcerdà (département of Sègre).
In 1814 a royal decree reestablished the independence and economy of Andorra.
During this period, Andorra's late medieval institutions and rural culture remained largely unchanged.
In 1866 the syndic Guillem d'Areny-Plandolit led the "reformist" group in a Council General of 24 members, elected by suffrage limited to heads of families, replaced the aristocratic oligarchy that previously ruled the state.
The New Reform ("Nova Reforma" or "Pla de Reforma") began after being ratified by both Co-Princes and established the basis of the constitution and symbols (such as the tricolor flag) of Andorra.
A new service economy arose as a demand of the inhabitants of the valleys and began to build infrastructures such as hotels, spa resorts, roads and telegraph lines.
The authorities of the Co-Princes ("veguer") banned casinos and betting houses throughout the country by establishing an economic conflict with the demand of the Andorran people.
The conflict led to the so-called "Revolution of 1881" or "Troubles of Andorra", when revolutionaries assaulted the house of the syndic during 8 December 1880 and established the "Provisional Revolutionary Council" led by Joan Pla i Calvo and Pere Baró i Mas, who granted the construction of casinos and spas to foreign companies.
During 7 and 9 June 1881, the loyalists of Canillo and Encamp reconquered the parishes of Ordino and Massana by establishing contact with the revolutionary forces in Escaldes-Engordany.
After a day of combat finally the Treaty of the Bridge of Escalls was signed on 10 June.
The Council was replaced and new elections were held.
But the economic situation worsened, as society was divided over the "Qüestió d'Andorra" (the "Andorran Question" in relation to the Eastern Question).
The struggles continued between pro-bishops, pro-French and nationalists who derived the troubles of Canillo in 1882 and 1885.
Andorra participated in the cultural movement of the Catalan Renaixença.
Between 1882 and 1887 the first academic schools were formed where trilingualism coexists with the knowledge of the official language, Catalan.
Some romantic authors from both France and Spain reported the awakening of the national consciousness of the country.
Jacint Verdaguer lived in Ordino during the 1880s where he wrote and share works related to the Renaixença with Joaquim de Riba, writer and photographer.
Fromental Halévy, for his part, had already premiered in 1848 the opera "Le Val d'Andorre" of great success in Europe, where the national consciousness of the valleys during the Peninsular War was exposed in the romantic work.
Andorra declared war on Imperial Germany during World War I, but did not take part directly in the fighting.
It is known that some Andorrans volunteered to take part in the conflict as part of the French Legions.
It remained in an official state of belligerency until 1958 as it was not included in the Treaty of Versailles.
In 1933, France occupied Andorra following social unrest which occurred before elections due the "Revolution of 1933" and the "FHASA strikes" ("Vagues de FHASA"); the revolt led by "Joves Andorrans" (a labour union group related to the Spanish CNT and FAI) called for political reforms, the universal suffrage vote of all Andorrans and acted in defense of the rights of local and foreign workers during the construction of FHASA's hydroelectric power station in Encamp.
The 5th April 1933 "Joves Andorrans" took the Andorran Parlamient under their custody in rebellion to their requests.
These actions were preceded by the arrival of Colonel René-Jules Baulard with 50 gendarmes and the mobilization of 200 local militias or "sometent" led by the Síndic Francesc Cairat.
On 12 July 1934, adventurer Boris Skossyreff issued a proclamation in Urgell, declaring himself "Boris I, King of Andorra", simultaneously declaring war on the Bishop of Urgell.
He was arrested by the Spanish authorities on 20 July and ultimately expelled from Spain.
From 1936 until 1940, a French military detachment was garrisoned in Andorra to secure the principality against disruption from the Spanish Civil War and Francoist Spain.
Francoist troops reached the Andorran border in the later stages of the war.
During World War II, Andorra remained neutral and was an important smuggling route between Vichy France and Spain.
Given its relative isolation, Andorra has existed outside the mainstream of European history, with few ties to countries other than France, Spain and Portugal.
In recent times, however, its thriving tourist industry along with developments in transport and communications have removed the country from its isolation.
Since 1976 the country sees the need to reform Andorran institutions due to the anachronisms in the field of sovereignty, human rights and the balance of powers as well as the need to adapt legislation to modern demands.
In 1982 a first separation of powers took place when instituting the "Govern d'Andorra", under the name of "Executive Board" ("Consell Executiu"), chaired by the first prime minister Òscar Ribas Reig with the approval of the Co-Princes.
In 1989 the Principality signed an agreement with the European Economic Community to regularize trade relations.
Its political system was modernized in 1993 after the Andorran constitutional referendum, when the constitution was drafted by the Co-Princes and the General Council and approved on 14 March by 74.2% of voters, with a 76% turnout.
The first elections under the new constitution were held later in the year.
The same year Andorra became a member of the United Nations and the Council of Europe.
Andorra is a parliamentary co-principality with the President of France and the Catholic Bishop of Urgell (Catalonia, Spain) as Co-Princes.
This peculiarity makes the President of France, in his capacity as Prince of Andorra, an elected reigning monarch, although he is not elected by a popular vote of the Andorran people.
The politics of Andorra take place in a framework of a parliamentary representative democracy, whereby the Head of Government is the chief executive, and of a pluriform multi-party system.
The current Head of Government is Antoni Martí of the Democrats for Andorra (DA).
Executive power is exercised by the government.
Legislative power is vested in both government and parliament.
The Parliament of Andorra is known as the General Council.
The General Council consists of between 28 and 42 Councillors.
The Councillors serve for four-year terms, and elections are held between the 30th and 40th days following the dissolution of the previous Council.
Half are elected in equal numbers by each of the seven administrative parishes, and the other half of the Councillors are elected in a single national constituency.
Fifteen days after the election, the Councillors hold their inauguration.
During this session, the Syndic General, who is the head of the General Council, and the Subsyndic General, his assistant, are elected.
Eight days later, the Council convenes once more.
During this session the Head of Government is chosen from among the Councillors.
Candidates can be proposed by a minimum of one-fifth of the Councillors.
The Council then elects the candidate with the absolute majority of votes to be Head of Government.
The Syndic General then notifies the Co-Princes, who in turn appoint the elected candidate as the Head of Government of Andorra.
The General Council is also responsible for proposing and passing laws.
Bills may be presented to the Council as Private Members' Bills by three of the local Parish Councils jointly or by at least one tenth of the citizens of Andorra.
The Council also approves the annual budget of the principality.
The government must submit the proposed budget for parliamentary approval at least two months before the previous budget expires.
If the budget is not approved by the first day of the next year, the previous budget is extended until a new one is approved.
Once any bill is approved, the Syndic General is responsible for presenting it to the Co-Princes so that they may sign and enact it.
If the Head of Government is not satisfied with the Council, he may request that the Co-Princes dissolve the Council and order new elections.
In turn, the Councillors have the power to remove the Head of Government from office.
After a motion of censure is approved by at least one-fifth of the Councillors, the Council will vote and if it receives the absolute majority of votes, the Head of Government is removed.
The judiciary is composed of the Magistrates Court, the Criminal Law Court, the High Court of Andorra, and the Constitutional Court.
The High Court of Justice is composed of five judges: one appointed by the Head of Government, one each by the Co-Princes, one by the Syndic General, and one by the Judges and Magistrates.
It is presided over by the member appointed by the Syndic General and the judges hold office for six-year terms.
The Magistrates and Judges are appointed by the High Court, as is the President of the Criminal Law Court.
The High Court also appoints members of the Office of the Attorney General.
The Constitutional Court is responsible for interpreting the Constitution and reviewing all appeals of unconstitutionality against laws and treaties.
It is composed of four judges, one appointed by each of the Co-Princes and two by the General Council.
They serve eight-year terms.
The Court is presided over by one of the Judges on a two-year rotation so that each judge at one point will preside over the Court.
Andorra does not have its own armed forces, although there is a small ceremonial army.
Responsibility for defending the nation rests primarily with France and Spain.
However, in case of emergencies or natural disasters, the "Sometent" (an alarm) is called and all able-bodied men between 21 and 60 of Andorran nationality must serve.
This is why all Andorrans, and especially the head of each house (usually the eldest able-bodied man of a house) should, by law, keep a rifle, even though the law also states that the police will offer a firearm in case of need.
Andorra is a full member of the United Nations (UN), the Organization for Security and Co-operation in Europe (OSCE), and has a special agreement with the European Union (EU).
Andorra has a small army, which has historically been raised or reconstituted at various dates, but has never in modern times amounted to a standing army.
The basic principle of Andorran defence is that all able-bodied men are available to fight if called upon by the sounding of the "Sometent".
Being a landlocked country, Andorra has no navy.
Prior to World War I, Andorra maintained an armed force of about 600 part-time militiamen.
This body was not liable for service outside the principality and was commanded by two officials ("viguiers") appointed by France and the Bishop of Urgell.
Despite not being involved in any fighting during the First World War, Andorra was technically the longest combatant, as the country was left out of the Versailles Peace Conference, technically remaining at war with Germany from its original declaration of war in 1914 until 24 September 1958 when Andorra officially declared peace with Germany.
In the modern era, the army has consisted of a very small body of volunteers willing to undertake ceremonial duties.
Uniforms were handed down from generation to generation within families and communities.
The army's role in internal security was largely taken over by the formation of the Police Corps of Andorra in 1931.
Brief civil disorder associated with the elections of 1933 led to assistance being sought from the French National Gendarmerie, with a detachment resident in Andorra for two months under the command of René-Jules Baulard.
The Andorran Army was reformed in the following year, with eleven soldiers appointed to supervisory roles.
The force consisted of six Corporals, one for each parish (although there are currently seven parishes, there were only six until 1978), plus four junior staff officers to co-ordinate action, and a commander with the rank of major.
It was the responsibility of the six corporals, each in his own parish, to be able to raise a fighting force from among the able-bodied men of the parish.
Today a small, twelve-man ceremonial unit remains the only permanent section of the Andorran Army, but all able-bodied men remain technically available for military service, with a requirement for each family to have access to a firearm.
The army has not fought for more than 700 years, and its main responsibility is to present the flag of Andorra at official ceremonial functions.
According to Marc Forné Molné, Andorra's military budget is strictly from voluntary donations, and the availability of full-time volunteers.
The myth that all members of the Andorran Army are ranked as officers is popularly maintained in many works of reference.
In reality, all those serving in the permanent ceremonial reserve hold ranks as officers, or non-commissioned officers, because the other ranks are considered to be the rest of the able-bodied male population, who may still be called upon by the "Sometent" to serve, although such a call has not been made in modern times.
Andorra maintains a small but modern and well-equipped internal police force, with around 240 police officers supported by civilian assistants.
The principal services supplied by the corps are uniformed community policing, criminal detection, border control, and traffic policing.
There are also small specialist units including police dogs, mountain rescue, and a bomb disposal team.
The "Grup d'Intervenció Policia d'Andorra" (GIPA) is a small special forces unit trained in counter-terrorism, and hostage recovery tasks.
Although it is the closest in style to an active military force, it is part of the Police Corps, and not the army.
As terrorist and hostage situations are a rare threat to the country, the GIPA is commonly assigned to prisoner escort duties, and at other times to routine policing.
The "Andorran Fire Brigade", with headquarters at Santa Coloma, operates from four modern fire stations, and has a staff of around 120 firefighters.
The service is equipped with 16 heavy appliances (fire tenders, turntable ladders, and specialist four-wheel drive vehicles), four light support vehicles (cars and vans) and four ambulances.
Historically, the families of the six ancient parishes of Andorra maintained local arrangements to assist each other in fighting fires.
The first fire pump purchased by the government was acquired in 1943.
Serious fires which lasted for two days in December 1959 led to calls for a permanent fire service, and the "Andorran Fire Brigade" was formed on 21 April 1961.
The fire service maintains full-time cover with five fire crews on duty at any time: two at the brigade's headquarters in Santa Coloma, and one crew at each of the other three fire stations.
Andorra consists of seven parishes:


Due to its location in the eastern Pyrenees mountain range, Andorra consists predominantly of rugged mountains, the highest being the Coma Pedrosa at , and the average elevation of Andorra is .
These are dissected by three narrow valleys in a Y shape that combine into one as the main stream, the Gran Valira river, leaves the country for Spain (at Andorra's lowest point of ).
Andorra's land area is .
Phytogeographically, Andorra belongs to the Atlantic European province of the Circumboreal Region within the Boreal Kingdom.
According to the WWF, the territory of Andorra belongs to the ecoregion of Pyrenees conifer and mixed forests.
Andorra has an alpine climate and continental climate.
Its higher elevation means there is, on average, more snow in winter, lower humidity, and it is slightly cooler in summer.
Tourism, the mainstay of Andorra's tiny, well-to-do economy, accounts for roughly 80% of GDP.
An estimated 10.2 million tourists visit annually, attracted by Andorra's duty-free status and by its summer and winter resorts.
One of the main sources of income in Andorra is tourism from ski resorts which total over of ski ground.
The sport brings in over 10 million visitors annually and an estimated 340 million euros per year, sustaining 2,000 direct and 10,000 indirect jobs at present since 2007.
The banking sector, with its tax haven status, also contributes substantially to the economy (the financial and insurance sector accounts for approximately 19% of GDP).
The financial system comprises five banking groups, one specialised credit entity, 8 investment undertaking management entities, 3 asset management companies and 29 insurance companies, 14 of which are branches of foreign insurance companies authorised to operate in the principality.
Agricultural production is limited, only 2% of the land is arable, and most food has to be imported.
Some tobacco is grown locally.
The principal livestock activity is domestic sheep raising.
Manufacturing output consists mainly of cigarettes, cigars, and furniture.
Andorra's natural resources include hydroelectric power, mineral water, timber, iron ore, and lead.
Andorra is not a member of the European Union, but enjoys a special relationship with it, such as being treated as an EU member for trade in manufactured goods (no tariffs) and as a non-EU member for agricultural products.
Andorra lacked a currency of its own and used both the French franc and the Spanish peseta in banking transactions until 31 December 1999, when both currencies were replaced by the EU's single currency, the euro.
Coins and notes of both the franc and the peseta remained legal tender in Andorra until 31 December 2002.
Andorra negotiated to issue its own euro coins, beginning in 2014.
Andorra has traditionally had one of the world's lowest unemployment rates.
In 2009 it stood at 2.9%.
Andorra has long benefited from its status as a tax haven, with revenues raised exclusively through import tariffs.
However, during the European sovereign-debt crisis of the 21st century, its tourist economy suffered a decline, partly caused by a drop in the prices of goods in Spain, which undercut Andorran duty-free shopping.
This led to a growth in unemployment.
On 1 January 2012, a business tax of 10% was introduced, followed by a sales tax of 2% a year later, which raised just over 14 million euros in its first quarter.
On 31 May 2013, it was announced that Andorra intended to legislate for the introduction of an income tax by the end of June, against a background of increasing dissatisfaction with the existence of tax havens among EU members.
The announcement was made following a meeting in Paris between the Head of Government Antoni Marti and the French President and Prince of Andorra, François Hollande.
Hollande welcomed the move as part of a process of Andorra "bringing its taxation in line with international standards".
The population of Andorra is estimated at ().
The Andorrans are a Romance ethnic group of originally Catalan descent.
The population has grown from 5,000 in 1900.
Two-thirds of residents lack Andorran nationality and do not have the right to vote in communal elections.
Moreover, they are not allowed to be elected as prime minister or to own more than 33% of the capital stock of a privately held company.
The historic and official language is Catalan, a Romance language.
The Andorran government encourages the use of Catalan.
It funds a Commission for Catalan Toponymy in Andorra (Catalan: "la Comissió de Toponímia d'Andorra"), and provides free Catalan classes to assist immigrants.
Andorran television and radio stations use Catalan.
Because of immigration, historical links, and close geographic proximity, Spanish, Portuguese and French are also commonly spoken.
Most Andorran residents can speak one or more of these, in addition to Catalan.
English is less commonly spoken among the general population, though it is understood to varying degrees in the major tourist resorts.
Andorra is one of only four European countries (together with France, Monaco, and Turkey) that have never signed the Council of Europe Framework Convention on National Minorities.
According to the "Observatori Social d'Andorra", the linguistic usage in Andorra is as follows:

The population of Andorra is predominantly (88.2%) Catholic.
Their patron saint is Our Lady of Meritxell.
Though it is not an official state religion, the constitution acknowledges a special relationship with the Catholic Church, offering some special privileges to that group.
Other Christian denominations include the Anglican Church, the Unification Church, the New Apostolic Church, and Jehovah's Witnesses.
The small Muslim community is primarily made up of North African immigrants.
There is a small community of Hindus and Bahá'ís, and roughly 100 Jews live in Andorra.
(See History of the Jews in Andorra.)
Children between the ages of 6 and 16 are required by law to have full-time education.
Education up to secondary level is provided free of charge by the government.
There are three systems of school, Andorran, French and Spanish, which use Catalan, French and Spanish languages respectively, as the main language of instruction.
Parents may choose which system their children attend.
All schools are built and maintained by Andorran authorities, but teachers in the French and Spanish schools are paid for the most part by France and Spain.
39% of Andorran children attend Andorran schools, 33% attend French schools, and 28% Spanish schools.
The Universitat d'Andorra (UdA) is the state public university and is the only university in Andorra.
It was established in 1997.
The university provides first-level degrees in nursing, computer science, business administration, and educational sciences, in addition to higher professional education courses.
The only two graduate schools in Andorra are the Nursing School and the School of Computer Science, the latter having a PhD programme.
The geographical complexity of the country as well as the small number of students prevents the University of Andorra from developing a full academic programme, and it serves principally as a centre for virtual studies, connected to Spanish and French universities.
The Virtual Studies Centre ("Centre d'Estudis Virtuals") at the University runs approximately twenty different academic degrees at both undergraduate and postgraduate levels in fields including tourism, law, Catalan philology, humanities, psychology, political sciences, audiovisual communication, telecommunications engineering, and East Asia studies.
The Centre also runs various postgraduate programmes and continuing-education courses for professionals.
Until the 20th century, Andorra had very limited transport links to the outside world, and development of the country was affected by its physical isolation.
Even now, the nearest major airports at Toulouse and Barcelona are both three hours' drive from Andorra.
Andorra has a road network of , of which is unpaved.
The two main roads out of Andorra la Vella are the CG-1 to the Spanish border, and the CG-2 to the French border via the Envalira Tunnel near El Pas de la Casa.
Bus services cover all metropolitan areas and many rural communities, with services on most major routes running half-hourly or more frequently during peak travel times.
There are frequent long-distance bus services from Andorra to Barcelona and Toulouse, plus a daily tour from the former city.
Bus services are mostly run by private companies, but some local ones are operated by the government.
There are no airports for fixed-wing aircraft within Andorra's borders but there are, however, heliports in La Massana (Camí Heliport), Arinsal and Escaldes-Engordany with commercial helicopter services and an airport located in the neighbouring Spanish comarca of Alt Urgell, south of the Andorran-Spanish border.
Since July 2015, Andorra–La Seu d'Urgell Airport has operated commercial flights to Madrid and Palma de Mallorca, and is the main hub for Air Andorra and Andorra Airlines.
As of 11 July 2018, there are no regular commercial flights at the airport.
Nearby airports located in Spain and France provide access to international flights for the principality.
The nearest airports are at Perpignan, France ( from Andorra) and Lleida, Spain ( from Andorra).
The largest nearby airports are at Toulouse, France ( from Andorra) and Barcelona, Spain ( from Andorra).
There are hourly bus services from both Barcelona and Toulouse airports to Andorra.
The nearest railway station is L'Hospitalet-près-l'Andorre east of Andorra which is on the -gauge line from Latour-de-Carol () southeast of Andorra, to Toulouse and on to Paris by the French high-speed trains.
This line is operated by the SNCF.
Latour-de-Carol has a scenic trainline to Villefranche-de-Conflent, as well as the SNCF's gauge line connecting to Perpignan, and the RENFE's -gauge line to Barcelona.
There are also direct Intercités de Nuit trains between L'Hospitalet-près-l'Andorre and Paris on certain dates.
In Andorra, mobile and fixed telephone and internet services are operated exclusively by the Andorran national telecommunications company, SOM, also known as Andorra Telecom (STA).
The same company also manages the technical infrastructure for national broadcasting of digital television and radio.
By the end of 2010, it was planned that every home in the country would have fibre-to-the-home for internet access at a minimum speed of 100 Mbit/s, and the availability was complete in June 2012.
There is only one Andorran television station, "Ràdio i Televisió d'Andorra" (RTVA).
"Radio Nacional d'Andorra" operates two radio stations, "Radio Andorra" and "Andorra Música".
There are three national newspapers, "Diari d'Andorra", "El Periòdic d'Andorra", and "Bondia" as well as several local newspapers.
There is also an amateur radio society.
Additional TV and radio stations from Spain and France are available via digital terrestrial television and IPTV.
The official and historic language is Catalan.
Thus the culture is Catalan, with its own specificity.
Andorra is home to folk dances like the contrapàs and marratxa, which survive in Sant Julià de Lòria especially.
Andorran folk music has similarities to the music of its neighbours, but is especially Catalan in character, especially in the presence of dances such as the sardana.
Other Andorran folk dances include contrapàs in Andorra la Vella and Saint Anne's dance in Escaldes-Engordany.
Andorra's national holiday is Our Lady of Meritxell Day, 8 September.
American folk artist Malvina Reynolds, intrigued by its defence budget of $4.90, wrote a song "Andorra".
Pete Seeger added verses, and sang "Andorra" on his 1962 album "The Bitter and the Sweet".
Andorra is famous for the practice of winter sports.
Popular sports played in Andorra include football, rugby union, basketball and roller hockey.
In roller hockey Andorra usually plays in CERH Euro Cup and in FIRS Roller Hockey World Cup.
In 2011, Andorra was the host country to the 2011 European League Final Eight.
The country is represented in association football by the Andorra national football team.
However, the team has had little success internationally because of Andorra's small population.
Football is governed in Andorra by the Andorran Football Federation - founded in 1994, it organizes the national competitions of association football (Primera Divisió, Copa Constitució and Supercopa) and futsal.
Andorra was admitted to UEFA and FIFA in the same year, 1996.
FC Andorra, a club based in Andorra la Vella founded in 1942, compete in the Spanish football league system.
Rugby is a traditional sport in Andorra, mainly influenced by the popularity in southern France.
The Andorra national rugby union team, nicknamed ""Els Isards"", has impressed on the international stage in rugby union and rugby sevens.
VPC Andorra XV is a rugby team based in Andorra la Vella actually playing in the French championship.
Basketball popularity has increased in the country since the 1990s, when the Andorran team BC Andorra played in the top league of Spain (Liga ACB).
After 18 years the club returned to the top league in 2014.
Other sports practised in Andorra include cycling, volleyball, judo, Australian Rules football, handball, swimming, gymnastics, tennis and motorsports.
In 2012, Andorra raised its first national cricket team and played a home match against the Dutch Fellowship of Fairly Odd Places Cricket Club, the first match played in the history of Andorra at an altitude of .
Andorra first participated at the Olympic Games in 1976.
The country has also appeared in every Winter Olympic Games since 1976.
Andorra competes in the Games of the Small States of Europe being twice the host country in 1991 and 2005.
As part of the Catalan cultural ambit, Andorra is home to a team of castellers, or Catalan human tower builders.
The , based in the town of Santa Coloma d'Andorra, are recognized by the , the governing body of castells.
Ariadna Tudel Cuberes and Sophie Dusautoir Bertrand earned the bronze medal in the women's team competition at the 2009 European Championship of Ski Mountaineering.
Joan Verdu Sanchez earned a bronze medal in Alpine Skiing at the 2012 Winter Youth Olympics.
In 2015, Marc Oliveras earned a silver medal in Alpine Skiing at the 2015 Winter Universiade, while Carmina Pallas earned a silver and a bronze medal in the same competition.
</doc>
<doc id="612" url="https://en.wikipedia.org/wiki?curid=612" title="Arithmetic mean">
Arithmetic mean

In mathematics and statistics, the arithmetic mean (, stress on third syllable of "arithmetic"), or simply the mean or average when the context is clear, is the sum of a collection of numbers divided by the count of numbers in the collection.
The collection is often a set of results of an experiment or an observational study, or frequently a set of results from a survey.
The term "arithmetic mean" is preferred in some contexts in mathematics and statistics because it helps distinguish it from other means, such as the geometric mean and the harmonic mean.
In addition to mathematics and statistics, the arithmetic mean is used frequently in many diverse fields such as economics, anthropology, and history, and it is used in almost every academic field to some extent.
For example, per capita income is the arithmetic average income of a nation's population.
While the arithmetic mean is often used to report central tendencies, it is not a robust statistic, meaning that it is greatly influenced by outliers (values that are very much larger or smaller than most of the values).
Notably, for skewed distributions, such as the distribution of income for which a few people's incomes are substantially greater than most people's, the arithmetic mean may not coincide with one's notion of "middle", and robust statistics, such as the median, may be a better description of central tendency.
The arithmetic mean (or mean or average), formula_1 (read formula_2 "bar"), is the mean of the formula_3 values formula_4.
The arithmetic mean is the most commonly used and readily understood measure of central tendency in a data set.
In statistics, the term average refers to any of the measures of central tendency.
The arithmetic mean of a set of observed data is defined as being equal to the sum of the numerical values of each and every observation divided by the total number of observations.
Symbolically, if we have a data set consisting of the values formula_5, then the arithmetic mean formula_6 is defined by the formula:

(See summation for an explanation of the summation operator).
For example, let us consider the monthly salary of 10 employees of a firm: 2500, 2700, 2400, 2300, 2550, 2650, 2750, 2450, 2600, 2400.
The arithmetic mean is

If the data set is a statistical population (i.e., consists of every possible observation and not just a subset of them), then the mean of that population is called the population mean.
If the data set is a statistical sample (a subset of the population), we call the statistic resulting from this calculation a sample mean.
The arithmetic mean has several properties that make it useful, especially as a measure of central tendency.
These include:


The arithmetic mean may be contrasted with the median.
The median is defined such that no more than half the values are larger than, and no more than half are smaller than, the median.
If elements in the data increase arithmetically, when placed in some order, then the median and arithmetic average are equal.
For example, consider the data sample formula_15.
The average is formula_16, as is the median.
However, when we consider a sample that cannot be arranged so as to increase arithmetically, such as formula_17, the median and arithmetic average can differ significantly.
In this case, the arithmetic average is 6.2 and the median is 4.
In general, the average value can vary significantly from most values in the sample, and can be larger or smaller than most of them.
There are applications of this phenomenon in many fields.
For example, since the 1980s, the median income in the United States has increased more slowly than the arithmetic average of income.
A weighted average, or weighted mean, is an average in which some data points count more heavily than others, in that they are given more weight in the calculation.
For example, the arithmetic mean of formula_18 and formula_19 is formula_20, or equivalently formula_21.
In contrast, a "weighted" mean in which the first number receives, for example, twice as much weight as the second (perhaps because it is assumed to appear twice as often in the general population from which these numbers were sampled) would be calculated as formula_22.
Here the weights, which necessarily sum to the value one, are formula_23 and formula_24, the former being twice the latter.
Note that the arithmetic mean (sometimes called the "unweighted average" or "equally weighted average") can be interpreted as a special case of a weighted average in which all the weights are equal to each other (equal to formula_25 in the above example, and equal to formula_26 in a situation with formula_3 numbers being averaged).
If a numerical property, and any sample of data from it, could take on any value from a continuous range, instead of, for example, just integers, then the probability of a number falling into some range of possible values can be described by integrating a continuous probability distribution across this range, even when the naive probability for a sample number taking one certain value from infinitely many is zero.
The analog of a weighted average in this context, in which there are an infinite number of possibilities for the precise value of the variable in each range, is called the "mean of the probability distribution".
A most widely encountered probability distribution is called the normal distribution; it has the property that all measures of its central tendency, including not just the mean but also the aforementioned median and the mode (the three M's), are equal to each other.
This equality does not hold for other probability distributions, as illustrated for the lognormal distribution here.
Particular care must be taken when using cyclic data, such as phases or angles.
Naïvely taking the arithmetic mean of 1° and 359° yields a result of 180°.
This is incorrect for two reasons:

In general application, such an oversight will lead to the average value artificially moving towards the middle of the numerical range.
A solution to this problem is to use the optimization formulation (viz., define the mean as the central point: the point about which one has the lowest dispersion), and redefine the difference as a modular distance (i.e., the distance on the circle: so the modular distance between 1° and 359° is 2°, not 358°).
The arithmetic mean is often denoted by a bar, for example as in formula_1 (read formula_2 "bar").
Some software (text processors, web browsers) may not display the x̄ symbol properly.
For example, the x̄ symbol in HTML is actually a combination of two codes - the base letter x plus a code for the line above (&#772: or ¯).
In some texts, such as pdfs, the x̄ symbol may be replaced by a cent (¢) symbol (Unicode &#162) when copied to text processor such as Microsoft Word.
</doc>
<doc id="615" url="https://en.wikipedia.org/wiki?curid=615" title="American Football Conference">
American Football Conference

The American Football Conference (AFC) is one of the two conferences of the National Football League (NFL), the highest professional level of American football in the United States.
This conference and its counterpart, the National Football Conference (NFC), currently contain 16 teams each, making up the 32 teams of the NFL.
Both conferences were created as part of the 1970 merger with the rival American Football League (AFL), with all ten of the former AFL teams and three NFL teams forming the AFC, and the remaining thirteen NFL clubs forming the NFC.
A series of league expansions and division realignments have occurred since the merger, thus making the current total of 16 clubs in each conference.
Since 2002, like the NFC, the AFC has 16 teams, organized into four divisions each with four teams: East, North, South and West.
Currently, the thirteen opponents each team faces over the 16-game regular season schedule are set using a pre-determined formula:

Each AFC team plays the other teams in their respective division twice (home and away) during the regular season, in addition to 10 other games assigned to their schedule by the NFL.
Two of these games are assigned on the basis of a particular team's final divisional standing from the previous season.
The remaining 8 games are split between the roster of two other NFL divisions.
This assignment shifts each year and will follow a standard cycle.
Using the 2012 regular season schedule as an example, each team in the AFC West plays against every team in the AFC North and NFC South.
In this way, non-divisional competition will be mostly among common opponents – the exception being the two games assigned based on the team's prior-season divisional standing.
At the end of each season, the winner of each division, in addition to the two remaining conference teams with the highest regular season records, proceed into the playoffs.
These teams consist of the four division winners and the top two wild card teams.
The AFC playoffs culminate in the AFC Championship Game with the winner receiving the Lamar Hunt Trophy.
The AFC Champion then plays the NFC Champion in the Super Bowl.
Both the AFC and the NFC were created after the NFL merged with the American Football League (AFL) in 1970.
The AFL began play in 1960 with eight teams, and added two more expansion clubs (the Miami Dolphins in 1966 and the Cincinnati Bengals in 1968) before the merger.
In order to equalize the number of teams in each conference, three NFL teams that predated the AFL's launch (the Cleveland Browns, Pittsburgh Steelers, and the then-Baltimore Colts) joined the ten former AFL teams to form the AFC.
The two AFL divisions AFL East and AFL West were more or less intact, while the Century Division, in which the Browns and the Steelers had played since 1967, was moved from the NFL to become the new AFC Central.
Upon the completion of the merger of the AFL and NFL in 1970, the newly-minted American Football Conference had already agreed upon their divisionalset up for the 1970 season; the National Foorball Conference, however, could not agree upon their setup, and one was chosen from a fishbowl on January 16, 1970.
Since the merger, five expansion teams have joined the AFC and two have left, thus making the current total 16.
When the Seattle Seahawks and the Tampa Bay Buccaneers joined the league in 1976, they were temporarily placed in the NFC and AFC respectively.
This arrangement lasted for one season only before the two teams switched conferences.
The Seahawks eventually returned to the NFC as a result of the 2002 realignment.
The expansion Jacksonville Jaguars joined the AFC in 1995.
There have been five teams that have relocated at least once.
In 1984, the Baltimore Colts relocated to Indianapolis in 1984.
In 1995, the Cleveland Browns had attempted to move to Baltimore; the resulting dispute between Cleveland and the team led to Modell establishing the Baltimore Ravens with the players and personnel from the Browns, while the Browns were placed in suspended operations before they were reinstated by the NFL.
The Ravens were treated as an expansion team.
In California, the Oakland Raiders relocated to Los Angeles in 1982, and back to Oakland in 1995, while the San Diego Chargers moved to Los Angeles in 2017.
The Houston Oilers moved to Tennessee in 1997, where they were renamed the Tennessee Oilers.
The team would change its name again, two years later, to the Tennessee Titans.
The NFL would again expand in 2002, adding the Houston Texans to the AFC.
With the exception of the aforementioned relocations since that time, the divisional setup has remained static ever since.
Between 1995 and 2017, the AFC has sent less than half of the 16 AFC teams to the Super Bowl with only 7 of the 16 individual teams making it.
New England Patriots (9 times), Denver Broncos (4 times), Pittsburgh Steelers (4 times), Baltimore Ravens (2 times), Indianapolis Colts (2 times), Oakland Raiders (1 time), and Tennessee Titans (1 time).
By contrast, the NFC has sent 13 of the 16 NFC teams during that same time frame with only the Detroit Lions, Minnesota Vikings, and Washington Redskins missing out on an appearance in the Super Bowl.
15 of the last 17 AFC champions have started one of just three quarterbacks - Tom Brady, Peyton Manning, and Ben Roethlisberger - in the Super Bowl.
The merged league created a new logo for the AFC that took elements of the old AFL logo, specifically the "A" and the six stars surrounding it.
The AFC logo basically remained unchanged from 1970 to 2009.
The 2010 NFL season introduced an updated AFC logo, with the most notable revision being the removal of two stars (leaving four representing the four divisions of the AFC), and moving the stars inside the letter, similar to the NFC logo.
</doc>
<doc id="620" url="https://en.wikipedia.org/wiki?curid=620" title="Animal Farm">
Animal Farm

Animal Farm is an allegorical novella by George Orwell, first published in England on 17 August 1945.
According to Orwell, the book reflects events leading up to the Russian Revolution of 1917 and then on into the Stalinist era of the Soviet Union.
Orwell, a democratic socialist, was a critic of Joseph Stalin and hostile to Moscow-directed Stalinism, an attitude that was critically shaped by his experiences during the Spanish Civil War.
The Soviet Union, he believed, had become a brutal dictatorship, built upon a cult of personality and enforced by a reign of terror.
In a letter to Yvonne Davet, Orwell described "Animal Farm" as a satirical tale against Stalin (""""), and in his essay "Why I Write" (1946), wrote that "Animal Farm" was the first book in which he tried, with full consciousness of what he was doing, "to fuse political purpose and artistic purpose into one whole".
The original title was "Animal Farm: A Fairy Story," but U.S.
publishers dropped the subtitle when it was published in 1946, and only one of the translations during Orwell's lifetime kept it.
Other titular variations include subtitles like "A Satire" and "A Contemporary Satire".
Orwell suggested the title ' for the French translation, which abbreviates to URSA, the Latin word for "bear", a symbol of Russia.
It also played on the French name of the Soviet Union, '.
Orwell wrote the book between November 1943 and February 1944, when the UK was in its wartime alliance with the Soviet Union and the British people and intelligentsia held Stalin in high esteem, a phenomenon Orwell hated.
The manuscript was initially rejected by a number of British and American publishers, including one of Orwell's own, Victor Gollancz, which delayed its publication.
It became a great commercial success when it did appear partly because international relations were transformed as the wartime alliance gave way to the Cold War.
"Time" magazine chose the book as one of the 100 best English-language novels (1923 to 2005); it also featured at number 31 on the Modern Library List of Best 20th-Century Novels.
It won a Retrospective Hugo Award in 1996 and is included in the Great Books of the Western World selection.
Old Major, the old boar on the Manor Farm, summons the animals on the farm together for a meeting, during which he refers to humans as "enemies" and teaches the animals a revolutionary song called "Beasts of England".
When Major dies, two young pigs, Snowball and Napoleon, assume command and consider it a duty to prepare for the Rebellion.
The animals revolt, driving the drunken, irresponsible farmer Mr. Jones, as well as Mrs. Jones and the other human caretakers and employees, off the farm, renaming it "Animal Farm".
They adopt the Seven Commandments of Animalism, the most important of which is, "All animals are equal".
The decree is painted in large letters on one side of the barn.
Snowball teaches the animals to read and write, while Napoleon educates young puppies on the principles of Animalism.
Food is plentiful, and the farm runs smoothly.
The pigs elevate themselves to positions of leadership and set aside special food items, ostensibly for their personal health.
Some time later, several men attack Animal Farm.
Jones and his men are making an attempt to recapture the farm, aided by several other farmers who are terrified of similar animal revolts.
Snowball and the animals, who are hiding in ambush, defeat the men by launching a surprise attack as soon as they enter the farmyard.
Snowball's popularity soars, and this event is proclaimed "The Battle of the Cowshed".
It is celebrated annually with the firing of a gun, on the anniversary of the Revolution.
Napoleon and Snowball vie for pre-eminence.
When Snowball announces his plans to modernize the farm by building a windmill, Napoleon has his dogs chase Snowball away and declares himself leader.
Napoleon enacts changes to the governance structure of the farm, replacing meetings with a committee of pigs who will run the farm.
Through a young pig named Squealer, Napoleon claims credit for the windmill idea.
The animals work harder with the promise of easier lives with the windmill.
When the animals find the windmill collapsed after a violent storm, Napoleon and Squealer convince the animals that Snowball is trying to sabotage their project.
Once Snowball becomes a scapegoat, Napoleon begins to purge the farm with his dogs, killing animals he accuses of consorting with his old rival.
When some animals recall the Battle of the Cowshed, Napoleon (who was nowhere to be found during the battle) frequently smears Snowball as a collaborator of Farmer Jones', while falsely representing himself as the hero of the battle.
"Beasts of England" is replaced with an anthem glorifying Napoleon, who appears to be adopting the lifestyle of a man.
The animals remain convinced that they are better off than they were under Mr. Jones.
Mr. Frederick, a neighbouring farmer, attacks the farm, using blasting powder to blow up the restored windmill.
Although the animals win the battle, they do so at great cost, as many, including Boxer, the workhorse, are wounded.
Despite his injuries, Boxer continues working harder and harder, until he collapses while working on the windmill.
Napoleon sends for a van to purportedly take Boxer to a veterinary surgeon, explaining that better care can be given there.
Benjamin, the cynical donkey who "could read as well as any pig", notices that the van belongs to a knacker and attempts a futile rescue.
Squealer quickly assures the animals that the van had been purchased from the knacker by an animal hospital, and the previous owner's signboard had not been repainted.
In a subsequent report, Squealer reports sadly to the animals that Boxer died peacefully at the animal hospital.
The pigs hold a festival one day after Boxer's death to further praise the glories of Animal Farm and have the animals work harder by taking on Boxer's ways.
However, the truth was that Napoleon had engineered the sale of Boxer to the knacker, allowing Napoleon and his inner circle to acquire money to buy whisky for themselves.
(In 1940s England, one way for farms to make money was to sell large animals to a knacker, who would kill the animal and boil its remains into animal glue.)
Years pass, the windmill is rebuilt, and another windmill is constructed, which makes the farm a good amount of income.
However, the ideals which Snowball discussed, including stalls with electric lighting, heating, and running water are forgotten, with Napoleon advocating that the happiest animals live simple lives.
In addition to Boxer, many of the animals who participated in the Revolution are dead, as is Farmer Jones, who died in another part of England.
The pigs start to resemble humans, as they walk upright, carry whips, and wear clothes.
The Seven Commandments are abridged to a single phrase: "All animals are equal, but some animals are more equal than others."
Napoleon holds a dinner party for the pigs and local farmers, with whom he celebrates a new alliance.
He abolishes the practice of the revolutionary traditions and restores the name "The Manor Farm".
As the animals outside gaze at the scene and look from pig to man, and from man to pig, and from pig to man again, they can no longer distinguish between the two.
George Orwell wrote the manuscript in 1943 and 1944 subsequent to his experiences during the Spanish Civil War, which he described in "Homage to Catalonia" (1938).
In the preface of a 1947 Ukrainian edition of "Animal Farm", he explained how escaping the communist purges in Spain taught him "how easily totalitarian propaganda can control the opinion of enlightened people in democratic countries".
This motivated Orwell to expose and strongly condemn what he saw as the Stalinist corruption of the original socialist ideals.
Immediately prior to writing the book, Orwell had quit the BBC.
He was also upset about a booklet for propagandists the Ministry of Information had put out.
The booklet included instructions on how to quell ideological fears of the Soviet Union, such as directions to claim that the Red Terror was a figment of Nazi imagination.
In the preface, Orwell also described the source of the idea of setting the book on a farm:
Orwell initially encountered difficulty getting the manuscript published, largely due to fears that the book might upset the alliance between Britain, the United States, and the Soviet Union.
Four publishers refused; one had initially accepted the work but declined it after consulting the Ministry of Information.
Eventually, Secker and Warburg published the first edition in 1945.
During the Second World War, it became clear to Orwell that anti-Soviet literature was not something which most major publishing houses would touch—including his regular publisher Gollancz.
He also submitted the manuscript to Faber and Faber, where the poet T. S. Eliot (who was a director of the firm) rejected it; Eliot wrote back to Orwell praising the book's "good writing" and "fundamental integrity", but declared that they would only accept it for publication if they had some sympathy for the viewpoint "which I take to be generally Trotskyite".
Eliot said he found the view "not convincing", and contended that the pigs were made out to be the best to run the farm; he posited that someone might argue "what was needed... was not more communism but more public-spirited pigs".
Orwell let André Deutsch, who was working for Nicholson & Watson in 1944, read the typescript, and Deutsch was convinced that Nicholson & Watson would want to publish it; however, they did not, and "lectured Orwell on what they perceived to be errors in "Animal Farm"."
In his "London Letter" on 17 April 1944 for "Partisan Review", Orwell wrote that it was "now next door to impossible to get anything overtly anti-Russian printed.
Anti-Russian books do appear, but mostly from Catholic publishing firms and always from a religious or frankly reactionary angle."
The publisher Jonathan Cape, who had initially accepted "Animal Farm", subsequently rejected the book after an official at the British Ministry of Information warned him off—although the civil servant who it is assumed gave the order was later found to be a Soviet spy.
Writing to Leonard Moore, a partner in the literary agency of Christy & Moore, publisher Jonathan Cape explained that the decision had been taken on the advice of a senior official in the Ministry of Information.
Such flagrant anti-Soviet bias was unacceptable, and the choice of pigs as the dominant class was thought to be especially offensive.
It may reasonably be assumed that the 'important official' was a man named Peter Smollett, who was later unmasked as a Soviet agent.
Orwell was suspicious of Smollett/Smolka, and he would be one of the names Orwell included in his list of Crypto-Communists and Fellow-Travellers sent to the Information Research Department in 1949.
Born Hans Peter Smolka in Vienna in 1912, he came to Britain in 1933 as an NKVD agent with the codename 'Abo', became a naturalised British subject in 1938, changed his name, and after the outbreak of World War II joined the Ministry of Information where he organised pro-Soviet propaganda, working with Kim Philby in 1943–45.
Smollett's family have rejected the accusation that he was a spy.
The publisher wrote to Orwell, saying:
Frederic Warburg also faced pressures against publication, even from people in his own office and from his wife Pamela, who felt that it was not the moment for ingratitude towards Stalin and the heroic Red Army, which had played a major part in defeating Hitler.
A Russian translation was printed in the paper "Posev", and in giving permission for a Russian translation of "Animal Farm", Orwell refused in advance all royalties.
A translation in Ukrainian, which was produced in Germany, was confiscated in large part by the American wartime authorities and handed over to the Soviet repatriation commission.
In October 1945, Orwell wrote to Frederic Warburg expressing interest in pursuing the possibility that the political cartoonist David Low might illustrate "Animal Farm".
Low had written a letter saying that he had had "a good time with "ANIMAL FARM"—an excellent bit of satire—it would illustrate perfectly."
Nothing came of this, and a trial issue produced by Secker & Warburg in 1956 illustrated by John Driver was abandoned, but the Folio Society published an edition in 1984 illustrated by Quentin Blake and an edition illustrated by the cartoonist Ralph Steadman was published by Secker & Warburg in 1995 to celebrate the fiftieth anniversary of the first edition of "Animal Farm".
Orwell originally wrote a preface complaining about British self-censorship and how the British people were suppressing criticism of the USSR, their World War II ally:

Although the first edition allowed space for the preface, it was not included, and as of June 2009 most editions of the book have not included it.
Secker and Warburg published the first edition of "Animal Farm" in 1945 without an introduction.
However, the publisher had provided space for a preface in the author's proof composited from the manuscript.
For reasons unknown, no preface was supplied, and the page numbers had to be renumbered at the last minute.
In 1972, Ian Angus found the original typescript titled "The Freedom of the Press", and Bernard Crick published it, together with his own introduction, in "The Times Literary Supplement" on 15 September 1972 as "How the essay came to be written".
Orwell's essay criticised British self-censorship by the press, specifically the suppression of unflattering descriptions of Stalin and the Soviet government.
The same essay also appeared in the Italian 1976 edition of "Animal Farm" with another introduction by Crick, claiming to be the first edition with the preface.
Other publishers were still declining to publish it.
Contemporary reviews of the work were not universally positive.
Writing in the American "New Republic" magazine, George Soule expressed his disappointment in the book, writing that it "puzzled and saddened me.
It seemed on the whole dull.
The allegory turned out to be a creaking machine for saying in a clumsy way things that have been said better directly."
Soule believed that the animals were not consistent enough with their real world inspirations, and said, "It seems to me that the failure of this book (commercially it is already assured of tremendous success) arises from the fact that the satire deals not with something the author has experienced, but rather with stereotyped ideas about a country which he probably does not know very well".
"The Guardian" on 24 August 1945 called "Animal Farm" "a delightfully humorous and caustic satire on the rule of the many by the few".
Tosco Fyvel, writing in "Tribune" on the same day, called the book "a gentle satire on a certain State and on the illusions of an age which may already be behind us."
Julian Symons responded, on 7 September, "Should we not expect, in "Tribune" at least, acknowledgement of the fact that it is a satire not at all gentle upon a particular State—Soviet Russia?
It seems to me that a reviewer should have the courage to identify Napoleon with Stalin, and Snowball with Trotsky, and express an opinion favourable or unfavourable to the author, upon a political ground.
In a hundred years time perhaps, "Animal Farm" may be simply a fairy story, today it is a political satire with a good deal of point."
"Animal Farm" has been subject to much comment in the decades since these early remarks.
The pigs Snowball, Napoleon, and Squealer adapt Old Major's ideas into "a complete system of thought", which they formally name Animalism, an allegoric reference to Communism, not to be confused with the philosophy Animalism.
Soon after, Napoleon and Squealer partake in activities associated with the humans (drinking alcohol, sleeping in beds, trading), which were explicitly prohibited by the Seven Commandments.
Squealer is employed to alter the Seven Commandments to account for this humanisation, an allusion to the Soviet government's revising of history in order to exercise control of the people's beliefs about themselves and their society.
The original commandments are:

These commandments are also distilled into the maxim "Four legs good, two legs bad!"
which is primarily used by the sheep on the farm, often to disrupt discussions and disagreements between animals on the nature of Animalism.
Later, Napoleon and his pigs secretly revise some commandments to clear themselves of accusations of law-breaking.
The changed commandments are as follows, with the changes bolded:

Eventually, these are replaced with the maxims, "All animals are equal, but some animals are more equal than others", and "Four legs good, two legs better!"
as the pigs become more human.
This is an ironic twist to the original purpose of the Seven Commandments, which were supposed to keep order within Animal Farm by uniting the animals together against the humans and preventing animals from following the humans' evil habits.
Through the revision of the commandments, Orwell demonstrates how simply political dogma can be turned into malleable propaganda.
Orwell biographer Jeffrey Meyers has written, "virtually every detail has political significance in this allegory."
Orwell himself wrote in 1946, "Of course I intended it primarily as a satire on the Russian revolution..[and] "that kind" of revolution (violent conspiratorial revolution, led by unconsciously power hungry people) can only lead to a change of masters [-] revolutions only effect a radical improvement when the masses are alert."
In a preface for a 1947 Ukrainian edition, he stated, "... for the past ten years I have been convinced that the destruction of the Soviet myth was essential if we wanted a revival of the socialist movement.
On my return from Spain [in 1937] I thought of exposing the Soviet myth in a story that could be easily understood by almost anyone and which could be easily translated into other languages."
The revolt of the animals against Farmer Jones is Orwell's analogy with the October 1917 Bolshevik Revolution.
The "Battle of the Cowshed" has been said to represent the allied invasion of Soviet Russia in 1918, and the defeat of the White Russians in the Russian Civil War.
The pigs' rise to pre-eminence mirrors the rise of a Stalinist bureaucracy in the USSR, just as Napoleon's emergence as the farm's sole leader reflects Stalin's emergence.
The pigs' appropriation of milk and apples for their own use, "the turning point of the story" as Orwell termed it in a letter to Dwight Macdonald, stands as an analogy for the crushing of the left-wing 1921 Kronstadt revolt against the Bolsheviks, and the difficult efforts of the animals to build the windmill suggest the various Five Year Plans.
The puppies controlled by Napoleon parallel the nurture of the secret police in the Stalinist structure, and the pigs' treatment of the other animals on the farm recalls the internal terror faced by the populace in the 1930s.
In chapter seven, when the animals confess their nonexistent crimes and are killed, Orwell directly alludes to the purges, confessions and show trials of the late 1930s.
These contributed to Orwell's conviction that the Bolshevik revolution had been corrupted and the Soviet system become rotten.
Peter Edgerly Firchow and Peter Davison consider that the "Battle of the Windmill" represents the Great Patriotic War (World War II), especially the Battle of Stalingrad and the Battle of Moscow.
During the battle, Orwell first wrote, "All the animals, including Napoleon" took cover.
Orwell had the publisher alter this to "All the animals except Napoleon" in recognition of Stalin's decision to remain in Moscow during the German advance.
Orwell requested the change after he met Joseph Czapski in Paris in March 1945.
Czapski, a survivor of the Katyn Massacre and an opponent of the Soviet regime, told Orwell, as Orwell wrote to Arthur Koestler, that it had been "the character [and] greatness of Stalin" that saved Russia from the German invasion.
Other connections that writers have suggested illustrate Orwell's telescoping of Russian history from 1917 to 1943 include the wave of rebelliousness that ran through the countryside after the Rebellion, which stands for the abortive revolutions in Hungary and in Germany (Ch IV); the conflict between Napoleon and Snowball (Ch V), paralleling "the two rival and quasi-Messianic beliefs that seemed pitted against one another: Trotskyism, with its faith in the revolutionary vocation of the proletariat of the West; and Stalinism with its glorification of Russia's socialist destiny"; Napoleon's dealings with Whymper and the Willingdon markets (Ch VI), paralleling the Treaty of Rapallo; and Frederick's forged bank notes, paralleling the Hitler-Stalin pact of August 1939, after which Frederick attacks Animal Farm without warning and destroys the windmill.
The book's close, with the pigs and men in a kind of rapprochement, reflected Orwell's view of the 1943 Teheran Conference that seemed to display the establishment of "the best possible relations between the USSR and the West"—but in reality were destined, as Orwell presciently predicted, to continue to unravel.
The disagreement between the allies and the start of the Cold War is suggested when Napoleon and Pilkington, both suspicious, "played an ace of spades simultaneously".
Similarly, the music in the novel, starting with "Beasts of England" and the later anthems, parallels "The Internationale" and its adoption and repudiation by the Soviet authorities as the Anthem of the USSR in the 1920s and 1930s.
In addition to the book's political symbolism, some critics have argued that "Animal Farm" can also be read as a more straightforward story about farm animals, reflecting Orwell's concern for the treatment of animals.
Critics supporting such readings, beginning in the 1970s with Marxist scholar Raymond Williams and later including Jeffrey Moussaieff Masson and Helen Tiffin, cite Orwell's description of his inspiration for setting the story on a farm, in which he writes that, "from the animals’ point of view,"

"Animal Farm" has been adapted to film twice.
Both differ from the novel and have been accused of taking significant liberties, including sanitising some aspects.
In 2012, a HFR-3D version of "Animal Farm", potentially directed by Andy Serkis, was announced.
A BBC radio version, produced by Rayner Heppenstall, was broadcast in January 1947.
Orwell listened to the production at his home in Canonbury Square, London, with Hugh Gordon Porteous, amongst others.
Orwell later wrote to Heppenstall that Porteous, "who had not read the book, grasped what was happening after a few minutes."
A further radio production, again using Orwell's own dramatisation of the book, was broadcast in January 2013 on BBC Radio 4.
Tamsin Greig narrated, and the cast included Nicky Henson as Napoleon, Toby Jones as the propagandist Squealer, and Ralph Ineson as Boxer.
A theatrical version, with music by Richard Peaslee and lyrics by Adrian Mitchell, was staged at the National Theatre London on 25 April 1984, directed by Peter Hall.
It toured nine cities in 1985.
A solo version, adapted and performed by Guy Masterson, premièred at the Traverse Theatre Edinburgh in January 1995 and has toured worldwide since.
In 1950 Norman Pett and his writing partner Don Freeman were secretly hired by the British Foreign Office to adapt "Animal Farm" into a comic strip.
This comic was not published in the U.K., but ran in Brazilian and Burmese newspapers.
A video game adaptation of "Animal Farm" was announced in August 2017.
Fully authorised by the estate of George Orwell, "Animal Farm" is created by an independent team formed specifically to deliver Orwell's vision in an interactive format.
On 17 July 2009, Amazon.com withdrew certain Amazon Kindle titles, including "Animal Farm" and "Nineteen Eighty-Four" by George Orwell, from sale, refunded buyers, and remotely deleted items from purchasers' devices after discovering that the publisher lacked rights to publish the titles in question.
Notes and annotations for the books made by users on their devices were also deleted.
After the move prompted outcry and comparisons to "Nineteen Eighty-Four" itself, Amazon spokesman Drew Herdener stated that the company is "[c]hanging our systems so that in the future we will not remove books from customers' devices in these circumstances."
</doc>
<doc id="621" url="https://en.wikipedia.org/wiki?curid=621" title="Amphibian">
Amphibian

Amphibians are ectothermic, tetrapod vertebrates of the class Amphibia.
Modern amphibians are all Lissamphibia.
They inhabit a wide variety of habitats, with most species living within terrestrial, fossorial, arboreal or freshwater aquatic ecosystems.
Thus amphibians typically start out as larvae living in water, but some species have developed behavioural adaptations to bypass this.
The young generally undergo metamorphosis from larva with gills to an adult air-breathing form with lungs.
Amphibians use their skin as a secondary respiratory surface and some small terrestrial salamanders and frogs lack lungs and rely entirely on their skin.
They are superficially similar to lizards but, along with mammals and birds, reptiles are amniotes and do not require water bodies in which to breed.
With their complex reproductive needs and permeable skins, amphibians are often ecological indicators; in recent decades there has been a dramatic decline in amphibian populations for many species around the globe.
The earliest amphibians evolved in the Devonian period from sarcopterygian fish with lungs and bony-limbed fins, features that were helpful in adapting to dry land.
They diversified and became dominant during the Carboniferous and Permian periods, but were later displaced by reptiles and other vertebrates.
Over time, amphibians shrank in size and decreased in diversity, leaving only the modern subclass Lissamphibia.
The three modern orders of amphibians are Anura (the frogs and toads), Urodela (the salamanders), and Apoda (the caecilians).
The number of known amphibian species is approximately 7,000, of which nearly 90% are frogs.
The smallest amphibian (and vertebrate) in the world is a frog from New Guinea ("Paedophryne amauensis") with a length of just .
The largest living amphibian is the Chinese giant salamander ("Andrias davidianus"), but this is dwarfed by the extinct "Prionosuchus" from the middle Permian of Brazil.
The study of amphibians is called batrachology, while the study of both reptiles and amphibians is called herpetology.
The word "amphibian" is derived from the Ancient Greek term ἀμφίβιος ("amphíbios"), which means "both kinds of life", "ἀμφί" meaning "of both kinds" and "βιος" meaning "life".
The term was initially used as a general adjective for animals that could live on land or in water, including seals and otters.
Traditionally, the class Amphibia includes all tetrapod vertebrates that are not amniotes.
Amphibia in its widest sense ("sensu lato") was divided into three subclasses, two of which are extinct:
The actual number of species in each group depends on the taxonomic classification followed.
The two most common systems are the classification adopted by the website AmphibiaWeb, University of California, Berkeley and the classification by herpetologist Darrel Frost and the American Museum of Natural History, available as the online reference database "Amphibian Species of the World".
The numbers of species cited above follows Frost and the total number of known amphibian species is over 7,000, of which nearly 90% are frogs.
With the phylogenetic classification, the taxon Labyrinthodontia has been discarded as it is a polyparaphyletic group without unique defining features apart from shared primitive characteristics.
Classification varies according to the preferred phylogeny of the author and whether they use a stem-based or a node-based classification.
Traditionally, amphibians as a class are defined as all tetrapods with a larval stage, while the group that includes the common ancestors of all living amphibians (frogs, salamanders and caecilians) and all their descendants is called Lissamphibia.
The phylogeny of Paleozoic amphibians is uncertain, and Lissamphibia may possibly fall within extinct groups, like the Temnospondyli (traditionally placed in the subclass Labyrinthodontia) or the Lepospondyli, and in some analyses even in the amniotes.
This means that advocates of phylogenetic nomenclature have removed a large number of basal Devonian and Carboniferous amphibian-type tetrapod groups that were formerly placed in Amphibia in Linnaean taxonomy, and included them elsewhere under cladistic taxonomy.
If the common ancestor of amphibians and amniotes is included in Amphibia, it becomes a paraphyletic group.
All modern amphibians are included in the subclass Lissamphibia, which is usually considered a clade, a group of species that have evolved from a common ancestor.
The three modern orders are Anura (the frogs and toads), Caudata (or Urodela, the salamanders), and Gymnophiona (or Apoda, the caecilians).
It has been suggested that salamanders arose separately from a Temnospondyl-like ancestor, and even that caecilians are the sister group of the advanced reptiliomorph amphibians, and thus of amniotes.
Although the fossils of several older proto-frogs with primitive characteristics are known, the oldest "true frog" is "Prosalirus bitis", from the Early Jurassic Kayenta Formation of Arizona.
It is anatomically very similar to modern frogs.
The oldest known caecilian is another Early Jurassic species, "Eocaecilia micropodia", also from Arizona.
The earliest salamander is "Beiyanerpeton jianpingensis" from the Late Jurassic of northeastern China.
Authorities disagree as to whether Salientia is a superorder that includes the order Anura, or whether Anura is a sub-order of the order Salientia.
The Lissamphibia are traditionally divided into three orders, but an extinct salamander-like family, the Albanerpetontidae, is now considered part of Lissamphibia alongside the superorder Salientia.
Furthermore, Salientia includes all three recent orders plus the Triassic proto-frog, "Triadobatrachus".
The first major groups of amphibians developed in the Devonian period, around 370 million years ago, from lobe-finned fish which were similar to the modern coelacanth and lungfish.
These ancient lobe-finned fish had evolved multi-jointed leg-like fins with digits that enabled them to crawl along the sea bottom.
Some fish had developed primitive lungs to help them breathe air when the stagnant pools of the Devonian swamps were low in oxygen.
They could also use their strong fins to hoist themselves out of the water and onto dry land if circumstances so required.
Eventually, their bony fins would evolve into limbs and they would become the ancestors to all tetrapods, including modern amphibians, reptiles, birds, and mammals.
Despite being able to crawl on land, many of these prehistoric tetrapodomorph fish still spent most of their time in the water.
They had started to develop lungs, but still breathed predominantly with gills.
Many examples of species showing transitional features have been discovered.
"Ichthyostega" was one of the first primitive amphibians, with nostrils and more efficient lungs.
It had four sturdy limbs, a neck, a tail with fins and a skull very similar to that of the lobe-finned fish, "Eusthenopteron".
Amphibians evolved adaptations that allowed them to stay out of the water for longer periods.
Their lungs improved and their skeletons became heavier and stronger, better able to support the weight of their bodies on land.
They developed "hands" and "feet" with five or more digits; the skin became more capable of retaining body fluids and resisting desiccation.
The fish's hyomandibula bone in the hyoid region behind the gills diminished in size and became the stapes of the amphibian ear, an adaptation necessary for hearing on dry land.
An affinity between the amphibians and the teleost fish is the multi-folded structure of the teeth and the paired supra-occipital bones at the back of the head, neither of these features being found elsewhere in the animal kingdom.
At the end of the Devonian period (360 million years ago), the seas, rivers and lakes were teeming with life while the land was the realm of early plants and devoid of vertebrates, though some, such as "Ichthyostega", may have sometimes hauled themselves out of the water.
It is thought they may have propelled themselves with their forelimbs, dragging their hindquarters in a similar manner to that used by the elephant seal.
In the early Carboniferous (360 to 345 million years ago), the climate became wet and warm.
Extensive swamps developed with mosses, ferns, horsetails and calamites.
Air-breathing arthropods evolved and invaded the land where they provided food for the carnivorous amphibians that began to adapt to the terrestrial environment.
There were no other tetrapods on the land and the amphibians were at the top of the food chain, occupying the ecological position currently held by the crocodile.
Though equipped with limbs and the ability to breathe air, most still had a long tapering body and strong tail.
They were the top land predators, sometimes reaching several metres in length, preying on the large insects of the period and the many types of fish in the water.
They still needed to return to water to lay their shell-less eggs, and even most modern amphibians have a fully aquatic larval stage with gills like their fish ancestors.
It was the development of the amniotic egg, which prevents the developing embryo from drying out, that enabled the reptiles to reproduce on land and which led to their dominance in the period that followed.
After the Carboniferous rainforest collapse amphibian dominance gave way to reptiles, and amphibians were further devastated by the Permian–Triassic extinction event.
During the Triassic Period (250 to 200 million years ago), the reptiles continued to out-compete the amphibians, leading to a reduction in both the amphibians' size and their importance in the biosphere.
According to the fossil record, Lissamphibia, which includes all modern amphibians and is the only surviving lineage, may have branched off from the extinct groups Temnospondyli and Lepospondyli at some period between the Late Carboniferous and the Early Triassic.
The relative scarcity of fossil evidence precludes precise dating, but the most recent molecular study, based on multilocus sequence typing, suggests a Late Carboniferous/Early Permian origin for extant amphibians.
The origins and evolutionary relationships between the three main groups of amphibians is a matter of debate.
A 2005 molecular phylogeny, based on rDNA analysis, suggests that salamanders and caecilians are more closely related to each other than they are to frogs.
It also appears that the divergence of the three groups took place in the Paleozoic or early Mesozoic (around 250 million years ago), before the breakup of the supercontinent Pangaea and soon after their divergence from the lobe-finned fish.
The briefness of this period, and the swiftness with which radiation took place, would help account for the relative scarcity of primitive amphibian fossils.
There are large gaps in the fossil record, but the discovery of a Gerobatrachus hottoni from the Early Permian in Texas in 2008 provided a missing link with many of the characteristics of modern frogs.
Molecular analysis suggests that the frog–salamander divergence took place considerably earlier than the palaeontological evidence indicates.
Newer research indicates that the common ancestor of all Lissamphibians lived about 315 million years ago, and that stereospondyls are the closest relatives to the caecilians.
As they evolved from lunged fish, amphibians had to make certain adaptations for living on land, including the need to develop new means of locomotion.
In the water, the sideways thrusts of their tails had propelled them forward, but on land, quite different mechanisms were required.
Their vertebral columns, limbs, limb girdles and musculature needed to be strong enough to raise them off the ground for locomotion and feeding.
Terrestrial adults discarded their lateral line systems and adapted their sensory systems to receive stimuli via the medium of the air.
They needed to develop new methods to regulate their body heat to cope with fluctuations in ambient temperature.
They developed behaviours suitable for reproduction in a terrestrial environment.
Their skins were exposed to harmful ultraviolet rays that had previously been absorbed by the water.
The skin changed to become more protective and prevent excessive water loss.
The superclass Tetrapoda is divided into four classes of vertebrate animals with four limbs.
Reptiles, birds and mammals are amniotes, the eggs of which are either laid or carried by the female and are surrounded by several membranes, some of which are impervious.
Lacking these membranes, amphibians require water bodies for reproduction, although some species have developed various strategies for protecting or bypassing the vulnerable aquatic larval stage.
They are not found in the sea with the exception of one or two frogs that live in brackish water in mangrove swamps; the Anderson's salamander meanwhile occurs in brackish or salt water lakes.
On land, amphibians are restricted to moist habitats because of the need to keep their skin damp.
The smallest amphibian (and vertebrate) in the world is a microhylid frog from New Guinea ("Paedophryne amauensis") first discovered in 2012.
It has an average length of and is part of a genus that contains four of the world's ten smallest frog species.
The largest living amphibian is the Chinese giant salamander ("Andrias davidianus") but this is a great deal smaller than the largest amphibian that ever existed—the extinct "Prionosuchus", a crocodile-like temnospondyl dating to 270 million years ago from the middle Permian of Brazil!
The largest frog is the African Goliath frog ("Conraua goliath"), which can reach and weigh .
Amphibians are ectothermic (cold-blooded) vertebrates that do not maintain their body temperature through internal physiological processes.
Their metabolic rate is low and as a result, their food and energy requirements are limited.
In the adult state, they have tear ducts and movable eyelids, and most species have ears that can detect airborne or ground vibrations.
They have muscular tongues, which in many species can be protruded.
Modern amphibians have fully ossified vertebrae with articular processes.
Their ribs are usually short and may be fused to the vertebrae.
Their skulls are mostly broad and short, and are often incompletely ossified.
Their skin contains little keratin and lacks scales, apart from a few fish-like scales in certain caecilians.
The skin contains many mucous glands and in some species, poison glands (a type of granular gland).
The hearts of amphibians have three chambers, two atria and one ventricle.
They have a urinary bladder and nitrogenous waste products are excreted primarily as urea.
Most amphibians lay their eggs in water and have aquatic larvae that undergo metamorphosis to become terrestrial adults.
Amphibians breathe by means of a pump action in which air is first drawn into the buccopharyngeal region through the nostrils.
These are then closed and the air is forced into the lungs by contraction of the throat.
They supplement this with gas exchange through the skin.
The order Anura (from the Ancient Greek "a(n)-" meaning "without" and "oura" meaning "tail") comprises the frogs and toads.
They usually have long hind limbs that fold underneath them, shorter forelimbs, webbed toes with no claws, no tails, large eyes and glandular moist skin.
Members of this order with smooth skins are commonly referred to as frogs, while those with warty skins are known as toads.
The difference is not a formal one taxonomically and there are numerous exceptions to this rule.
Members of the family Bufonidae are known as the "true toads".
Frogs range in size from the Goliath frog ("Conraua goliath") of West Africa to the "Paedophryne amauensis", first described in Papua New Guinea in 2012, which is also the smallest known vertebrate.
Although most species are associated with water and damp habitats, some are specialised to live in trees or in deserts.
They are found worldwide except for polar areas.
Anura is divided into three suborders that are broadly accepted by the scientific community, but the relationships between some families remain unclear.
Future molecular studies should provide further insights into their evolutionary relationships.
The suborder Archaeobatrachia contains four families of primitive frogs.
These are Ascaphidae, Bombinatoridae, Discoglossidae and Leiopelmatidae which have few derived features and are probably paraphyletic with regard to other frog lineages.
The six families in the more evolutionarily advanced suborder Mesobatrachia are the fossorial Megophryidae, Pelobatidae, Pelodytidae, Scaphiopodidae and Rhinophrynidae and the obligatorily aquatic Pipidae.
These have certain characteristics that are intermediate between the two other suborders.
Neobatrachia is by far the largest suborder and includes the remaining families of modern frogs, including most common species.
Ninety-six percent of the over 5,000 extant species of frog are neobatrachians.
The order Caudata (from the Latin "cauda" meaning "tail") consists of the salamanders—elongated, low-slung animals that mostly resemble lizards in form.
This is a symplesiomorphic trait and they are no more closely related to lizards than they are to mammals.
Salamanders lack claws, have scale-free skins, either smooth or covered with tubercles, and tails that are usually flattened from side to side and often finned.
They range in size from the Chinese giant salamander ("Andrias davidianus"), which has been reported to grow to a length of , to the diminutive "Thorius pennatulus" from Mexico which seldom exceeds in length.
Salamanders have a mostly Laurasian distribution, being present in much of the Holarctic region of the northern hemisphere.
The family Plethodontidae is also found in Central America and South America north of the Amazon basin; South America was apparently invaded from Central America by about the start of the Miocene, 23 million years ago.
Urodela is a name sometimes used for all the extant species of salamanders.
Members of several salamander families have become paedomorphic and either fail to complete their metamorphosis or retain some larval characteristics as adults.
Most salamanders are under long.
They may be terrestrial or aquatic and many spend part of the year in each habitat.
When on land, they mostly spend the day hidden under stones or logs or in dense vegetation, emerging in the evening and night to forage for worms, insects and other invertebrates.
The suborder Cryptobranchoidea contains the primitive salamanders.
A number of fossil cryptobranchids have been found, but there are only three living species, the Chinese giant salamander ("Andrias davidianus"), the Japanese giant salamander ("Andrias japonicus") and the hellbender ("Cryptobranchus alleganiensis") from North America.
These large amphibians retain several larval characteristics in their adult state; gills slits are present and the eyes are unlidded.
A unique feature is their ability to feed by suction, depressing either the left side of their lower jaw or the right.
The males excavate nests, persuade females to lay their egg strings inside them, and guard them.
As well as breathing with lungs, they respire through the many folds in their thin skin, which has capillaries close to the surface.
The suborder Salamandroidea contains the advanced salamanders.
They differ from the cryptobranchids by having fused prearticular bones in the lower jaw, and by using internal fertilisation.
In salamandrids, the male deposits a bundle of sperm, the spermatophore, and the female picks it up and inserts it into her cloaca where the sperm is stored until the eggs are laid.
The largest family in this group is Plethodontidae, the lungless salamanders, which includes 60% of all salamander species.
The family Salamandridae includes the true salamanders and the name "newt" is given to members of its subfamily Pleurodelinae.
The third suborder, Sirenoidea, contains the four species of sirens, which are in a single family, Sirenidae.
Members of this order are eel-like aquatic salamanders with much reduced forelimbs and no hind limbs.
Some of their features are primitive while others are derived.
Fertilisation is likely to be external as sirenids lack the cloacal glands used by male salamandrids to produce spermatophores and the females lack spermathecae for sperm storage.
Despite this, the eggs are laid singly, a behaviour not conducive for external fertilisation.
The order Gymnophiona (from the Greek "gymnos" meaning "naked" and "ophis" meaning "serpent") or Apoda (from the Latin "an-" meaning "without" and the Greek "poda" meaning "legs") comprises the caecilians.
These are long, cylindrical, limbless animals with a snake- or worm-like form.
The adults vary in length from 8 to 75 centimetres (3 to 30 inches) with the exception of Thomson's caecilian ("Caecilia thompsoni"), which can reach .
A caecilian's skin has a large number of transverse folds and in some species contains tiny embedded dermal scales.
It has rudimentary eyes covered in skin, which are probably limited to discerning differences in light intensity.
It also has a pair of short tentacles near the eye that can be extended and which have tactile and olfactory functions.
Most caecilians live underground in burrows in damp soil, in rotten wood and under plant debris, but some are aquatic.
Most species lay their eggs underground and when the larvae hatch, they make their way to adjacent bodies of water.
Others brood their eggs and the larvae undergo metamorphosis before the eggs hatch.
A few species give birth to live young, nourishing them with glandular secretions while they are in the oviduct.
Caecilians have a mostly Gondwanan distribution, being found in tropical regions of Africa, Asia and Central and South America.
The structure contains some typical characteristics common to terrestrial vertebrates, such as the presence of highly cornified outer layers, renewed periodically through a moulting process controlled by the pituitary and thyroid glands.
Local thickenings (often called warts) are common, such as those found on toads.
The outside of the skin is shed periodically mostly in one piece, in contrast to mammals and birds where it is shed in flakes.
Amphibians often eat the sloughed skin.
Caecilians are unique among amphibians in having mineralized dermal scales embedded in the dermis between the furrows in the skin.
The similarity of these to the scales of bony fish is largely superficial.
Lizards and some frogs have somewhat similar osteoderms forming bony deposits in the dermis, but this is an example of convergent evolution with similar structures having arisen independently in diverse vertebrate lineages.
Amphibian skin is permeable to water.
Gas exchange can take place through the skin (cutaneous respiration) and this allows adult amphibians to respire without rising to the surface of water and to hibernate at the bottom of ponds.
To compensate for their thin and delicate skin, amphibians have evolved mucous glands, principally on their heads, backs and tails.
The secretions produced by these help keep the skin moist.
In addition, most species of amphibian have granular glands that secrete distasteful or poisonous substances.
Some amphibian toxins can be lethal to humans while others have little effect.
The main poison-producing glands, the paratoids, produce the neurotoxin bufotoxin and are located behind the ears of toads, along the backs of frogs, behind the eyes of salamanders and on the upper surface of caecilians.
The skin colour of amphibians is produced by three layers of pigment cells called chromatophores.
These three cell layers consist of the melanophores (occupying the deepest layer), the guanophores (forming an intermediate layer and containing many granules, producing a blue-green colour) and the lipophores (yellow, the most superficial layer).
The colour change displayed by many species is initiated by hormones secreted by the pituitary gland.
Unlike bony fish, there is no direct control of the pigment cells by the nervous system, and this results in the colour change taking place more slowly than happens in fish.
A vividly coloured skin usually indicates that the species is toxic and is a warning sign to predators.
Amphibians have a skeletal system that is structurally homologous to other tetrapods, though with a number of variations.
They all have four limbs except for the legless caecilians and a few species of salamander with reduced or no limbs.
The bones are hollow and lightweight.
The musculoskeletal system is strong to enable it to support the head and body.
The bones are fully ossified and the vertebrae interlock with each other by means of overlapping processes.
The pectoral girdle is supported by muscle, and the well-developed pelvic girdle is attached to the backbone by a pair of sacral ribs.
The ilium slopes forward and the body is held closer to the ground than is the case in mammals.
In most amphibians, there are four digits on the fore foot and five on the hind foot, but no claws on either.
Some salamanders have fewer digits and the amphiumas are eel-like in appearance with tiny, stubby legs.
The sirens are aquatic salamanders with stumpy forelimbs and no hind limbs.
The caecilians are limbless.
They burrow in the manner of earthworms with zones of muscle contractions moving along the body.
On the surface of the ground or in water they move by undulating their body from side to side.
In frogs, the hind legs are larger than the fore legs, especially so in those species that principally move by jumping or swimming.
In the walkers and runners the hind limbs are not so large, and the burrowers mostly have short limbs and broad bodies.
The feet have adaptations for the way of life, with webbing between the toes for swimming, broad adhesive toe pads for climbing, and keratinised tubercles on the hind feet for digging (frogs usually dig backwards into the soil).
In most salamanders, the limbs are short and more or less the same length and project at right angles from the body.
Locomotion on land is by walking and the tail often swings from side to side or is used as a prop, particularly when climbing.
In their normal gait, only one leg is advanced at a time in the manner adopted by their ancestors, the lobe-finned fish.
Some salamanders in the genus "Aneides" and certain plethodontids climb trees and have long limbs, large toepads and prehensile tails.
In aquatic salamanders and in frog tadpoles, the tail has dorsal and ventral fins and is moved from side to side as a means of propulsion.
Adult frogs do not have tails and caecilians have only very short ones.
Salamanders use their tails in defence and some are prepared to jettison them to save their lives in a process known as autotomy.
Certain species in the Plethodontidae have a weak zone at the base of the tail and use this strategy readily.
The tail often continues to twitch after separation which may distract the attacker and allow the salamander to escape.
Both tails and limbs can be regenerated.
Adult frogs are unable to regrow limbs but tadpoles can do so.
Amphibians have a juvenile stage and an adult stage, and the circulatory systems of the two are distinct.
In the juvenile (or tadpole) stage, the circulation is similar to that of a fish; the two-chambered heart pumps the blood through the gills where it is oxygenated, and is spread around the body and back to the heart in a single loop.
In the adult stage, amphibians (especially frogs) lose their gills and develop lungs.
They have a heart that consists of a single ventricle and two atria.
When the ventricle starts contracting, deoxygenated blood is pumped through the pulmonary artery to the lungs.
Continued contraction then pumps oxygenated blood around the rest of the body.
Mixing of the two bloodstreams is minimized by the anatomy of the chambers.
The nervous system is basically the same as in other vertebrates, with a central brain, a spinal cord, and nerves throughout the body.
The amphibian brain is less well developed than that of reptiles, birds and mammals but is similar in morphology and function to that of a fish.
It is believed amphibians are capable of perceiving pain.
The brain consists of equal parts, cerebrum, midbrain and cerebellum.
Various parts of the cerebrum process sensory input, such as smell in the olfactory lobe and sight in the optic lobe, and it is additionally the centre of behaviour and learning.
The cerebellum is the center of muscular coordination and the medulla oblongata controls some organ functions including heartbeat and respiration.
The brain sends signals through the spinal cord and nerves to regulate activity in the rest of the body.
The pineal body, known to regulate sleep patterns in humans, is thought to produce the hormones involved in hibernation and aestivation in amphibians.
Tadpoles retain the lateral line system of their ancestral fishes, but this is lost in terrestrial adult amphibians.
Some caecilians possess electroreceptors that allow them to locate objects around them when submerged in water.
The ears are well developed in frogs.
There is no external ear, but the large circular eardrum lies on the surface of the head just behind the eye.
This vibrates and sound is transmitted through a single bone, the stapes, to the inner ear.
Only high-frequency sounds like mating calls are heard in this way, but low-frequency noises can be detected through another mechanism.
There is a patch of specialized haircells, called "papilla amphibiorum", in the inner ear capable of detecting deeper sounds.
Another feature, unique to frogs and salamanders, is the columella-operculum complex adjoining the auditory capsule which is involved in the transmission of both airborne and seismic signals.
The ears of salamanders and caecilians are less highly developed than those of frogs as they do not normally communicate with each other through the medium of sound.
The eyes of tadpoles lack lids, but at metamorphosis, the cornea becomes more dome-shaped, the lens becomes flatter, and eyelids and associated glands and ducts develop.
The adult eyes are an improvement on invertebrate eyes and were a first step in the development of more advanced vertebrate eyes.
They allow colour vision and depth of focus.
In the retinas are green rods, which are receptive to a wide range of wavelengths.
Many amphibians catch their prey by flicking out an elongated tongue with a sticky tip and drawing it back into the mouth before seizing the item with their jaws.
Some use inertial feeding to help them swallow the prey, repeatedly thrusting their head forward sharply causing the food to move backwards in their mouth by inertia.
Most amphibians swallow their prey whole without much chewing so they possess voluminous stomachs.
The short oesophagus is lined with cilia that help to move the food to the stomach and mucus produced by glands in the mouth and pharynx eases its passage.
The enzyme chitinase produced in the stomach helps digest the chitinous cuticle of arthropod prey.
Amphibians possess a pancreas, liver and gall bladder.
The liver is usually large with two lobes.
Its size is determined by its function as a glycogen and fat storage unit, and may change with the seasons as these reserves are built or used up.
Adipose tissue is another important means of storing energy and this occurs in the abdomen (in internal structures called fat bodies), under the skin and, in some salamanders, in the tail.
There are two kidneys located dorsally, near the roof of the body cavity.
Their job is to filter the blood of metabolic waste and transport the urine via ureters to the urinary bladder where it is stored before being passed out periodically through the cloacal vent.
Larvae and most aquatic adult amphibians excrete the nitrogen as ammonia in large quantities of dilute urine, while terrestrial species, with a greater need to conserve water, excrete the less toxic product urea.
Some tree frogs with limited access to water excrete most of their metabolic waste as uric acid.
The lungs in amphibians are primitive compared to those of amniotes, possessing few internal septa and large alveoli, and consequently having a comparatively slow diffusion rate for oxygen entering the blood.
Ventilation is accomplished by buccal pumping.
Most amphibians, however, are able to exchange gases with the water or air via their skin.
To enable sufficient cutaneous respiration, the surface of their highly vascularised skin must remain moist to allow the oxygen to diffuse at a sufficiently high rate.
Because oxygen concentration in the water increases at both low temperatures and high flow rates, aquatic amphibians in these situations can rely primarily on cutaneous respiration, as in the Titicaca water frog and the hellbender salamander.
In air, where oxygen is more concentrated, some small species can rely solely on cutaneous gas exchange, most famously the plethodontid salamanders, which have neither lungs nor gills.
Many aquatic salamanders and all tadpoles have gills in their larval stage, with some (such as the axolotl) retaining gills as aquatic adults.
For the purpose of reproduction most amphibians require fresh water although some lay their eggs on land and have developed various means of keeping them moist.
A few (e.g.
"Fejervarya raja") can inhabit brackish water, but there are no true marine amphibians.
There are reports, however, of particular amphibian populations unexpectedly invading marine waters.
Such was the case with the Black Sea invasion of the natural hybrid "Pelophylax esculentus" reported in 2010.
Several hundred frog species in adaptive radiations (e.g., "Eleutherodactylus", the Pacific "Platymantis", the Australo-Papuan microhylids, and many other tropical frogs), however, do not need any water for breeding in the wild.
They reproduce via direct development, an ecological and evolutionary adaptation that has allowed them to be completely independent from free-standing water.
Almost all of these frogs live in wet tropical rainforests and their eggs hatch directly into miniature versions of the adult, passing through the tadpole stage within the egg.
Reproductive success of many amphibians is dependent not only on the quantity of rainfall, but the seasonal timing.
In the tropics, many amphibians breed continuously or at any time of year.
In temperate regions, breeding is mostly seasonal, usually in the spring, and is triggered by increasing day length, rising temperatures or rainfall.
Experiments have shown the importance of temperature, but the trigger event, especially in arid regions, is often a storm.
In anurans, males usually arrive at the breeding sites before females and the vocal chorus they produce may stimulate ovulation in females and the endocrine activity of males that are not yet reproductively active.
In caecilians, fertilisation is internal, the male extruding an intromittent organ, the phallodeum, and inserting it into the female cloaca.
The paired Müllerian glands inside the male cloaca secrete a fluid which resembles that produced by mammalian prostate glands and which may transport and nourish the sperm.
Fertilisation probably takes place in the oviduct.
The majority of salamanders also engage in internal fertilisation.
In most of these, the male deposits a spermatophore, a small packet of sperm on top of a gelatinous cone, on the substrate either on land or in the water.
The female takes up the sperm packet by grasping it with the lips of the cloaca and pushing it into the vent.
The spermatozoa move to the spermatheca in the roof of the cloaca where they remain until ovulation which may be many months later.
Courtship rituals and methods of transfer of the spermatophore vary between species.
In some, the spermatophore may be placed directly into the female cloaca while in others, the female may be guided to the spermatophore or restrained with an embrace called amplexus.
Certain primitive salamanders in the families Sirenidae, Hynobiidae and Cryptobranchidae practice external fertilisation in a similar manner to frogs, with the female laying the eggs in water and the male releasing sperm onto the egg mass.
With a few exceptions, frogs use external fertilisation.
The male grasps the female tightly with his forelimbs either behind the arms or in front of the back legs, or in the case of "Epipedobates tricolor", around the neck.
They remain in amplexus with their cloacae positioned close together while the female lays the eggs and the male covers them with sperm.
Roughened nuptial pads on the male's hands aid in retaining grip.
Often the male collects and retains the egg mass, forming a sort of basket with the hind feet.
An exception is the granular poison frog ("Oophaga granulifera") where the male and female place their cloacae in close proximity while facing in opposite directions and then release eggs and sperm simultaneously.
The tailed frog ("Ascaphus truei") exhibits internal fertilisation.
The "tail" is only possessed by the male and is an extension of the cloaca and used to inseminate the female.
This frog lives in fast-flowing streams and internal fertilisation prevents the sperm from being washed away before fertilisation occurs.
The sperm may be retained in storage tubes attached to the oviduct until the following spring.
Most frogs can be classified as either prolonged or explosive breeders.
Typically, prolonged breeders congregate at a breeding site, the males usually arriving first, calling and setting up territories.
Other satellite males remain quietly nearby, waiting for their opportunity to take over a territory.
The females arrive sporadically, mate selection takes place and eggs are laid.
The females depart and territories may change hands.
More females appear and in due course, the breeding season comes to an end.
Explosive breeders on the other hand are found where temporary pools appear in dry regions after rainfall.
These frogs are typically fossorial species that emerge after heavy rains and congregate at a breeding site.
They are attracted there by the calling of the first male to find a suitable place, perhaps a pool that forms in the same place each rainy season.
The assembled frogs may call in unison and frenzied activity ensues, the males scrambling to mate with the usually smaller number of females.
There is a direct competition between males to win the attention of the females in salamanders and newts, with elaborate courtship displays to keep the female's attention long enough to get her interested in choosing him to mate with.
Some species store sperm through long breeding seasons, as the extra time may allow for interactions with rival sperm.
Most amphibians go through metamorphosis, a process of significant morphological change after birth.
In typical amphibian development, eggs are laid in water and larvae are adapted to an aquatic lifestyle.
Frogs, toads and salamanders all hatch from the egg as larvae with external gills.
Metamorphosis in amphibians is regulated by thyroxine concentration in the blood, which stimulates metamorphosis, and prolactin, which counteracts thyroxine's effect.
Specific events are dependent on threshold values for different tissues.
Because most embryonic development is outside the parental body, it is subject to many adaptations due to specific environmental circumstances.
For this reason tadpoles can have horny ridges instead of teeth, whisker-like skin extensions or fins.
They also make use of a sensory lateral line organ similar to that of fish.
After metamorphosis, these organs become redundant and will be reabsorbed by controlled cell death, called apoptosis.
The variety of adaptations to specific environmental circumstances among amphibians is wide, with many discoveries still being made.
The egg of an amphibian is typically surrounded by a transparent gelatinous covering secreted by the oviducts and containing mucoproteins and mucopolysaccharides.
This capsule is permeable to water and gases, and swells considerably as it absorbs water.
The ovum is at first rigidly held, but in fertilised eggs the innermost layer liquefies and allows the embryo to move freely.
This also happens in salamander eggs, even when they are unfertilised.
Eggs of some salamanders and frogs contain unicellular green algae.
These penetrate the jelly envelope after the eggs are laid and may increase the supply of oxygen to the embryo through photosynthesis.
They seem to both speed up the development of the larvae and reduce mortality.
Most eggs contain the pigment melanin which raises their temperature through the absorption of light and also protects them against ultraviolet radiation.
Caecilians, some plethodontid salamanders and certain frogs lay eggs underground that are unpigmented.
In the wood frog ("Rana sylvatica"), the interior of the globular egg cluster has been found to be up to warmer than its surroundings, which is an advantage in its cool northern habitat.
The eggs may be deposited singly or in small groups, or may take the form of spherical egg masses, rafts or long strings.
In terrestrial caecilians, the eggs are laid in grape-like clusters in burrows near streams.
The amphibious salamander "Ensatina" attaches its similar clusters by stalks to underwater stems and roots.
The greenhouse frog ("Eleutherodactylus planirostris") lays eggs in small groups in the soil where they develop in about two weeks directly into juvenile frogs without an intervening larval stage.
The tungara frog ("Physalaemus pustulosus") builds a floating nest from foam to protect its eggs.
First a raft is built, then eggs are laid in the centre, and finally a foam cap is overlaid.
The foam has anti-microbial properties.
It contains no detergents but is created by whipping up proteins and lectins secreted by the female.
The eggs of amphibians are typically laid in water and hatch into free-living larvae that complete their development in water and later transform into either aquatic or terrestrial adults.
In many species of frog and in most lungless salamanders (Plethodontidae), direct development takes place, the larvae growing within the eggs and emerging as miniature adults.
Many caecilians and some other amphibians lay their eggs on land, and the newly hatched larvae wriggle or are transported to water bodies.
Some caecilians, the alpine salamander ("Salamandra atra") and some of the African live-bearing toads ("Nectophrynoides spp.")
are viviparous.
Their larvae feed on glandular secretions and develop within the female's oviduct, often for long periods.
Other amphibians, but not caecilians, are ovoviviparous.
The eggs are retained in or on the parent's body, but the larvae subsist on the yolks of their eggs and receive no nourishment from the adult.
The larvae emerge at varying stages of their growth, either before or after metamorphosis, according to their species.
The toad genus "Nectophrynoides" exhibits all of these developmental patterns among its dozen or so members.
Frog larvae are known as tadpoles and typically have oval bodies and long, vertically flattened tails with fins.
The free-living larvae are normally fully aquatic, but the tadpoles of some species (such as "Nannophrys ceylonensis") are semi-terrestrial and live among wet rocks.
Tadpoles have cartilaginous skeletons, gills for respiration (external gills at first, internal gills later), lateral line systems and large tails that they use for swimming.
Newly hatched tadpoles soon develop gill pouches that cover the gills.
The lungs develop early and are used as accessory breathing organs, the tadpoles rising to the water surface to gulp air.
Some species complete their development inside the egg and hatch directly into small frogs.
These larvae do not have gills but instead have specialised areas of skin through which respiration takes place.
While tadpoles do not have true teeth, in most species, the jaws have long, parallel rows of small keratinized structures called keradonts surrounded by a horny beak.
Front legs are formed under the gill sac and hind legs become visible a few days later.
Iodine and T4 (over stimulate the spectacular apoptosis [programmed cell death] of the cells of the larval gills, tail and fins) also stimulate the evolution of nervous systems transforming the aquatic, vegetarian tadpole into the terrestrial, carnivorous frog with better neurological, visuospatial, olfactory and cognitive abilities for hunting.
In fact, tadpoles developing in ponds and streams are typically herbivorous.
Pond tadpoles tend to have deep bodies, large caudal fins and small mouths; they swim in the quiet waters feeding on growing or loose fragments of vegetation.
Stream dwellers mostly have larger mouths, shallow bodies and caudal fins; they attach themselves to plants and stones and feed on the surface films of algae and bacteria.
They also feed on diatoms, filtered from the water through the gills, and stir up the sediment at bottom of the pond, ingesting edible fragments.
They have a relatively long, spiral-shaped gut to enable them to digest this diet.
Some species are carnivorous at the tadpole stage, eating insects, smaller tadpoles and fish.
Young of the Cuban tree frog ("Osteopilus septentrionalis") can occasionally be cannibalistic, the younger tadpoles attacking a larger, more developed tadpole when it is undergoing metamorphosis.
At metamorphosis, rapid changes in the body take place as the lifestyle of the frog changes completely.
The spiral‐shaped mouth with horny tooth ridges is reabsorbed together with the spiral gut.
The animal develops a large jaw, and its gills disappear along with its gill sac.
Eyes and legs grow quickly, and a tongue is formed.
There are associated changes in the neural networks such as development of stereoscopic vision and loss of the lateral line system.
All this can happen in about a day.
A few days later, the tail is reabsorbed, due to the higher thyroxine concentration required for this to take place.
At hatching, a typical salamander larva has eyes without lids, teeth in both upper and lower jaws, three pairs of feathery external gills, a somewhat laterally flattened body and a long tail with dorsal and ventral fins.
The forelimbs may be partially developed and the hind limbs are rudimentary in pond-living species but may be rather more developed in species that reproduce in moving water.
Pond-type larvae often have a pair of balancers, rod-like structures on either side of the head that may prevent the gills from becoming clogged up with sediment.
Some members of the genera "Ambystoma" and "Dicamptodon" have larvae that never fully develop into the adult form, but this varies with species and with populations.
The northwestern salamander ("Ambystoma gracile") is one of these and, depending on environmental factors, either remains permanently in the larval state, a condition known as neoteny, or transforms into an adult.
Both of these are able to breed.
Neoteny occurs when the animal's growth rate is very low and is usually linked to adverse conditions such as low water temperatures that may change the response of the tissues to the hormone thyroxine.
Other factors that may inhibit metamorphosis include lack of food, lack of trace elements and competition from conspecifics.
The tiger salamander ("Ambystoma tigrinum") also sometimes behaves in this way and may grow particularly large in the process.
The adult tiger salamander is terrestrial, but the larva is aquatic and able to breed while still in the larval state.
When conditions are particularly inhospitable on land, larval breeding may allow continuation of a population that would otherwise die out.
There are fifteen species of obligate neotenic salamanders, including species of "Necturus", "Proteus" and "Amphiuma", and many examples of facultative ones that adopt this strategy under appropriate environmental circumstances.
Lungless salamanders in the family Plethodontidae are terrestrial and lay a small number of unpigmented eggs in a cluster among damp leaf litter.
Each egg has a large yolk sac and the larva feeds on this while it develops inside the egg, emerging fully formed as a juvenile salamander.
The female salamander often broods the eggs.
In the genus "Ensatinas", the female has been observed to coil around them and press her throat area against them, effectively massaging them with a mucous secretion.
In newts and salamanders, metamorphosis is less dramatic than in frogs.
This is because the larvae are already carnivorous and continue to feed as predators when they are adults so few changes are needed to their digestive systems.
Their lungs are functional early, but the larvae do not make as much use of them as do tadpoles.
Their gills are never covered by gill sacs and are reabsorbed just before the animals leave the water.
Other changes include the reduction in size or loss of tail fins, the closure of gill slits, thickening of the skin, the development of eyelids, and certain changes in dentition and tongue structure.
Salamanders are at their most vulnerable at metamorphosis as swimming speeds are reduced and transforming tails are encumbrances on land.
Adult salamanders often have an aquatic phase in spring and summer, and a land phase in winter.
For adaptation to a water phase, prolactin is the required hormone, and for adaptation to the land phase, thyroxine.
External gills do not return in subsequent aquatic phases because these are completely absorbed upon leaving the water for the first time.
Most terrestrial caecilians that lay eggs do so in burrows or moist places on land near bodies of water.
The development of the young of "Ichthyophis glutinosus", a species from Sri Lanka, has been much studied.
The eel-like larvae hatch out of the eggs and make their way to water.
They have three pairs of external red feathery gills, a blunt head with two rudimentary eyes, a lateral line system and a short tail with fins.
They swim by undulating their body from side to side.
They are mostly active at night, soon lose their gills and make sorties onto land.
Metamorphosis is gradual.
By the age of about ten months they have developed a pointed head with sensory tentacles near the mouth and lost their eyes, lateral line systems and tails.
The skin thickens, embedded scales develop and the body divides into segments.
By this time, the caecilian has constructed a burrow and is living on land.
In the majority of species of caecilians, the young are produced by viviparity.
"Typhlonectes compressicauda", a species from South America, is typical of these.
Up to nine larvae can develop in the oviduct at any one time.
They are elongated and have paired sac-like gills, small eyes and specialised scraping teeth.
At first, they feed on the yolks of the eggs, but as this source of nourishment declines they begin to rasp at the ciliated epithelial cells that line the oviduct.
This stimulates the secretion of fluids rich in lipids and mucoproteins on which they feed along with scrapings from the oviduct wall.
They may increase their length sixfold and be two-fifths as long as their mother before being born.
By this time they have undergone metamorphosis, lost their eyes and gills, developed a thicker skin and mouth tentacles, and reabsorbed their teeth.
A permanent set of teeth grow through soon after birth.
The ringed caecilian ("Siphonops annulatus") has developed a unique adaptation for the purposes of reproduction.
The progeny feed on a skin layer that is specially developed by the adult in a phenomenon known as maternal dermatophagy.
The brood feed as a batch for about seven minutes at intervals of approximately three days which gives the skin an opportunity to regenerate.
Meanwhile, they have been observed to ingest fluid exuded from the maternal cloaca.
The care of offspring among amphibians has been little studied but, in general, the larger the number of eggs in a batch, the less likely it is that any degree of parental care takes place.
Nevertheless, it is estimated that in up to 20% of amphibian species, one or both adults play some role in the care of the young.
Those species that breed in smaller water bodies or other specialised habitats tend to have complex patterns of behaviour in the care of their young.
Many woodland salamanders lay clutches of eggs under dead logs or stones on land.
The black mountain salamander ("Desmognathus welteri") does this, the mother brooding the eggs and guarding them from predation as the embryos feed on the yolks of their eggs.
When fully developed, they break their way out of the egg capsules and disperse as juvenile salamanders.
The male hellbender, a primitive salamander, excavates an underwater nest and encourages females to lay there.
The male then guards the site for the two or three months before the eggs hatch, using body undulations to fan the eggs and increase their supply of oxygen.
The male "Colostethus subpunctatus", a tiny frog, protects the egg cluster which is hidden under a stone or log.
When the eggs hatch, the male transports the tadpoles on his back, stuck there by a mucous secretion, to a temporary pool where he dips himself into the water and the tadpoles drop off.
The male midwife toad ("Alytes obstetricans") winds egg strings round his thighs and carries the eggs around for up to eight weeks.
He keeps them moist and when they are ready to hatch, he visits a pond or ditch and releases the tadpoles.
The female gastric-brooding frog ("Rheobatrachus spp.")
reared larvae in her stomach after swallowing either the eggs or hatchlings; however, this stage was never observed before the species became extinct.
The tadpoles secrete a hormone that inhibits digestion in the mother whilst they develop by consuming their very large yolk supply.
The pouched frog ("Assa darlingtoni") lays eggs on the ground.
When they hatch, the male carries the tadpoles around in brood pouches on his hind legs.
The aquatic Surinam toad ("Pipa pipa") raises its young in pores on its back where they remain until metamorphosis.
The granular poison frog ("Oophaga granulifera") is typical of a number of tree frogs in the poison dart frog family Dendrobatidae.
Its eggs are laid on the forest floor and when they hatch, the tadpoles are carried one by one on the back of an adult to a suitable water-filled crevice such as the axil of a leaf or the rosette of a bromeliad.
The female visits the nursery sites regularly and deposits unfertilised eggs in the water and these are consumed by the tadpoles.
With a few exceptions, adult amphibians are predators, feeding on virtually anything that moves that they can swallow.
The diet mostly consists of small prey that do not move too fast such as beetles, caterpillars, earthworms and spiders.
The sirens ("Siren spp.")
often ingest aquatic plant material with the invertebrates on which they feed and a Brazilian tree frog ("Xenohyla truncata") includes a large quantity of fruit in its diet.
The Mexican burrowing toad ("Rhinophrynus dorsalis") has a specially adapted tongue for picking up ants and termites.
It projects it with the tip foremost whereas other frogs flick out the rear part first, their tongues being hinged at the front.
Food is mostly selected by sight, even in conditions of dim light.
Movement of the prey triggers a feeding response.
Frogs have been caught on fish hooks baited with red flannel and green frogs ("Rana clamitans") have been found with stomachs full of elm seeds that they had seen floating past.
Toads, salamanders and caecilians also use smell to detect prey.
This response is mostly secondary because salamanders have been observed to remain stationary near odoriferous prey but only feed if it moves.
Cave-dwelling amphibians normally hunt by smell.
Some salamanders seem to have learned to recognize immobile prey when it has no smell, even in complete darkness.
Amphibians usually swallow food whole but may chew it lightly first to subdue it.
They typically have small hinged pedicellate teeth, a feature unique to amphibians.
The base and crown of these are composed of dentine separated by an uncalcified layer and they are replaced at intervals.
Salamanders, caecilians and some frogs have one or two rows of teeth in both jaws, but some frogs ("Rana spp.")
lack teeth in the lower jaw, and toads ("Bufo spp.")
have no teeth.
In many amphibians there are also vomerine teeth attached to a facial bone in the roof of the mouth.
The tiger salamander ("Ambystoma tigrinum") is typical of the frogs and salamanders that hide under cover ready to ambush unwary invertebrates.
Others amphibians, such as the "Bufo spp."
toads, actively search for prey, while the Argentine horned frog ("Ceratophrys ornata") lures inquisitive prey closer by raising its hind feet over its back and vibrating its yellow toes.
Among leaf litter frogs in Panama, frogs that actively hunt prey have narrow mouths and are slim, often brightly coloured and toxic, while ambushers have wide mouths and are broad and well-camouflaged.
Caecilians do not flick their tongues, but catch their prey by grabbing it with their slightly backward-pointing teeth.
The struggles of the prey and further jaw movements work it inwards and the caecilian usually retreats into its burrow.
The subdued prey is gulped down whole.
When they are newly hatched, frog larvae feed on the yolk of the egg.
When this is exhausted some move on to feed on bacteria, algal crusts, detritus and raspings from submerged plants.
Water is drawn in through their mouths, which are usually at the bottom of their heads, and passes through branchial food traps between their mouths and their gills where fine particles are trapped in mucus and filtered out.
Others have specialised mouthparts consisting of a horny beak edged by several rows of labial teeth.
They scrape and bite food of many kinds as well as stirring up the bottom sediment, filtering out larger particles with the papillae around their mouths.
Some, such as the spadefoot toads, have strong biting jaws and are carnivorous or even cannibalistic.
The calls made by caecilians and salamanders are limited to occasional soft squeaks, grunts or hisses and have not been much studied.
A clicking sound sometimes produced by caecilians may be a means of orientation, as in bats, or a form of communication.
Most salamanders are considered voiceless, but the California giant salamander ("Dicamptodon ensatus") has vocal cords and can produce a rattling or barking sound.
Some species of salamander emit a quiet squeak or yelp if attacked.
Frogs are much more vocal, especially during the breeding season when they use their voices to attract mates.
The presence of a particular species in an area may be more easily discerned by its characteristic call than by a fleeting glimpse of the animal itself.
In most species, the sound is produced by expelling air from the lungs over the vocal cords into an air sac or sacs in the throat or at the corner of the mouth.
This may distend like a balloon and acts as a resonator, helping to transfer the sound to the atmosphere, or the water at times when the animal is submerged.
The main vocalisation is the male's loud advertisement call which seeks to both encourage a female to approach and discourage other males from intruding on its territory.
This call is modified to a quieter courtship call on the approach of a female or to a more aggressive version if a male intruder draws near.
Calling carries the risk of attracting predators and involves the expenditure of much energy.
Other calls include those given by a female in response to the advertisement call and a release call given by a male or female during unwanted attempts at amplexus.
When a frog is attacked, a distress or fright call is emitted, often resembling a scream.
The usually nocturnal Cuban tree frog ("Osteopilus septentrionalis") produces a rain call when there is rainfall during daylight hours.
Little is known of the territorial behaviour of caecilians, but some frogs and salamanders defend home ranges.
These are usually feeding, breeding or sheltering sites.
Males normally exhibit such behaviour though in some species, females and even juveniles are also involved.
Although in many frog species, females are larger than males, this is not the case in most species where males are actively involved in territorial defence.
Some of these have specific adaptations such as enlarged teeth for biting or spines on the chest, arms or thumbs.
In salamanders, defence of a territory involves adopting an aggressive posture and if necessary attacking the intruder.
This may involve snapping, chasing and sometimes biting, occasionally causing the loss of a tail.
The behaviour of red back salamanders ("Plethodon cinereus") has been much studied.
91% of marked individuals that were later recaptured were within a metre (yard) of their original daytime retreat under a log or rock.
A similar proportion, when moved experimentally a distance of , found their way back to their home base.
The salamanders left odour marks around their territories which averaged in size and were sometimes inhabited by a male and female pair.
These deterred the intrusion of others and delineated the boundaries between neighbouring areas.
Much of their behaviour seemed stereotyped and did not involve any actual contact between individuals.
An aggressive posture involved raising the body off the ground and glaring at the opponent who often turned away submissively.
If the intruder persisted, a biting lunge was usually launched at either the tail region or the naso-labial grooves.
Damage to either of these areas can reduce the fitness of the rival, either because of the need to regenerate tissue or because it impairs its ability to detect food.
In frogs, male territorial behaviour is often observed at breeding locations; calling is both an announcement of ownership of part of this resource and an advertisement call to potential mates.
In general, a deeper voice represents a heavier and more powerful individual, and this may be sufficient to prevent intrusion by smaller males.
Much energy is used in the vocalization and it takes a toll on the territory holder who may be displaced by a fitter rival if he tires.
There is a tendency for males to tolerate the holders of neighbouring territories while vigorously attacking unknown intruders.
Holders of territories have a "home advantage" and usually come off better in an encounter between two similar-sized frogs.
If threats are insufficient, chest to chest tussles may take place.
Fighting methods include pushing and shoving, deflating the opponent's vocal sac, seizing him by the head, jumping on his back, biting, chasing, splashing, and ducking him under the water.
Amphibians have soft bodies with thin skins, and lack claws, defensive armour, or spines.
Nevertheless, they have evolved various defence mechanisms to keep themselves alive.
The first line of defence in salamanders and frogs is the mucous secretion that they produce.
This keeps their skin moist and makes them slippery and difficult to grip.
The secretion is often sticky and distasteful or toxic.
Snakes have been observed yawning and gaping when trying to swallow African clawed frogs ("Xenopus laevis"), which gives the frogs an opportunity to escape.
Caecilians have been little studied in this respect, but the Cayenne caecilian ("Typhlonectes compressicauda") produces toxic mucus that has killed predatory fish in a feeding experiment in Brazil.
In some salamanders, the skin is poisonous.
The rough-skinned newt ("Taricha granulosa") from North America and other members of its genus contain the neurotoxin tetrodotoxin (TTX), the most toxic non-protein substance known and almost identical to that produced by pufferfish.
Handling the newts does not cause harm, but ingestion of even the most minute amounts of the skin is deadly.
In feeding trials, fish, frogs, reptiles, birds and mammals were all found to be susceptible.
The only predators with some tolerance to the poison are certain populations of common garter snake ("Thamnophis sirtalis").
In locations where both snake and salamander co-exist, the snakes have developed immunity through genetic changes and they feed on the amphibians with impunity.
Coevolution occurs with the newt increasing its toxic capabilities at the same rate as the snake further develops its immunity.
Some frogs and toads are toxic, the main poison glands being at the side of the neck and under the warts on the back.
These regions are presented to the attacking animal and their secretions may be foul-tasting or cause various physical or neurological symptoms.
Altogether, over 200 toxins have been isolated from the limited number of amphibian species that have been investigated.
Poisonous species often use bright colouring to warn potential predators of their toxicity.
These warning colours tend to be red or yellow combined with black, with the fire salamander ("Salamandra salamandra") being an example.
Once a predator has sampled one of these, it is likely to remember the colouration next time it encounters a similar animal.
In some species, such as the fire-bellied toad ("Bombina spp.
"), the warning colouration is on the belly and these animals adopt a defensive pose when attacked, exhibiting their bright colours to the predator.
The frog "Allobates zaparo" is not poisonous, but mimics the appearance of other toxic species in its locality, a strategy that may deceive predators.
Many amphibians are nocturnal and hide during the day, thereby avoiding diurnal predators that hunt by sight.
Other amphibians use camouflage to avoid being detected.
They have various colourings such as mottled browns, greys and olives to blend into the background.
Some salamanders adopt defensive poses when faced by a potential predator such as the North American northern short-tailed shrew ("Blarina brevicauda").
Their bodies writhe and they raise and lash their tails which makes it difficult for the predator to avoid contact with their poison-producing granular glands.
A few salamanders will autotomise their tails when attacked, sacrificing this part of their anatomy to enable them to escape.
The tail may have a constriction at its base to allow it to be easily detached.
The tail is regenerated later, but the energy cost to the animal of replacing it is significant.
Some frogs and toads inflate themselves to make themselves look large and fierce, and some spadefoot toads ("Pelobates spp") scream and leap towards the attacker.
Giant salamanders of the genus "Andrias", as well as Ceratophrine and "Pyxicephalus" frogs possess sharp teeth and are capable of drawing blood with a defensive bite.
The blackbelly salamander ("Desmognathus quadramaculatus") can bite an attacking common garter snake ("Thamnophis sirtalis") two or three times its size on the head and often manages to escape.
In amphibians, there is evidence of habituation, associative learning through both classical and instrumental learning, and discrimination abilities.
In one experiment, when offered live fruit flies ("Drosophila virilis"), salamanders choose the larger of 1 vs 2 and 2 vs 3.
Frogs can distinguish between low numbers (1 vs 2, 2 vs 3, but not 3 vs 4) and large numbers (3 vs 6, 4 vs 8, but not 4 vs 6) of prey.
This is irrespective of other characteristics, i.e. surface area, volume, weight and movement, although discrimination among large numbers may be based on surface area.
Dramatic declines in amphibian populations, including population crashes and mass localized extinction, have been noted since the late 1980s from locations all over the world, and amphibian declines are thus perceived to be one of the most critical threats to global biodiversity.
In 2004, the International Union for Conservation of Nature (IUCN) reported stating that currently birds, mammals, and amphibians extinction rates were at minimum 48 times greater than natural extinction rates—possibly 1,024 times higher.
In 2006 there were believed to be 4,035 species of amphibians that depended on water at some stage during their life cycle.
Of these, 1,356 (33.6%) were considered to be threatened and this figure is likely to be an underestimate because it excludes 1,427 species for which there was insufficient data to assess their status.
A number of causes are believed to be involved, including habitat destruction and modification, over-exploitation, pollution, introduced species, climate change, endocrine-disrupting pollutants, destruction of the ozone layer (ultraviolet radiation has shown to be especially damaging to the skin, eyes, and eggs of amphibians), and diseases like chytridiomycosis.
However, many of the causes of amphibian declines are still poorly understood, and are a topic of ongoing discussion.
With their complex reproductive needs and permeable skins, amphibians are often considered to be ecological indicators.
In many terrestrial ecosystems, they constitute one of the largest parts of the vertebrate biomass.
Any decline in amphibian numbers will affect the patterns of predation.
The loss of carnivorous species near the top of the food chain will upset the delicate ecosystem balance and may cause dramatic increases in opportunistic species.
In the Middle East, a growing appetite for eating frog legs and the consequent gathering of them for food was linked to an increase in mosquitoes.
Predators that feed on amphibians are affected by their decline.
The western terrestrial garter snake ("Thamnophis elegans") in California is largely aquatic and depends heavily on two species of frog that are decreasing in numbers, the Yosemite toad ("Bufo canorus") and the mountain yellow-legged frog ("Rana muscosa"), putting the snake's future at risk.
If the snake were to become scarce, this would affect birds of prey and other predators that feed on it.
Meanwhile, in the ponds and lakes, fewer frogs means fewer tadpoles.
These normally play an important role in controlling the growth of algae and also forage on detritus that accumulates as sediment on the bottom.
A reduction in the number of tadpoles may lead to an overgrowth of algae, resulting in depletion of oxygen in the water when the algae later die and decompose.
Aquatic invertebrates and fish might then die and there would be unpredictable ecological consequences.
A global strategy to stem the crisis was released in 2005 in the form of the Amphibian Conservation Action Plan.
Developed by over eighty leading experts in the field, this call to action details what would be required to curtail amphibian declines and extinctions over the following five years and how much this would cost.
The Amphibian Specialist Group of the IUCN is spearheading efforts to implement a comprehensive global strategy for amphibian conservation.
Amphibian Ark is an organization that was formed to implement the ex-situ conservation recommendations of this plan, and they have been working with zoos and aquaria around the world, encouraging them to create assurance colonies of threatened amphibians.
One such project is the Panama Amphibian Rescue and Conservation Project that built on existing conservation efforts in Panama to create a country-wide response to the threat of chytridiomycosis.
</doc>
<doc id="624" url="https://en.wikipedia.org/wiki?curid=624" title="Alaska">
Alaska

Alaska (; ; ; ) is a U.S.
state in the northwest extremity of North America.
The Canadian administrative divisions of British Columbia and Yukon border the state to the east, its most extreme western part is Attu Island, and it has a maritime border with Russia (Chukotka Autonomous Okrug) to the west across the Bering Strait.
To the north are the Chukchi and Beaufort seas—the southern parts of the Arctic Ocean.
The Pacific Ocean lies to the south and southwest.
It is the largest state in the United States by area and the 
seventh largest subnational division in the world.
In addition, it is the 3rd least populous and the most sparsely populated of the 50 United States; nevertheless, it is by far the most populous territory located mostly north of the 60th parallel in North America: its population—estimated at 738,432 by the U.S.
Census Bureau in 2015— is more than quadruple the combined populations of Northern Canada and Greenland.
Approximately half of Alaska's residents live within the Anchorage metropolitan area.
Alaska's economy is dominated by the fishing, natural gas, and oil industries, resources which it has in abundance.
Military bases and tourism are also a significant part of the economy.
The United States purchased Alaska from the Russian Empire on March 30, 1867, for 7.2 million U.S.
dollars at approximately two cents per acre ($4.74/km).
The area went through several administrative changes before becoming organized as a territory on May 11, 1912.
It was admitted as the 49th state of the U.S.
on January 3, 1959.
The name "Alaska" () was introduced in the Russian colonial period when it was used to refer to the Alaska Peninsula.
It was derived from an Aleut-language idiom, which figuratively refers to the mainland.
Literally, it means "object to which the action of the sea is directed".
Alaska is the northernmost and westernmost state in the United States and has the most easterly longitude in the United States because the Aleutian Islands extend into the Eastern Hemisphere.
Alaska is the only non-contiguous U.S.
state on continental North America; about of British Columbia (Canada) separates Alaska from Washington.
It is technically part of the continental U.S., but is sometimes not included in colloquial use; Alaska is not part of the contiguous U.S., often called "the Lower 48".
The capital city, Juneau, is situated on the mainland of the North American continent but is not connected by road to the rest of the North American highway system.
The state is bordered by Yukon and British Columbia in Canada, to the east, the Gulf of Alaska and the Pacific Ocean to the south and southwest, the Bering Sea, Bering Strait, and Chukchi Sea to the west and the Arctic Ocean to the north.
Alaska's territorial waters touch Russia's territorial waters in the Bering Strait, as the Russian Big Diomede Island and Alaskan Little Diomede Island are only apart.
Alaska has a longer coastline than all the other U.S.
states combined.
Alaska is the largest state in the United States by total area at , over twice the size of Texas, the next largest state.
Alaska is larger than all but 18 sovereign countries.
Counting territorial waters, Alaska is larger than the combined area of the next three largest states: Texas, California, and Montana.
It is also larger than the combined area of the 22 smallest U.S.
states.
There are no officially defined borders demarcating the various regions of Alaska, but there are six widely accepted regions:

The most populous region of Alaska, containing Anchorage, the Matanuska-Susitna Valley and the Kenai Peninsula.
Rural, mostly unpopulated areas south of the Alaska Range and west of the Wrangell Mountains also fall within the definition of South Central, as do the Prince William Sound area and the communities of Cordova and Valdez.
Also referred to as the Panhandle or Inside Passage, this is the region of Alaska closest to the rest of the United States.
As such, this was where most of the initial non-indigenous settlement occurred in the years following the Alaska Purchase.
The region is dominated by the Alexander Archipelago as well as the Tongass National Forest, the largest national forest in the United States.
It contains the state capital Juneau, the former capital Sitka, and Ketchikan, at one time Alaska's largest city.
The Alaska Marine Highway provides a vital surface transportation link throughout the area, as only three communities (Haines, Hyder and Skagway) enjoy direct connections to the contiguous North American road system.
Officially designated in 1963.
The Interior is the largest region of Alaska; much of it is uninhabited wilderness.
Fairbanks is the only large city in the region.
Denali National Park and Preserve is located here.
"Denali" is the highest mountain in North America.
Southwest Alaska is a sparsely inhabited region stretching some inland from the Bering Sea.
Most of the population lives along the coast.
Kodiak Island is also located in Southwest.
The massive Yukon–Kuskokwim Delta, one of the largest river deltas in the world, is here.
Portions of the Alaska Peninsula are considered part of Southwest, with the remaining portions included with the Aleutian Islands (see below).
The North Slope is mostly tundra peppered with small villages.
The area is known for its massive reserves of crude oil, and contains both the National Petroleum Reserve–Alaska and the Prudhoe Bay Oil Field.
The city of Utqiagvik, formerly known as Barrow, is the northernmost city in the United States and is located here.
The Northwest Arctic area, anchored by Kotzebue and also containing the Kobuk River valley, is often regarded as being part of this region.
However, the respective Inupiat of the North Slope and of the Northwest Arctic seldom consider themselves to be one people.
More than 300 small volcanic islands make up this chain, which stretches over into the Pacific Ocean.
Some of these islands fall in the Eastern Hemisphere, but the International Date Line was drawn west of 180° to keep the whole state, and thus the entire North American continent, within the same legal day.
Two of the islands, Attu and Kiska, were occupied by Japanese forces during World War II.
With its myriad islands, Alaska has nearly of tidal shoreline.
The Aleutian Islands chain extends west from the southern tip of the Alaska Peninsula.
Many active volcanoes are found in the Aleutians and in coastal regions.
Unimak Island, for example, is home to Mount Shishaldin, which is an occasionally smoldering volcano that rises to above the North Pacific.
It is the most perfect volcanic cone on Earth, even more symmetrical than Japan's Mount Fuji.
The chain of volcanoes extends to Mount Spurr, west of Anchorage on the mainland.
Geologists have identified Alaska as part of Wrangellia, a large region consisting of multiple states and Canadian provinces in the Pacific Northwest, which is actively undergoing continent building.
One of the world's largest tides occurs in Turnagain Arm, just south of Anchorage, where tidal differences can be more than .
Alaska has more than three million lakes.
Marshlands and wetland permafrost cover (mostly in northern, western and southwest flatlands).
Glacier ice covers about of Alaska.
The Bering Glacier is the largest glacier in North America, covering alone.
According to an October 1998 report by the United States Bureau of Land Management, approximately 65% of Alaska is owned and managed by the U.S.
federal government as public lands, including a multitude of national forests, national parks, and national wildlife refuges.
Of these, the Bureau of Land Management manages , or 23.8% of the state.
The Arctic National Wildlife Refuge is managed by the United States Fish and Wildlife Service.
It is the world's largest wildlife refuge, comprising .
Of the remaining land area, the state of Alaska owns , its entitlement under the Alaska Statehood Act.
A portion of that acreage is occasionally ceded to organized boroughs, under the statutory provisions pertaining to newly formed boroughs.
Smaller portions are set aside for rural subdivisions and other homesteading-related opportunities.
These are not very popular due to the often remote and roadless locations.
The University of Alaska, as a land grant university, also owns substantial acreage which it manages independently.
Another are owned by 12 regional, and scores of local, Native corporations created under the Alaska Native Claims Settlement Act (ANCSA) of 1971.
Regional Native corporation Doyon, Limited often promotes itself as the largest private landowner in Alaska in advertisements and other communications.
Provisions of ANCSA allowing the corporations' land holdings to be sold on the open market starting in 1991 were repealed before they could take effect.
Effectively, the corporations hold title (including subsurface title in many cases, a privilege denied to individual Alaskans) but cannot sell the land.
Individual Native allotments can be and are sold on the open market, however.
Various private interests own the remaining land, totaling about one percent of the state.
Alaska is, by a large margin, the state with the smallest percentage of private land ownership when Native corporation holdings are excluded.
The climate in Southeast Alaska is a mid-latitude oceanic climate (Köppen climate classification: "Cfb") in the southern sections and a subarctic oceanic climate (Köppen "Cfc") in the northern parts.
On an annual basis, Southeast is both the wettest and warmest part of Alaska with milder temperatures in the winter and high precipitation throughout the year.
Juneau averages over of precipitation a year, and Ketchikan averages over .
This is also the only region in Alaska in which the average daytime high temperature is above freezing during the winter months.
The climate of Anchorage and south central Alaska is mild by Alaskan standards due to the region's proximity to the seacoast.
While the area gets less rain than southeast Alaska, it gets more snow, and days tend to be clearer.
On average, Anchorage receives of precipitation a year, with around of snow, although there are areas in the south central which receive far more snow.
It is a subarctic climate () due to its brief, cool summers.
The climate of Western Alaska is determined in large part by the Bering Sea and the Gulf of Alaska.
It is a subarctic oceanic climate in the southwest and a continental subarctic climate farther north.
The temperature is somewhat moderate considering how far north the area is.
This region has a tremendous amount of variety in precipitation.
An area stretching from the northern side of the Seward Peninsula to the Kobuk River valley (i. e., the region around Kotzebue Sound) is technically a desert, with portions receiving less than of precipitation annually.
On the other extreme, some locations between Dillingham and Bethel average around of precipitation.
The climate of the interior of Alaska is subarctic.
Some of the highest and lowest temperatures in Alaska occur around the area near Fairbanks.
The summers may have temperatures reaching into the 90s °F (the low-to-mid 30s °C), while in the winter, the temperature can fall below .
Precipitation is sparse in the Interior, often less than a year, but what precipitation falls in the winter tends to stay the entire winter.
The highest and lowest recorded temperatures in Alaska are both in the Interior.
The highest is in Fort Yukon (which is just inside the arctic circle) on June 27, 1915, making Alaska tied with Hawaii as the state with the lowest high temperature in the United States.
The lowest official Alaska temperature is in Prospect Creek on January 23, 1971, one degree above the lowest temperature recorded in continental North America (in Snag, Yukon, Canada).
The climate in the extreme north of Alaska is Arctic () with long, very cold winters and short, cool summers.
Even in July, the average low temperature in Utqiagvik is .
Precipitation is light in this part of Alaska, with many places averaging less than per year, mostly as snow which stays on the ground almost the entire year.
Numerous indigenous peoples occupied Alaska for thousands of years before the arrival of European peoples to the area.
Linguistic and DNA studies done here have provided evidence for the settlement of North America by way of the Bering land bridge.
At the Upward Sun River site in the Tanana River Valley in Alaska, remains of a six-week-old infant were found.
The baby's DNA showed that she belonged to a population that was genetically separate from other native groups present elsewhere in the New World at the end of the Pleistocene.
Ben Potter, the University of Alaska Fairbanks archaeologist who unearthed the remains at the Upward River Sun site in 2013, named this new group Ancient Beringians.
The Tlingit people developed a society with a matrilineal kinship system of property inheritance and descent in what is today Southeast Alaska, along with parts of British Columbia and the Yukon.
Also in Southeast were the Haida, now well known for their unique arts.
The Tsimshian people came to Alaska from British Columbia in 1887, when President Grover Cleveland, and later the U.S.
Congress, granted them permission to settle on Annette Island and found the town of Metlakatla.
All three of these peoples, as well as other indigenous peoples of the Pacific Northwest Coast, experienced smallpox outbreaks from the late 18th through the mid-19th century, with the most devastating epidemics occurring in the 1830s and 1860s, resulting in high fatalities and social disruption.
The Aleutian Islands are still home to the Aleut people's seafaring society, although they were the first Native Alaskans to be exploited by Russians.
Western and Southwestern Alaska are home to the Yup'ik, while their cousins the Alutiiq ~ Sugpiaq lived in what is now Southcentral Alaska.
The Gwich'in people of the northern Interior region are Athabaskan and primarily known today for their dependence on the caribou within the much-contested Arctic National Wildlife Refuge.
The North Slope and Little Diomede Island are occupied by the widespread Inupiat people.
Some researchers believe that the first Russian settlement in Alaska was established in the 17th century.
According to this hypothesis, in 1648 several koches of Semyon Dezhnyov's expedition came ashore in Alaska by storm and founded this settlement.
This hypothesis is based on the testimony of Chukchi geographer Nikolai Daurkin, who had visited Alaska in 1764–1765 and who had reported on a village on the Kheuveren River, populated by "bearded men" who "pray to the icons".
Some modern researchers associate Kheuveren with Koyuk River.
The first European vessel to reach Alaska is generally held to be the "St.
Gabriel" under the authority of the surveyor M. S. Gvozdev and assistant navigator I. Fyodorov on August 21, 1732, during an expedition of Siberian cossak A. F. Shestakov and Belarusian explorer Dmitry Pavlutsky (1729–1735).
Another European contact with Alaska occurred in 1741, when Vitus Bering led an expedition for the Russian Navy aboard the "St.
Peter".
After his crew returned to Russia with sea otter pelts judged to be the finest fur in the world, small associations of fur traders began to sail from the shores of Siberia toward the Aleutian Islands.
The first permanent European settlement was founded in 1784.
Between 1774 and 1800, Spain sent several expeditions to Alaska in order to assert its claim over the Pacific Northwest.
In 1789 a Spanish settlement and fort were built in Nootka Sound.
These expeditions gave names to places such as Valdez, Bucareli Sound, and Cordova.
Later, the Russian-American Company carried out an expanded colonization program during the early-to-mid-19th century.
Sitka, renamed New Archangel from 1804 to 1867, on Baranof Island in the Alexander Archipelago in what is now Southeast Alaska, became the capital of Russian America.
It remained the capital after the colony was transferred to the United States.
The Russians never fully colonized Alaska, and the colony was never very profitable.
Evidence of Russian settlement in names and churches survive throughout southeast Alaska.
On March 30, 1867, the United States purchased Alaska from the Russian Empire for the sum of $7.2 million.
It was not until October of that year that the commissioners arrived in Sitka and the formal transfer was arranged.
The formal flag-raising took place at Fort Sitka on October 18, 1867.
The original ceremony included 250 uniformed U.S.
soldiers, who marched to the governor's house at "Castle Hill".
Here the Russian troops lowered the Russian flag and the U.S.
flag was raised.
This event is celebrated as Alaska Day, a legal holiday on the 18th of October.
William H. Seward, the United States Secretary of State, negotiated the Alaska Purchase (also known as Seward's Folly) with the Russians in 1867 for $7.2 million.
Alaska was loosely governed by the military initially, and was administered as a district starting in 1884, with a governor appointed by the President of the United States.
A federal district court was headquartered in Sitka.
For most of Alaska's first decade under the United States flag, Sitka was the only community inhabited by American settlers.
They organized a "provisional city government", which was Alaska's first municipal government, but not in a legal sense.
Legislation allowing Alaskan communities to legally incorporate as cities did not come about until 1900, and home rule for cities was extremely limited or unavailable until statehood took effect in 1959.
Starting in the 1890s and stretching in some places to the early 1910s, gold rushes in Alaska and the nearby Yukon Territory brought thousands of miners and settlers to Alaska.
Alaska was officially incorporated as an organized territory in 1912.
Alaska's capital, which had been in Sitka until 1906, was moved north to Juneau.
Construction of the Alaska Governor's Mansion began that same year.
European immigrants from Norway and Sweden also settled in southeast Alaska, where they entered the fishing and logging industries.
During World War II, the Aleutian Islands Campaign focused on the three outer Aleutian Islands – Attu, Agattu and Kiska – that were invaded by Japanese troops and occupied between June 1942 and August 1943.
During the occupation, one Aleut civilian was killed by Japanese troops and nearly fifty were interned in Japan, where about half of them died.
Unalaska/Dutch Harbor became a significant base for the United States Army Air Forces and Navy submariners.
The United States Lend-Lease program involved the flying of American warplanes through Canada to Fairbanks and then Nome; Soviet pilots took possession of these aircraft, ferrying them to fight the German invasion of the Soviet Union.
The construction of military bases contributed to the population growth of some Alaskan cities.
Statehood for Alaska was an important cause of James Wickersham early in his tenure as a congressional delegate.
Decades later, the statehood movement gained its first real momentum following a territorial referendum in 1946.
The Alaska Statehood Committee and Alaska's Constitutional Convention would soon follow.
Statehood supporters also found themselves fighting major battles against political foes, mostly in the U.S.
Congress but also within Alaska.
Statehood was approved by Congress on July 7, 1958.
Alaska was officially proclaimed a state on January 3, 1959.
In 1960, the Census Bureau reported Alaska's population as 77.2% White, 3% Black, and 18.8% American Indian and Alaska Native.
On March 27, 1964, the massive Good Friday earthquake killed 133 people and destroyed several villages and portions of large coastal communities, mainly by the resultant tsunamis and landslides.
It was the second-most-powerful earthquake in the recorded history of the world, with a moment magnitude of 9.2.
It was over one thousand times more powerful than the 1989 San Francisco earthquake.
The time of day (5:36 pm), time of year and location of the epicenter were all cited as factors in potentially sparing thousands of lives, particularly in Anchorage.
The 1968 discovery of oil at Prudhoe Bay and the 1977 completion of the Trans-Alaska Pipeline System led to an oil boom.
Royalty revenues from oil have funded large state budgets from 1980 onward.
That same year, not coincidentally, Alaska repealed its state income tax.
In 1989, the "Exxon Valdez" hit a reef in the Prince William Sound, spilling over of crude oil over of coastline.
Today, the battle between philosophies of development and conservation is seen in the contentious debate over oil drilling in the Arctic National Wildlife Refuge and the proposed Pebble Mine.
The Alaska Heritage Resources Survey (AHRS) is a restricted inventory of all reported historic and prehistoric sites within the state of Alaska; it is maintained by the Office of History and Archaeology.
The survey's inventory of cultural resources includes objects, structures, buildings, sites, districts, and travel ways, with a general provision that they are over 50 years old.
, over 35,000 sites have been reported.
The United States Census Bureau estimates that the population of Alaska was 738,432 on July 1, 2015, a 3.97% increase since the 2010 United States Census.
In 2010, Alaska ranked as the 47th state by population, ahead of North Dakota, Vermont, and Wyoming (and Washington, D.C.).
Estimates show North Dakota ahead .
Alaska is the least densely populated state, and one of the most sparsely populated areas in the world, at , with the next state, Wyoming, at .
Alaska is the largest U.S.
state by area, and the tenth wealthiest (per capita income).
, the state's unemployment rate was 6.6%.
, it is one of 14 U.S.
states that still has only one telephone area code.
According to the 2010 United States Census, Alaska, had a population of 710,231.
In terms of race and ethnicity, the state was 66.7% White (64.1% Non-Hispanic White), 14.8% American Indian and Alaska Native, 5.4% Asian, 3.3% Black or African American, 1.0% Native Hawaiian and Other Pacific Islander, 1.6% from Some Other Race, and 7.3% from Two or More Races.
Hispanics or Latinos of any race made up 5.5% of the population.
, 50.7% of Alaska's population younger than one year of age belonged to minority groups (i.e., did not have two parents of non-Hispanic white ancestry).
According to the 2011 American Community Survey, 83.4% of people over the age of five speak only English at home.
About 3.5% speak Spanish at home.
About 2.2% speak another Indo-European language at home and about 4.3% speak an Asian language (including Tagalog) at home.
About 5.3% speak other languages at home.
The Alaska Native Language Center at the University of Alaska Fairbanks claims that at least 20 Alaskan native languages exist and there are also some languages with different dialects.
Most of Alaska's native languages belong to either the Eskimo–Aleut or Na-Dene language families however some languages are thought to be isolates (e.g.
Haida) or have not yet been classified (e.g.
Tsimshianic).
nearly all of Alaska's native languages were classified as either threatened, shifting, moribund, nearly extinct, or dormant languages.
A total of 5.2% of Alaskans speak one of the state's 20 indigenous languages, known locally as "native languages".
In October 2014, the governor of Alaska signed a bill declaring the state's 20 indigenous languages as official languages.
This bill gave the languages symbolic recognition as official languages, though they have not been adopted for official use within the government.
The 20 languages that were included in the bill are:

According to statistics collected by the Association of Religion Data Archives from 2010, about 34% of Alaska residents were members of religious congregations.
100,960 people identified as Evangelical Protestants, 50,866 as Roman Catholic, and 32,550 as mainline Protestants.
Roughly 4% are Mormon, 0.5% are Jewish, 1% are Muslim, 0.5% are Buddhist, and 0.5% are Hindu.
The largest religious denominations in Alaska were the Catholic Church with 50,866 adherents, non-denominational Evangelical Protestants with 38,070 adherents, The Church of Jesus Christ of Latter-day Saints with 32,170 adherents, and the Southern Baptist Convention with 19,891 adherents.
Alaska has been identified, along with Pacific Northwest states Washington and Oregon, as being the least religious states of the USA, in terms of church membership.
In 1795, the First Russian Orthodox Church was established in Kodiak.
Intermarriage with Alaskan Natives helped the Russian immigrants integrate into society.
As a result, an increasing number of Russian Orthodox churches gradually became established within Alaska.
Alaska also has the largest Quaker population (by percentage) of any state.
In 2009 there were 6,000 Jews in Alaska (for whom observance of halakha may pose special problems).
Alaskan Hindus often share venues and celebrations with members of other Asian religious communities, including Sikhs and Jains.
Estimates for the number of Muslims in Alaska range from 2,000 to 5,000.
The Islamic Community Center of Anchorage began efforts in the late 1990s to construct a mosque in Anchorage.
They broke ground on a building in south Anchorage in 2010 and were nearing completion in late 2014.
When completed, the mosque will be the first in the state and one of the northernmost mosques in the world.
The 2007 gross state product was $44.9 billion, 45th in the nation.
Its per capita personal income for 2007 was $40,042, ranking 15th in the nation.
According to a 2013 study by Phoenix Marketing International, Alaska had the fifth-largest number of millionaires per capita in the United States, with a ratio of 6.75 percent.
The oil and gas industry dominates the Alaskan economy, with more than 80% of the state's revenues derived from petroleum extraction.
Alaska's main export product (excluding oil and natural gas) is seafood, primarily salmon, cod, Pollock and crab.
Agriculture represents a very small fraction of the Alaskan economy.
Agricultural production is primarily for consumption within the state and includes nursery stock, dairy products, vegetables, and livestock.
Manufacturing is limited, with most foodstuffs and general goods imported from elsewhere.
Employment is primarily in government and industries such as natural resource extraction, shipping, and transportation.
Military bases are a significant component of the economy in the Fairbanks North Star, Anchorage and Kodiak Island boroughs, as well as Kodiak.
Federal subsidies are also an important part of the economy, allowing the state to keep taxes low.
Its industrial outputs are crude petroleum, natural gas, coal, gold, precious metals, zinc and other mining, seafood processing, timber and wood products.
There is also a growing service and tourism sector.
Tourists have contributed to the economy by supporting local lodging.
Alaska has vast energy resources, although its oil reserves have been largely depleted.
Major oil and gas reserves were found in the Alaska North Slope (ANS) and Cook Inlet basins, but according to the Energy Information Administration, by February 2014 Alaska had fallen to fourth place in the nation in crude oil production after Texas, North Dakota, and California.
Prudhoe Bay on Alaska's North Slope is still the second highest-yielding oil field in the United States, typically producing about , although by early 2014 North Dakota's Bakken Formation was producing over .
Prudhoe Bay was the largest conventional oil field ever discovered in North America, but was much smaller than Canada's enormous Athabasca oil sands field, which by 2014 was producing about of unconventional oil, and had hundreds of years of producible reserves at that rate.
The Trans-Alaska Pipeline can transport and pump up to of crude oil per day, more than any other crude oil pipeline in the United States.
Additionally, substantial coal deposits are found in Alaska's bituminous, sub-bituminous, and lignite coal basins.
The United States Geological Survey estimates that there are of undiscovered, technically recoverable gas from natural gas hydrates on the Alaskan North Slope.
Alaska also offers some of the highest hydroelectric power potential in the country from its numerous rivers.
Large swaths of the Alaskan coastline offer wind and geothermal energy potential as well.
Alaska's economy depends heavily on increasingly expensive diesel fuel for heating, transportation, electric power and light.
Although wind and hydroelectric power are abundant and underdeveloped, proposals for statewide energy systems (e.g.
with special low-cost electric interties) were judged uneconomical (at the time of the report, 2001) due to low (less than 50¢/gal) fuel prices, long distances and low population.
The cost of a gallon of gas in urban Alaska today is usually 30–60¢ higher than the national average; prices in rural areas are generally significantly higher but vary widely depending on transportation costs, seasonal usage peaks, nearby petroleum development infrastructure and many other factors.
The Alaska Permanent Fund is a constitutionally authorized appropriation of oil revenues, established by voters in 1976 to manage a surplus in state petroleum revenues from oil, largely in anticipation of the then recently constructed Trans-Alaska Pipeline System.
The fund was originally proposed by Governor Keith Miller on the eve of the 1969 Prudhoe Bay lease sale, out of fear that the legislature would spend the entire proceeds of the sale (which amounted to $900 million) at once.
It was later championed by Governor Jay Hammond and Kenai state representative Hugh Malone.
It has served as an attractive political prospect ever since, diverting revenues which would normally be deposited into the general fund.
The Alaska Constitution was written so as to discourage dedicating state funds for a particular purpose.
The Permanent Fund has become the rare exception to this, mostly due to the political climate of distrust existing during the time of its creation.
From its initial principal of $734,000, the fund has grown to $50 billion as a result of oil royalties and capital investment programs.
Most if not all the principal is invested conservatively outside Alaska.
This has led to frequent calls by Alaskan politicians for the Fund to make investments within Alaska, though such a stance has never gained momentum.
Starting in 1982, dividends from the fund's annual growth have been paid out each year to eligible Alaskans, ranging from an initial $1,000 in 1982 (equal to three years' payout, as the distribution of payments was held up in a lawsuit over the distribution scheme) to $3,269 in 2008 (which included a one-time $1,200 "Resource Rebate").
Every year, the state legislature takes out 8% from the earnings, puts 3% back into the principal for inflation proofing, and the remaining 5% is distributed to all qualifying Alaskans.
To qualify for the Permanent Fund Dividend, one must have lived in the state for a minimum of 12 months, maintain constant residency subject to allowable absences, and not be subject to court judgments or criminal convictions which fall under various disqualifying classifications or may subject the payment amount to civil garnishment.
The Permanent Fund is often considered to be one of the leading examples of a "Basic Income" policy in the world.
The cost of goods in Alaska has long been higher than in the contiguous 48 states.
Federal government employees, particularly United States Postal Service (USPS) workers and active-duty military members, receive a Cost of Living Allowance usually set at 25% of base pay because, while the cost of living has gone down, it is still one of the highest in the country.
Rural Alaska suffers from extremely high prices for food and consumer goods compared to the rest of the country, due to the relatively limited transportation infrastructure.
Due to the northern climate and short growing season, relatively little farming occurs in Alaska.
Most farms are in either the Matanuska Valley, about northeast of Anchorage, or on the Kenai Peninsula, about southwest of Anchorage.
The short 100-day growing season limits the crops that can be grown, but the long sunny summer days make for productive growing seasons.
The primary crops are potatoes, carrots, lettuce, and cabbage.
The Tanana Valley is another notable agricultural locus, especially the Delta Junction area, about southeast of Fairbanks, with a sizable concentration of farms growing agronomic crops; these farms mostly lie north and east of Fort Greely.
This area was largely set aside and developed under a state program spearheaded by Hammond during his second term as governor.
Delta-area crops consist predominantly of barley and hay.
West of Fairbanks lies another concentration of small farms catering to restaurants, the hotel and tourist industry, and community-supported agriculture.
Alaskan agriculture has experienced a surge in growth of market gardeners, small farms and farmers' markets in recent years, with the highest percentage increase (46%) in the nation in growth in farmers' markets in 2011, compared to 17% nationwide.
The peony industry has also taken off, as the growing season allows farmers to harvest during a gap in supply elsewhere in the world, thereby filling a niche in the flower market.
Alaska, with no counties, lacks county fairs.
However, a small assortment of state and local fairs (with the Alaska State Fair in Palmer the largest), are held mostly in the late summer.
The fairs are mostly located in communities with historic or current agricultural activity, and feature local farmers exhibiting produce in addition to more high-profile commercial activities such as carnival rides, concerts and food.
"Alaska Grown" is used as an agricultural slogan.
Alaska has an abundance of seafood, with the primary fisheries in the Bering Sea and the North Pacific.
Seafood is one of the few food items that is often cheaper within the state than outside it.
Many Alaskans take advantage of salmon seasons to harvest portions of their household diet while fishing for subsistence, as well as sport.
This includes fish taken by hook, net or wheel.
Hunting for subsistence, primarily caribou, moose, and Dall sheep is still common in the state, particularly in remote Bush communities.
An example of a traditional native food is Akutaq, the Eskimo ice cream, which can consist of reindeer fat, seal oil, dried fish meat and local berries.
Alaska's reindeer herding is concentrated on Seward Peninsula, where wild caribou can be prevented from mingling and migrating with the domesticated reindeer.
Most food in Alaska is transported into the state from "Outside", and shipping costs make food in the cities relatively expensive.
In rural areas, subsistence hunting and gathering is an essential activity because imported food is prohibitively expensive.
Although most small towns and villages in Alaska lie along the coastline, the cost of importing food to remote villages can be high, because of the terrain and difficult road conditions, which change dramatically, due to varying climate and precipitation changes.
The cost of transport can reach as high as 50¢ per pound ($1.10/kg) or more in some remote areas, during the most difficult times, if these locations can be reached at all during such inclement weather and terrain conditions.
The cost of delivering a of milk is about $3.50 in many villages where per capita income can be $20,000 or less.
Fuel cost per gallon is routinely 20–30¢ higher than the continental United States average, with only Hawaii having higher prices.
Alaska has few road connections compared to the rest of the U.S.
The state's road system covers a relatively small area of the state, linking the central population centers and the Alaska Highway, the principal route out of the state through Canada.
The state capital, Juneau, is not accessible by road, only a car ferry, which has spurred several debates over the decades about moving the capital to a city on the road system, or building a road connection from Haines.
The western part of Alaska has no road system connecting the communities with the rest of Alaska.
One unique feature of the Alaska Highway system is the Anton Anderson Memorial Tunnel, an active Alaska Railroad tunnel recently upgraded to provide a paved roadway link with the isolated community of Whittier on Prince William Sound to the Seward Highway about southeast of Anchorage at Portage.
At , the tunnel was the longest road tunnel in North America until 2007.
The tunnel is the longest combination road and rail tunnel in North America.
Built around 1915, the Alaska Railroad (ARR) played a key role in the development of Alaska through the 20th century.
It links north Pacific shipping through providing critical infrastructure with tracks that run from Seward to Interior Alaska by way of South Central Alaska, passing through Anchorage, Eklutna, Wasilla, Talkeetna, Denali, and Fairbanks, with spurs to Whittier, Palmer and North Pole.
The cities, towns, villages, and region served by ARR tracks are known statewide as "The Railbelt".
In recent years, the ever-improving paved highway system began to eclipse the railroad's importance in Alaska's economy.
The railroad played a vital role in Alaska's development, moving freight into Alaska while transporting natural resources southward (i.e., coal from the Usibelli coal mine near Healy to Seward and gravel from the Matanuska Valley to Anchorage).
It is well known for its summertime tour passenger service.
The Alaska Railroad was one of the last railroads in North America to use cabooses in regular service and still uses them on some gravel trains.
It continues to offer one of the last flag stop routes in the country.
A stretch of about of track along an area north of Talkeetna remains inaccessible by road; the railroad provides the only transportation to rural homes and cabins in the area.
Until construction of the Parks Highway in the 1970s, the railroad provided the only land access to most of the region along its entire route.
In northern Southeast Alaska, the White Pass and Yukon Route also partly runs through the state from Skagway northwards into Canada (British Columbia and Yukon Territory), crossing the border at White Pass Summit.
This line is now mainly used by tourists, often arriving by cruise liner at Skagway.
It was featured in the 1983 BBC television series "Great Little Railways."
The Alaska Rail network is not connected to Outside.
In 2000, the U.S.
Congress authorized $6 million to study the feasibility of a rail link between Alaska, Canada, and the lower 48.
Alaska Rail Marine provides car float service between Whittier and Seattle.
Many cities, towns and villages in the state do not have road or highway access; the only modes of access involve travel by air, river, or the sea.
Alaska's well-developed state-owned ferry system (known as the Alaska Marine Highway) serves the cities of southeast, the Gulf Coast and the Alaska Peninsula.
The ferries transport vehicles as well as passengers.
The system also operates a ferry service from Bellingham, Washington and Prince Rupert, British Columbia, in Canada through the Inside Passage to Skagway.
The Inter-Island Ferry Authority also serves as an important marine link for many communities in the Prince of Wales Island region of Southeast and works in concert with the Alaska Marine Highway.
In recent years, cruise lines have created a summertime tourism market, mainly connecting the Pacific Northwest to Southeast Alaska and, to a lesser degree, towns along Alaska's gulf coast.
The population of Ketchikan may rise by over 10,000 people on many days during the summer, as up to four large cruise ships at a time can dock, debarking thousands of passengers.
Cities not served by road, sea, or river can be reached only by air, foot, dogsled, or snowmachine, accounting for Alaska's extremely well developed bush air services—an Alaskan novelty.
Anchorage and, to a lesser extent Fairbanks, is served by many major airlines.
Because of limited highway access, air travel remains the most efficient form of transportation in and out of the state.
Anchorage recently completed extensive remodeling and construction at Ted Stevens Anchorage International Airport to help accommodate the upsurge in tourism (in 2012–2013, Alaska received almost 2 million visitors).
Regular flights to most villages and towns within the state that are commercially viable are challenging to provide, so they are heavily subsidized by the federal government through the Essential Air Service program.
Alaska Airlines is the only major airline offering in-state travel with jet service (sometimes in combination cargo and passenger Boeing 737-400s) from Anchorage and Fairbanks to regional hubs like Bethel, Nome, Kotzebue, Dillingham, Kodiak, and other larger communities as well as to major Southeast and Alaska Peninsula communities.
The bulk of remaining commercial flight offerings come from small regional commuter airlines such as Ravn Alaska, PenAir, and Frontier Flying Service.
The smallest towns and villages must rely on scheduled or chartered bush flying services using general aviation aircraft such as the Cessna Caravan, the most popular aircraft in use in the state.
Much of this service can be attributed to the Alaska bypass mail program which subsidizes bulk mail delivery to Alaskan rural communities.
The program requires 70% of that subsidy to go to carriers who offer passenger service to the communities.
Many communities have small air taxi services.
These operations originated from the demand for customized transport to remote areas.
Perhaps the most quintessentially Alaskan plane is the bush seaplane.
The world's busiest seaplane base is Lake Hood, located next to Ted Stevens Anchorage International Airport, where flights bound for remote villages without an airstrip carry passengers, cargo, and many items from stores and warehouse clubs.
In 2006 Alaska had the highest number of pilots per capita of any U.S.
state.
Another Alaskan transportation method is the dogsled.
In modern times (that is, any time after the mid-late 1920s), dog mushing is more of a sport than a true means of transportation.
Various races are held around the state, but the best known is the Iditarod Trail Sled Dog Race, a trail from Anchorage to Nome (although the distance varies from year to year, the official distance is set at ).
The race commemorates the famous 1925 serum run to Nome in which mushers and dogs like Togo and Balto took much-needed medicine to the diphtheria-stricken community of Nome when all other means of transportation had failed.
Mushers from all over the world come to Anchorage each March to compete for cash, prizes, and prestige.
The "Serum Run" is another sled dog race that more accurately follows the route of the famous 1925 relay, leaving from the community of Nenana (southwest of Fairbanks) to Nome.
In areas not served by road or rail, primary transportation in summer is by all-terrain vehicle and in winter by snowmobile or "snow machine", as it is commonly referred to in Alaska.
Alaska's internet and other data transport systems are provided largely through the two major telecommunications companies: GCI and Alaska Communications.
GCI owns and operates what it calls the Alaska United Fiber Optic system and as of late 2011 Alaska Communications advertised that it has "two fiber optic paths to the lower 48 and two more across Alaska.
In January 2011, it was reported that a $1 billion project to connect Asia and rural Alaska was being planned, aided in part by $350 million in stimulus from the federal government.
Like all other U.S.
states, Alaska is governed as a republic, with three branches of government: an executive branch consisting of the Governor of Alaska and his appointees which head executive departments; a legislative branch consisting of the Alaska House of Representatives and Alaska Senate; and a judicial branch consisting of the Alaska Supreme Court and lower courts.
The state of Alaska employs approximately 16,000 people statewide.
The Alaska Legislature consists of a 40-member House of Representatives and a 20-member Senate.
Senators serve four-year terms and House members two.
The Governor of Alaska serves four-year terms.
The lieutenant governor runs separately from the governor in the primaries, but during the general election, the nominee for governor and nominee for lieutenant governor run together on the same ticket.
Alaska's court system has four levels: the Alaska Supreme Court, the Alaska Court of Appeals, the superior courts and the district courts.
The superior and district courts are trial courts.
Superior courts are courts of general jurisdiction, while district courts only hear certain types of cases, including misdemeanor criminal cases and civil cases valued up to $100,000.
The Supreme Court and the Court of Appeals are appellate courts.
The Court of Appeals is required to hear appeals from certain lower-court decisions, including those regarding criminal prosecutions, juvenile delinquency, and habeas corpus.
The Supreme Court hears civil appeals and may in its discretion hear criminal appeals.
Although in its early years of statehood Alaska was a Democratic state, since the early 1970s it has been characterized as Republican-leaning.
Local political communities have often worked on issues related to land use development, fishing, tourism, and individual rights.
Alaska Natives, while organized in and around their communities, have been active within the Native corporations.
These have been given ownership over large tracts of land, which require stewardship.
Alaska was formerly the only state in which possession of one ounce or less of marijuana in one's home was completely legal under state law, though the federal law remains in force.
The state has an independence movement favoring a vote on secession from the United States, with the Alaskan Independence Party.
Six Republicans and four Democrats have served as governor of Alaska.
In addition, Republican Governor Wally Hickel was elected to the office for a second term in 1990 after leaving the Republican party and briefly joining the Alaskan Independence Party ticket just long enough to be reelected.
He officially rejoined the Republican party in 1994.
Alaska's voter initiative making marijuana legal took effect on February 24, 2015, placing Alaska alongside Colorado and Washington as the first three U.S.
states where recreational marijuana is legal.
The new law means people over age 21 can consume small amounts of pot – if they can find it.
There is a rather lengthy and involved application process, per Alaska Measure 2 (2014).
The first legal marijuana store opened in Valdez in October 2016.
To finance state government operations, Alaska depends primarily on petroleum revenues and federal subsidies.
This allows it to have the lowest individual tax burden in the United States.
It is one of five states with no state sales tax, one of seven states that do not levy an individual income tax, and one of the two states that has neither.
The Department of Revenue Tax Division reports regularly on the state's revenue sources.
The Department also issues an annual summary of its operations, including new state laws that directly affect the tax division.
While Alaska has no state sales tax, 89 municipalities collect a local sales tax, from 1.0–7.5%, typically 3–5%.
Other local taxes levied include raw fish taxes, hotel, motel, and bed-and-breakfast 'bed' taxes, severance taxes, liquor and tobacco taxes, gaming (pull tabs) taxes, tire taxes and fuel transfer taxes.
A part of the revenue collected from certain state taxes and license fees (such as petroleum, aviation motor fuel, telephone cooperative) is shared with municipalities in Alaska.
Fairbanks has one of the highest property taxes in the state as no sales or income taxes are assessed in the Fairbanks North Star Borough (FNSB).
A sales tax for the FNSB has been voted on many times, but has yet to be approved, leading lawmakers to increase taxes dramatically on goods such as liquor and tobacco.
In 2014 the Tax Foundation ranked Alaska as having the fourth most "business friendly" tax policy, behind only Wyoming, South Dakota, and Nevada.
Alaska regularly supports Republicans in presidential elections and has done so since statehood.
Republicans have won the state's electoral college votes in all but one election that it has participated in (1964).
No state has voted for a Democratic presidential candidate fewer times.
Alaska was carried by Democratic nominee Lyndon B. Johnson during his landslide election in 1964, while the 1960 and 1968 elections were close.
Since 1972, however, Republicans have carried the state by large margins.
In 2008, Republican John McCain defeated Democrat Barack Obama in Alaska, 59.49% to 37.83%.
McCain's running mate was Sarah Palin, the state's governor and the first Alaskan on a major party ticket.
Obama lost Alaska again in 2012, but he captured 40% of the state's vote in that election, making him the first Democrat to do so since 1968.
The Alaska Bush, central Juneau, midtown and downtown Anchorage, and the areas surrounding the University of Alaska Fairbanks campus and Ester have been strongholds of the Democratic Party.
The Matanuska-Susitna Borough, the majority of Fairbanks (including North Pole and the military base), and South Anchorage typically have the strongest Republican showing.
, well over half of all registered voters have chosen "Non-Partisan" or "Undeclared" as their affiliation, despite recent attempts to close primaries to unaffiliated voters.
Because of its population relative to other U.S.
states, Alaska has only one member in the U.S.
House of Representatives.
This seat is held by Republican Don Young, who was re-elected to his 21st consecutive term in 2012.
Alaska's at-large congressional district is one of the largest parliamentary constituencies in the world by area.
In 2008, Governor Sarah Palin became the first Republican woman to run on a national ticket when she became John McCain's running mate.
She continued to be a prominent national figure even after resigning from the governor's job in July 2009.
Alaska's United States Senators belong to Class 2 and Class 3.
In 2008, Democrat Mark Begich, mayor of Anchorage, defeated long-time Republican senator Ted Stevens.
Stevens had been convicted on seven felony counts of failing to report gifts on Senate financial discloser forms one week before the election.
The conviction was set aside in April 2009 after evidence of prosecutorial misconduct emerged.
Republican Frank Murkowski held the state's other senatorial position.
After being elected governor in 2002, he resigned from the Senate and appointed his daughter, State Representative Lisa Murkowski as his successor.
She won full six-year terms in 2004 and 2010.
Valerie Davidson is currently serving as Lieutenant Governor following the resignation of Byron Mallott.
Alaska is not divided into counties, as most of the other U.S.
states, but it is divided into "boroughs".
Many of the more densely populated parts of the state are part of Alaska's 16 boroughs, which function somewhat similarly to counties in other states.
However, unlike county-equivalents in the other 49 states, the boroughs do not cover the entire land area of the state.
The area not part of any borough is referred to as the Unorganized Borough.
The Unorganized Borough has no government of its own, but the U.S.
Census Bureau in cooperation with the state divided the Unorganized Borough into 11 census areas solely for the purposes of statistical analysis and presentation.
A "recording district" is a mechanism for administration of the public record in Alaska.
The state is divided into 34 recording districts which are centrally administered under a State Recorder.
All recording districts use the same acceptance criteria, fee schedule, etc., for accepting documents into the public record.
Whereas many U.S.
states use a three-tiered system of decentralization—state/county/township—most of Alaska uses only two tiers—state/borough.
Owing to the low population density, most of the land is located in the Unorganized Borough.
As the name implies, it has no intermediate borough government but is administered directly by the state government.
In 2000, 57.71% of Alaska's area has this status, with 13.05% of the population.
Anchorage merged the city government with the Greater Anchorage Area Borough in 1975 to form the Municipality of Anchorage, containing the city proper and the communities of Eagle River, Chugiak, Peters Creek, Girdwood, Bird, and Indian.
Fairbanks has a separate borough (the Fairbanks North Star Borough) and municipality (the City of Fairbanks).
The state's most populous city is Anchorage, home to 278,700 people in 2006, 225,744 of whom live in the urbanized area.
The richest location in Alaska by per capita income is Halibut Cove ($89,895).
Yakutat City, Sitka, Juneau, and Anchorage are the four largest cities in the U.S.
by area.
As reflected in the 2010 United States Census, Alaska has a total of 355 incorporated cities and census-designated places (CDPs).
The tally of cities includes four unified municipalities, essentially the equivalent of a consolidated city–county.
The majority of these communities are located in the rural expanse of Alaska known as "The Bush" and are unconnected to the contiguous North American road network.
The table at the bottom of this section lists the 100 largest cities and census-designated places in Alaska, in population order.
Of Alaska's 2010 Census population figure of 710,231, 20,429 people, or 2.88% of the population, did not live in an incorporated city or census-designated place.
Approximately three-quarters of that figure were people who live in urban and suburban neighborhoods on the outskirts of the city limits of Ketchikan, Kodiak, Palmer and Wasilla.
CDPs have not been established for these areas by the United States Census Bureau, except that seven CDPs were established for the Ketchikan-area neighborhoods in the 1980 Census (Clover Pass, Herring Cove, Ketchikan East, Mountain Point, North Tongass Highway, Pennock Island and Saxman East), but have not been used since.
The remaining population was scattered throughout Alaska, both within organized boroughs and in the Unorganized Borough, in largely remote areas.
The Alaska Department of Education and Early Development administers many school districts in Alaska.
In addition, the state operates a boarding school, Mt.
Edgecumbe High School in Sitka, and provides partial funding for other boarding schools, including Nenana Student Living Center in Nenana and The Galena Interior Learning Academy in Galena.
There are more than a dozen colleges and universities in Alaska.
Accredited universities in Alaska include the University of Alaska Anchorage, University of Alaska Fairbanks, University of Alaska Southeast, and Alaska Pacific University.
Alaska is the only state that has no institutions that are part of NCAA Division I.

The Alaska Department of Labor and Workforce Development operates AVTEC, Alaska's Institute of Technology.
Campuses in Seward and Anchorage offer 1 week to 11-month training programs in areas as diverse as Information Technology, Welding, Nursing, and Mechanics.
Alaska has had a problem with a "brain drain".
Many of its young people, including most of the highest academic achievers, leave the state after high school graduation and do not return.
, Alaska did not have a law school or medical school.
The University of Alaska has attempted to combat this by offering partial four-year scholarships to the top 10% of Alaska high school graduates, via the Alaska Scholars Program.
The Alaska State Troopers are Alaska's statewide police force.
They have a long and storied history, but were not an official organization until 1941.
Before the force was officially organized, law enforcement in Alaska was handled by various federal agencies.
Larger towns usually have their own local police and some villages rely on "Public Safety Officers" who have police training but do not carry firearms.
In much of the state, the troopers serve as the only police force available.
In addition to enforcing traffic and criminal law, wildlife Troopers enforce hunting and fishing regulations.
Due to the varied terrain and wide scope of the Troopers' duties, they employ a wide variety of land, air, and water patrol vehicles.
Many rural communities in Alaska are considered "dry", having outlawed the importation of alcoholic beverages.
Suicide rates for rural residents are higher than urban.
Domestic abuse and other violent crimes are also at high levels in the state; this is in part linked to alcohol abuse.
Alaska has the highest rate of sexual assault in the nation, especially in rural areas.
The average age of sexually assaulted victims is 16 years old.
In four out of five cases, the suspects were relatives, friends or acquaintances.
Some of Alaska's popular annual events are the Iditarod Trail Sled Dog Race that starts in Anchorage and ends in Nome, World Ice Art Championships in Fairbanks, the Blueberry Festival and Alaska Hummingbird Festival in Ketchikan, the Sitka Whale Fest, and the Stikine River Garnet Fest in Wrangell.
The Stikine River attracts the largest springtime concentration of American bald eagles in the world.
The Alaska Native Heritage Center celebrates the rich heritage of Alaska's 11 cultural groups.
Their purpose is to encourage cross-cultural exchanges among all people and enhance self-esteem among Native people.
The Alaska Native Arts Foundation promotes and markets Native art from all regions and cultures in the State, using the internet.
Influences on music in Alaska include the traditional music of Alaska Natives as well as folk music brought by later immigrants from Russia and Europe.
Prominent musicians from Alaska include singer Jewel, traditional Aleut flautist Mary Youngblood, folk singer-songwriter Libby Roderick, Christian music singer-songwriter Lincoln Brewster, metal/post hardcore band 36 Crazyfists and the groups Pamyua and Portugal.
The Man.
There are many established music festivals in Alaska, including the Alaska Folk Festival, the Fairbanks Summer Arts Festival, the Anchorage Folk Festival, the Athabascan Old-Time Fiddling Festival, the Sitka Jazz Festival, and the Sitka Summer Music Festival.
The most prominent orchestra in Alaska is the Anchorage Symphony Orchestra, though the Fairbanks Symphony Orchestra and Juneau Symphony are also notable.
The Anchorage Opera is currently the state's only professional opera company, though there are several volunteer and semi-professional organizations in the state as well.
The official state song of Alaska is "Alaska's Flag", which was adopted in 1955; it celebrates the flag of Alaska.
Alaska's first independent picture entirely made in Alaska was "The Chechahcos", produced by Alaskan businessman Austin E. Lathrop and filmed in and around Anchorage.
Released in 1924 by the Alaska Moving Picture Corporation, it was the only film the company made.
One of the most prominent movies filmed in Alaska is MGM's "Eskimo/Mala The Magnificent", starring Alaska Native Ray Mala.
In 1932 an expedition set out from MGM's studios in Hollywood to Alaska to film what was then billed as "The Biggest Picture Ever Made".
Upon arriving in Alaska, they set up "Camp Hollywood" in Northwest Alaska, where they lived during the duration of the filming.
Louis B. Mayer spared no expense in spite of the remote location, going so far as to hire the chef from the Hotel Roosevelt in Hollywood to prepare meals.
When "Eskimo" premiered at the Astor Theatre in New York City, the studio received the largest amount of feedback in its history to that point.
"Eskimo" was critically acclaimed and released worldwide; as a result, Mala became an international movie star.
"Eskimo" won the first Oscar for Best Film Editing at the Academy Awards, and showcased and preserved aspects of Inupiat culture on film.
The 1983 Disney movie "Never Cry Wolf" was at least partially shot in Alaska.
The 1991 film "White Fang", based on Jack London's novel and starring Ethan Hawke, was filmed in and around Haines.
Steven Seagal's 1994 "On Deadly Ground", starring Michael Caine, was filmed in part at the Worthington Glacier near Valdez.
The 1999 John Sayles film "Limbo", starring David Strathairn, Mary Elizabeth Mastrantonio, and Kris Kristofferson, was filmed in Juneau.
The psychological thriller "Insomnia", starring Al Pacino and Robin Williams, was shot in Canada, but was set in Alaska.
The 2007 film directed by Sean Penn, "Into The Wild", was partially filmed and set in Alaska.
The film, which is based on the novel of the same name, follows the adventures of Christopher McCandless, who died in a remote abandoned bus along the Stampede Trail west of Healy in 1992.
Many films and television shows set in Alaska are not filmed there; for example, "Northern Exposure", set in the fictional town of Cicely, Alaska, was filmed in Roslyn, Washington.
The 2007 horror feature "30 Days of Night" is set in Barrow, Alaska, but was filmed in New Zealand.
Many reality television shows are filmed in Alaska.
In 2011 the "Anchorage Daily News" found ten set in the state.
U.S.
federal government

Alaska state government


</doc>
<doc id="627" url="https://en.wikipedia.org/wiki?curid=627" title="Agriculture">
Agriculture

Agriculture is the cultivation of land and breeding of animals and plants to provide food, fiber, medicinal plants and other products to sustain and enhance life.
Agriculture was the key development in the rise of sedentary human civilization, whereby farming of domesticated species created food surpluses that enabled people to live in cities.
The study of agriculture is known as agricultural science.
The history of agriculture dates back thousands of years; people gathered wild grains at least 105,000 years ago and began to plant them around 11,500 years ago before they became domesticated.
Pigs, sheep, and cattle were domesticated over 10,000 years ago.
Crops originate from at least 11 regions of the world.
Industrial agriculture based on large-scale monoculture has in the past century come to dominate agricultural output, though about 2 billion people worldwide still depend on subsistence agriculture.
Modern agronomy, plant breeding, agrochemicals such as pesticides and fertilizers, and technological developments have sharply increased yields from cultivation, but at the same time have caused widespread ecological and environmental damage.
Selective breeding and modern practices in animal husbandry have similarly increased the output of meat, but have raised concerns about animal welfare and environmental damage through contributions to global warming, depletion of aquifers, deforestation, antibiotic resistance, and growth hormones in industrially produced meat.
Genetically modified organisms are widely used, although they are banned in several countries.
The major agricultural products can be broadly grouped into foods, fibers, fuels, and raw materials (such as rubber).
Classes of foods include cereals (grains), vegetables, fruits, oils, meat, milk, fungi and eggs.
Over one-third of the world's workers are employed in agriculture, second only to the service sector, although the number of agricultural workers in developed countries has decreased significantly over the past several centuries.
The word "agriculture" is a late Middle English adaptation of Latin "agricultūra", from "ager", "field", which in its turn came from Greek αγρός, and "cultūra", "cultivation" or "growing".
Agriculture usually refers to human activities, although it is also observed in certain species of ant, termite and ambrosia beetle.
Agriculture is defined with varying scopes, in its broadest sense using natural resources to "produce commodities which maintain life, including food, fiber, forest products, horticultural crops, and their related services".
Thus defined, it includes arable farming, horticulture, animal husbandry, and forestry, but horticulture and forestry are in practice often excluded.
The development of agriculture enabled the human population to grow many times larger than could be sustained by hunting and gathering.
Agriculture began independently in different parts of the globe, and included a diverse range of taxa.
At least 11 separate regions of the Old and New World were involved as independent centers of origin.
Wild grains were collected and eaten from at least 105,000 years ago.
From around 11,500 years ago, the eight Neolithic founder crops, emmer and einkorn wheat, hulled barley, peas, lentils, bitter vetch, chick peas and flax were cultivated in the Levant.
Rice was domesticated in China between 11,500 and 6,200 BC with earliest known cultivation from 5,700 BC, followed by mung, soy and azuki beans.
Sheep were domesticated in Mesopotamia between 13,000 and 11,000 years ago.
Cattle were domesticated from the wild aurochs in the areas of modern Turkey and Pakistan some 10,500 years ago.
Domestic pigs had multiple centres of origin in Eurasia, including Europe, East Asia and Southwest Asia, where wild boar were first domesticated about 10,500 years ago.
In the Andes of South America, the potato was domesticated between 10,000 and 7,000 years ago, along with beans, coca, llamas, alpacas, and guinea pigs.
Sugarcane and some root vegetables were domesticated in New Guinea around 9,000 years ago.
Sorghum was domesticated in the Sahel region of Africa by 7,000 years ago.
Cotton was domesticated in Peru by 5,600 years ago, and was independently domesticated in Eurasia.
In Mesoamerica, wild teosinte was domesticated to maize by 6,000 years ago.
Scholars have developed a number of hypotheses to explain the historical origins of agriculture.
Studies of the transition from hunter-gatherer to agricultural societies indicate an initial period of intensification and increasing sedentism; examples are the Natufian culture in the Levant, and the Early Chinese Neolithic in China.
Then, wild stands that had previously been harvested started to be planted, and gradually came to be domesticated.
In Eurasia, the Sumerians started to live in villages from about 8,000 BC, relying on the Tigris and Euphrates rivers and a canal system for irrigation.
Ploughs appear in pictographs around 3,000 BC; seed-ploughs around 2,300 BC.
Farmers grew wheat, barley, vegetables such as lentils and onions, and fruits including dates, grapes, and figs.
Ancient Egyptian agriculture relied on the Nile River and its seasonal flooding.
Farming started in the predynastic period at the end of the Paleolithic, after 10,000 BC.
Staple food crops were grains such as wheat and barley, alongside industrial crops such as flax and papyrus.
In India, wheat, barley, and jujube were domesticated by 9,000 BC, soon followed by sheep and goats.
Cattle, sheep and goats were domesticated in Mehrgarh culture by 8,000–6,000 BC.
Cotton was cultivated by the 5th-4th millennium BC.
There is archeological evidence of an animal-drawn plough from 2,500 BC in the Indus Valley Civilization.
In China, from the 5th century BC there was a nationwide granary system and widespread silk farming.
Water-powered grain mills were in use by the 1st century BC, followed by irrigation.
By the late 2nd century, heavy ploughs had been developed with iron ploughshares and mouldboards.
These slowly spread westwards across Eurasia.
Asian rice was domesticated 8,200–13,500 years ago – depending on the molecular clock estimate that is used – on the Pearl River in southern China with a single genetic origin from the wild rice "Oryza rufipogon".
In ancient Greece and Rome, the major cereals were wheat, emmer, and barley, alongside vegetables including peas, beans, and olives.
Sheep and goats were kept mainly for dairy products.
In the Americas, crops domesticated in Mesoamerica (apart from teosinte) include squash, beans, and cocoa.
Cocoa was being domesticated by the Mayo Chinchipe of the upper Amazon around 3,000 BC.
The turkey was probably domesticated in Mexico or the American Southwest.
The Aztecs developed irrigation systems, formed terraced hillsides, fertilized their soil, and developed chinampas or artificial islands.
The Mayas used extensive canal and raised field systems to farm swampland from 400 BC.
Coca was domesticated in the Andes, as were the peanut, tomato, tobacco, and pineapple.
Cotton was domesticated in Peru by 3,600 BC.
Animals, too, including llamas, alpacas, and guinea pigs were domesticated in the region.
In North America, the indigenous people of the East domesticated crops such as sunflower, tobacco, squash and "Chenopodium".
Wild foods including wild rice and maple sugar were harvested.
The domesticated strawberry is a hybrid of a Chilean and a North American species, developed by breeding in Europe and North America.
The indigenous people of the Southwest and the Pacific Northwest practiced forest gardening and fire-stick farming.
The natives controlled fire on a regional scale to create a low-intensity fire ecology which sustained a low-density agriculture in loose rotation; a sort of "wild" permaculture.
A system of companion planting called the Three Sisters was developed on the Great Plains, the three crops being winter squash, maize, and climbing beans.
Indigenous Australians, long supposed to have been nomadic hunter-gatherers, practised systematic burning to enhance natural productivity in fire-stick farming.
The Gunditjmara and other groups developed eel farming and fish trapping systems from some 5,000 years ago.
There is evidence of 'intensification' across the whole continent over that period.
In two regions of Australia, the central west coast and eastern central Australia, early agriculture with crops of yams, native millet, and bush onions may have been practised in permanent settlements.
In the Middle Ages, both in the Islamic world and in Europe, agriculture was transformed with improved techniques and the diffusion of crop plants, including the introduction of sugar, rice, cotton and fruit trees such as the orange to Europe by way of Al-Andalus.
After 1492, the Columbian exchange brought New World crops such as maize, potatoes, tomatoes, sweet potatoes and manioc to Europe, and Old World crops such as wheat, barley, rice and turnips, and livestock including horses, cattle, sheep and goats to the Americas.
Irrigation, crop rotation, and fertilizers were greatly developed in the past 200 years, starting with the British Agricultural Revolution, allowing global population to rise significantly.
Since 1900, agriculture in the developed nations, and to a lesser extent in the developing world, has seen large rises in productivity as human labor has been replaced by mechanization, and assisted by synthetic fertilizers, pesticides, and selective breeding.
The Haber-Bosch method allowed the synthesis of ammonium nitrate fertilizer on an industrial scale, greatly increasing crop yields and sustaining a further increase in global population.
Modern agriculture has raised political issues including water pollution, biofuels, genetically modified organisms, tariffs and farm subsidies, leading to alternative approaches such as the organic movement.
Pastoralism involves managing domesticated animals.
In nomadic pastoralism, herds of livestock are moved from place to place in search of pasture, fodder, and water.
This type of farming is practised in arid and semi-arid regions of Sahara, Central Asia and some parts of India.
In shifting cultivation, a small area of a forest is cleared by cutting down all the trees and the area is burned.
The land is then used for growing crops for several years.
When the soil becomes less fertile, the area is then abandoned.
Another patch of land is selected and the process is repeated.
This type of farming is practiced mainly in areas with abundant rainfall where the forest regenerates quickly.
This practice is used in Northeast India, Southeast Asia, and the Amazon Basin.
Subsistence farming is practiced to satisfy family or local needs alone, with little left over for transport elsewhere.
It is intensively practiced in Monsoon Asia and South-East Asia.
If the typical subsistence farmer is equivalent to a smallholder, then there are an estimated 2.5 billion such farmers in 2018, cultivating about 60% of the earth's arable land.
In intensive farming, the crops are cultivated to maximise profit, with a low fallow ratio and a high use of inputs.
This type of farming is practiced mainly in highly developed countries.
In the past century, agriculture has been characterized by increased productivity, the substitution of synthetic fertilizers and pesticides for labor, water pollution, and farm subsidies.
In recent years there has been a backlash against the environmental effects of conventional agriculture, resulting in the organic, regenerative, and sustainable agriculture movements.
One of the major forces behind this movement has been the European Union, which first certified organic food in 1991 and began reform of its Common Agricultural Policy (CAP) in 2005 to phase out commodity-linked farm subsidies, also known as decoupling.
The growth of organic farming has renewed research in alternative technologies such as integrated pest management and selective breeding.
Recent mainstream technological developments include genetically modified food.
Demand for non-food biofuel crops, development of former farm lands, rising transportation costs, climate change, growing consumer demand in China and India, and population growth, are threatening food security in many parts of the world.
The International Fund for Agricultural Development posits that an increase in smallholder agriculture may be part of the solution to concerns about food prices and overall food security, given the favorable experience of Vietnam.
Soil degradation and diseases such as stem rust are major concerns globally; approximately 40% of the world's agricultural land is seriously degraded.
By 2015, the agricultural output of China was the largest in the world, followed by the European Union, India and the United States.
Economists measure the total factor productivity of agriculture and by this measure agriculture in the United States is roughly 1.7 times more productive than it was in 1948.
Following the three-sector theory, the number of people employed in agriculture and other primary activities (such as fishing) can be more than 80% in the least developed countries, and less than 2% in the most highly developed countries.
Since the Industrial Revolution, many countries have made the transition to developed economies, and the proportion of people working in agriculture has steadily fallen.
During the 16th century in Europe, for example, between 55 and 75% of the population was engaged in agriculture; by the 19th century, this had dropped to between 35 and 65%.
In the same countries today, the figure is less than 10%.
At the start of the 21st century, some one billion people, or over 1/3 of the available work force, were employed in agriculture.
It constitutes approximately 70% of the global employment of children, and in many countries employs the largest percentage of women of any industry.
The service sector overtook the agricultural sector as the largest global employer in 2007.
Agriculture, specifically farming, remains a hazardous industry, and farmers worldwide remain at high risk of work-related injuries, lung disease, noise-induced hearing loss, skin diseases, as well as certain cancers related to chemical use and prolonged sun exposure.
On industrialized farms, injuries frequently involve the use of agricultural machinery, and a common cause of fatal agricultural injuries in developed countries is tractor rollovers.
Pesticides and other chemicals used in farming can also be hazardous to worker health, and workers exposed to pesticides may experience illness or have children with birth defects.
As an industry in which families commonly share in work and live on the farm itself, entire families can be at risk for injuries, illness, and death.
Ages 0–6 may be an especially vulnerable population in agriculture; common causes of fatal injuries among young farm workers include drowning, machinery and motor accidents, including with all-terrain vehicles.
The International Labour Organization considers agriculture "one of the most hazardous of all economic sectors".
It estimates that the annual work-related death toll among agricultural employees is at least 170,000, twice the average rate of other jobs.
In addition, incidences of death, injury and illness related to agricultural activities often go unreported.
The organization has developed the Safety and Health in Agriculture Convention, 2001, which covers the range of risks in the agriculture occupation, the prevention of these risks and the role that individuals and organizations engaged in agriculture should play.
In America, agriculture has been identified by the National Institute for Occupational Safety and Health as a priority industry sector in the National Occupational Research Agenda to identify and provide intervention strategies for occupational health and safety issues.
In the European Union, the European Agency for Safety and Health at Work has issued guidelines on implementing health and safety directives in agriculture, livestock farming, horticulture, and forestry.
Overall production varies by country as listed.
Cropping systems vary among farms depending on the available resources and constraints; geography and climate of the farm; government policy; economic, social and political pressures; and the philosophy and culture of the farmer.
Shifting cultivation (or slash and burn) is a system in which forests are burnt, releasing nutrients to support cultivation of annual and then perennial crops for a period of several years.
Then the plot is left fallow to regrow forest, and the farmer moves to a new plot, returning after many more years (10–20).
This fallow period is shortened if population density grows, requiring the input of nutrients (fertilizer or manure) and some manual pest control.
Annual cultivation is the next phase of intensity in which there is no fallow period.
This requires even greater nutrient and pest control inputs.
Further industrialization led to the use of monocultures, when one cultivar is planted on a large acreage.
Because of the low biodiversity, nutrient use is uniform and pests tend to build up, necessitating the greater use of pesticides and fertilizers.
Multiple cropping, in which several crops are grown sequentially in one year, and intercropping, when several crops are grown at the same time, are other kinds of annual cropping systems known as polycultures.
In subtropical and arid environments, the timing and extent of agriculture may be limited by rainfall, either not allowing multiple annual crops in a year, or requiring irrigation.
In all of these environments perennial crops are grown (coffee, chocolate) and systems are practiced such as agroforestry.
In temperate environments, where ecosystems were predominantly grassland or prairie, highly productive annual farming is the dominant agricultural system.
Important categories of food crops include cereals, legumes, forage, fruits and vegetables.
Natural fibers include cotton, wool, hemp, silk and flax.
Specific crops are cultivated in distinct growing regions throughout the world.
Production is listed in millions of metric tons, based on FAO estimates.
Animal husbandry is the breeding and raising of animals for meat, milk, eggs, or wool), and for work and transport.
Working animals, including horses, mules, oxen, water buffalo, camels, llamas, alpacas, donkeys, and dogs, have for centuries been used to help cultivate fields, harvest crops, wrangle other animals, and transport farm products to buyers.
Livestock production systems can be defined based on feed source, as grassland-based, mixed, and landless.
, 30% of Earth's ice- and water-free area was used for producing livestock, with the sector employing approximately 1.3 billion people.
Between the 1960s and the 2000s, there was a significant increase in livestock production, both by numbers and by carcass weight, especially among beef, pigs and chickens, the latter of which had production increased by almost a factor of 10.
Non-meat animals, such as milk cows and egg-producing chickens, also showed significant production increases.
Global cattle, sheep and goat populations are expected to continue to increase sharply through 2050.
Aquaculture or fish farming, the production of fish for human consumption in confined operations, is one of the fastest growing sectors of food production, growing at an average of 9% a year between 1975 and 2007.
During the second half of the 20th century, producers using selective breeding focused on creating livestock breeds and crossbreeds that increased production, while mostly disregarding the need to preserve genetic diversity.
This trend has led to a significant decrease in genetic diversity and resources among livestock breeds, leading to a corresponding decrease in disease resistance and local adaptations previously found among traditional breeds.
Grassland based livestock production relies upon plant material such as shrubland, rangeland, and pastures for feeding ruminant animals.
Outside nutrient inputs may be used, however manure is returned directly to the grassland as a major nutrient source.
This system is particularly important in areas where crop production is not feasible because of climate or soil, representing 30–40 million pastoralists.
Mixed production systems use grassland, fodder crops and grain feed crops as feed for ruminant and monogastric (one stomach; mainly chickens and pigs) livestock.
Manure is typically recycled in mixed systems as a fertilizer for crops.
Landless systems rely upon feed from outside the farm, representing the de-linking of crop and livestock production found more prevalently in Organisation for Economic Co-operation and Development member countries.
Synthetic fertilizers are more heavily relied upon for crop production and manure utilization becomes a challenge as well as a source for pollution.
Industrialized countries use these operations to produce much of the global supplies of poultry and pork.
Scientists estimate that 75% of the growth in livestock production between 2003 and 2030 will be in confined animal feeding operations, sometimes called factory farming.
Much of this growth is happening in developing countries in Asia, with much smaller amounts of growth in Africa.
Some of the practices used in commercial livestock production, including the usage of growth hormones, are controversial.
Tillage is the practice of breaking up the soil with tools such as the plow or harrow to prepare for planting, for nutrient incorporation, or for pest control.
Tillage varies in intensity from conventional to no-till.
It may improve productivity by warming the soil, incorporating fertilizer and controlling weeds, but also renders soil more prone to erosion, triggers the decomposition of organic matter releasing CO, and reduces the abundance and diversity of soil organisms.
Pest control includes the management of weeds, insects, mites, and diseases.
Chemical (pesticides), biological (biocontrol), mechanical (tillage), and cultural practices are used.
Cultural practices include crop rotation, culling, cover crops, intercropping, composting, avoidance, and resistance.
Integrated pest management attempts to use all of these methods to keep pest populations below the number which would cause economic loss, and recommends pesticides as a last resort.
Nutrient management includes both the source of nutrient inputs for crop and livestock production, and the method of utilization of manure produced by livestock.
Nutrient inputs can be chemical inorganic fertilizers, manure, green manure, compost and minerals.
Crop nutrient use may also be managed using cultural techniques such as crop rotation or a fallow period.
Manure is used either by holding livestock where the feed crop is growing, such as in managed intensive rotational grazing, or by spreading either dry or liquid formulations of manure on cropland or pastures.
Water management is needed where rainfall is insufficient or variable, which occurs to some degree in most regions of the world.
Some farmers use irrigation to supplement rainfall.
In other areas such as the Great Plains in the U.S.
and Canada, farmers use a fallow year to conserve soil moisture to use for growing a crop in the following year.
Agriculture represents 70% of freshwater use worldwide.
According to a report by the International Food Policy Research Institute, agricultural technologies will have the greatest impact on food production if adopted in combination with each other; using a model that assessed how eleven technologies could impact agricultural productivity, food security and trade by 2050, the International Food Policy Research Institute found that the number of people at risk from hunger could be reduced by as much as 40% and food prices could be reduced by almost half.
Payment for ecosystem services is a method of providing additional incentives to encourage farmers to conserve some aspects of the environment.
Measures might include paying for reforestation upstream of a city, to improve the supply of fresh water.
Crop alteration has been practiced by humankind for thousands of years, since the beginning of civilization.
Altering crops through breeding practices changes the genetic make-up of a plant to develop crops with more beneficial characteristics for humans, for example, larger fruits or seeds, drought-tolerance, or resistance to pests.
Significant advances in plant breeding ensued after the work of geneticist Gregor Mendel.
His work on dominant and recessive alleles, although initially largely ignored for almost 50 years, gave plant breeders a better understanding of genetics and breeding techniques.
Crop breeding includes techniques such as plant selection with desirable traits, self-pollination and cross-pollination, and molecular techniques that genetically modify the organism.
Domestication of plants has, over the centuries increased yield, improved disease resistance and drought tolerance, eased harvest and improved the taste and nutritional value of crop plants.
Careful selection and breeding have had enormous effects on the characteristics of crop plants.
Plant selection and breeding in the 1920s and 1930s improved pasture (grasses and clover) in New Zealand.
Extensive X-ray and ultraviolet induced mutagenesis efforts (i.e. primitive genetic engineering) during the 1950s produced the modern commercial varieties of grains such as wheat, corn (maize) and barley.
The Green Revolution popularized the use of conventional hybridization to sharply increase yield by creating "high-yielding varieties".
For example, average yields of corn (maize) in the US have increased from around 2.5 tons per hectare (t/ha) (40 bushels per acre) in 1900 to about 9.4 t/ha (150 bushels per acre) in 2001.
Similarly, worldwide average wheat yields have increased from less than 1 t/ha in 1900 to more than 2.5 t/ha in 1990.
South American average wheat yields are around 2 t/ha, African under 1 t/ha, and Egypt and Arabia up to 3.5 to 4 t/ha with irrigation.
In contrast, the average wheat yield in countries such as France is over 8 t/ha.
Variations in yields are due mainly to variation in climate, genetics, and the level of intensive farming techniques (use of fertilizers, chemical pest control, growth control to avoid lodging).
Genetically modified organisms (GMO) are organisms whose genetic material has been altered by genetic engineering techniques generally known as recombinant DNA technology.
Genetic engineering has expanded the genes available to breeders to utilize in creating desired germlines for new crops.
Increased durability, nutritional content, insect and virus resistance and herbicide tolerance are a few of the attributes bred into crops through genetic engineering.
For some, GMO crops cause food safety and food labeling concerns.
Numerous countries have placed restrictions on the production, import or use of GMO foods and crops.
Currently a global treaty, the Biosafety Protocol, regulates the trade of GMOs.
There is ongoing discussion regarding the labeling of foods made from GMOs, and while the EU currently requires all GMO foods to be labeled, the US does not.
Herbicide-resistant seed has a gene implanted into its genome that allows the plants to tolerate exposure to herbicides, including glyphosate.
These seeds allow the farmer to grow a crop that can be sprayed with herbicides to control weeds without harming the resistant crop.
Herbicide-tolerant crops are used by farmers worldwide.
With the increasing use of herbicide-tolerant crops, comes an increase in the use of glyphosate-based herbicide sprays.
In some areas glyphosate resistant weeds have developed, causing farmers to switch to other herbicides.
Some studies also link widespread glyphosate usage to iron deficiencies in some crops, which is both a crop production and a nutritional quality concern, with potential economic and health implications.
Other GMO crops used by growers include insect-resistant crops, which have a gene from the soil bacterium "Bacillus thuringiensis" (Bt), which produces a toxin specific to insects.
These crops resist damage by insects.
Some believe that similar or better pest-resistance traits can be acquired through traditional breeding practices, and resistance to various pests can be gained through hybridization or cross-pollination with wild species.
In some cases, wild species are the primary source of resistance traits; some tomato cultivars that have gained resistance to at least 19 diseases did so through crossing with wild populations of tomatoes.
Agriculture imposes multiple external costs upon society through effects such as pesticide damage to nature (especially herbicides and insecticides), nutrient runoff, excessive water usage, and loss of natural environment.
A 2000 assessment of agriculture in the UK determined total external costs for 1996 of £2,343 million, or £208 per hectare.
A 2005 analysis of these costs in the US concluded that cropland imposes approximately $5 to $16 billion ($30 to $96 per hectare), while livestock production imposes $714 million.
Both studies, which focused solely on the fiscal impacts, concluded that more should be done to internalize external costs.
Neither included subsidies in their analysis, but they noted that subsidies also influence the cost of agriculture to society.
Agriculture seeks to increase yield and to reduce costs.
Yield increases with inputs such as fertilisers and removal of pathogens, predators, and competitors (such as weeds).
Costs decrease with increasing scale of farm units, such as making fields larger; this means removing hedges, ditches and other areas of habitat.
Pesticides kill insects, plants and fungi.
These and other measures have cut biodiversity to very low levels on intensively farmed land.
In 2010, the International Resource Panel of the United Nations Environment Programme assessed the environmental impacts of consumption and production.
It found that agriculture and food consumption are two of the most important drivers of environmental pressures, particularly habitat change, climate change, water use and toxic emissions.
Agriculture is the main source of toxins released into the environment, including insecticides, especially those used on cotton.
The 2011 UNEP Green Economy report states that "[a]gricultural operations, excluding land use changes, produce approximately 13 per cent of anthropogenic global GHG emissions.
This includes GHGs emitted by the use of inorganic fertilisers agro-chemical pesticides and herbicides; (GHG emissions resulting from production of these inputs are included in industrial emissions); and fossil fuel-energy inputs.
"On average we find that the total amount of fresh residues from agricultural and forestry production for second- generation biofuel production amounts to 3.8 billion tonnes per year between 2011 and 2050 (with an average annual growth rate of 11 per cent throughout the period analysed, accounting for higher growth during early years, 48 per cent for 2011–2020 and an average 2 per cent annual expansion after 2020)."
A senior UN official and co-author of a UN report detailing this problem, Henning Steinfeld, said "Livestock are one of the most significant contributors to today's most serious environmental problems".
Livestock production occupies 70% of all land used for agriculture, or 30% of the land surface of the planet.
It is one of the largest sources of greenhouse gases, responsible for 18% of the world's greenhouse gas emissions as measured in CO equivalents.
By comparison, all transportation emits 13.5% of the CO.
It produces 65% of human-related nitrous oxide (which has 296 times the global warming potential of CO) and 37% of all human-induced methane (which is 23 times as warming as CO.)
It also generates 64% of the ammonia emission.
Livestock expansion is cited as a key factor driving deforestation; in the Amazon basin 70% of previously forested area is now occupied by pastures and the remainder used for feedcrops.
Through deforestation and land degradation, livestock is also driving reductions in biodiversity.
Furthermore, the UNEP states that "methane emissions from global livestock are projected to increase by 60 per cent by 2030 under current practices and consumption patterns."
Land transformation, the use of land to yield goods and services, is the most substantial way humans alter the Earth's ecosystems, and is considered the driving force in the loss of biodiversity.
Estimates of the amount of land transformed by humans vary from 39 to 50%.
Land degradation, the long-term decline in ecosystem function and productivity, is estimated to be occurring on 24% of land worldwide, with cropland overrepresented.
The UN-FAO report cites land management as the driving factor behind degradation and reports that 1.5 billion people rely upon the degrading land.
Degradation can be deforestation, desertification, soil erosion, mineral depletion, or chemical degradation (acidification and salinization).
Eutrophication, excessive nutrients in aquatic ecosystems resulting in algal blooms and anoxia, leads to fish kills, loss of biodiversity, and renders water unfit for drinking and other industrial uses.
Excessive fertilization and manure application to cropland, as well as high livestock stocking densities cause nutrient (mainly nitrogen and phosphorus) runoff and leaching from agricultural land.
These nutrients are major nonpoint pollutants contributing to eutrophication of aquatic ecosystems and pollution of groundwater, with harmful effects on human populations.
Fertilisers also reduce terrestrial biodiversity by increasing competition for light, favouring those species that are able to benefit from the added nutrients.
Agriculture accounts for 70 percent of withdrawals of freshwater resources.
Agriculture is a major draw on water from aquifers, and currently draws from those underground water sources at an unsustainable rate.
It is long known that aquifers in areas as diverse as northern China, the Upper Ganges and the western US are being depleted, and new research extends these problems to aquifers in Iran, Mexico and Saudi Arabia.
Increasing pressure is being placed on water resources by industry and urban areas, meaning that water scarcity is increasing and agriculture is facing the challenge of producing more food for the world's growing population with reduced water resources.
Agricultural water usage can also cause major environmental problems, including the destruction of natural wetlands, the spread of water-borne diseases, and land degradation through salinization and waterlogging, when irrigation is performed incorrectly.
Pesticide use has increased since 1950 to 2.5million short tons annually worldwide, yet crop loss from pests has remained relatively constant.
The World Health Organization estimated in 1992 that three million pesticide poisonings occur annually, causing 220,000 deaths.
Pesticides select for pesticide resistance in the pest population, leading to a condition termed the "pesticide treadmill" in which pest resistance warrants the development of a new pesticide.
An alternative argument is that the way to "save the environment" and prevent famine is by using pesticides and intensive high yield farming, a view exemplified by a quote heading the Center for Global Food Issues website: 'Growing more per acre leaves more land for nature'.
However, critics argue that a trade-off between the environment and a need for food is not inevitable, and that pesticides simply replace good agronomic practices such as crop rotation.
The Push–pull agricultural pest management technique involves intercropping, using plant aromas to repel pests from crops (push) and to lure them to a place from which they can then be removed (pull).
Global warming and agriculture are interrelated on a global scale.
Global warming affects agriculture through changes in average temperatures, rainfall, and weather extremes (like storms and heat waves); changes in pests and diseases; changes in atmospheric carbon dioxide and ground-level ozone concentrations; changes in the nutritional quality of some foods; and changes in sea level.
Global warming is already affecting agriculture, with effects unevenly distributed across the world.
Future climate change will probably negatively affect crop production in low latitude countries, while effects in northern latitudes may be positive or negative.
Global warming will probably increase the risk of food insecurity for some vulnerable groups, such as the poor.
Animal husbandry is also responsible for greenhouse gas production of and a percentage of the world's methane, and future land infertility, and the displacement of wildlife.
Agriculture contributes to climate change by anthropogenic emissions of greenhouse gases, and by the conversion of non-agricultural land such as forest for agricultural use.
Agriculture, forestry and land-use change contributed around 20 to 25% to global annual emissions in 2010.
A range of policies can reduce the risk of negative climate change impacts on agriculture, and greenhouse gas emissions from the agriculture sector.
Current farming methods have resulted in over-stretched water resources, high levels of erosion and reduced soil fertility.
There is not enough water to continue farming using current practices; therefore how critical water, land, and ecosystem resources are used to boost crop yields must be reconsidered.
A solution would be to give value to ecosystems, recognizing environmental and livelihood tradeoffs, and balancing the rights of a variety of users and interests.
Inequities that result when such measures are adopted would need to be addressed, such as the reallocation of water from poor to rich, the clearing of land to make way for more productive farmland, or the preservation of a wetland system that limits fishing rights.
Technological advancements help provide farmers with tools and resources to make farming more sustainable.
Technology permits innovations like conservation tillage, a farming process which helps prevent land loss to erosion, reduces water pollution, and enhances carbon sequestration.
According to a report by the International Food Policy Research Institute (IFPRI), agricultural technologies will have the greatest impact on food production if adopted in combination with each other; using a model that assessed how eleven technologies could impact agricultural productivity, food security and trade by 2050, IFPRI found that the number of people at risk from hunger could be reduced by as much as 40% and food prices could be reduced by almost half.
The caloric demand of Earth's projected population, with current climate change predictions, can be satisfied by additional improvement of agricultural methods, expansion of agricultural areas, and a sustainability-oriented consumer mindset.
Since the 1940s, agricultural productivity has increased dramatically, due largely to the increased use of energy-intensive mechanization, fertilizers and pesticides.
The vast majority of this energy input comes from fossil fuel sources.
Between the 1960s and the 1980s, the Green Revolution transformed agriculture around the globe, with world grain production increasing significantly (between 70% and 390% for wheat and 60% to 150% for rice, depending on geographic area) as world population doubled.
Heavy reliance on petrochemicals has raised concerns that oil shortages could increase costs and reduce agricultural output.
Industrialized agriculture depends on fossil fuels in two fundamental ways: direct consumption on the farm and manufacture of inputs used on the farm.
Direct consumption includes the use of lubricants and fuels to operate farm vehicles and machinery.
Indirect consumption includes the manufacture of fertilizers, pesticides, and farm machinery.
In particular, the production of nitrogen fertilizer can account for over half of agricultural energy usage.
Together, direct and indirect consumption by US farms accounts for about 2% of the nation's energy use.
Direct and indirect energy consumption by U.S.
farms peaked in 1979, and has since gradually declined.
Food systems encompass not just agriculture but off-farm processing, packaging, transporting, marketing, consumption, and disposal of food and food-related items.
Agriculture accounts for less than one-fifth of food system energy use in the US.
Agricultural economics refers to economics as it relates to the "production, distribution and consumption of [agricultural] goods and services".
Combining agricultural production with general theories of marketing and business as a discipline of study began in the late 1800s, and grew significantly through the 20th century.
Although the study of agricultural economics is relatively recent, major trends in agriculture have significantly affected national and international economies throughout history, ranging from tenant farmers and sharecropping in the post-American Civil War Southern United States to the European feudal system of manorialism.
In the United States, and elsewhere, food costs attributed to food processing, distribution, and agricultural marketing, sometimes referred to as the value chain, have risen while the costs attributed to farming have declined.
This is related to the greater efficiency of farming, combined with the increased level of value addition (e.g.
more highly processed products) provided by the supply chain.
Market concentration has increased in the sector as well, and although the total effect of the increased market concentration is likely increased efficiency, the changes redistribute economic surplus from producers (farmers) and consumers, and may have negative implications for rural communities.
National government policies can significantly change the economic marketplace for agricultural products, in the form of taxation, subsidies, tariffs and other measures.
Since at least the 1960s, a combination of trade restrictions, exchange rate policies and subsidies have affected farmers in both the developing and the developed world.
In the 1980s, non-subsidized farmers in developing countries experienced adverse effects from national policies that created artificially low global prices for farm products.
Between the mid-1980s and the early 2000s, several international agreements limited agricultural tariffs, subsidies and other trade restrictions.
However, , there was still a significant amount of policy-driven distortion in global agricultural product prices.
The three agricultural products with the greatest amount of trade distortion were sugar, milk and rice, mainly due to taxation.
Among the oilseeds, sesame had the greatest amount of taxation, but overall, feed grains and oilseeds had much lower levels of taxation than livestock products.
Since the 1980s, policy-driven distortions have seen a greater decrease among livestock products than crops during the worldwide reforms in agricultural policy.
Despite this progress, certain crops, such as cotton, still see subsidies in developed countries artificially deflating global prices, causing hardship in developing countries with non-subsidized farmers.
Unprocessed commodities such as corn, soybeans, and cattle are generally graded to indicate quality, affecting the price the producer receives.
Commodities are generally reported by production quantities, such as volume, number or weight.
Agricultural science is a broad multidisciplinary field of biology that encompasses the parts of exact, natural, economic and social sciences used in the practice and understanding of agriculture.
It covers topics such as agronomy, plant breeding and genetics, plant pathology, crop modelling, soil science, entomology, production techniques and improvement, study of pests and their management, and study of adverse environmental effects such as soil degradation, waste management, and bioremediation.
The scientific study of agriculture began in the 18th century, when Johann Friedrich Mayer conducted experiments on the use of gypsum (hydrated calcium sulphate) as a fertilizer.
Research became more systematic when in 1843, John Lawes and Henry Gilbert began a set of long-term agronomy field experiments at Rothamsted Research Station in England; some of them, such as the Park Grass Experiment, are still running.
In America, the Hatch Act of 1887 provided funding for what it was the first to call "agricultural science", driven by farmers' interest in fertilizers.
In agricultural entomology, the USDA began to research biological control in 1881; it instituted its first large program in 1905, searching Europe and Japan for natural enemies of the gypsy moth and brown-tail moth, establishing parasitoids (such as solitary wasps) and predators of both pests in the USA.
Agricultural policy is the set of government decisions and actions relating to domestic agriculture and imports of foreign agricultural products.
Governments usually implement agricultural policies with the goal of achieving a specific outcome in the domestic agricultural product markets.
Some overarching themes include risk management and adjustment (including policies related to climate change, food safety and natural disasters), economic stability (including policies related to taxes), natural resources and environmental sustainability (especially water policy), research and development, and market access for domestic commodities (including relations with global organizations and agreements with other countries).
Agricultural policy can also touch on food quality, ensuring that the food supply is of a consistent and known quality, food security, ensuring that the food supply meets the population's needs, and conservation.
Policy programs can range from financial programs, such as subsidies, to encouraging producers to enroll in voluntary quality assurance programs.
There are many influences on the creation of agricultural policy, including consumers, agribusiness, trade lobbies and other groups.
Agribusiness interests hold a large amount of influence over policy making, in the form of lobbying and campaign contributions.
Political action groups, including those interested in environmental issues and labor unions, also provide influence, as do lobbying organizations representing individual agricultural commodities.
The Food and Agriculture Organization of the United Nations (FAO) leads international efforts to defeat hunger and provides a forum for the negotiation of global agricultural regulations and agreements.
Dr. Samuel Jutzi, director of FAO's animal production and health division, states that lobbying by large corporations has stopped reforms that would improve human health and the environment.
For example, proposals in 2010 for a voluntary code of conduct for the livestock industry that would have provided incentives for improving standards for health, and environmental regulations, such as the number of animals an area of land can support without long-term damage, were successfully defeated due to large food company pressure.
</doc>
<doc id="628" url="https://en.wikipedia.org/wiki?curid=628" title="Aldous Huxley">
Aldous Huxley

Aldous Leonard Huxley (26 July 1894 – 22 November 1963) was an English writer, novelist, philosopher, and prominent member of the Huxley family.
He graduated from Balliol College at the University of Oxford with a first-class honours degree in English literature.
The author of nearly fifty books, Huxley was best known for his novels (among them "Brave New World", set in a dystopian future); for nonfiction works, such as "The Doors of Perception", in which he recalls his experiences taking psychedelic drugs; and for his wide-ranging essays.
Early in his career, Huxley published short stories and poetry, and edited the literary magazine "Oxford Poetry".
He went on to publish travel writing, film stories, satire, and screenplays.
He spent the latter part of his life in the United States, living in Los Angeles from 1937 until his death.
Huxley was a humanist and pacifist.
He became interested in spiritual subjects such as parapsychology and philosophical mysticism, and in particular universalism.
By the end of his life, Huxley was widely acknowledged as one of the pre-eminent intellectuals of his time.
He was nominated for the Nobel Prize in Literature seven times.
In 1962, a year before he died, Huxley was elected Companion of Literature by the Royal Society of Literature.
Huxley was born in Godalming, Surrey, England, in 1894.
He was the third son of the writer and schoolmaster Leonard Huxley, who edited "Cornhill Magazine", and his first wife, Julia Arnold, who founded Prior's Field School.
Julia was the niece of poet and critic Matthew Arnold and the sister of Mrs. Humphry Ward.
Aldous was the grandson of Thomas Henry Huxley, the zoologist, agnostic, and controversialist ("Darwin's Bulldog").
His brother Julian Huxley and half-brother Andrew Huxley also became outstanding biologists.
Aldous had another brother, Noel Trevelyan Huxley (1891–1914), who committed suicide after a period of clinical depression.
As a child, Huxley's nickname was "Ogie," short for "Ogre."
He was described by his brother, Julian, as someone who frequently "[contemplated] the strangeness of things".
According to his cousin and contemporary, Gervas Huxley, he had an early interest in drawing.
Huxley's education began in his father's well-equipped botanical laboratory, after which he enrolled at Hillside School near Godalming.
He was taught there by his own mother for several years until she became terminally ill.
After Hillside, he went on to Eton College.
His mother died in 1908 when he was 14.
In 1911 he contracted the eye disease (keratitis punctata) which "left [him] practically blind for two to three years."
This "ended his early dreams of becoming a doctor."
In October 1913, Huxley entered Balliol College, Oxford, where he studied English literature.
In January 1916, he volunteered for the British Army in the Great War, however was rejected on health grounds, being half-blind in one eye.
His eyesight later partly recovered.
In 1916 he edited "Oxford Poetry" and in June of that year graduated BA with first class honours.
His brother Julian wrote:

Following his years at Balliol, Huxley, being financially indebted to his father, decided to find employment.
From April to July 1917, he was in charge of ordering supplies at the Air Ministry .
He taught French for a year at Eton, where Eric Blair (who was to take the pen name George Orwell) and Steven Runciman were among his pupils.
He was mainly remembered as being an incompetent schoolmaster unable to keep order in class.
Nevertheless, Blair and others spoke highly of his excellent command of language.
Significantly, Huxley also worked for a time during the 1920s at Brunner and Mond, an advanced chemical plant in Billingham in County Durham, northeast England.
According to the introduction to the latest edition of his science fiction novel "Brave New World" (1932), the experience he had there of "an ordered universe in a world of planless incoherence" was an important source for the novel.
Huxley completed his first (unpublished) novel at the age of 17 and began writing seriously in his early 20s, establishing himself as a successful writer and social satirist.
His first published novels were social satires, "Crome Yellow" (1921), "Antic Hay" (1923), "Those Barren Leaves" (1925), and "Point Counter Point" (1928).
"Brave New World" was Huxley's fifth novel and first dystopian work.
In the 1920s he was also a contributor to "Vanity Fair" and British "Vogue" magazines.
During World War I, Huxley spent much of his time at Garsington Manor near Oxford, home of Lady Ottoline Morrell, working as a farm labourer.
There he met several Bloomsbury figures, including Bertrand Russell, Alfred North Whitehead, and Clive Bell.
Later, in "Crome Yellow" (1921) he caricatured the Garsington lifestyle.
Jobs were very scarce, but in 1919 John Middleton Murry was reorganising the "Athenaeum" and invited Huxley to join the staff.
He accepted immediately, and quickly married the Belgian refugee Maria Nys, also at Garsington.
They lived with their young son in Italy part of the time during the 1920s, where Huxley would visit his friend D. H. Lawrence.
Following Lawrence's death in 1930, Huxley edited Lawrence's letters (1932).
Works of this period included important novels on the dehumanising aspects of scientific progress, most famously "Brave New World", and on pacifist themes (for example, "Eyeless in Gaza").
In "Brave New World", set in a dystopian London, Huxley portrays a society operating on the principles of mass production and Pavlovian conditioning.
Huxley was strongly influenced by F. Matthias Alexander and included him as a character in "Eyeless in Gaza".
Starting from this period, Huxley began to write and edit non-fiction works on pacifist issues, including "Ends and Means", "An Encyclopedia of Pacifism", and "Pacifism and Philosophy", and was an active member of the Peace Pledge Union.
In 1937 Huxley moved to Hollywood with his wife Maria, son Matthew Huxley, and friend Gerald Heard.
He lived in the U.S., mainly in southern California, until his death, and also for a time in Taos, New Mexico, where he wrote "Ends and Means" (published in 1937).
The book contains tracts on war, religion, nationalism and ethics.
Heard introduced Huxley to Vedanta (Upanishad-centered philosophy), meditation, and vegetarianism through the principle of ahimsa.
In 1938, Huxley befriended Jiddu Krishnamurti, whose teachings he greatly admired.
Huxley and Krishnamurti entered into an enduring exchange (sometimes edging on debate) over many years, with Krishnamurti representing the more rarefied, detached, ivory-tower perspective and Huxley, with his pragmatic concerns, the more socially and historically informed position.
Huxley provided an introduction to Krishnamurti's quintessential statement, "The First and Last Freedom" (1954).
Huxley also became a Vedantist in the circle of Hindu Swami Prabhavananda, and introduced Christopher Isherwood to this circle.
Not long after, Huxley wrote his book on widely held spiritual values and ideas, "The Perennial Philosophy", which discussed the teachings of renowned mystics of the world.
Huxley's book affirmed a sensibility that insists there are realities beyond the generally accepted "five senses" and that there is genuine meaning for humans beyond both sensual satisfactions and sentimentalities.
Huxley became a close friend of Remsen Bird, president of Occidental College.
He spent much time at the college, which is in the Eagle Rock neighbourhood of Los Angeles.
The college appears as "Tarzana College" in his satirical novel "After Many a Summer" (1939).
The novel won Huxley a British literary award, the 1939 James Tait Black Memorial Prize for fiction.
Huxley also incorporated Bird into the novel.
During this period, Huxley earned a substantial income as a Hollywood screenwriter; Christopher Isherwood, in his autobiography "My Guru and His Disciple", states that Huxley earned more than $3,000 per week (an enormous sum in those days) as a screenwriter, and that he used much of it to transport Jewish and left-wing writer and artist refugees from Hitler's Germany to the U.S.
In March 1938, his friend Anita Loos, a novelist and screenwriter, put him in touch with Metro-Goldwyn-Mayer who hired Huxley for "Madame Curie", which was originally to star Greta Garbo and be directed by George Cukor.
(Eventually, the film was completed by MGM in 1943 with a different director and cast.)
Huxley received screen credit for "Pride and Prejudice" (1940) and was paid for his work on a number of other films, including "Jane Eyre" (1944).
Huxley was commissioned by Walt Disney in 1945 to write a script based on "Alice's Adventures in Wonderland" and the biography of the story's author, Lewis Carroll.
The script was not used, however.
Huxley wrote an introduction to the posthumous publication of J. D. Unwin's 1940 book "Hopousia or The Sexual and Economic Foundations of a New Society".
On 21 October 1949, Huxley wrote to George Orwell, author of "Nineteen Eighty-Four", congratulating him on "how fine and how profoundly important the book is."
In his letter to Orwell, he predicted:
Huxley had deeply felt apprehensions about the future the developed world might make for itself.
From these, he made some warnings in his writings and talks.
In a 1958 televised interview conducted by journalist Mike Wallace, Huxley outlined several major concerns: the difficulties and dangers of world overpopulation; the tendency toward distinctly hierarchical social organisation; the crucial importance of evaluating the use of technology in mass societies susceptible to persuasion; the tendency to promote modern politicians to a naive public as well-marketed commodities.
In 1953 Huxley and Maria applied for United States citizenship and presented themselves for examination.
When Huxley refused to bear arms for the U.S.
and would not state that his objections were based on religious ideals, the only excuse allowed under the McCarran Act, the judge had to adjourn the proceedings.
He withdrew his application.
Nevertheless, he remained in the U.S.
In 1959 Huxley turned down an offer of a Knight Bachelor by the Macmillan government without putting forward a reason; his brother Julian had been knighted in 1958, while another brother Andrew would be knighted in 1974.
Beginning in 1939 and continuing until his death in 1963, Huxley had an extensive association with the Vedanta Society of Southern California, founded and headed by Swami Prabhavananda.
Together with Gerald Heard, Christopher Isherwood, and other followers he was initiated by the Swami and was taught meditation and spiritual practices.
In 1944 Huxley wrote the introduction to the "Bhagavad Gita: The Song of God", translated by Swami Prabhavananda and Christopher Isherwood, which was published by the Vedanta Society of Southern California.
From 1941 until 1960, Huxley contributed 48 articles to "Vedanta and the West", published by the society.
He also served on the editorial board with Isherwood, Heard, and playwright John van Druten from 1951 through 1962.
Huxley also occasionally lectured at the Hollywood and Santa Barbara Vedanta temples.
Two of those lectures have been released on CD: "Knowledge and Understanding" and "Who Are We?"
from 1955.
Nonetheless, Huxley's agnosticism, together with his speculative propensity, made it difficult for him to fully embrace any form of institutionalised religion.
In the spring of 1953, Huxley had his first experience with psychedelic drugs, in this case, mescaline.
Huxley had initiated a correspondence with Dr. Humphry Osmond, a British psychiatrist then employed in a Canadian institution, and eventually asked him to supply a dose of mescaline; Osmond obliged and supervised Huxley's session in southern California.
After the publication of "The Doors of Perception", in which he recounted this experience, Huxley and Swami Prabhavananda disagreed about the meaning and importance of the psychedelic drug experience, which may have caused the relationship to cool, but Huxley continued to write articles for the society's journal, lecture at the temple, and attend social functions.
There are differing accounts about the details of the quality of Huxley's eyesight at specific points in his life.
About 1939 Huxley encountered the Bates method for better eyesight, and a teacher, Margaret Darst Corbett, who was able to teach the method to him.
In 1940, Huxley relocated from Hollywood to a "ranchito" in the high desert hamlet of Llano, California, in northern Los Angeles County.
Huxley then said that his sight improved dramatically with the Bates Method and the extreme and pure natural lighting of the southwestern American desert.
He reported that, for the first time in more than 25 years, he was able to read without glasses and without strain.
He even tried driving a car along the dirt road beside the ranch.
He wrote a book about his successes with the Bates Method, "The Art of Seeing", which was published in 1942 (U.S.
), 1943 (UK).
The book contained some generally disputed theories, and its publication created a growing degree of popular controversy about Huxley's eyesight.
It was, and is, widely believed that Huxley was nearly blind since the illness in his teens, despite the partial recovery that had enabled him to study at Oxford.
For example, some ten years after publication of "The Art of Seeing", in 1952, Bennett Cerf was present when Huxley spoke at a Hollywood banquet, wearing no glasses and apparently reading his paper from the lectern without difficulty: "Then suddenly he faltered—and the disturbing truth became obvious.
He wasn't reading his address at all.
He had learned it by heart.
To refresh his memory he brought the paper closer and closer to his eyes.
When it was only an inch or so away he still couldn't read it, and had to fish for a magnifying glass in his pocket to make the typing visible to him.
It was an agonising moment."
Brazilian author João Ubaldo Ribeiro, who as a young journalist spent several evenings in the Huxleys' company in the late 1950s, wrote that Huxley had said to him, with a wry smile: "I can hardly see at all.
And I don't give a damn, really."
Ribeiro then proceeds to confirm Bennett Cerf's experience, as described above.
On the other hand, Huxley's second wife, Laura Archera, would later emphasise in her biographical account, "This Timeless Moment": "One of the great achievements of his life: that of having regained his sight."
After revealing a letter she wrote to the "Los Angeles Times" disclaiming the label of Huxley as a "poor fellow who can hardly see" by Walter C. Alvarez, she tempered this: "Although I feel it was an injustice to treat Aldous as though he were blind, it is true there were many indications of his impaired vision.
For instance, although Aldous did not wear glasses, he would quite often use a magnifying lens."
Laura Huxley proceeded to elaborate a few nuances of inconsistency peculiar to Huxley's vision.
Her account, in this respect, is discernibly congruent with the following sample of Huxley's own words from "The Art of Seeing": "The most characteristic fact about the functioning of the total organism, or any part of the organism, is that it is not constant, but highly variable."
Nevertheless, the topic of Huxley's eyesight continues to endure similar, significant controversy, regardless of how trivial a subject matter it might initially appear.
American popular science author Steven Johnson, in his book "Mind Wide Open", quotes Huxley about his difficulties with visual encoding: "I am and, for as long as I can remember, I have always been a poor visualizer.
Words, even the pregnant words of poets, do not evoke pictures in my mind.
No hypnagogic visions greet me on the verge of sleep.
When I recall something, the memory does not present itself to me as a vividly seen event or object.
By an effort of the will, I can evoke a not very vivid image of what happened yesterday afternoon ...".
Huxley married Maria Nys (10 September 1899 – 12 February 1955), a Belgian he met at Garsington, Oxfordshire, in 1919.
They had one child, Matthew Huxley (19 April 1920 – 10 February 2005), who had a career as an author, anthropologist, and prominent epidemiologist.
In 1955, Maria Huxley died of cancer.
In 1956 Huxley married Laura Archera (1911–2007), also an author as well as a violinist and psychotherapist.
She wrote "This Timeless Moment", a biography of Huxley.
She told the story of their marriage through Mary Ann Braubach's 2010 documentary, "Huxley on Huxley".
In 1960 Aldous Huxley was diagnosed with laryngeal cancer and, in the years that followed, with his health deteriorating, he wrote the Utopian novel "Island", and gave lectures on "Human Potentialities" both at the University of California's San Francisco Medical Center and at the Esalen Institute.
These lectures were fundamental to the beginning of the Human Potential Movement.
Huxley was a close friend of Jiddu Krishnamurti and Rosalind Rajagopal and was involved in the creation of the Happy Valley School, now Besant Hill School of Happy Valley, in Ojai, California.
The most substantial collection of Huxley's few remaining papers, following the destruction of most in a fire, is at the Library of the University of California, Los Angeles.
Some are also at the Stanford University Libraries.
On 9 April 1962, Huxley was informed he was elected Companion of Literature by the Royal Society of Literature, the senior literary organisation in Britain, and he accepted the title via letter on 28 April 1962.
The correspondence between Huxley and the society are kept at the Cambridge University Library.
The society invited Huxley to appear at a banquet and give a lecture at Somerset House, London in June 1963.
Huxley wrote a draft of the speech he intended to give at the society; however, his deteriorating health meant he was not able to attend.
On his deathbed, unable to speak owing to advanced laryngeal cancer, Huxley made a written request to his wife Laura for "LSD, 100 µg, intramuscular."
According to her account of his death in "This Timeless Moment", she obliged with an injection at 11:20 a.m.
and a second dose an hour later; Huxley died aged 69, at 5:20 p.m.
(Los Angeles time), on 22 November 1963.
Media coverage of Huxley's death—as with that of the author C. S. Lewis–was overshadowed by the assassination of U.S.
president John F. Kennedy on the same day.
In an article for "New York" magazine titled “The Eclipsed Celebrity Death Club”, Christopher Bonanos wrote, 

This coincidence served as the basis for Peter Kreeft's book "Between Heaven and Hell: A Dialog Somewhere Beyond Death with John F. Kennedy, C. S. Lewis, & Aldous Huxley", which imagines a conversation among the three men taking place in Purgatory following their deaths.
Huxley's memorial service took place in London in December 1963; it was led by his older brother Julian, and on 27 October 1971 his ashes were interred in the family grave at the Watts Cemetery, home of the Watts Mortuary Chapel in Compton, Guildford, Surrey, England.
Huxley had been a long-time friend of Russian composer Igor Stravinsky, who later dedicated his last orchestral composition to Huxley.
Stravinsky began "Variations" in Santa Fé, New Mexico, in July 1963, and completed the composition in Hollywood on 28 October 1964.
It was first performed in Chicago on 17 April 1965, by the Chicago Symphony Orchestra conducted by Robert Craft.
</doc>
<doc id="630" url="https://en.wikipedia.org/wiki?curid=630" title="Ada">
Ada

Ada may refer to:
















</doc>
<doc id="632" url="https://en.wikipedia.org/wiki?curid=632" title="Aberdeen (disambiguation)">
Aberdeen (disambiguation)

Aberdeen is a city in Scotland, United Kingdom.
Aberdeen may also refer to:


















</doc>
<doc id="633" url="https://en.wikipedia.org/wiki?curid=633" title="Algae">
Algae

Algae (; singular alga ) is an informal term for a large, diverse group of photosynthetic eukaryotic organisms that are not necessarily closely related, and is thus polyphyletic.
Included organisms range from unicellular microalgae genera, such as "Chlorella" and the diatoms, to multicellular forms, such as the giant kelp, a large brown alga which may grow up to 50 m in length.
Most are aquatic and autotrophic and lack many of the distinct cell and tissue types, such as stomata, xylem, and phloem, which are found in land plants.
The largest and most complex marine algae are called seaweeds, while the most complex freshwater forms are the Charophyta, a division of green algae which includes, for example, "Spirogyra" and the stoneworts.
No definition of algae is generally accepted.
One definition is that algae "have chlorophyll as their primary photosynthetic pigment and lack a sterile covering of cells around their reproductive cells".
Although cyanobacteria are often referred to as "blue-green algae", most authorities exclude all prokaryotes from the definition of algae.
Algae constitute a polyphyletic group since they do not include a common ancestor, and although their plastids seem to have a single origin, from cyanobacteria, they were acquired in different ways.
Green algae are examples of algae that have primary chloroplasts derived from endosymbiotic cyanobacteria.
Diatoms and brown algae are examples of algae with secondary chloroplasts derived from an endosymbiotic red alga.
Algae exhibit a wide range of reproductive strategies, from simple asexual cell division to complex forms of sexual reproduction.
Algae lack the various structures that characterize land plants, such as the phyllids (leaf-like structures) of bryophytes, rhizoids in nonvascular plants, and the roots, leaves, and other organs found in tracheophytes (vascular plants).
Most are phototrophic, although some are mixotrophic, deriving energy both from photosynthesis and uptake of organic carbon either by osmotrophy, myzotrophy, or phagotrophy.
Some unicellular species of green algae, many golden algae, euglenids, dinoflagellates, and other algae have become heterotrophs (also called colorless or apochlorotic algae), sometimes parasitic, relying entirely on external energy sources and have limited or no photosynthetic apparatus.
Some other heterotrophic organisms, such as the apicomplexans, are also derived from cells whose ancestors possessed plastids, but are not traditionally considered as algae.
Algae have photosynthetic machinery ultimately derived from cyanobacteria that produce oxygen as a by-product of photosynthesis, unlike other photosynthetic bacteria such as purple and green sulfur bacteria.
Fossilized filamentous algae from the Vindhya basin have been dated back to 1.6 to 1.7 billion years ago.
The singular "alga" is the Latin word for "seaweed" and retains that meaning in English.
The etymology is obscure.
Although some speculate that it is related to Latin "algēre", "be cold", no reason is known to associate seaweed with temperature.
A more likely source is "alliga", "binding, entwining".
The Ancient Greek word for seaweed was φῦκος ("phŷcos"), which could mean either the seaweed (probably red algae) or a red dye derived from it.
The Latinization, "fūcus", meant primarily the cosmetic rouge.
The etymology is uncertain, but a strong candidate has long been some word related to the Biblical פוך ("pūk"), "paint" (if not that word itself), a cosmetic eye-shadow used by the ancient Egyptians and other inhabitants of the eastern Mediterranean.
It could be any color: black, red, green, or blue.
Accordingly, the modern study of marine and freshwater algae is called either phycology or algology, depending on whether the Greek or Latin root is used.
The name "Fucus" appears in a number of taxa.
The algae contain chloroplasts that are similar in structure to cyanobacteria.
Chloroplasts contain circular DNA like that in cyanobacteria and are interpreted as representing reduced endosymbiotic cyanobacteria.
However, the exact origin of the chloroplasts is different among separate lineages of algae, reflecting their acquisition during different endosymbiotic events.
The table below describes the composition of the three major groups of algae.
Their lineage relationships are shown in the figure in the upper right.
Many of these groups contain some members that are no longer photosynthetic.
Some retain plastids, but not chloroplasts, while others have lost plastids entirely.
Phylogeny based on plastid not nucleocytoplasmic genealogy:

Linnaeus, in "Species Plantarum" (1753), the starting point for modern botanical nomenclature, recognized 14 genera of algae, of which only four are currently considered among algae.
In "Systema Naturae", Linnaeus described the genera "Volvox" and "Corallina", and a species of "Acetabularia" (as "Madrepora"), among the animals.
In 1768, Samuel Gottlieb Gmelin (1744–1774) published the "Historia Fucorum", the first work dedicated to marine algae and the first book on marine biology to use the then new binomial nomenclature of Linnaeus.
It included elaborate illustrations of seaweed and marine algae on folded leaves.
W.H.Harvey (1811—1866) and Lamouroux (1813) were the first to divide macroscopic algae into four divisions based on their pigmentation.
This is the first use of a biochemical criterion in plant systematics.
Harvey's four divisions are: red algae (Rhodospermae), brown algae (Melanospermae), green algae (Chlorospermae), and Diatomaceae.
At this time, microscopic algae were discovered and reported by a different group of workers (e.g., O. F. Müller and Ehrenberg) studying the Infusoria (microscopic organisms).
Unlike macroalgae, which were clearly viewed as plants, microalgae were frequently considered animals because they are often motile.
Even the nonmotile (coccoid) microalgae were sometimes merely seen as stages of the lifecycle of plants, macroalgae, or animals.
Although used as a taxonomic category in some pre-Darwinian classifications, e.g., Linnaeus (1753), de Jussieu (1789), Horaninow (1843), Agassiz (1859), Wilson & Cassin (1864), in further classifications, the "algae" are seen as an artificial, polyphyletic group.
Throughout the 20th century, most classifications treated the following groups as divisions or classes of algae: cyanophytes, rhodophytes, chrysophytes, xanthophytes, bacillariophytes, phaeophytes, pyrrhophytes (cryptophytes and dinophytes), euglenophytes, and chlorophytes.
Later, many new groups were discovered (e.g., Bolidophyceae), and others were splintered from older groups: charophytes and glaucophytes (from chlorophytes), many heterokontophytes (e.g., synurophytes from chrysophytes, or eustigmatophytes from xanthophytes), haptophytes (from chrysophytes), and chlorarachniophytes (from xanthophytes).
With the abandonment of plant-animal dichotomous classification, most groups of algae (sometimes all) were included in Protista, later also abandoned in favour of Eukaryota.
However, as a legacy of the older plant life scheme, some groups that were also treated as protozoans in the past still have duplicated classifications (see ambiregnal protists).
Some parasitic algae (e.g., the green algae "Prototheca" and "Helicosporidium", parasites of metazoans, or "Cephaleuros", parasites of plants) were originally classified as fungi, sporozoans, or protistans of "incertae sedis", while others (e.g., the green algae "Phyllosiphon" and "Rhodochytrium", parasites of plants, or the red algae "Pterocladiophila" and "Gelidiocolax mammillatus", parasites of other red algae, or the dinoflagellates "Oodinium", parasites of fish) had their relationship with algae conjectured early.
In other cases, some groups were originally characterized as parasitic algae (e.g., "Chlorochytrium"), but later were seen as endophytic algae.
Some filamentous bacteria (e.g., "Beggiatoa") were originally seen as algae.
Furthermore, groups like the apicomplexans are also parasites derived from ancestors that possessed plastids, but are not included in any group traditionally seen as algae.
The first land plants probably evolved from shallow freshwater charophyte algae much like "Chara" almost 500 million years ago.
These probably had an isomorphic alternation of generations and were probably filamentous.
Fossils of isolated land plant spores suggest land plants may have been around as long as 475 million years ago.
A range of algal morphologies is exhibited, and convergence of features in unrelated groups is common.
The only groups to exhibit three-dimensional multicellular thalli are the reds and browns, and some chlorophytes.
Apical growth is constrained to subsets of these groups: the florideophyte reds, various browns, and the charophytes.
The form of charophytes is quite different from those of reds and browns, because they have distinct nodes, separated by internode 'stems'; whorls of branches reminiscent of the horsetails occur at the nodes.
Conceptacles are another polyphyletic trait; they appear in the coralline algae and the Hildenbrandiales, as well as the browns.
Most of the simpler algae are unicellular flagellates or amoeboids, but colonial and nonmotile forms have developed independently among several of the groups.
Some of the more common organizational levels, more than one of which may occur in the lifecycle of a species, are

In three lines, even higher levels of organization have been reached, with full tissue differentiation.
These are the brown algae,—some of which may reach 50 m in length (kelps)—the red algae, and the green algae.
The most complex forms are found among the charophyte algae (see Charales and Charophyta), in a lineage that eventually led to the higher land plants.
The innovation that defines these nonalgal plants is the presence of female reproductive organs with protective cell layers that protect the zygote and developing embryo.
Hence, the land plants are referred to as the Embryophytes.
Many algae, particularly members of the Characeae, have served as model experimental organisms to understand the mechanisms of the water permeability of membranes, osmoregulation, turgor regulation, salt tolerance, cytoplasmic streaming, and the generation of action potentials.
Phytohormones are found not only in higher plants, but in algae, too.
Some species of algae form symbiotic relationships with other organisms.
In these symbioses, the algae supply photosynthates (organic substances) to the host organism providing protection to the algal cells.
The host organism derives some or all of its energy requirements from the algae.
Examples are:

Lichens are defined by the International Association for Lichenology to be "an association of a fungus and a photosynthetic symbiont resulting in a stable vegetative body having a specific structure."
The fungi, or mycobionts, are mainly from the Ascomycota with a few from the Basidiomycota.
In nature they do not occur separate from lichens.
It is unknown when they began to associate.
One mycobiont associates with the same phycobiont species, rarely two, from the green algae, except that alternatively, the mycobiont may associate with a species of cyanobacteria (hence "photobiont" is the more accurate term).
A photobiont may be associated with many different mycobionts or may live independently; accordingly, lichens are named and classified as fungal species.
The association is termed a morphogenesis because the lichen has a form and capabilities not possessed by the symbiont species alone (they can be experimentally isolated).
The photobiont possibly triggers otherwise latent genes in the mycobiont.
Trentepohlia is an example of a common green alga genus worldwide that can grow on its own or be lichenised.
Lichen thus share some of the habitat and often similar appearance with specialized species of algae ("aerophytes") growing on exposed surfaces such as tree trunks and rocks and sometimes discoloring them.
Coral reefs are accumulated from the calcareous exoskeletons of marine invertebrates of the order Scleractinia (stony corals).
These animals metabolize sugar and oxygen to obtain energy for their cell-building processes, including secretion of the exoskeleton, with water and carbon dioxide as byproducts.
Dinoflagellates (algal protists) are often endosymbionts in the cells of the coral-forming marine invertebrates, where they accelerate host-cell metabolism by generating sugar and oxygen immediately available through photosynthesis using incident light and the carbon dioxide produced by the host.
Reef-building stony corals (hermatypic corals) require endosymbiotic algae from the genus "Symbiodinium" to be in a healthy condition.
The loss of "Symbiodinium" from the host is known as coral bleaching, a condition which leads to the deterioration of a reef.
Endosymbiontic green algae live close to the surface of some sponges, for example, breadcrumb sponges ("Halichondria panicea").
The alga is thus protected from predators; the sponge is provided with oxygen and sugars which can account for 50 to 80% of sponge growth in some species.
Rhodophyta, Chlorophyta, and Heterokontophyta, the three main algal divisions, have lifecycles which show considerable variation and complexity.
In general, an asexual phase exists where the seaweed's cells are diploid, a sexual phase where the cells are haploid, followed by fusion of the male and female gametes.
Asexual reproduction permits efficient population increases, but less variation is possible.
Commonly, in sexual reproduction of unicellular and colonial algae, two specialized, sexually compatible, haploid gametes make physical contact and fuse to form a zygote.
To ensure a successful mating, the development and release of gametes is highly synchronized and regulated; pheromones may play a key role in these processes.
Sexual reproduction allows for more variation and provides the benefit of efficient recombinational repair of DNA damages during meiosis, a key stage of the sexual cycle.
However, sexual reproduction is more costly than asexual reproduction.
Meiosis has been shown to occur in many different species of algae.
The "Algal Collection of the US National Herbarium" (located in the National Museum of Natural History) consists of approximately 320,500 dried specimens, which, although not exhaustive (no exhaustive collection exists), gives an idea of the order of magnitude of the number of algal species (that number remains unknown).
Estimates vary widely.
For example, according to one standard textbook, in the British Isles the "UK Biodiversity Steering Group Report" estimated there to be 20,000 algal species in the UK.
Another checklist reports only about 5,000 species.
Regarding the difference of about 15,000 species, the text concludes: "It will require many detailed field surveys before it is possible to provide a reliable estimate of the total number of species ..."

Regional and group estimates have been made, as well:
and so on, but lacking any scientific basis or reliable sources, these numbers have no more credibility than the British ones mentioned above.
Most estimates also omit microscopic algae, such as phytoplankton.
The most recent estimate suggests 72,500 algal species worldwide.
The distribution of algal species has been fairly well studied since the founding of phytogeography in the mid-19th century.
Algae spread mainly by the dispersal of spores analogously to the dispersal of Plantae by seeds and spores.
This dispersal can be accomplished by air, water, or other organisms.
Due to this, spores can be found in a variety of environments: fresh and marine waters, air, soil, and in or on other organisms.
Whether a spore is to grow into an organism depends on the combination of the species and the environmental conditions where the spore lands.
The spores of freshwater algae are dispersed mainly by running water and wind, as well as by living carriers.
However, not all bodies of water can carry all species of algae, as the chemical composition of certain water bodies limits the algae that can survive within them.
Marine spores are often spread by ocean currents.
Ocean water presents many vastly different habitats based on temperature and nutrient availability, resulting in phytogeographic zones, regions, and provinces.
To some degree, the distribution of algae is subject to floristic discontinuities caused by geographical features, such as Antarctica, long distances of ocean or general land masses.
It is, therefore, possible to identify species occurring by locality, such as "Pacific algae" or "North Sea algae".
When they occur out of their localities, hypothesizing a transport mechanism is usually possible, such as the hulls of ships.
For example, "Ulva reticulata" and "U. fasciata" travelled from the mainland to Hawaii in this manner.
Mapping is possible for select species only: "there are many valid examples of confined distribution patterns."
For example, "Clathromorphum" is an arctic genus and is not mapped far south of there.
However, scientists regard the overall data as insufficient due to the "difficulties of undertaking such studies."
Algae are prominent in bodies of water, common in terrestrial environments, and are found in unusual environments, such as on snow and ice.
Seaweeds grow mostly in shallow marine waters, under deep; however, some such as Navicula pennata have been recorded to a depth of .
The various sorts of algae play significant roles in aquatic ecology.
Microscopic forms that live suspended in the water column (phytoplankton) provide the food base for most marine food chains.
In very high densities (algal blooms), these algae may discolor the water and outcompete, poison, or asphyxiate other life forms.
Algae can be used as indicator organisms to monitor pollution in various aquatic systems.
In many cases, algal metabolism is sensitive to various pollutants.
Due to this, the species composition of algal populations may shift in the presence of chemical pollutants.
To detect these changes, algae can be sampled from the environment and maintained in laboratories with relative ease.
On the basis of their habitat, algae can be categorized as: aquatic (planktonic, benthic, marine, freshwater, lentic, lotic), terrestrial, aerial (subareial), lithophytic, halophytic (or euryhaline), psammon, thermophilic, cryophilic, epibiont (epiphytic, epizoic), endosymbiont (endophytic, endozoic), parasitic, calcifilic or lichenic (phycobiont).
In classical Chinese, the word is used both for "algae" and (in the modest tradition of the imperial scholars) for "literary talent".
The third island in Kunming Lake beside the Summer Palace in Beijing is known as the Zaojian Tang Dao, which thus simultaneously means "Island of the Algae-Viewing Hall" and "Island of the Hall for Reflecting on Literary Talent".
Agar, a gelatinous substance derived from red algae, has a number of commercial uses.
It is a good medium on which to grow bacteria and fungi, as most microorganisms cannot digest agar.
Alginic acid, or alginate, is extracted from brown algae.
Its uses range from gelling agents in food, to medical dressings.
Alginic acid also has been used in the field of biotechnology as a biocompatible medium for cell encapsulation and cell immobilization.
Molecular cuisine is also a user of the substance for its gelling properties, by which it becomes a delivery vehicle for flavours.
Between 100,000 and 170,000 wet tons of "Macrocystis" are harvested annually in New Mexico for alginate extraction and abalone feed.
To be competitive and independent from fluctuating support from (local) policy on the long run, biofuels should equal or beat the cost level of fossil fuels.
Here, algae-based fuels hold great promise, directly related to the potential to produce more biomass per unit area in a year than any other form of biomass.
The break-even point for algae-based biofuels is estimated to occur by 2025.
For centuries, seaweed has been used as a fertilizer; George Owen of Henllys writing in the 16th century referring to drift weed in South Wales:This kind of ore they often gather and lay on great heapes, where it heteth and rotteth, and will have a strong and loathsome smell; when being so rotten they cast on the land, as they do their muck, and thereof springeth good corn, especially barley ... After spring-tydes or great rigs of the sea, they fetch it in sacks on horse backes, and carie the same three, four, or five miles, and cast it on the lande, which doth very much better the ground for corn and grass.
Today, algae are used by humans in many ways; for example, as fertilizers, soil conditioners, and livestock feed.
Aquatic and microscopic species are cultured in clear tanks or ponds and are either harvested or used to treat effluents pumped through the ponds.
Algaculture on a large scale is an important type of aquaculture in some places.
Maerl is commonly used as a soil conditioner.
Naturally growing seaweeds are an important source of food, especially in Asia.
They provide many vitamins including: A, B, B, B, niacin, and C, and are rich in iodine, potassium, iron, magnesium, and calcium.
In addition, commercially cultivated microalgae, including both algae and cyanobacteria, are marketed as nutritional supplements, such as spirulina, "Chlorella" and the vitamin-C supplement from "Dunaliella", high in beta-carotene.
Algae are national foods of many nations: China consumes more than 70 species, including "fat choy", a cyanobacterium considered a vegetable; Japan, over 20 species; Ireland, dulse; Chile, cochayuyo.
Laver is used to make "laver bread" in Wales, where it is known as "bara lawr"; in Korea, "gim"; in Japan, "nori" and "aonori".
It is also used along the west coast of North America from California to British Columbia, in Hawaii and by the Māori of New Zealand.
Sea lettuce and badderlocks are salad ingredients in Scotland, Ireland, Greenland, and Iceland.
Algae is being considered a potential solution for world hunger problem.
The oils from some algae have high levels of unsaturated fatty acids.
For example, "Parietochloris incisa" is very high in arachidonic acid, where it reaches up to 47% of the triglyceride pool.
Some varieties of algae favored by vegetarianism and veganism contain the long-chain, essential omega-3 fatty acids, docosahexaenoic acid (DHA) and eicosapentaenoic acid (EPA).
Fish oil contains the omega-3 fatty acids, but the original source is algae (microalgae in particular), which are eaten by marine life such as copepods and are passed up the food chain.
Algae have emerged in recent years as a popular source of omega-3 fatty acids for vegetarians who cannot get long-chain EPA and DHA from other vegetarian sources such as flaxseed oil, which only contains the short-chain alpha-linolenic acid (ALA).
Agricultural Research Service scientists found that 60–90% of nitrogen runoff and 70–100% of phosphorus runoff can be captured from manure effluents using a horizontal algae scrubber, also called an algal turf scrubber (ATS).
Scientists developed the ATS, which consists of shallow, 100-foot raceways of nylon netting where algae colonies can form, and studied its efficacy for three years.
They found that algae can readily be used to reduce the nutrient runoff from agricultural fields and increase the quality of water flowing into rivers, streams, and oceans.
Researchers collected and dried the nutrient-rich algae from the ATS and studied its potential as an organic fertilizer.
They found that cucumber and corn seedlings grew just as well using ATS organic fertilizer as they did with commercial fertilizers.
Algae scrubbers, using bubbling upflow or vertical waterfall versions, are now also being used to filter aquaria and ponds.
Various polymers can be created from algae, which can be especially useful in the creation of bioplastics.
These include hybrid plastics, cellulose based plastics, poly-lactic acid, and bio-polyethylene.
Several companies have begun to produce algae polymers commercially, including for use in flip-flops and in surf boards.
The alga "Stichococcus bacillaris" has been seen to colonize silicone resins used at archaeological sites; biodegrading the synthetic substance.
The natural pigments (carotenoids and chlorophylls) produced by algae can be used as alternatives to chemical dyes and coloring agents.
The presence of some individual algal pigments, together with specific pigment concentration ratios, are taxon-specific: analysis of their concentrations with various analytical methods, particularly high-performance liquid chromatography, can therefore offer deep insight into the taxonomic composition and relative abundance of natural algae populations in sea water samples.
Carrageenan, from the red alga "Chondrus crispus", is used as a stabilizer in milk products.
</doc>
<doc id="634" url="https://en.wikipedia.org/wiki?curid=634" title="Analysis of variance">
Analysis of variance

Analysis of variance (ANOVA) is a collection of statistical models and their associated estimation procedures (such as the "variation" among and between groups) used to analyze the differences among group means in a sample.
ANOVA was developed by statistician and evolutionary biologist Ronald Fisher.
In the ANOVA setting, the observed variance in a particular variable is partitioned into components attributable to different sources of variation.
In its simplest form, ANOVA provides a statistical test of whether the population means of several groups are equal, and therefore generalizes the "t"-test to more than two groups.
ANOVA is useful for comparing (testing) three or more group means for statistical significance.
It is conceptually similar to multiple two-sample t-tests, but is more conservative, resulting in fewer type I errors, and is therefore suited to a wide range of practical problems.
While the analysis of variance reached fruition in the 20th century, antecedents extend centuries into the past according to Stigler.
These include hypothesis testing, the partitioning of sums of squares, experimental techniques and the additive model.
Laplace was performing hypothesis testing in the 1770s.
The development of least-squares methods by Laplace and Gauss circa 1800 provided an improved method of combining observations (over the existing practices then used in astronomy and geodesy).
It also initiated much study of the contributions to sums of squares.
Laplace knew how to estimate a variance from a residual (rather than a total) sum of squares.
By 1827, Laplace was using least squares methods to address ANOVA problems regarding measurements of atmospheric tides.
Before 1800, astronomers had isolated observational errors resulting 
from reaction times (the "personal equation") and had developed methods of reducing the errors.
The experimental methods used in the study of the personal equation were later accepted by the emerging field of psychology which developed strong (full factorial) experimental methods to which randomization and blinding were soon added.
An eloquent non-mathematical explanation of the additive effects model was
available in 1885.
Ronald Fisher introduced the term variance and proposed its formal analysis in a 1918 article "The Correlation Between Relatives on the Supposition of Mendelian Inheritance".
His first application of the analysis of variance was published in 1921.
Analysis of variance became widely known after being included in Fisher's 1925 book "Statistical Methods for Research Workers".
Randomization models were developed by several researchers.
The first was published in Polish by Jerzy Neyman in 1923.
One of the attributes of ANOVA that ensured its early popularity was computational elegance.
The structure of the additive model allows solution for the additive coefficients by simple algebra rather than by matrix calculations.
In the era of mechanical calculators this simplicity was critical.
The determination of statistical significance also required access to tables of the F function which were supplied by early statistics texts.
The analysis of variance can be used as an exploratory tool to explain observations.
A dog show provides an example.
A dog show is not a random sampling of the breed: it is typically limited to dogs that are adult, pure-bred, and exemplary.
A histogram of dog weights from a show might plausibly be rather complex, like the yellow-orange distribution shown in the illustrations.
Suppose we wanted to predict the weight of a dog based on a certain set of characteristics of each dog.
One way to do that is to "explain" the distribution of weights by dividing the dog population into groups based on those characteristics.
A successful grouping will split dogs such that (a) each group has a low variance of dog weights (meaning the group is relatively homogeneous) and (b) the mean of each group is distinct (if two groups have the same mean, then it isn't reasonable to conclude that the groups are, in fact, separate in any meaningful way).
In the illustrations to the right, groups are identified as "X", "X", etc.
In the first illustration, the dogs are divided according to the product (interaction) of two binary groupings: young vs old, and short-haired vs long-haired (e.g., group 1 is young, short-haired dogs, group 2 is young, long-haired dogs, etc.).
Since the distributions of dog weight within each of the groups (shown in blue) has a relatively large variance, and since the means are very similar across groups, grouping dogs by these characteristics does not produce an effective way to explain the variation in dog weights: knowing which group a dog is in doesn't allow us to predict its weight much better than simply knowing the dog is in a dog show.
Thus, this grouping fails to explain the variation in the overall distribution (yellow-orange).
An attempt to explain the weight distribution by grouping dogs as "pet vs working breed" and "less athletic vs more athletic" would probably be somewhat more successful (fair fit).
The heaviest show dogs are likely to be big strong working breeds, while breeds kept as pets tend to be smaller and thus lighter.
As shown by the second illustration, the distributions have variances that are considerably smaller than in the first case, and the means are more distinguishable.
However, the significant overlap of distributions, for example, means that we cannot distinguish "X" and "X" reliably.
Grouping dogs according to a coin flip might produce distributions that look similar.
An attempt to explain weight by breed is likely to produce a very good fit.
All Chihuahuas are light and all St Bernards are heavy.
The difference in weights between Setters and Pointers does not justify separate breeds.
The analysis of variance provides the formal tools to justify these intuitive judgments.
A common use of the method is the analysis of experimental data or the development of models.
The method has some advantages over correlation: not all of the data must be numeric and one result of the method is a judgment in the confidence in an explanatory relationship.
ANOVA is a form of statistical hypothesis testing heavily used in the analysis of experimental data.
A test result (calculated from the null hypothesis and the sample) is called statistically significant if it is deemed unlikely to have occurred by chance, "assuming the truth of the null hypothesis".
A statistically significant result, when a probability (p-value) is less than a pre-specified threshold (significance level), justifies the rejection of the null hypothesis, but only if the a priori probability of the null hypothesis is not high.
In the typical application of ANOVA, the null hypothesis is that all groups are random samples from the same population.
For example, when studying the effect of different treatments on similar samples of patients, the null hypothesis would be that all treatments have the same effect (perhaps none).
Rejecting the null hypothesis is taken to mean that the differences in observed effects between treatment groups are unlikely to be due to random chance.
By construction, hypothesis testing limits the rate of Type I errors (false positives) to a significance level.
Experimenters also wish to limit Type II errors (false negatives).
The rate of Type II errors depends largely on sample size (the rate is larger for smaller samples), significance 
level (when the standard of proof is high, the chances of overlooking 
a discovery are also high) and effect size (a smaller effect size is more prone to Type II error).
The terminology of ANOVA is largely from the statistical 
design of experiments.
The experimenter adjusts factors and 
measures responses in an attempt to determine an effect.
Factors are 
assigned to experimental units by a combination of randomization and 
blocking to ensure the validity of the results.
Blinding keeps the
weighing impartial.
Responses show a variability that is partially 
the result of the effect and is partially random error.
ANOVA is the synthesis of several ideas and it is used for multiple 
purposes.
As a consequence, it is difficult to define concisely or precisely.
"Classical" ANOVA for balanced data does three things at once:

In short, ANOVA is a statistical tool used in several ways to develop and confirm an explanation for the observed data.
Additionally:

As a result:
ANOVA "has long enjoyed the status of being the most used (some would 
say abused) statistical technique in psychological research."
ANOVA "is probably the most useful technique in the field of 
statistical inference."
ANOVA is difficult to teach, particularly for complex experiments, with split-plot designs being notorious.
In some cases the proper 
application of the method is best determined by problem pattern recognition 
followed by the consultation of a classic authoritative test.
(Condensed from the "NIST Engineering Statistics Handbook": Section 5.7.
A 
Glossary of DOE Terminology.)
There are three classes of models used in the analysis of variance, and these are outlined here.
The fixed-effects model (class I) of analysis of variance applies to situations in which the experimenter applies one or more treatments to the subjects of the experiment to see whether the response variable values change.
This allows the experimenter to estimate the ranges of response variable values that the treatment would generate in the population as a whole.
Random-effects model (class II) is used when the treatments are not fixed.
This occurs when the various factor levels are sampled from a larger population.
Because the levels themselves are random variables, some assumptions and the method of contrasting the treatments (a multi-variable generalization of simple differences) differ from the fixed-effects model.
A mixed-effects model (class III) contains experimental factors of both fixed and random-effects types, with appropriately different interpretations and analysis for the two types.
Example:
Teaching experiments could be performed by a college or university department 
to find a good introductory textbook, with each text considered a 
treatment.
The fixed-effects model would compare a list of candidate 
texts.
The random-effects model would determine whether important 
differences exist among a list of randomly selected texts.
The 
mixed-effects model would compare the (fixed) incumbent texts to 
randomly selected alternatives.
Defining fixed and random effects has proven elusive, with competing 
definitions arguably leading toward a linguistic quagmire.
The analysis of variance has been studied from several approaches, the most common of which uses a linear model that relates the response to the treatments and blocks.
Note that the model is linear in parameters but may be nonlinear across factor levels.
Interpretation is easy when data is balanced across factors but much deeper understanding is needed for unbalanced data.
The analysis of variance can be presented in terms of a linear model, which makes the following assumptions about the probability distribution of the responses:

The separate assumptions of the textbook model imply that the errors are independently, identically, and normally distributed for fixed effects models, that is, that the errors (formula_1) are independent and

In a randomized controlled experiment, the treatments are randomly assigned to experimental units, following the experimental protocol.
This randomization is objective and declared before the experiment is carried out.
The objective random-assignment is used to test the significance of the null hypothesis, following the ideas of C. S. Peirce and Ronald Fisher.
This design-based analysis was discussed and developed by Francis J. Anscombe at Rothamsted Experimental Station and by Oscar Kempthorne at Iowa State University.
Kempthorne and his students make an assumption of "unit treatment additivity", which is discussed in the books of Kempthorne and David R. Cox.
In its simplest form, the assumption of unit-treatment additivity states that the observed response formula_3 from experimental unit formula_4 when receiving treatment formula_5 can be written as the sum of the unit's response formula_6 and the treatment-effect formula_7, that is 
The assumption of unit-treatment additivity implies that, for every treatment formula_5, the formula_5th treatment has exactly the same effect formula_11 on every experiment unit.
The assumption of unit treatment additivity usually cannot be directly falsified, according to Cox and Kempthorne.
However, many "consequences" of treatment-unit additivity can be falsified.
For a randomized experiment, the assumption of unit-treatment additivity "implies" that the variance is constant for all treatments.
Therefore, by contraposition, a necessary condition for unit-treatment additivity is that the variance is constant.
The use of unit treatment additivity and randomization is similar to the design-based inference that is standard in finite-population survey sampling.
Kempthorne uses the randomization-distribution and the assumption of "unit treatment additivity" to produce a "derived linear model", very similar to the textbook model discussed previously.
The test statistics of this derived linear model are closely approximated by the test statistics of an appropriate normal linear model, according to approximation theorems and simulation studies.
However, there are differences.
For example, the randomization-based analysis results in a small but (strictly) negative correlation between the observations.
In the randomization-based analysis, there is "no assumption" of a "normal" distribution and certainly "no assumption" of "independence".
On the contrary, "the observations are dependent"!
The randomization-based analysis has the disadvantage that its exposition involves tedious algebra and extensive time.
Since the randomization-based analysis is complicated and is closely approximated by the approach using a normal linear model, most teachers emphasize the normal linear model approach.
Few statisticians object to model-based analysis of balanced randomized experiments.
However, when applied to data from non-randomized experiments or observational studies, model-based analysis lacks the warrant of randomization.
For observational data, the derivation of confidence intervals must use "subjective" models, as emphasized by Ronald Fisher and his followers.
In practice, the estimates of treatment-effects from observational studies generally are often inconsistent.
In practice, "statistical models" and observational data are useful for suggesting hypotheses that should be treated very cautiously by the public.
The normal-model based ANOVA analysis assumes the independence, normality and 
homogeneity of the variances of the residuals.
The 
randomization-based analysis assumes only the homogeneity of the 
variances of the residuals (as a consequence of unit-treatment 
additivity) and uses the randomization procedure of the experiment.
Both these analyses require homoscedasticity, as an assumption for the normal-model analysis and as a consequence of randomization and additivity for the randomization-based analysis.
However, studies of processes that 
change variances rather than means (called dispersion effects) have 
been successfully conducted using ANOVA.
There are
"no" necessary assumptions for ANOVA in its full generality, but the
"F"-test used for ANOVA hypothesis testing has assumptions and practical 
limitations which are of continuing interest.
Problems which do not satisfy the assumptions of ANOVA can often be transformed to satisfy the assumptions.
The property of unit-treatment additivity is not invariant under a "change of scale", so statisticians often use transformations to achieve unit-treatment additivity.
If the response variable is expected to follow a parametric family of probability distributions, then the statistician may specify (in the protocol for the experiment or observational study) that the responses be transformed to stabilize the variance.
Also, a statistician may specify that logarithmic transforms be applied to the responses, which are believed to follow a multiplicative model.
According to Cauchy's functional equation theorem, the logarithm is the only continuous transformation that transforms real multiplication to addition.
ANOVA is used in the analysis of comparative experiments, those in 
which only the difference in outcomes is of interest.
The statistical
significance of the experiment is determined by a ratio of two 
variances.
This ratio is independent of several possible alterations
to the experimental observations: Adding a constant to all 
observations does not alter significance.
Multiplying all 
observations by a constant does not alter significance.
So ANOVA 
statistical significance result is independent of constant bias and 
scaling errors as well as the units used in expressing observations.
In the era of mechanical calculation it was common to 
subtract a constant from all observations (when equivalent to 
dropping leading digits) to simplify data entry.
This is an example of data
coding.
The calculations of ANOVA can be characterized as computing a number
of means and variances, dividing two variances and comparing the ratio 
to a handbook value to determine statistical significance.
Calculating 
a treatment effect is then trivial, "the effect of any treatment is 
estimated by taking the difference between the mean of the 
observations which receive the treatment and the general mean".
ANOVA uses traditional standardized terminology.
The definitional 
equation of sample variance is
formula_12, where the 
divisor is called the degrees of freedom (DF), the summation is called 
the sum of squares (SS), the result is called the mean square (MS) and 
the squared terms are deviations from the sample mean.
ANOVA 
estimates 3 sample variances: a total variance based on all the 
observation deviations from the grand mean, an error variance based on 
all the observation deviations from their appropriate 
treatment means, and a treatment variance.
The treatment variance is
based on the deviations of treatment means from the grand mean, the 
result being multiplied by the number of observations in each 
treatment to account for the difference between the variance of 
observations and the variance of means.
The fundamental technique is a partitioning of the total sum of squares "SS" into components related to the effects used in the model.
For example, the model for a simplified ANOVA with one type of treatment at different levels.
The number of degrees of freedom "DF" can be partitioned in a similar way: one of these components (that for error) specifies a chi-squared distribution which describes the associated sum of squares, while the same is true for "treatments" if there is no treatment effect.
See also Lack-of-fit sum of squares.
The "F"-test is used for comparing the factors of the total deviation.
For example, in one-way, or single-factor ANOVA, statistical significance is tested for by comparing the F test statistic

where "MS" is mean square, formula_17 = number of treatments and 
formula_18 = total number of cases

to the "F"-distribution with formula_19, formula_20 degrees of freedom.
Using the "F"-distribution is a natural candidate because the test statistic is the ratio of two scaled sums of squares each of which follows a scaled chi-squared distribution.
The expected value of F is formula_21 (where n is the treatment sample size)
which is 1 for no treatment effect.
As values of F increase above 1, the evidence is increasingly inconsistent with the null hypothesis.
Two apparent experimental methods of increasing F are increasing the sample size and reducing the error variance by tight experimental controls.
There are two methods of concluding the ANOVA hypothesis test, both of which produce the same result:
The ANOVA "F"-test is known to be nearly optimal in the sense of minimizing false negative errors for a fixed rate of false positive errors (i.e. maximizing power for a fixed significance level).
For example, to test the hypothesis that various medical treatments have exactly the same effect, the "F"-test's "p"-values closely approximate the permutation test's p-values: The approximation is particularly close when the design is balanced.
Such permutation tests characterize tests with maximum power against all alternative hypotheses, as observed by Rosenbaum.
The ANOVA "F"-test (of the null-hypothesis that all treatments have exactly the same effect) is recommended as a practical test, because of its robustness against many alternative distributions.
ANOVA consists of separable parts; partitioning sources of variance 
and hypothesis testing can be used individually.
ANOVA is used to 
support other statistical tools.
Regression is first used to fit more 
complex models to data, then ANOVA is used to compare models with the 
objective of selecting simple(r) models that adequately describe the 
data.
"Such models could be fit without any reference to ANOVA, but 
ANOVA tools could then be used to make some sense of the fitted models, 
and to test hypotheses about batches of coefficients."
"[W]e think of the analysis of variance as a way of understanding and structuring 
multilevel models—not as an alternative to regression but as a tool 
for summarizing complex high-dimensional inferences ..."

The simplest experiment suitable for ANOVA analysis is the completely 
randomized experiment with a single factor.
More complex experiments 
with a single factor involve constraints on randomization and include 
completely randomized blocks and Latin squares (and variants: 
Graeco-Latin squares, etc.).
The more complex experiments share many 
of the complexities of multiple factors.
A relatively complete 
discussion of the analysis (models, data summaries, ANOVA table) of 
the completely randomized experiment is 
available.
ANOVA generalizes to the study of the effects of multiple factors.
When the experiment includes observations at all combinations of 
levels of each factor, it is termed factorial.
Factorial experiments 
are more efficient than a series of single factor experiments and the 
efficiency grows as the number of factors increases.
Consequently, factorial designs are heavily used.
The use of ANOVA to study the effects of multiple factors has a complication.
In a 3-way ANOVA with factors x, y and z, the ANOVA model includes terms for the main effects (x, y, z) and terms for interactions (xy, xz, yz, xyz).
All terms require hypothesis tests.
The proliferation of interaction terms increases the risk that some hypothesis test will produce a false positive by chance.
Fortunately, experience says that high order interactions are rare.
The ability to detect interactions is a major advantage of multiple 
factor ANOVA.
Testing one factor at a time hides interactions, but 
produces apparently inconsistent experimental results.
Caution is advised when encountering interactions; Test 
interaction terms first and expand the analysis beyond ANOVA if 
interactions are found.
Texts vary in their recommendations regarding 
the continuation of the ANOVA procedure after encountering an 
interaction.
Interactions complicate the interpretation of 
experimental data.
Neither the calculations of significance nor the 
estimated treatment effects can be taken at face value.
"A 
significant interaction will often mask the significance of main effects."
Graphical methods are recommended
to enhance understanding.
Regression is often useful.
A lengthy discussion of interactions is available in Cox (1958).
Some interactions can be removed (by transformations) while others cannot.
A variety of techniques are used with multiple factor ANOVA to reduce expense.
One technique used in factorial designs is to minimize replication (possibly no replication with support of analytical trickery) and to combine groups when effects are found to be statistically (or practically) insignificant.
An experiment with many insignificant factors may collapse into one with a few factors supported by many replications.
Several fully worked numerical examples are available.
A 
simple case uses one-way (a single factor) analysis.
A more complex case uses two-way (two-factor) analysis.
Some analysis is required in support of the "design" of the experiment while other analysis is performed after changes in the factors are formally found to produce statistically significant changes in the responses.
Because experimentation is iterative, the results of one experiment alter plans for following experiments.
In the design of an experiment, the number of experimental units is planned to satisfy the goals of the experiment.
Experimentation is often sequential.
Early experiments are often designed to provide mean-unbiased estimates of treatment effects and of experimental error.
Later experiments are often designed to test a hypothesis that a treatment effect has an important magnitude; in this case, the number of experimental units is chosen so that the experiment is within budget and has adequate power, among other goals.
Reporting sample size analysis is generally required in psychology.
"Provide information on sample size and the process that led to sample size decisions."
The analysis, which is written in the experimental protocol before the experiment is conducted, is examined in grant applications and administrative review boards.
Besides the power analysis, there are less formal methods for selecting the number of experimental units.
These include graphical methods based on limiting
the probability of false negative errors, graphical methods based on an expected variation increase (above the residuals) and methods based on achieving a desired confident interval.
Power analysis is often applied in the context of ANOVA in order to assess the probability of successfully rejecting the null hypothesis if we assume a certain ANOVA design, effect size in the population, sample size and significance level.
Power analysis can assist in study design by determining what sample size would be required in order to have a reasonable chance of rejecting the null hypothesis when the alternative hypothesis is true.
Several standardized measures of effect have been proposed for ANOVA to summarize the strength of the association between a predictor(s) and the dependent variable or the overall standardized difference of the complete model.
Standardized effect-size estimates facilitate comparison of findings across studies and disciplines.
However, while standardized effect sizes are commonly used in much of the professional literature, a non-standardized measure of effect size that has immediately "meaningful" units may be preferable for reporting purposes.
It is always appropriate to carefully consider outliers.
They have a disproportionate impact on statistical conclusions and are often the result of errors.
It is prudent to verify that the assumptions of ANOVA have been met.
Residuals are examined or analyzed to confirm homoscedasticity and gross normality.
Residuals should have the appearance of (zero mean normal distribution) noise when plotted as a function of anything including time and 
modeled data values.
Trends hint at interactions among factors or among observations.
One rule of thumb: "If the largest standard deviation is less than twice the smallest standard deviation, we can use methods based on the assumption of equal standard deviations and our results 
will still be approximately correct."
A statistically significant effect in ANOVA is often followed up with one or more different follow-up tests.
This can be done in order to assess which groups are different from which other groups or to test various other focused hypotheses.
Follow-up tests are often distinguished in terms of whether they are planned (a priori) or post hoc.
Planned tests are determined before looking at the data and post hoc tests are performed after looking at the data.
Often one of the "treatments" is none, so the treatment group can act as a control.
Dunnett's test (a modification of the t-test) tests whether each of the other treatment groups has the same mean as the control.
Post hoc tests such as Tukey's range test most commonly compare every group mean with every other group mean and typically incorporate some method of controlling for Type I errors.
Comparisons, which are most commonly planned, can be either simple or compound.
Simple comparisons compare one group mean with one other group mean.
Compound comparisons typically compare two sets of groups means where one set has two or more groups (e.g., compare average group means of group A, B and C with group D).
Comparisons can also look at tests of trend, such as linear and quadratic relationships, when the independent variable involves ordered levels.
Following ANOVA with pair-wise multiple-comparison tests has been criticized on several grounds.
There are many such tests (10 in one table) and recommendations regarding their use are vague or conflicting.
There are several types of ANOVA.
Many statisticians base ANOVA on the design of the experiment, especially on the protocol that specifies the random assignment of treatments to subjects; the protocol's description of the assignment mechanism should include a specification of the structure of the treatments and of any blocking.
It is also common to apply ANOVA to observational data using an appropriate statistical model.
Some popular designs use the following types of ANOVA:

Balanced experiments (those with an equal sample size for each treatment) are relatively easy to interpret; Unbalanced 
experiments offer more complexity.
For single-factor (one-way) ANOVA, the adjustment for unbalanced data is easy, but the unbalanced analysis lacks both robustness and power.
For more complex designs the lack of balance leads to further complications.
"The orthogonality property of main effects and interactions present in balanced data does not carry over to the unbalanced case.
This means that the usual analysis of variance techniques do not apply.
Consequently, the analysis of unbalanced factorials is much more difficult than that for balanced designs."
In the general case, "The analysis of variance can also be applied to unbalanced data, but then the sums of squares, mean squares, and "F"-ratios will depend on the order in which the sources of variation 
are considered."
The simplest techniques for handling unbalanced data restore balance by either throwing out data or by synthesizing missing data.
More complex techniques use regression.
ANOVA is (in part) a significance test.
The American Psychological Association holds the view that simply reporting significance is insufficient and that reporting confidence bounds is preferred.
While ANOVA is conservative (in maintaining a significance level) against multiple comparisons in one dimension, it is not conservative against comparisons in multiple dimensions.
ANOVA is considered to be a special case of linear regression which in turn is a special case of the general linear model.
All consider the observations to be the sum of a model (fit) and a residual (error) to be minimized.
The Kruskal–Wallis test and the Friedman test are nonparametric tests, which do not rely on an assumption of normality.
Below we make clear the connection between multi-way ANOVA and linear regression.
Linearly re-order the data so that formula_22 observation is associated with a response formula_23 and factors formula_24 where formula_25 denotes the different factors and formula_26 is the total number of factors.
In one-way ANOVA formula_27 and in two-way ANOVA formula_28.
Furthermore, we assume the formula_29 factor has formula_30 levels, namely formula_31.
Now, we can one-hot encode the factors into the formula_32 dimensional vector formula_33.
The one-hot encoding function formula_34 is defined such that the formula_35 entry of formula_36 is
formula_37
The vector formula_33 is the concatenation of all of the above vectors for all formula_39.
Thus, formula_40.
In order to obtain a fully general formula_26-way interaction ANOVA we must also concatenate every additional interaction term in the vector formula_33 and then add an intercept term.
Let that vector be formula_43.
With this notation in place, we now have the exact connection with linear regression.
We simply regress response formula_23 against the vector formula_43.
However, there is a concern about identifiability.
In order to overcome such issues we assume that the sum of the parameters within each set of interactions is equal to zero.
From here, one can use "F"-statistics or other methods to determine the relevance of the individual factors.
We can consider the 2-way interaction example where we assume that the first factor has 2 levels and the second factor has 3 levels.
Define formula_46 if formula_47 and formula_48 if formula_49, i.e. formula_50 is the one-hot encoding of the first factor and formula_39 is the one-hot encoding of the second factor.
With that,
formula_52
where the last term is an intercept term.
For a more concrete example suppose that
formula_53
Then,
formula_54





</doc>
<doc id="639" url="https://en.wikipedia.org/wiki?curid=639" title="Alkane">
Alkane

In organic chemistry, an alkane, or paraffin (a historical name that also has other meanings), is an acyclic saturated hydrocarbon.
In other words, an alkane consists of hydrogen and carbon atoms arranged in a tree structure in which all the carbon–carbon bonds are single.
Alkanes have the general chemical formula CH.
The alkanes range in complexity from the simplest case of methane (CH), where "n" = 1 (sometimes called the parent molecule), to arbitrarily large and complex molecules, like pentacontane (CH) or 6-ethyl-2-methyl-5-(1-methylethyl)octane, an isomer of tetradecane (CH)

IUPAC defines alkanes as "acyclic branched or unbranched hydrocarbons having the general formula , and therefore consisting entirely of hydrogen atoms and saturated carbon atoms".
However, some sources use the term to denote "any" saturated hydrocarbon, including those that are either monocyclic (i.e. the cycloalkanes) or polycyclic, despite their having a different general formula (i.e. cycloalkanes are CH).
In an alkane, each carbon atom is sp-hybridized with 4 sigma bonds (either C–C or C–H), and each hydrogen atom is joined to one of the carbon atoms (in a C–H bond).
The longest series of linked carbon atoms in a molecule is known as its carbon skeleton or carbon backbone.
The number of carbon atoms may be thought of as the size of the alkane.
One group of the higher alkanes are waxes, solids at standard ambient temperature and pressure (SATP), for which the number of carbons in the carbon backbone is greater than about 17.
With their repeated –CH units, the alkanes constitute a homologous series of organic compounds in which the members differ in molecular mass by multiples of 14.03 u (the total mass of each such methylene-bridge unit, which comprises a single carbon atom of mass 12.01 u and two hydrogen atoms of mass ~1.01 u each).
Alkanes are not very reactive and have little biological activity.
They can be viewed as molecular trees upon which can be hung the more active/reactive functional groups of biological molecules.
The alkanes have two main commercial sources: petroleum (crude oil) and natural gas.
An alkyl group, generally abbreviated with the symbol R, is a functional group that, like an alkane, consists solely of single-bonded carbon and hydrogen atoms connected acyclically—for example, a methyl or ethyl group.
Saturated hydrocarbons are hydrocarbons having only single covalent bonds between their carbons.
They can be:

According to the definition by IUPAC, the former two are alkanes, whereas the third group is called cycloalkanes.
Saturated hydrocarbons can also combine any of the linear, cyclic (e.g., polycyclic) and branching structures; the general formula is , where "k" is the number of independent loops.
Alkanes are the acyclic (loopless) ones, corresponding to "k" = 0.
Alkanes with more than three carbon atoms can be arranged in various different ways, forming structural isomers.
The simplest isomer of an alkane is the one in which the carbon atoms are arranged in a single chain with no branches.
This isomer is sometimes called the "n"-isomer ("n" for "normal", although it is not necessarily the most common).
However the chain of carbon atoms may also be branched at one or more points.
The number of possible isomers increases rapidly with the number of carbon atoms.
For example, for acyclic alkanes:

Branched alkanes can be chiral.
For example, 3-methylhexane and its higher homologues are chiral due to their stereogenic center at carbon atom number 3.
In addition to the alkane isomers, the chain of carbon atoms may form one or more loops.
Such compounds are called cycloalkanes.
Stereoisomers and cyclic compounds are excluded when calculating the number of isomers above.
The IUPAC nomenclature (systematic way of naming compounds) for alkanes is based on identifying hydrocarbon chains.
Unbranched, saturated hydrocarbon chains are named systematically with a Greek numerical prefix denoting the number of carbons and the suffix "-ane".
In 1866, August Wilhelm von Hofmann suggested systematizing nomenclature by using the whole sequence of vowels a, e, i, o and u to create suffixes -ane, -ene, -ine (or -yne), -one, -une, for the hydrocarbons CH, CH, CH, CH, CH.
Now, the first three name hydrocarbons with single, double and triple bonds; "-one" represents a ketone; "-ol" represents an alcohol or OH group; "-oxy-" means an ether and refers to oxygen between two carbons, so that methoxymethane is the IUPAC name for dimethyl ether.
It is difficult or impossible to find compounds with more than one IUPAC name.
This is because shorter chains attached to longer chains are prefixes and the convention includes brackets.
Numbers in the name, referring to which carbon a group is attached to, should be as low as possible so that 1- is implied and usually omitted from names of organic compounds with only one side-group.
Symmetric compounds will have two ways of arriving at the same name.
Straight-chain alkanes are sometimes indicated by the prefix ""n"-" (for "normal") where a non-linear isomer exists.
Although this is not strictly necessary, the usage is still common in cases where there is an important difference in properties between the straight-chain and branched-chain isomers, e.g., "n"-hexane or 2- or 3-methylpentane.
Alternative names for this group are: linear paraffins or "n"-paraffins.
The members of the series (in terms of number of carbon atoms) are named as follows:

The first four names were derived from methanol, ether, propionic acid and butyric acid, respectively (hexadecane is also sometimes referred to as cetane).
Alkanes with five or more carbon atoms are named by adding the suffix -ane to the appropriate numerical multiplier prefix with elision of any terminal vowel ("-a" or "-o") from the basic numerical term.
Hence, pentane, CH; hexane, CH; heptane, CH; octane, CH; etc.
The prefix is generally Greek, however alkanes with a carbon atom count ending in nine, for example nonane, use the Latin prefix non-.
For a more complete list, see List of alkanes.
Simple branched alkanes often have a common name using a prefix to distinguish them from linear alkanes, for example "n"-pentane, isopentane, and neopentane.
IUPAC naming conventions can be used to produce a systematic name.
The key steps in the naming of more complicated branched alkanes are as follows:

Though technically distinct from the alkanes, this class of hydrocarbons is referred to by some as the "cyclic alkanes."
As their description implies, they contain one or more rings.
Simple cycloalkanes have a prefix "cyclo-" to distinguish them from alkanes.
Cycloalkanes are named as per their acyclic counterparts with respect to the number of carbon atoms in their backbones, e.g., cyclopentane (CH) is a cycloalkane with 5 carbon atoms just like pentane (CH), but they are joined up in a five-membered ring.
In a similar manner, propane and cyclopropane, butane and cyclobutane, etc.
Substituted cycloalkanes are named similarly to substituted alkanes — the cycloalkane ring is stated, and the substituents are according to their position on the ring, with the numbering decided by the Cahn–Ingold–Prelog priority rules.
The trivial (non-systematic) name for alkanes is "paraffins".
Together, alkanes are known as the "paraffin series".
Trivial names for compounds are usually historical artifacts.
They were coined before the development of systematic names, and have been retained due to familiar usage in industry.
Cycloalkanes are also called naphthenes.
It is almost certain that the term "paraffin" stems from the petrochemical industry.
Branched-chain alkanes are called "isoparaffins".
The use of the term "paraffin" is a general term and often does not distinguish between pure compounds and mixtures of isomers, i.e., compounds of the same chemical formula, e.g., pentane and isopentane.
The following trivial names are retained in the IUPAC system:

All alkanes are colorless.
Alkanes with the lowest molecular weights are gasses, those of intermediate molecular weight are liquids, and the heaviest are waxy solids.
Alkanes experience intermolecular van der Waals forces.
Stronger intermolecular van der Waals forces give rise to greater boiling points of alkanes.
There are two determinants for the strength of the van der Waals forces:

Under standard conditions, from CH to CH alkanes are gaseous; from CH to CH they are liquids; and after CH they are solids.
As the boiling point of alkanes is primarily determined by weight, it should not be a surprise that the boiling point has almost a linear relationship with the size (molecular weight) of the molecule.
As a rule of thumb, the boiling point rises 20–30 °C for each carbon added to the chain; this rule applies to other homologous series.
A straight-chain alkane will have a boiling point higher than a branched-chain alkane due to the greater surface area in contact, thus the greater van der Waals forces, between adjacent molecules.
For example, compare isobutane (2-methylpropane) and n-butane (butane), which boil at −12 and 0 °C, and 2,2-dimethylbutane and 2,3-dimethylbutane which boil at 50 and 58 °C, respectively.
For the latter case, two molecules 2,3-dimethylbutane can "lock" into each other better than the cross-shaped 2,2-dimethylbutane, hence the greater van der Waals forces.
On the other hand, cycloalkanes tend to have higher boiling points than their linear counterparts due to the locked conformations of the molecules, which give a plane of intermolecular contact.
The melting points of the alkanes follow a similar trend to boiling points for the same reason as outlined above.
That is, (all other things being equal) the larger the molecule the higher the melting point.
There is one significant difference between boiling points and melting points.
Solids have more rigid and fixed structure than liquids.
This rigid structure requires energy to break down.
Thus the better put together solid structures will require more energy to break apart.
For alkanes, this can be seen from the graph above (i.e., the blue line).
The odd-numbered alkanes have a lower trend in melting points than even numbered alkanes.
This is because even numbered alkanes pack well in the solid phase, forming a well-organized structure, which requires more energy to break apart.
The odd-numbered alkanes pack less well and so the "looser" organized solid packing structure requires less energy to break apart.
The melting points of branched-chain alkanes can be either higher or lower than those of the corresponding straight-chain alkanes, again depending on the ability of the alkane in question to pack well in the solid phase: This is particularly true for isoalkanes (2-methyl isomers), which often have melting points higher than those of the linear analogues.
Alkanes do not conduct electricity in any way, nor are they substantially polarized by an electric field.
For this reason, they do not form hydrogen bonds and are insoluble in polar solvents such as water.
Since the hydrogen bonds between individual water molecules are aligned away from an alkane molecule, the coexistence of an alkane and water leads to an increase in molecular order (a reduction in entropy).
As there is no significant bonding between water molecules and alkane molecules, the second law of thermodynamics suggests that this reduction in entropy should be minimized by minimizing the contact between alkane and water: Alkanes are said to be hydrophobic in that they repel water.
Their solubility in nonpolar solvents is relatively good, a property that is called lipophilicity.
Different alkanes are, for example, miscible in all proportions among themselves.
The density of the alkanes usually increases with the number of carbon atoms but remains less than that of water.
Hence, alkanes form the upper layer in an alkane–water mixture.
The molecular structure of the alkanes directly affects their physical and chemical characteristics.
It is derived from the electron configuration of carbon, which has four valence electrons.
The carbon atoms in alkanes are always sp-hybridized, that is to say that the valence electrons are said to be in four equivalent orbitals derived from the combination of the 2s orbital and the three 2p orbitals.
These orbitals, which have identical energies, are arranged spatially in the form of a tetrahedron, the angle of cos(−) ≈ 109.47° between them.
An alkane molecule has only C–H and C–C single bonds.
The former result from the overlap of an sp orbital of carbon with the 1s orbital of a hydrogen; the latter by the overlap of two sp orbitals on different carbon atoms.
The bond lengths amount to 1.09 × 10 m for a C–H bond and 1.54 × 10 m for a C–C bond.
The spatial arrangement of the bonds is similar to that of the four sp orbitals—they are tetrahedrally arranged, with an angle of 109.47° between them.
Structural formulae that represent the bonds as being at right angles to one another, while both common and useful, do not correspond with the reality.
The structural formula and the bond angles are not usually sufficient to completely describe the geometry of a molecule.
There is a further degree of freedom for each carbon–carbon bond: the torsion angle between the atoms or groups bound to the atoms at each end of the bond.
The spatial arrangement described by the torsion angles of the molecule is known as its conformation.
Ethane forms the simplest case for studying the conformation of alkanes, as there is only one C–C bond.
If one looks down the axis of the C–C bond, one will see the so-called Newman projection.
The hydrogen atoms on both the front and rear carbon atoms have an angle of 120° between them, resulting from the projection of the base of the tetrahedron onto a flat plane.
However, the torsion angle between a given hydrogen atom attached to the front carbon and a given hydrogen atom attached to the rear carbon can vary freely between 0° and 360°.
This is a consequence of the free rotation about a carbon–carbon single bond.
Despite this apparent freedom, only two limiting conformations are important: eclipsed conformation and staggered conformation.
The two conformations, also known as rotamers, differ in energy: The staggered conformation is 12.6 kJ/mol lower in energy (more stable) than the eclipsed conformation (the least stable).
This difference in energy between the two conformations, known as the torsion energy, is low compared to the thermal energy of an ethane molecule at ambient temperature.
There is constant rotation about the C–C bond.
The time taken for an ethane molecule to pass from one staggered conformation to the next, equivalent to the rotation of one CH group by 120° relative to the other, is of the order of 10 seconds.
The case of higher alkanes is more complex but based on similar principles, with the antiperiplanar conformation always being the most favored around each carbon–carbon bond.
For this reason, alkanes are usually shown in a zigzag arrangement in diagrams or in models.
The actual structure will always differ somewhat from these idealized forms, as the differences in energy between the conformations are small compared to the thermal energy of the molecules: Alkane molecules have no fixed structural form, whatever the models may suggest.
Virtually all organic compounds contain carbon–carbon, and carbon–hydrogen bonds, and so show some of the features of alkanes in their spectra.
Alkanes are notable for having no other groups, and therefore for the "absence" of other characteristic spectroscopic features of a different functional group like –OH, –CHO, –COOH etc.
The carbon–hydrogen stretching mode gives a strong absorption between 2850 and 2960 cm, while the carbon–carbon stretching mode absorbs between 800 and 1300 cm.
The carbon–hydrogen bending modes depend on the nature of the group: methyl groups show bands at 1450 cm and 1375 cm, while methylene groups show bands at 1465 cm and 1450 cm.
Carbon chains with more than four carbon atoms show a weak absorption at around 725 cm.
The proton resonances of alkanes are usually found at "δ" = 0.5–1.5.
The carbon-13 resonances depend on the number of hydrogen atoms attached to the carbon: "δ" = 8–30 (primary, methyl, –CH), 15–55 (secondary, methylene, –CH–), 20–60 (tertiary, methyne, C–H) and quaternary.
The carbon-13 resonance of quaternary carbon atoms is characteristically weak, due to the lack of nuclear Overhauser effect and the long relaxation time, and can be missed in weak samples, or samples that have not been run for a sufficiently long time.
Alkanes have a high ionization energy, and the molecular ion is usually weak.
The fragmentation pattern can be difficult to interpret, but, in the case of branched chain alkanes, the carbon chain is preferentially cleaved at tertiary or quaternary carbons due to the relative stability of the resulting free radicals.
The fragment resulting from the loss of a single methyl group ("M" − 15) is often absent, and other fragments are often spaced by intervals of fourteen mass units, corresponding to sequential loss of CH groups.
Alkanes are only weakly reactive with ionic and other polar substances.
The acid dissociation constant (pK) values of all alkanes are above 60, hence they are practically inert to acids and bases (see: carbon acids).
This inertness is the source of the term "paraffins" (with the meaning here of "lacking affinity").
In crude oil the alkane molecules have remained chemically unchanged for millions of years.
However redox reactions of alkanes, in particular with oxygen and the halogens, are possible as the carbon atoms are in a strongly reduced condition; in the case of methane, the lowest possible oxidation state for carbon (−4) is reached.
Reaction with oxygen ("if" present in sufficient quantity to satisfy the reaction stoichiometry) leads to combustion without any smoke, producing carbon dioxide and water.
Free radical halogenation reactions occur with halogens, leading to the production of haloalkanes.
In addition, alkanes have been shown to interact with, and bind to, certain transition metal complexes in C–H bond activation.
Free radicals, molecules with unpaired electrons, play a large role in most reactions of alkanes, such as cracking and reformation where long-chain alkanes are converted into shorter-chain alkanes and straight-chain alkanes into branched-chain isomers.
In highly branched alkanes, the bond angle may differ significantly from the optimal value (109.5°) in order to allow the different groups sufficient space.
This causes a tension in the molecule, known as steric hindrance, and can substantially increase the reactivity.
All alkanes react with oxygen in a combustion reaction, although they become increasingly difficult to ignite as the number of carbon atoms increases.
The general equation for complete combustion is:

In the absence of sufficient oxygen, carbon monoxide or even soot can be formed, as shown below:

For example, methane:

See the alkane heat of formation table for detailed data.
The standard enthalpy change of combustion, Δ"H", for alkanes increases by about 650 kJ/mol per CH group.
Branched-chain alkanes have lower values of Δ"H" than straight-chain alkanes of the same number of carbon atoms, and so can be seen to be somewhat more stable.
Alkanes react with halogens in a so-called "free radical halogenation" reaction.
The hydrogen atoms of the alkane are progressively replaced by halogen atoms.
Free radicals are the reactive species that participate in the reaction, which usually leads to a mixture of products.
The reaction is highly exothermic, and can lead to an explosion.
These reactions are an important industrial route to halogenated hydrocarbons.
There are three steps:

Experiments have shown that all halogenation produces a mixture of all possible isomers, indicating that all hydrogen atoms are susceptible to reaction.
The mixture produced, however, is not a statistical mixture: Secondary and tertiary hydrogen atoms are preferentially replaced due to the greater stability of secondary and tertiary free-radicals.
An example can be seen in the monobromination of propane:

Cracking breaks larger molecules into smaller ones.
This can be done with a thermal or catalytic method.
The thermal cracking process follows a homolytic mechanism with formation of free-radicals.
The catalytic cracking process involves the presence of acid catalysts (usually solid acids such as silica-alumina and zeolites), which promote a heterolytic (asymmetric) breakage of bonds yielding pairs of ions of opposite charges, usually a carbocation and the very unstable hydride anion.
Carbon-localized free radicals and cations are both highly unstable and undergo processes of chain rearrangement, C–C scission in position beta (i.e., cracking) and intra- and intermolecular hydrogen transfer or hydride transfer.
In both types of processes, the corresponding reactive intermediates (radicals, ions) are permanently regenerated, and thus they proceed by a self-propagating chain mechanism.
The chain of reactions is eventually terminated by radical or ion recombination.
Dragan and his colleague were the first to report about isomerization in alkanes.
Isomerization and reformation are processes in which straight-chain alkanes are heated in the presence of a platinum catalyst.
In isomerization, the alkanes become branched-chain isomers.
In other words, it does not lose any carbons or hydrogens, keeping the same molecular weight.
In reformation, the alkanes become cycloalkanes or aromatic hydrocarbons, giving off hydrogen as a by-product.
Both of these processes raise the octane number of the substance.
Butane is the most common alkane that is put under the process of isomerization, as it makes many branched alkanes with high octane numbers.
Alkanes will react with steam in the presence of a nickel catalyst to give hydrogen.
Alkanes can be chlorosulfonated and nitrated, although both reactions require special conditions.
The fermentation of alkanes to carboxylic acids is of some technical importance.
In the Reed reaction, sulfur dioxide, chlorine and light convert hydrocarbons to sulfonyl chlorides.
Nucleophilic Abstraction can be used to separate an alkane from a metal.
Alkyl groups can be transferred from one compound to another by transmetalation reactions.
Alkanes form a small portion of the atmospheres of the outer gas planets such as Jupiter (0.1% methane, 2 ppm ethane), Saturn (0.2% methane, 5 ppm ethane), Uranus (1.99% methane, 2.5 ppm ethane) and Neptune (1.5% methane, 1.5 ppm ethane).
Titan (1.6% methane), a satellite of Saturn, was examined by the "Huygens" probe, which indicated that Titan's atmosphere periodically rains liquid methane onto the moon's surface.
Also on Titan the Cassini mission has imaged seasonal methane/ethane lakes near the polar regions of Titan.
Methane and ethane have also been detected in the tail of the comet Hyakutake.
Chemical analysis showed that the abundances of ethane and methane were roughly equal, which is thought to imply that its ices formed in interstellar space, away from the Sun, which would have evaporated these volatile molecules.
Alkanes have also been detected in meteorites such as carbonaceous chondrites.
Traces of methane gas (about 0.0002% or 1745 ppb) occur in the Earth's atmosphere, produced primarily by methanogenic microorganisms, such as Archaea in the gut of ruminants.
The most important commercial sources for alkanes are natural gas and oil.
Natural gas contains primarily methane and ethane, with some propane and butane: oil is a mixture of liquid alkanes and other hydrocarbons.
These hydrocarbons were formed when marine animals and plants (zooplankton and phytoplankton) died and sank to the bottom of ancient seas and were covered with sediments in an anoxic environment and converted over many millions of years at high temperatures and high pressure to their current form.
Natural gas resulted thereby for example from the following reaction:

These hydrocarbon deposits, collected in porous rocks trapped beneath impermeable cap rocks, comprise commercial oil fields.
They have formed over millions of years and once exhausted cannot be readily replaced.
The depletion of these hydrocarbons reserves is the basis for what is known as the energy crisis.
Methane is also present in what is called biogas, produced by animals and decaying matter, which is a possible renewable energy source.
Alkanes have a low solubility in water, so the content in the oceans is negligible; however, at high pressures and low temperatures (such as at the bottom of the oceans), methane can co-crystallize with water to form a solid methane clathrate (methane hydrate).
Although this cannot be commercially exploited at the present time, the amount of combustible energy of the known methane clathrate fields exceeds the energy content of all the natural gas and oil deposits put together.
Methane extracted from methane clathrate is, therefore, a candidate for future fuels.
Acyclic alkanes occur in nature in various ways.
Certain types of bacteria can metabolize alkanes: they prefer even-numbered carbon chains as they are easier to degrade than odd-numbered chains.
On the other hand, certain archaea, the methanogens, produce large quantities of methane by the metabolism of carbon dioxide or other oxidized organic compounds.
The energy is released by the oxidation of hydrogen:

Methanogens are also the producers of marsh gas in wetlands, and release about two billion tonnes of methane per year—the atmospheric content of this gas is produced nearly exclusively by them.
The methane output of cattle and other herbivores, which can release 30 to 50 gallons per day, and of termites, is also due to methanogens.
They also produce this simplest of all alkanes in the intestines of humans.
Methanogenic archaea are, hence, at the end of the carbon cycle, with carbon being released back into the atmosphere after having been fixed by photosynthesis.
It is probable that our current deposits of natural gas were formed in a similar way.
Alkanes also play a role, if a minor role, in the biology of the three eukaryotic groups of organisms: fungi, plants and animals.
Some specialized yeasts, e.g., "Candida tropicale", "Pichia" sp., "Rhodotorula" sp., can use alkanes as a source of carbon or energy.
The fungus "Amorphotheca resinae" prefers the longer-chain alkanes in aviation fuel, and can cause serious problems for aircraft in tropical regions.
In plants, the solid long-chain alkanes are found in the plant cuticle and epicuticular wax of many species, but are only rarely major constituents.
They protect the plant against water loss, prevent the leaching of important minerals by the rain, and protect against bacteria, fungi, and harmful insects.
The carbon chains in plant alkanes are usually odd-numbered, between 27 and 33 carbon atoms in length and are made by the plants by decarboxylation of even-numbered fatty acids.
The exact composition of the layer of wax is not only species-dependent but changes also with the season and such environmental factors as lighting conditions, temperature or humidity.
More volatile short-chain alkanes are also produced by and found in plant tissues.
The Jeffrey pine is noted for producing exceptionally high levels of "n"-heptane in its resin, for which reason its distillate was designated as the zero point for one octane rating.
Floral scents have also long been known to contain volatile alkane components, and "n"-nonane is a significant component in the scent of some roses.
Emission of gaseous and volatile alkanes such as ethane, pentane, and hexane by plants has also been documented at low levels, though they are not generally considered to be a major component of biogenic air pollution.
Edible vegetable oils also typically contain small fractions of biogenic alkanes with a wide spectrum of carbon numbers, mainly 8 to 35, usually peaking in the low to upper 20s, with concentrations up to dozens of milligrams per kilogram (parts per million by weight) and sometimes over a hundred for the total alkane fraction.
Alkanes are found in animal products, although they are less important than unsaturated hydrocarbons.
One example is the shark liver oil, which is approximately 14% pristane (2,6,10,14-tetramethylpentadecane, CH).
They are important as pheromones, chemical messenger materials, on which insects depend for communication.
In some species, e.g.
the support beetle "Xylotrechus colonus", pentacosane (CH), 3-methylpentaicosane (CH) and 9-methylpentaicosane (CH) are transferred by body contact.
With others like the tsetse fly "Glossina morsitans morsitans", the pheromone contains the four alkanes 2-methylheptadecane (CH), 17,21-dimethylheptatriacontane (CH), 15,19-dimethylheptatriacontane (CH) and 15,19,23-trimethylheptatriacontane (CH), and acts by smell over longer distances.
Waggle-dancing honey bees produce and release two alkanes, tricosane and pentacosane.
One example, in which both plant and animal alkanes play a role, is the ecological relationship between the sand bee ("Andrena nigroaenea") and the early spider orchid ("Ophrys sphegodes"); the latter is dependent for pollination on the former.
Sand bees use pheromones in order to identify a mate; in the case of "A. nigroaenea", the females emit a mixture of tricosane (CH), pentacosane (CH) and heptacosane (CH) in the ratio 3:3:1, and males are attracted by specifically this odor.
The orchid takes advantage of this mating arrangement to get the male bee to collect and disseminate its pollen; parts of its flower not only resemble the appearance of sand bees but also produce large quantities of the three alkanes in the same ratio as female sand bees.
As a result, numerous males are lured to the blooms and attempt to copulate with their imaginary partner: although this endeavor is not crowned with success for the bee, it allows the orchid to transfer its pollen,
which will be dispersed after the departure of the frustrated male to different blooms.
As stated earlier, the most important source of alkanes is natural gas and crude oil.
Alkanes are separated in an oil refinery by fractional distillation and processed into many different products.
The Fischer–Tropsch process is a method to synthesize liquid hydrocarbons, including alkanes, from carbon monoxide and hydrogen.
This method is used to produce substitutes for petroleum distillates.
There is usually little need for alkanes to be synthesized in the laboratory, since they are usually commercially available.
Also, alkanes are generally unreactive chemically or biologically, and do not undergo functional group interconversions cleanly.
When alkanes are produced in the laboratory, it is often a side-product of a reaction.
For example, the use of "n"-butyllithium as a strong base gives the conjugate acid, "n"-butane as a side-product:

However, at times it may be desirable to make a section of a molecule into an alkane-like functionality (alkyl group) using the above or similar methods.
For example, an ethyl group is an alkyl group; when this is attached to a hydroxy group, it gives ethanol, which is not an alkane.
To do so, the best-known methods are hydrogenation of alkenes:

Alkanes or alkyl groups can also be prepared directly from alkyl halides in the Corey–House–Posner–Whitesides reaction.
The Barton–McCombie deoxygenation removes hydroxyl groups from alcohols e.g.
and the Clemmensen reduction removes carbonyl groups from aldehydes and ketones to form alkanes or alkyl-substituted compounds e.g.
:

The applications of alkanes depend on the number of carbon atoms.
The first four alkanes are used mainly for heating and cooking purposes, and in some countries for electricity generation.
Methane and ethane are the main components of natural gas; they are normally stored as gases under pressure.
It is, however, easier to transport them as liquids: This requires both compression and cooling of the gas.
Propane and butane are gases at atmospheric pressure that can be liquefied at fairly low pressures and are commonly known as liquified petroleum gas (LPG).
Propane is used in propane gas burners and as a fuel for road vehicles, butane in space heaters and disposable cigarette lighters.
Both are used as propellants in aerosol sprays.
From pentane to octane the alkanes are highly volatile liquids.
They are used as fuels in internal combustion engines, as they vaporize easily on entry into the combustion chamber without forming droplets, which would impair the uniformity of the combustion.
Branched-chain alkanes are preferred as they are much less prone to premature ignition, which causes knocking, than their straight-chain homologues.
This propensity to premature ignition is measured by the octane rating of the fuel, where 2,2,4-trimethylpentane ("isooctane") has an arbitrary value of 100, and heptane has a value of zero.
Apart from their use as fuels, the middle alkanes are also good solvents for nonpolar substances.
Alkanes from nonane to, for instance, hexadecane (an alkane with sixteen carbon atoms) are liquids of higher viscosity, less and less suitable for use in gasoline.
They form instead the major part of diesel and aviation fuel.
Diesel fuels are characterized by their cetane number, cetane being an old name for hexadecane.
However, the higher melting points of these alkanes can cause problems at low temperatures and in polar regions, where the fuel becomes too thick to flow correctly.
Alkanes from hexadecane upwards form the most important components of fuel oil and lubricating oil.
In the latter function, they work at the same time as anti-corrosive agents, as their hydrophobic nature means that water cannot reach the metal surface.
Many solid alkanes find use as paraffin wax, for example, in candles.
This should not be confused however with true wax, which consists primarily of esters.
Alkanes with a chain length of approximately 35 or more carbon atoms are found in bitumen, used, for example, in road surfacing.
However, the higher alkanes have little value and are usually split into lower alkanes by cracking.
Some synthetic polymers such as polyethylene and polypropylene are alkanes with chains containing hundreds of thousands of carbon atoms.
These materials are used in innumerable applications, and billions of kilograms of these materials are made and used each year.
Alkanes are chemically very inert apolar molecules which are not very reactive as organic compounds.
This inertness yields serious ecological issues if they are released into the environment.
Due to their lack of functional groups and low water solubility, alkanes show poor bioavailability for microorganisms.
There are, however, some microorganisms possessing the metabolic capacity to utilize n-alkanes as both carbon and energy sources.
Some bacterial species are highly specialised in degrading alkanes; these are referred to as hydrocarbonoclastic bacteria.
Methane is flammable, explosive and dangerous to inhale, because it is a colorless, odorless gas, special caution must be taken around methane.
Ethane is also extremely flammable, dangerous to inhale and explosive.
Both of these may cause suffocation.
Similarly, propane is flammable and explosive.
It may cause drowsiness or unconsciousness if inhaled.
Butane has the same hazards to consider as propane.
Alkanes also pose a threat to the environment.
Branched alkanes have a lower biodegradability than unbranched alkanes.
However, methane is ranked as the most dangerous greenhouse gas.
Although the amount of methane in the atmosphere is low, it does pose a threat to the environment.
</doc>
<doc id="640" url="https://en.wikipedia.org/wiki?curid=640" title="Appellate procedure in the United States">
Appellate procedure in the United States

United States appellate procedure involves the rules and regulations for filing appeals in state courts and federal courts.
The nature of an appeal can vary greatly depending on the type of case and the rules of the court in the jurisdiction where the case was prosecuted.
There are many types of standard of review for appeals, such as "de novo" and abuse of discretion.
However, most appeals begin when a party files a petition for review to a higher court for the purpose of overturning the lower court's decision.
An appellate court is a court that hears cases on appeal from another court.
Depending on the particular legal rules that apply to each circumstance, a party to a court case who is unhappy with the result might be able to challenge that result in an appellate court on specific grounds.
These grounds typically could include errors of law, fact, procedure or due process.
In different jurisdictions, appellate courts are also called appeals courts, courts of appeals, superior courts, or supreme courts.
The specific procedures for appealing, including even whether there is a right of appeal from a particular type of decision, can vary greatly from state to state.
The right to file an appeal can also vary from state to state; for example, the New Jersey Constitution vests judicial power in a Supreme Court, a Superior Court, and other courts of limited jurisdiction, with an appellate court being part of the Superior Court.
A party who files an appeal is called an "appellant", "plaintiff in error", "petitioner" or "pursuer", and a party on the other side is called a "appellee".
A "cross-appeal" is an appeal brought by the respondent.
For example, suppose at trial the judge found for the plaintiff and ordered the defendant to pay $50,000.
If the defendant files an appeal arguing that he should not have to pay any money, then the plaintiff might file a cross-appeal arguing that the defendant should have to pay $200,000 instead of $50,000.
The appellant is the party who, having lost part or all their claim in a lower court decision, is appealing to a higher court to have their case reconsidered.
This is usually done on the basis that the lower court judge erred in the application of law, but it may also be possible to appeal on the basis of court misconduct, or that a finding of fact was entirely unreasonable to make on the evidence.
The appellant in the new case can be either the plaintiff (or claimant), defendant, third-party intervenor, or respondent (appellee) from the lower case, depending on who was the losing party.
The winning party from the lower court, however, is now the respondent.
In unusual cases the appellant can be the victor in the court below, but still appeal.
An appellee is the party to an appeal in which the lower court judgment was in its favor.
The appellee is required to respond to the petition, oral arguments, and legal briefs of the appellant.
In general, the appellee takes the procedural posture that the lower court's decision should be affirmed.
An appeal "as of right" is one that is guaranteed by statute or some underlying constitutional or legal principle.
The appellate court cannot refuse to listen to the appeal.
An appeal "by leave" or "permission" requires the appellant to obtain leave to appeal; in such a situation either or both of the lower court and the court may have the discretion to grant or refuse the appellant's demand to appeal the lower court's decision.
In the Supreme Court, review in most cases is available only if the Court exercises its discretion and grants a writ of certiorari.
In tort, equity, or other civil matters either party to a previous case may file an appeal.
In criminal matters, however, the state or prosecution generally has no appeal "as of right".
And due to the double jeopardy principle, the state or prosecution may never appeal a jury or bench verdict of acquittal.
But in some jurisdictions, the state or prosecution may appeal "as of right" from a trial court's dismissal of an indictment in whole or in part or from a trial court's granting of a defendant's suppression motion.
Likewise, in some jurisdictions, the state or prosecution may appeal an issue of law "by leave" from the trial court or the appellate court.
The ability of the prosecution to appeal a decision in favor of a defendant varies significantly internationally.
All parties must present grounds to appeal, or it will not be heard.
By convention in some law reports, the appellant is named first.
This can mean that where it is the defendant who appeals, the name of the case in the law reports reverses (in some cases twice) as the appeals work their way up the court hierarchy.
This is not always true, however.
In the federal courts, the parties' names always stay in the same order as the lower court when an appeal is taken to the circuit courts of appeals, and are re-ordered only if the appeal reaches the Supreme Court.
Many jurisdictions recognize two types of appeals, particularly in the criminal context.
The first is the traditional "direct" appeal in which the appellant files an appeal with the next higher court of review.
The second is the collateral appeal or post-conviction petition, in which the petitioner-appellant files the appeal in a court of first instance—usually the court that tried the case.
The key distinguishing factor between direct and collateral appeals is that the former occurs in state courts, and the latter in federal courts.
Relief in post-conviction is rare and is most often found in capital or violent felony cases.
The typical scenario involves an incarcerated defendant locating DNA evidence demonstrating the defendant's actual innocence.
"Appellate review" is the general term for the process by which courts with appellate jurisdiction take jurisdiction of matters decided by lower courts.
It is distinguished from judicial review, which refers to the court's overriding constitutional or statutory right to determine if a legislative act or administrative decision is defective for jurisdictional or other reasons (which may vary by jurisdiction).
In most jurisdictions the normal and preferred way of seeking appellate review is by filing an appeal of the final judgment.
Generally, an appeal of the judgment will also allow appeal of all other orders or rulings made by the trial court in the course of the case.
This is because such orders cannot be appealed "as of right".
However, certain critical interlocutory court orders, such as the denial of a request for an interim injunction, or an order holding a person in contempt of court, can be appealed immediately although the case may otherwise not have been fully disposed of.
There are two distinct forms of appellate review, "direct" and "collateral".
For example, a criminal defendant may be convicted in state court, and lose on "direct appeal" to higher state appellate courts, and if unsuccessful, mount a "collateral" action such as filing for a writ of habeas corpus in the federal courts.
Generally speaking, "[d]irect appeal statutes afford defendants the opportunity to challenge the merits of a judgment and allege errors of law or fact.
... [Collateral review], on the other hand, provide[s] an independent and civil inquiry into the validity of a conviction and sentence, and as such are generally limited to challenges to constitutional, jurisdictional, or other fundamental violations that occurred at trial."
"Graham v. Borgen", 483 F 3d.
475 (7th Cir.
2007) (no.
04-4103) (slip op.
at 7) (citation omitted).
In Anglo-American common law courts, appellate review of lower court decisions may also be obtained by filing a petition for review by prerogative writ in certain cases.
There is no corresponding right to a writ in any pure or continental civil law legal systems, though some mixed systems such as Quebec recognize these prerogative writs.
After exhausting the first appeal as of right, defendants usually petition the highest state court to review the decision.
This appeal is known as a direct appeal.
The highest state court, generally known as the Supreme Court, exercises discretion over whether it will review the case.
On direct appeal, a prisoner challenges the grounds of the conviction based on an error that occurred at trial or some other stage in the adjudicative process.
An appellant's claim(s) must usually be preserved at trial.
This means that the defendant had to object to the error when it occurred in the trial.
Because constitutional claims are of great magnitude, appellate courts might be more lenient to review the claim even if it was not preserved.
For example, Connecticut applies the following standard to review unpreserved claims: 1.the record is adequate to review the alleged claim of error; 2. the claim is of constitutional magnitude alleging the violation of a fundamental right; 3. the alleged constitutional violation clearly exists and clearly deprived the defendant of a fair trial; 4. if subject to harmless error analysis, the state has failed to demonstrate harmlessness of the alleged constitutional violation beyond a reasonable doubt.
All States have a post-conviction relief process.
Similar to federal post-conviction relief, an appellant can petition the court to correct alleged fundamental errors that were not corrected on direct review.
Typical claims might include ineffective assistance of counsel and actual innocence based on new evidence.
These proceedings are normally separate from the direct appeal, however some states allow for collateral relief to be sought on direct appeal.
After direct appeal, the conviction is considered final.
An appeal from the post conviction court proceeds just as a direct appeal.
That is, it goes to the intermediate appellate court, followed by the highest court.
If the petition is granted the appellant could be released from incarceration, the sentence could be modified, or a new trial could be ordered.
A "notice of appeal" is a form or document that in many cases is required to begin an appeal.
The form is completed by the appellant or by the appellant's legal representative.
The nature of this form can vary greatly from country to country and from court to court within a country.
The specific rules of the legal system will dictate exactly how the appeal is officially begun.
For example, the appellant might have to file the notice of appeal with the appellate court, or with the court from which the appeal is taken, or both.
Some courts have samples of a notice of appeal on the court's own web site.
In New Jersey, for example, the Administrative Office of the Court has promulgated a form of notice of appeal for use by appellants, though using this exact form is not mandatory and the failure to use it is not a jurisdictional defect provided that all pertinent information is set forth in whatever form of notice of appeal is used.
The deadline for beginning an appeal can often be very short: traditionally, it is measured in days, not months.
This can vary from country to country, as well as within a country, depending on the specific rules in force.
In the U.S.
federal court system, criminal defendants must file a notice of appeal within 10 days of the entry of either the judgment or the order being appealed, or the right to appeal is forfeited.
Generally speaking the appellate court examines the record of evidence presented in the trial court and the law that the lower court applied and decides whether that decision was legally sound or not.
The appellate court will typically be deferential to the lower court's findings of fact (such as whether a defendant committed a particular act), unless clearly erroneous, and so will focus on the court's application of the law to those facts (such as whether the act found by the court to have occurred fits a legal definition at issue).
If the appellate court finds no defect, it "affirms" the judgment.
If the appellate court does find a legal defect in the decision "below" (i.e., in the lower court), it may "modify" the ruling to correct the defect, or it may nullify ("reverse" or "vacate") the whole decision or any part of it.
It may, in addition, send the case back ("remand" or "remit") to the lower court for further proceedings to remedy the defect.
In some cases, an appellate court may review a lower court decision "de novo" (or completely), challenging even the lower court's findings of fact.
This might be the proper standard of review, for example, if the lower court resolved the case by granting a pre-trial motion to dismiss or motion for summary judgment which is usually based only upon written submissions to the trial court and not on any trial testimony.
Another situation is where appeal is by way of "re-hearing".
Certain jurisdictions permit certain appeals to cause the trial to be heard afresh in the appellate court.
Sometimes, the appellate court finds a defect in the procedure the parties used in filing the appeal and dismisses the appeal without considering its merits, which has the same effect as affirming the judgment below.
(This would happen, for example, if the appellant waited too long, under the appellate court's rules, to file the appeal.)
Generally, there is no trial in an appellate court, only consideration of the record of the evidence presented to the trial court and all the pre-trial and trial court proceedings are reviewed—unless the appeal is by way of re-hearing, new evidence will usually only be considered on appeal in "very" rare instances, for example if that material evidence was unavailable to a party for some very significant reason such as prosecutorial misconduct.
In some systems, an appellate court will only consider the written decision of the lower court, together with any written evidence that was before that court and is relevant to the appeal.
In other systems, the appellate court will normally consider the record of the lower court.
In those cases the record will first be certified by the lower court.
The appellant has the opportunity to present arguments for the granting of the appeal and the appellee (or respondent) can present arguments against it.
Arguments of the parties to the appeal are presented through their appellate lawyers, if represented, or "pro se" if the party has not engaged legal representation.
Those arguments are presented in written briefs and sometimes in oral argument to the court at a hearing.
At such hearings each party is allowed a brief presentation at which the appellate judges ask questions based on their review of the record below and the submitted briefs.
In an adversarial system, appellate courts do not have the power to review lower court decisions unless a party appeals it.
Therefore, if a lower court has ruled in an improper manner, or against legal precedent, that judgment will stand if not appealed – even if it might have been overturned on appeal.
The United States legal system generally recognizes two types of appeals: a trial "de novo" or an appeal on the record.
A trial de novo is usually available for review of informal proceedings conducted by some minor judicial tribunals in proceedings that do not provide all the procedural attributes of a formal judicial trial.
If unchallenged, these decisions have the power to settle more minor legal disputes once and for all.
If a party is dissatisfied with the finding of such a tribunal, one generally has the power to request a trial "de novo" by a court of record.
In such a proceeding, all issues and evidence may be developed newly, as though never heard before, and one is not restricted to the evidence heard in the lower proceeding.
Sometimes, however, the decision of the lower proceeding is itself admissible as evidence, thus helping to curb frivolous appeals.
In some cases, an application for "trial de novo" effectively erases the prior trial as if it had never taken place.
The Supreme Court of Virginia has stated that '"This Court has repeatedly held that the effect of an appeal to circuit court is to "annul the judgment of the inferior tribunal as completely as if there had been no previous trial."'
The only exception to this is that if a defendant appeals a conviction for a crime having multiple levels of offenses, where they are convicted on a lesser offense, the appeal is of the lesser offense; the conviction represents an acquittal of the more serious offenses.
"[A] trial on the same charges in the circuit court does not violate double jeopardy principles, .
.
.
subject only to the limitation that conviction in [the] district court for an offense lesser included in the one charged constitutes an acquittal of the greater offense,
permitting trial de novo in the circuit court only for the lesser-included offense."
In an appeal on the record from a decision in a judicial proceeding, both appellant and respondent are bound to base their arguments wholly on the proceedings and body of evidence as they were presented in the lower tribunal.
Each seeks to prove to the higher court that the result they desired was the just result.
Precedent and case law figure prominently in the arguments.
In order for the appeal to succeed, the appellant must prove that the lower court committed reversible error, that is, an impermissible action by the court acted to cause a result that was unjust, and which would not have resulted had the court acted properly.
Some examples of reversible error would be erroneously instructing the jury on the law applicable to the case, permitting seriously improper argument by an attorney, admitting or excluding evidence improperly, acting outside the court's jurisdiction, injecting bias into the proceeding or appearing to do so, juror misconduct, etc.
The failure to formally object at the time, to what one views as improper action in the lower court, may result in the affirmance of the lower court's judgment on the grounds that one did not "preserve the issue for appeal" by objecting.
In cases where a judge rather than a jury decided issues of fact, an appellate court will apply an "abuse of discretion" standard of review.
Under this standard, the appellate court gives deference to the lower court's view of the evidence, and reverses its decision only if it were a clear abuse of discretion.
This is usually defined as a decision outside the bounds of reasonableness.
On the other hand, the appellate court normally gives less deference to a lower court's decision on issues of law, and may reverse if it finds that the lower court applied the wrong legal standard.
In some cases, an appellant may successfully argue that the law under which the lower decision was rendered was unconstitutional or otherwise invalid, or may convince the higher court to order a new trial on the basis that evidence earlier sought was concealed or only recently discovered.
In the case of new evidence, there must be a high probability that its presence or absence would have made a material difference in the trial.
Another issue suitable for appeal in criminal cases is effective assistance of counsel.
If a defendant has been convicted and can prove that his lawyer did not adequately handle his case and that there is a reasonable probability that the result of the trial would have been different had the lawyer given competent representation, he is entitled to a new trial.
A lawyer traditionally starts an oral argument to any appellate court with the words "May it please the court."
After an appeal is heard, the "mandate" is a formal notice of a decision by a court of appeal; this notice is transmitted to the trial court and, when filed by the clerk of the trial court, constitutes the final judgment on the case, unless the appeal court has directed further proceedings in the trial court.
The mandate is distinguished from the appeal court's opinion, which sets out the legal reasoning for its decision.
In some jurisdictions the mandate is known as the "remittitur".
The result of an appeal can be:

There can be multiple outcomes, so that the reviewing court can affirm some rulings, reverse others and remand the case all at the same time.
Remand is not required where there is nothing left to do in the case.
"Generally speaking, an appellate court's judgment provides 'the final directive of the appeals courts as to the matter appealed, setting out with specificity the court's determination that the action appealed from should be affirmed, reversed, remanded or modified'".
Some reviewing courts who have discretionary review may send a case back without comment other than "review improvidently granted".
In other words, after looking at the case, they chose not to say anything.
The result for the case of "review improvidently granted" is effectively the same as affirmed, but without that extra higher court stamp of approval.
</doc>
<doc id="642" url="https://en.wikipedia.org/wiki?curid=642" title="Answer (law)">
Answer (law)

In law, an Answer was originally a solemn assertion in opposition to someone or something, and thus generally any counter-statement or defense, a reply to a question or response, or objection, or a correct solution of a problem.
In the common law, an Answer is the first pleading by a defendant, usually filed and served upon the plaintiff within a certain strict time limit after a civil complaint or criminal information or indictment has been served upon the defendant.
It may have been preceded by an "optional" "pre-answer" motion to dismiss or demurrer; if such a motion is unsuccessful, the defendant "must" file an answer to the complaint or risk an adverse default judgment.
In a criminal case, there is usually an arraignment or some other kind of appearance before the defendant comes to court.
The pleading in the criminal case, which is entered on the record in open court, is usually either guilty or not guilty.
Generally speaking in private, civil cases there is no plea entered of guilt or innocence.
There is only a judgment that grants money damages or some other kind of equitable remedy such as restitution or a permanent injunction.
Criminal cases may lead to fines or other punishment, such as imprisonment.
The famous Latin "Responsa Prudentium" ("answers of the learned ones") were the accumulated views of many successive generations of Roman lawyers, a body of legal opinion which gradually became authoritative.
During debates of a contentious nature, deflection, colloquially known as 'changing the topic', has been widely observed, and is often seen as a failure to answer a question.
</doc>
<doc id="643" url="https://en.wikipedia.org/wiki?curid=643" title="Appellate court">
Appellate court

An appellate court, commonly called an appeals court, court of appeals (American English), appeal court (British English), court of second instance or second instance court, is any court of law that is empowered to hear an appeal of a trial court or other lower tribunal.
In most jurisdictions, the court system is divided into at least three levels: the trial court, which initially hears cases and reviews evidence and testimony to determine the facts of the case; at least one intermediate appellate court; and a supreme court (or court of last resort) which primarily reviews the decisions of the intermediate courts.
A jurisdiction's supreme court is that jurisdiction's highest appellate court.
Appellate courts nationwide can operate under varying rules.
The authority of appellate courts to review the decisions of lower courts varies widely from one jurisdiction to another.
In some areas, the appellate court has limited powers of review.
Generally, an appellate court's judgment provides the final directive of the appeals courts as to the matter appealed, setting out with specificity the court's determination that the action appealed from should be affirmed, reversed, remanded or modified.
The Court of Appeal of New Zealand, located in Wellington, is New Zealand's principal intermediate appellate court.
In practice, most appeals are resolved at this intermediate appellate level, rather than in the Supreme Court.
The Court of Appeal of Sri Lanka, located in Colombo, is the second senior court in the Sri Lankan legal system.
In the United States, both state and federal appellate courts are usually restricted to examining whether the lower court made the correct legal determinations, rather than hearing direct evidence and determining what the facts of the case were.
Furthermore, U.S.
appellate courts are usually restricted to hearing appeals based on matters that were originally brought up before the trial court.
Hence, such an appellate court will not consider an appellant's argument if it is based on a theory that is raised for the first time in the appeal.
In most U.S.
states, and in U.S.
federal courts, parties before the court are allowed one appeal as of right.
This means that a party who is unsatisfied with the outcome of a trial may bring an appeal to contest that outcome.
However, appeals may be costly, and the appellate court must find an error on the part of the court below that justifies upsetting the verdict.
Therefore, only a small proportion of trial court decisions result in appeals.
Some appellate courts, particularly supreme courts, have the power of discretionary review, meaning that they can decide whether they will hear an appeal brought in a particular case.
Many U.S.
jurisdictions title their appellate court an court of appeal or court of appeals.
Historically, others have titled their appellate court a court of errors (or court of errors and appeals), on the premise that it was intended to correct errors made by lower courts.
Examples of such courts include the New Jersey Court of Errors and Appeals (which existed from 1844 to 1947), the Connecticut Supreme Court of Errors (which has been renamed the Connecticut Supreme Court), the Kentucky Court of Errors (renamed the Kentucky Supreme Court), and the Mississippi High Court of Errors and Appeals (since renamed the Supreme Court of Mississippi).
In some jurisdictions, a court able to hear appeals is known as an appellate division.
The phrase "court of appeals" most often refers to intermediate appellate courts.
However, the Maryland and New York systems are different.
The Maryland Court of Appeals and the New York Court of Appeals are the highest appellate courts in those states.
The New York Supreme Court is a trial court of general jurisdiction.
Depending on the system, certain courts may serve as both trial courts and appellate courts, hearing appeals of decisions made by courts with more limited jurisdiction.
Some jurisdictions have specialized appellate courts, such as the Texas Court of Criminal Appeals, which only hears appeals raised in criminal cases, and the U.S.
Court of Appeals for the Federal Circuit, which has general jurisdiction but derives most of its caseload from patent cases, on one hand, and appeals from the Court of Federal Claims on the other.
</doc>
<doc id="649" url="https://en.wikipedia.org/wiki?curid=649" title="Arraignment">
Arraignment

Arraignment is a formal reading of a criminal charging document in the presence of the defendant.
In response to arraignment, the defendant is expected to enter a plea.
Acceptable pleas vary among jurisdiction but generally include peremptory (setting out reasons why a trial cannot proceed), not guilty, guilty, the Alford (a type of guilty plea where the defendant asserts innocence but concedes that they will be found guilty, only used in the United States) and "nolo contendere" (or no contest).
In Australia, arraignment is the first of eleven stages in a criminal trial, and involves the clerk of the court reading out the indictment.
The judge will testify during the indictment process.
In every province in Canada except British Columbia, defendants are arraigned on the day of their trial (?).
In British Columbia, arraignment takes places in one of the first few court appearances by the defendant or their lawyer.
The defendant is asked whether he or she pleads guilty or not guilty to each charge.
In France, the general rule is that one cannot remain in police custody for more than 24 hours from the time of the arrest.
However, police custody can last another 24 hours in specific circumstances, especially if the offence is punishable by at least one year's imprisonment, or if the investigation is deemed to require the extra time, and can last up to 96 hours in certain cases involving terrorism, drug trafficking or organised crime.
The police needs to have the consent of the prosecutor (in the vast majority of cases, the prosecutor will consent).
In Germany, if one has been arrested and taken into custody by the police one must be brought before a judge as soon as possible and at the latest on the day after the arrest.
At the first appearance, the accused is read the charges and asked for a plea.
The available pleas are, guilty, not guilty, and no plea.
No plea allows the defendant to get legal advice on the plea, which must be made on the second appearance.
In South Africa, arraignment is defined as the calling upon the accused to appear, the informing of the accused of the crime charged against him, the demanding of the accused whether he be guilty or not guilty, and the entering of his plea.
His plea having been entered he is said to stand arraigned.
In England, Wales, and Northern Ireland, arraignment is the first of eleven stages in a criminal trial, and involves the clerk of the court reading out the indictment.
In England and Wales, the police cannot legally detain anyone for more than 24 hours without charging them unless an officer with the rank of superintendent (or above) authorises detention for a further 12 hours (36 hours total), or a judge (who will be a magistrate) authorises detention by the police before charge for up to a maximum of 96 hours, but for terrorism-related offences people can be held by the police for up to 28 days before charge.
If they are not released after being charged, they should be brought before a court as soon as practicable.
The United States has 57 court systems – one each for federal (1), state (50), commonwealth (2), territorial (3), and federal district (1) jurisdictions.
Each jurisdiction establishes its own rules of criminal procedure.
All 57 sets of rules must comply with certain provisions of the U.S.
Constitution (e.g.
the privilege against self-incrimination, prohibition of excessive bail, etc.)
but otherwise are bound only by their own local constitutions and statutes.
At federal level, the Federal Rules of Criminal Procedure provide that "arraignment shall [consist of an] open...reading [of] the indictment...to the defendant...and call[] on him to plead thereto.
He/she shall be given a copy of the indictment...before he/she is called upon to plead."
Federal arraignment takes place in two stages.
The first is called the initial arraignment and must take place within 48 hours of an individual's arrest, 72 hours if the individual was arrested on the weekend and not able to go before a judge until Monday.
During this arraignment the defendant is informed of the pending legal charges and is informed of his or her right to retain counsel.
The presiding judge also decides at what amount, if any, to set bail.
During the second arraignment, a post-indictment arraignment or PIA, the defendant is allowed to enter a plea.
In New York, most people arrested must be released if they are not arraigned within 24 hours.
In California, arraignments must be conducted without unnecessary delay and, in any event, within 48 hours of arrest, excluding weekends and holidays.
Thus, an individual arrested without a warrant, in some cases, may be held for as long as 168 hours (7 days) without arraignment or charge.
For example, both Thanksgiving Day and the day after Thanksgiving are state holidays in California.
So, if someone was arrested at 9:00 am on the Tuesday before Thanksgiving, the 48-hour period would not expire at 9:00 am on Thursday as it normally would, because that is a holiday; and the next day is a holiday; and the two days after that are the weekend; so the arraignment need not take place until 9:00 am on Monday morning following the arrest on the previous Tuesday.
In Michigan, the procedure differs for misdemenours and felonies.
A defendant is allowed to enter a plea when arraigned for a misdemeanour but no plea is entered by a defendant charged with a felony.
In either case, the judge explains the defendant's rights to him/her, advises of his/her right to an attorney, sets pre-release bail (except when inappropriate), and sets a date for a pre-trial conference (misdemeanour only) or pre-exam conference (felonies only, and only in some counties).
Because of the Michigan Constitution's guarantee of a speedy arraignment, a least one judge in every Michigan District Court is on 24-hour call every day, including weekends and holidays, so that the arraignment can be held as soon after the arrest as possible.
The wording of the arraignment varies from jurisdiction to jurisdiction.
However, it generally conforms with the following principles:

Video arraignment is the act of conducting the arraignment process using some form of videoconferencing technology.
Use of video arraignment system allows the courts to conduct the requisite arraignment process without the need to transport the defendant to the courtroom by using an audio-visual link between the location where the defendant is being held and the courtroom.
Use of the video arraignment process addresses the problems associated with having to transport defendants.
The transportation of defendants requires time, puts additional demands on the public safety organizations to provide for the safety of the public, court personnel and for the security of the population held in detention.
It also addresses the rising costs of transportation.
If the defendant pleads guilty, an evidentiary hearing usually follows.
The court is not required to accept a guilty plea.
During the hearing, the judge assesses the offense, the mitigating factors, and the defendant's character, and passes sentence.
If the defendant pleads not guilty, a date is set for a preliminary hearing or a trial.
In the past, a defendant who refused to plead (or "stood mute") was subject to peine forte et dure (Law French for "strong and hard punishment").
Today in common-law jurisdictions, the court enters a plea of not guilty for a defendant who refuses to enter a plea.
The rationale for this is the defendant's right to silence.
This is also often the stage at which arguments for or against pre-trial release and bail may be made, depending on the alleged crime and jurisdiction.
</doc>
<doc id="651" url="https://en.wikipedia.org/wiki?curid=651" title="America the Beautiful">
America the Beautiful

"America the Beautiful" is an American patriotic song.
The lyrics were written by Katharine Lee Bates, and the music was composed by church organist and choirmaster Samuel A. Ward at Grace Episcopal Church in Newark, New Jersey.
The two never met.
Bates originally wrote the words as a poem, "Pikes Peak", first published in the Fourth of July edition of the church periodical "The Congregationalist" in 1895.
At that time, the poem was titled "America" for publication.
Ward had originally written the music, "Materna", for the hymn "O Mother dear, Jerusalem" in 1882, though it was not first published until 1892.
Ward's music combined with the Bates poem was first published in 1910 and titled "America the Beautiful".
The song is one of the most popular of the many U.S.
patriotic songs.
In 1893, at the age of 33, Bates, an English professor at Wellesley College, had taken a train trip to Colorado Springs, Colorado, to teach a short summer school session at Colorado College.
Several of the sights on her trip inspired her, and they found their way into her poem, including the World's Columbian Exposition in Chicago, the "White City" with its promise of the future contained within its gleaming white buildings; the wheat fields of America's heartland Kansas, through which her train was riding on July 16; and the majestic view of the Great Plains from high atop Pikes Peak.
On the pinnacle of that mountain, the words of the poem started to come to her, and she wrote them down upon returning to her hotel room at the original Antlers Hotel.
The poem was initially published two years later in "The Congregationalist" to commemorate the Fourth of July.
It quickly caught the public's fancy.
Amended versions were published in 1904 and 1911.
The first known melody written for the song was sent in by Silas Pratt when the poem was published in "The Congregationalist."
By 1900, at least 75 different melodies had been written.
A hymn tune composed in 1882 by Samuel A. Ward, the organist and choir director at Grace Church, Newark, was generally considered the best music as early as 1910 and is still the popular tune today.
Just as Bates had been inspired to write her poem, Ward, too, was inspired.
The tune came to him while he was on a ferryboat trip from Coney Island back to his home in New York City after a leisurely summer day and he immediately wrote it down.
Supposedly, he was so anxious to capture the tune in his head, he asked fellow passenger friend Harry Martin for his shirt cuff to write the tune on.
He composed the tune for the old hymn "O Mother Dear, Jerusalem", retitling the work "Materna".
Ward's music combined with Bates's poem was first published together in 1910 and titled "America the Beautiful".
Ward died in 1903, not knowing the national stature his music would attain since the music was only first applied to the song in 1904.
Bates was more fortunate since the song's popularity was well established by the time of her death in 1929.
At various times in the more than 100 years that have elapsed since the song was written, particularly during the John F. Kennedy administration, there have been efforts to give "America the Beautiful" legal status either as a national hymn or as a national anthem equal to, or in place of, "The Star-Spangled Banner", but so far this has not yet succeeded.
Proponents prefer "America the Beautiful" for various reasons, saying it is easier to sing, more melodic, and more adaptable to new orchestrations while still remaining as easily recognizable as "The Star-Spangled Banner".
Some prefer "America the Beautiful" over "The Star-Spangled Banner" due to the latter's war-oriented imagery.
Others prefer "The Star-Spangled Banner" for the same reason.
While that national dichotomy has stymied any effort at changing the tradition of the national anthem, "America the Beautiful" continues to be held in high esteem by a large number of Americans, and was even being considered "before" 1931, as a candidate to become the national anthem of the United States.
This song was used as the background music of the television broadcast of the Tiangong-1 launch.
The song is often included in songbooks in a wide variety of religious congregations in the United States.
Bing Crosby included the song in a medley on his album "101 Gang Songs" (1961).
In 1976, while the United States celebrated its bicentennial, a soulful version popularized by Ray Charles peaked at number 98 on the US R&B Charts.
Ray Charles did this again in 1984 to Re-Elect Ronald Reagan.
Ray Charles did this yet again in Miami, Florida in 1999.
Three different renditions of the song have entered the Hot Country Songs charts.
The first was by Charlie Rich, which went to number 22 in 1976.
A second, by Mickey Newbury, peaked at number 82 in 1980.
An all-star version of "America the Beautiful" performed by country singers Trace Adkins, Sherrié Austin, Billy Dean, Vince Gill, Carolyn Dawn Johnson, Toby Keith, Brenda Lee, Lonestar, Lyle Lovett, Lila McCann, Lorrie Morgan, Jamie O'Neal, The Oak Ridge Boys, Collin Raye, Kenny Rogers, Keith Urban and Phil Vassar reached number 58 in July 2001.
The song re-entered the chart following the September 11 attacks.
A punk rock adaptation of the song was recorded in 1976 by New York band The Dictators, and released on their album "Every Day is Saturday".
Popularity of the song increased greatly following the September 11 attacks; at some sporting events it was sung in addition to the traditional singing of the national anthem.
During the first taping of the "Late Show with David Letterman" following the attacks, CBS newsman Dan Rather cried briefly as he quoted the fourth verse.
For Super Bowl XLVIII, The Coca-Cola Company aired a multilingual version of the song, sung in several different languages.
The commercial received some criticism on social media sites, such as Twitter and Facebook, and from some conservatives, such as Glenn Beck.
Despite the controversies, Coca-Cola later reused the Super Bowl ad during Super Bowl LI, the opening ceremonies of the 2014 Winter Olympics and 2016 Summer Olympics and for patriotic holidays.
"From sea to shining sea", originally used in the charters of some of the English Colonies in North America, is an American idiom meaning "from the Atlantic Ocean to the Pacific Ocean" (or vice versa).
Other songs that have used this phrase include the American patriotic song "God Bless the U.S.A."
and Schoolhouse Rock's "Elbow Room".
The phrase and the song are also the namesake of the Shining Sea Bikeway, a bike path in Bates's hometown of Falmouth, Massachusetts.
The phrase is similar to the Latin phrase """" ("From sea to sea"), which serves as the official motto of Canada.
"Purple mountain majesties" refers to the shade of the Pikes Peak in Colorado Springs, Colorado, which inspired Bates to write the poem.
Lynn Sherr's 2001 book "America the Beautiful" discusses the origins of the song and the backgrounds of its authors in depth.
The book points out that the poem has the same meter as that of "Auld Lang Syne"; the songs can be sung interchangeably.
Additionally, Sherr discusses the evolution of the lyrics, for instance, changes to the original third verse written by Bates.
Melinda M. Ponder, in her 2017 biography "Katharine Lee Bates: From Sea to Shining Sea", draws heavily on Bates's diaries and letters to trace the history of the poem and its place in American culture.
The song appears in Ellen Raskin's "The Westing Game".
</doc>
<doc id="653" url="https://en.wikipedia.org/wiki?curid=653" title="Assistive technology">
Assistive technology

Assistive technology is an umbrella term that includes assistive, adaptive, and rehabilitative devices for people with disabilities or elderly population while also including the process used in selecting, locating, and using them.
People who have disabilities often have difficulty performing activities of daily living (ADLs) independently, or even with assistance.
ADLs are self-care activities that include toileting, mobility (ambulation), eating, bathing, dressing and grooming.
Assistive technology can ameliorate the effects of disabilities that limit the ability to perform ADLs.
Assistive technology promotes greater independence by enabling people to perform tasks they were formerly unable to accomplish, or had great difficulty accomplishing, by providing enhancements to, or changing methods of interacting with, the technology needed to accomplish such tasks.
For example, wheelchairs provide independent mobility for those who cannot walk, while assistive eating devices can enable people who cannot feed themselves to do so.
Due to assistive technology, people with disabilities have an opportunity of a more positive and easygoing lifestyle, with an increase in "social participation," "security and control," and a greater chance to "reduce institutional costs without significantly increasing household expenses."
The term adaptive technology is often used as the synonym for assistive technology; however, they are different terms.
Assistive technology refers to "any item, piece of equipment, or product system, whether acquired commercially, modified, or customized, that is used to increase, maintain, or improve functional capabilities of individuals with disabilities", while adaptive technology covers items that are specifically designed for persons with disabilities and would seldom be used by non-disabled persons.
In other words, "assistive technology is any object or system that increases or maintains the capabilities of people with disabilities," while adaptive technology is "any object or system that is specifically designed for the purpose of increasing or maintaining the capabilities of people with disabilities."
Consequently, adaptive technology is a subset of assistive technology.
Adaptive technology often refers specifically to electronic and information technology access.
Wheelchairs are devices that can be manually propelled or electrically propelled, and that include a seating system and are designed to be a substitute for the normal mobility that most people enjoy.
Wheelchairs and other mobility devices allow people to perform mobility-related activities of daily living which include feeding, toileting, dressing, grooming, and bathing.
The devices come in a number of variations where they can be propelled either by hand or by motors where the occupant uses electrical controls to manage motors and seating control actuators through a joystick, sip-and-puff control, or other input devices.
Often there are handles behind the seat for someone else to do the pushing or input devices for caregivers.
Wheelchairs are used by people for whom walking is difficult or impossible due to illness, injury, or disability.
People with both sitting and walking disability often need to use a wheelchair or walker.
Patient transfer devices generally allow patients with impaired mobility to be moved by caregivers between beds, wheelchairs, commodes, toilets, chairs, stretchers, shower benches, automobiles, swimming pools, and other patient support systems (i.e., radiology, surgical, or examining tables).
The most common devices are Patient lifts (for vertical transfer), Transfer benches, stretcher or convertible chairs (for lateral, supine transfer), sit-to-stand lifts (for moving patients from one seated position to another i.e., from wheelchairs to commodes), air bearing inflatable mattresses (for supine transfer i.e., transfer from a gurney to an operating room table), and sliding boards (usually used for transfer from a bed to a wheelchair).
Highly dependent patients who cannot assist their caregiver in moving them often require a Patient lift (a floor or ceiling-suspended sling lift) which though invented in 1955 and in common use since the early 1960s is still considered the state-of-the-art transfer device by OSHA and the American Nursing Association.
A walker or walking frame or Rollator is a tool for disabled people who need additional support to maintain balance or stability while walking.
It consists of a frame that is about waist high, approximately twelve inches deep and slightly wider than the user.
Walkers are also available in other sizes, such as for children, or for heavy people.
Modern walkers are height-adjustable.
The front two legs of the walker may or may not have wheels attached depending on the strength and abilities of the person using it.
It is also common to see caster wheels or glides on the back legs of a walker with wheels on the front.
A prosthesis, prosthetic, or prosthetic limb is a device that replaces a missing body part.
It is part of the field of biomechatronics, the science of using mechanical devices with human muscle, skeleton, and nervous systems to assist or enhance motor control lost by trauma, disease, or defect.
Prostheses are typically used to replace parts lost by injury (traumatic) or missing from birth (congenital) or to supplement defective body parts.
Inside the body, artificial heart valves are in common use with artificial hearts and lungs seeing less common use but under active technology development.
Other medical devices and aids that can be considered prosthetics include hearing aids, artificial eyes, palatal obturator, gastric bands, and dentures.
Prostheses are specifically "not" orthoses, although given certain circumstances a prosthesis might end up performing some or all of the same functionary benefits as an orthosis.
Prostheses are technically the complete finished item.
For instance, a C-Leg knee alone is "not" a prosthesis, but only a prosthetic "component".
The complete prosthesis would consist of the attachment system  to the residual limb — usually a "socket", and all the attachment hardware components all the way down to and including the terminal device.
Keep this in mind as nomenclature is often interchanged.
The terms "prosthetic" and "orthotic" are adjectives used to describe devices such as a prosthetic knee.
The terms "prosthetics" and "orthotics" are used to describe the respective allied health fields.
Many people with serious visual impairments live independently, using a wide range of tools and techniques.
Examples of assistive technology for visually impairment include screen readers, screen magnifiers, Braille embossers, desktop video magnifiers, and voice recorders.
Screen readers are used to help the visually impaired to easily access electronic information.
These software programs run on a computer in order to convey the displayed information through voice (text-to-speech) or braille (refreshable braille displays) in combination with magnification for low vision users in some cases.
There are a variety of platforms and applications available for a variety of costs with differing feature sets.
One example of screen readers is Apple VoiceOver.
This software is provided free of charge on all Apple devices.
Apple VoiceOver includes the option to magnify the screen, control the keyboard, and provide verbal descriptions to describe what is happening on the screen.
There are thirty languages to select from.
It also has the capacity to read aloud file content, as well as web pages, E-mail messages, and word processing files.
Braille is a system of raised dots formed into units called braille cells.
A full braille cell is made up of six dots, with two parallel rows of three dots, but other combinations and quantities of dots represent other letters, numbers, punctuation marks, or words.
People can then use their fingers to read the code of raised dots.
A braille embosser is, simply put, a printer for braille.
Instead of a standard printer adding ink onto a page, the braille embosser imprints the raised dots of braille onto a page.
Some braille embossers combine both braille and ink so the documents can be read with either sight or touch.
A refreshable braille display or braille terminal is an electro-mechanical device for displaying braille characters, usually by means of round-tipped pins raised through holes in a flat surface.
Computer users who cannot use a computer monitor use it to read a braille output version of the displayed text.
Desktop video magnifiers are electronic devices that use a camera and a display screen to perform digital magnification of printed materials.
They enlarge printed pages for those with low vision.
A camera connects to a monitor that displays real-time images, and the user can control settings such as magnification, focus, contrast, underlining, highlighting, and other screen preferences.
They come in a variety of sizes and styles; some are small and portable with handheld cameras, while others are much larger and mounted on a fixed stand.
A screen magnifier is software that interfaces with a computer's graphical output to present enlarged screen content.
It allows users to enlarge the texts and graphics on their computer screens for easier viewing.
Similar to desktop video magnifiers, this technology assists people with low vision.
After the user loads the software into their computer's memory, it serves as a kind of "computer magnifying glass."
Wherever the computer cursor moves, it enlarges the area around it.
This allows greater computer accessibility for a wide range of visual abilities.
A large-print keyboard has large letters printed on the keys.
On the keyboard shown, the round buttons at the top control software which can magnify the screen (zoom in), change the background color of the screen, or make the mouse cursor on the screen larger.
The "bump dots" on the keys, installed in this case by the organization using the keyboards, help the user find the right keys in a tactile way.
Assistive technology for navigation has exploded on the IEEE Xplore database since 2000, with over 7,500 engineering articles written on assistive technologies and visual impairment in the past 25 years, and over 1,300 articles on solving the problem of navigation for people who are blind or visually impaired.
As well, over 600 articles on augmented reality and visual impairment have appeared in the engineering literature since 2000.
Most of these articles were published within the past 5 years, and the number of articles in this area is increasing every year.
GPS, accelerometers, gyroscopes, and cameras can pinpoint the exact location of the user and provide information on what's in the immediate vicinity, and assistance in getting to a destination.
Wearable technology are smart electronic devices that can be worn on the body as an implant or an accessory.
New technologies are exploring how the visually impaired can receive visual information through wearable devices.
Some wearable devices for visual impairment include:

Personal emergency response systems (PERS), or Telecare (UK term), are a particular sort of assistive technology that use electronic sensors connected to an alarm system to help caregivers manage risk and help vulnerable people stay independent at home longer.
An example would be the systems being put in place for senior people such as fall detectors, thermometers (for hypothermia risk), flooding and unlit gas sensors (for people with mild dementia).
Notably, these alerts can be customized to the particular person's risks.
When the alert is triggered, a message is sent to a caregiver or contact center who can respond appropriately.
In human–computer interaction, computer accessibility (also known as accessible computing) refers to the accessibility of a computer system to all people, regardless of disability or severity of impairment, examples include web accessibility guidelines.
Another approach is for the user to present a token to the computer terminal, such as a smart card, that has configuration information to adjust the computer speed, text size, etc.
to their particular needs.
This is useful where users want to access public computer based terminals in Libraries, ATM, Information kiosks etc.
The concept is encompassed by the CEN EN 1332-4 Identification Card Systems - Man-Machine Interface.
This development of this standard has been supported in Europe by SNAPI and has been successfully incorporated into the Lasseo specifications, but with limited success due to the lack of interest from public computer terminal suppliers.
People in the d/Deaf and hard of hearing community have a more difficult time receiving auditory information as compared to hearing individuals.
These individuals often rely on visual and tactile mediums for receiving and communicating information.
The use of assistive technology and devices provides this community with various solutions to auditory communication needs by providing higher sound (for those who are hard of hearing), tactile feedback, visual cues and improved technology access.
Individuals who are deaf or hard of hearing utilize a variety of assistive technologies that provide them with different access to information in numerous environments.
Most devices either provide amplified sound or alternate ways to access information through vision and/or vibration.
These technologies can be grouped into three general categories: Hearing Technology, alerting devices, and communication support.
A hearing aid or deaf aid is an electroacoustic device which is designed to amplify sound for the wearer, usually with the aim of making speech more intelligible, and to correct impaired hearing as measured by audiometry.
This type of assistive technology helps people with hearing loss participate more fully in their hearing communities by allowing them to hear more clearly.
They amplify any and all sound waves through use of a microphone, amplifier, and speaker.
There is a wide variety of hearing aids available, including digital, in-the-ear, in-the-canal, behind-the-ear, and on-the-body aids.
Assistive listening devices include FM, infrared, and loop assistive listening devices.
This type of technology allows people with hearing difficulties to focus on a speaker or subject by getting rid of extra background noises and distractions, making places like auditoriums, classrooms, and meetings much easier to participate in.
The assistive listening device usually uses a microphone to capture an audio source near to its origin and broadcast it wirelessly over an FM (Frequency Modulation) transmission, IR (Infra Red) transmission, IL (Induction Loop) transmission, or other transmission methods.
The person who is listening may use an FM/IR/IL Receiver to tune into the signal and listen at his/her preferred volume.
This type of assistive technology allows users to amplify the volume and clarity of their phone calls so that they can easily partake in this medium of communication.
There are also options to adjust the frequency and tone of a call to suit their individual hearing needs.
Additionally, there is a wide variety of amplified telephones to choose from, with different degrees of amplification.
For example, a phone with 26 to 40 decibel is generally sufficient for mild hearing loss, while a phone with 71 to 90 decibel is better for more severe hearing loss.
Augmentative and alternative communication (AAC) is an umbrella term that encompasses methods of communication for those with impairments or restrictions on the production or comprehension of spoken or written language.
AAC systems are extremely diverse and depend on the capabilities of the user.
They may be as basic as pictures on a board that are used to request food, drink, or other care; or they can be advanced speech generating devices, based on speech synthesis, that are capable of storing hundreds of phrases and words.
Assistive Technology for Cognition (ATC) is the use of technology (usually high tech) to augment and assist cognitive processes such as attention, memory, self-regulation, navigation, emotion recognition and management, planning, and sequencing activity.
Systematic reviews of the field have found that the number of ATC are growing rapidly, but have focused on memory and planning, that there is emerging evidence for efficacy, that a lot of scope exists to develop new ATC.
Examples of ATC include: NeuroPage which prompts users about meetings, Wakamaru, which provides companionship and reminds users to take medicine and calls for help if something is wrong, and telephone Reassurance systems.
Memory aids are any type of assistive technology that helps a user learn and remember certain information.
Many memory aids are used for cognitive impairments such as reading, writing, or organizational difficulties.
For example, a Smartpen records handwritten notes by creating both a digital copy and an audio recording of the text.
Users simply tap certain parts of their notes, the pen saves it, and reads it back to them.
From there, the user can also download their notes onto a computer for increased accessibility.
Digital voice recorders are also used to record "in the moment" information for fast and easy recall at a later time.
Educational software is software that assists people with reading, learning, comprehension, and organizational difficulties.
Any accommodation software such as text readers, notetakers, text enlargers, organization tools, word predictions, and talking word processors falls under the category of educational software.
Adaptive eating devices include items commonly used by the general population like spoons and forks and plates.
However they become assistive technology when they are modified to accommodate the needs of people who have difficultly using standard cutlery due to a disabling condition.
Common modifications include increasing the size of the utensil handle to make it easier to grasp.
Plates and bowls may have a guard on the edge that stops food being pushed off of the dish when it is being scooped.
More sophisticated equipment for eating includes manual and powered feeding devices.
These devices support those who have little or no hand and arm function and enable them to eat independently.
Assistive technology in sports is an area of technology design that is growing.
Assistive technology is the array of new devices created to enable sports enthusiasts who have disabilities to play.
Assistive technology may be used in adaptive sports, where an existing sport is modified to enable players with a disability to participate; or, assistive technology may be used to invent completely new sports with athletes with disabilities exclusively in mind.
An increasing number of people with disabilities are participating in sports, leading to the development of new assistive technology.
Assistive technology devices can be simple, or "low-tech", or they may use highly advanced technology.
"Low-tech" devices can include velcro gloves and adaptive bands and tubes.
"High-tech" devices can include all-terrain wheelchairs and adaptive bicycles.
Accordingly, assistive technology can be found in sports ranging from local community recreation to the elite Paralympic Games.
More complex assistive technology devices have been developed over time, and as a result, sports for people with disabilities "have changed from being a clinical therapeutic tool to an increasingly competition-oriented activity".
In the United States there are two major pieces of legislation that govern the use of assistive technology within the school system.
The first is Section 504 of the Rehabilitation Act of 1973 and the second being the Individuals with Disabilities Education Act (IDEA) which was first enacted in 1975 under the name The Education for All Handicapped Children Act.
In 2004, during the reauthorization period for IDEA, the National Instructional Material Access Center (NIMAC) was created which provided a repository of accessible text including publisher's textbooks to students with a qualifying disability.
Files provided are in XML format and used as a starting platform for braille readers, screen readers, and other digital text software.
IDEA defines assistive technology as follows: "any item, piece of equipment, or product system, whether acquired commercially off the shelf, modified, or customized, that is used to increase, maintain, or improve functional capabilities of a child with a disability.
(B) Exception.--The term does not include a medical device that is surgically implanted, or the replacement of such device."
Assistive technology in this area is broken down into low, mid, and high tech categories.
Low tech encompasses equipment that is often low cost and does not include batteries or requires charging.
Examples include adapted paper and pencil grips for writing or masks and color overlays for reading.
Mid tech supports used in the school setting include the use of handheld spelling dictionaries and portable word processors used to keyboard writing.
High tech supports involve the use of tablet devices and computers with accompanying software.
Software supports for writing include the use of auditory feedback while keyboarding, word prediction for spelling, and speech to text.
Supports for reading include the use of text to speech (TTS) software and font modification via access to digital text.
Limited supports are available for math instruction and mostly consist of grid based software to allow younger students to keyboard equations and auditory feedback of more complex equations using MathML and Daisy.
One of the largest problems that affect people with disabilities is discomfort with prostheses.
An experiment performed in Massachusetts utilized 20 people with various sensors attached to their arms.
The subjects tried different arm exercises, and the sensors recorded their movements.
All of the data helped engineers develop new engineering concepts for prosthetics.
Assistive technology may attempt to improve the ergonomics of the devices themselves such as Dvorak and other alternative keyboard layouts, which offer more ergonomic layouts of the keys.
Assistive technology devices have been created to enable people with disabilities to use modern touch screen mobile computers such as the iPad, iPhone and iPod touch.
The Pererro is a plug and play adapter for iOS devices which uses the built in Apple VoiceOver feature in combination with a basic switch.
This brings touch screen technology to those who were previously unable to use it.
Apple, with the release of iOS 7 had introduced the ability to navigate apps using switch control.
Switch access could be activated either through an external bluetooth connected switch, single touch of the screen, or use of right and left head turns using the device's camera.
Additional accessibility features include the use of Assistive Touch which allows a user to access multi-touch gestures through pre-programmed onscreen buttons.
For users with physical disabilities a large variety of switches are available and customizable to the user's needs varying in size, shape, or amount of pressure required for activation.
Switch access may be placed near any area of the body which has consistent and reliable mobility and less subject to fatigue.
Common sites include the hands, head, and feet.
Eye gaze and head mouse systems can also be used as an alternative mouse navigation.
A user may utilize single or multiple switch sites and the process often involves a scanning through items on a screen and activating the switch once the desired object is highlighted.
The form of home automation called assistive domotics focuses on making it possible for elderly and disabled people to live independently.
Home automation is becoming a viable option for the elderly and disabled who would prefer to stay in their own homes rather than move to a healthcare facility.
This field uses much of the same technology and equipment as home automation for security, entertainment, and energy conservation but tailors it towards elderly and disabled users.
For example, automated prompts and reminders utilize motion sensors and pre-recorded audio messages; an automated prompt in the kitchen may remind the resident to turn off the oven, and one by the front door may remind the resident to lock the door.
Overall, assistive technology aims to allow people with disabilities to "participate more fully in all aspects of life (home, school, and community)" and increases their opportunities for "education, social interactions, and potential for meaningful employment".
It creates greater independence and control for disabled individuals.
For example, in one study of 1,342 infants, toddlers and preschoolers, all with some kind of developmental, physical, sensory, or cognitive disability, the use of assistive technology created improvements in child development.
These included improvements in "cognitive, social, communication, literacy, motor, adaptive, and increases in engagement in learning activities".
Additionally, it has been found to lighten caregiver load.
Both family and professional caregivers benefit from assistive technology.
Through its use, the time that a family member or friend would need to care for a patient significantly decreases.
However, studies show that care time for a professional caregiver increases when assistive technology is used.
Nonetheless, their work load is significantly easier as the assistive technology frees them of having to perform certain tasks.
There are several platforms that use machine learning to identify the appropriate assistive device to suggest to patients, making assistive devices more accessible.
</doc>
<doc id="655" url="https://en.wikipedia.org/wiki?curid=655" title="Abacus">
Abacus

The abacus ("plural" abaci or abacuses), also called a counting frame, is a calculating tool that was in use in Europe, China and Russia, centuries before the adoption of the written Hindu–Arabic numeral system.
The exact origin of the abacus is still unknown.
Today, abacuses are often constructed as a bamboo frame with beads sliding on wires, but originally they were beans or stones moved in grooves in sand or on tablets of wood, stone, or metal.
Abacuses come in different designs.
Some designs, like the bead frame consisting of beads divided into tens, are used mainly to teach arithmetic, although they remain popular in the post-Soviet states as a tool.
Other designs, such as the Japanese soroban, have been used for practical calculations even involving several digits.
For any particular abacus design, there usually are numerous different methods to perform a certain type of calculation, which may include basic operations like addition and multiplication, or even more complex ones, such as calculating square roots.
Some of these methods may work with non-natural numbers (numbers such as and ).
Although today many use calculators and computers instead of abacuses to calculate, abacuses still remain in common use in some countries.
Merchants, traders and clerks in some parts of Eastern Europe, Russia, China and Africa use abacuses, and they are still used to teach arithmetic to children.
Some people who are unable to use a calculator because of visual impairment may use an abacus.
The use of the word "abacus" dates before 1387 AD, when a Middle English work borrowed the word from Latin to describe a sandboard abacus.
The Latin word came from Greek ἄβαξ "abax" which means something without base, and improperly, any piece of rectangular board or plank.
Alternatively, without reference to ancient texts on etymology, it has been suggested that it means "a square tablet strewn with dust", or "drawing-board covered with dust (for the use of mathematics)" (the exact shape of the Latin perhaps reflects the genitive form of the Greek word, ἄβακoς "abakos").
Whereas the table strewn with dust definition is popular, there are those that do not place credence in this at all and in fact state that it is not proven.
Greek ἄβαξ itself is probably a borrowing of a Northwest Semitic, perhaps Phoenician, word akin to Hebrew "ʾābāq" (אבק), "dust" (or in post-Biblical sense meaning "sand used as a writing surface").
The preferred plural of "abacus" is a subject of disagreement, with both "abacuses" and "abaci" (hard "c") in use.
The user of an abacus is called an "abacist".
The period 2700–2300 BC saw the first appearance of the Sumerian abacus, a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system.
Some scholars point to a character from the Babylonian cuneiform which may have been derived from a representation of the abacus.
It is the belief of Old Babylonian scholars such as Carruccio that Old Babylonians "may have used the abacus for the operations of addition and subtraction; however, this primitive device proved difficult to use for more complex calculations".
The use of the abacus in Ancient Egypt is mentioned by the Greek historian Herodotus, who writes that the Egyptians manipulated the pebbles from right to left, opposite in direction to the Greek left-to-right method.
Archaeologists have found ancient disks of various sizes that are thought to have been used as counters.
However, wall depictions of this instrument have not been discovered.
During the Achaemenid Empire, around 600 BC the Persians first began to use the abacus.
Under the Parthian, Sassanian and Iranian empires, scholars concentrated on exchanging knowledge and inventions with the countries around them – India, China, and the Roman Empire, when it is thought to have been exported to other countries.
The earliest archaeological evidence for the use of the Greek abacus dates to the 5th century BC.
Also Demosthenes (384 BC–322 BC) talked of the need to use pebbles for calculations too difficult for your head.
A play by Alexis from the 4th century BC mentions an abacus and pebbles for accounting, and both Diogenes and Polybius mention men that sometimes stood for more and sometimes for less, like the pebbles on an abacus.
The Greek abacus was a table of wood or marble, pre-set with small counters in wood or metal for mathematical calculations.
This Greek abacus saw use in Achaemenid Persia, the Etruscan civilization, Ancient Rome and, until the French Revolution, the Western Christian world.
A tablet found on the Greek island Salamis in 1846 AD (the Salamis Tablet), dates back to 300 BC, making it the oldest counting board discovered so far.
It is a slab of white marble long, wide, and thick, on which are 5 groups of markings.
In the center of the tablet is a set of 5 parallel lines equally divided by a vertical line, capped with a semicircle at the intersection of the bottom-most horizontal line and the single vertical line.
Below these lines is a wide space with a horizontal crack dividing it.
Below this crack is another group of eleven parallel lines, again divided into two sections by a line perpendicular to them, but with the semicircle at the top of the intersection; the third, sixth and ninth of these lines are marked with a cross where they intersect with the vertical line.
Also from this time frame the "Darius Vase" was unearthed in 1851.
It was covered with pictures including a "treasurer" holding a wax tablet in one hand while manipulating counters on a table with the other.
The earliest known written documentation of the Chinese abacus dates to the 2nd century BC.
The Chinese abacus, known as the "suanpan" (算盤, lit.
"calculating tray"), is typically tall and comes in various widths depending on the operator.
It usually has more than seven rods.
There are two beads on each rod in the upper deck and five beads each in the bottom.
The beads are usually rounded and made of a hardwood.
The beads are counted by moving them up or down towards the beam; beads moved toward the beam are counted, while those moved away from it are not.
The "suanpan" can be reset to the starting position instantly by a quick movement along the horizontal axis to spin all the beads away from the horizontal beam at the center.
"Suanpan" can be used for functions other than counting.
Unlike the simple counting board used in elementary schools, very efficient suanpan techniques have been developed to do multiplication, division, addition, subtraction, square root and cube root operations at high speed.
There are currently schools teaching students how to use it.
In the long scroll "Along the River During the Qingming Festival" painted by Zhang Zeduan during the Song dynasty (960–1297), a "suanpan" is clearly visible beside an account book and doctor's prescriptions on the counter of an apothecary's (Feibao).
The similarity of the Roman abacus to the Chinese one suggests that one could have inspired the other, as there is some evidence of a trade relationship between the Roman Empire and China.
However, no direct connection can be demonstrated, and the similarity of the abacuses may be coincidental, both ultimately arising from counting with five fingers per hand.
Where the Roman model (like most modern Korean and Japanese) has 4 plus 1 bead per decimal place, the standard "suanpan" has 5 plus 2.
(Incidentally, this allows use with a hexadecimal numeral system, which was used for traditional Chinese measures of weight.)
Instead of running on wires as in the Chinese, Korean, and Japanese models, the beads of Roman model run in grooves, presumably making arithmetic calculations much slower.
Another possible source of the "suanpan" is Chinese counting rods, which operated with a decimal system but lacked the concept of zero as a place holder.
The zero was probably introduced to the Chinese in the Tang dynasty (618–907) when travel in the Indian Ocean and the Middle East would have provided direct contact with India, allowing them to acquire the concept of zero and the decimal point from Indian merchants and mathematicians.
The normal method of calculation in ancient Rome, as in Greece, was by moving counters on a smooth table.
Originally pebbles ("calculi") were used.
Later, and in medieval Europe, jetons were manufactured.
Marked lines indicated units, fives, tens etc.
as in the Roman numeral system.
This system of 'counter casting' continued into the late Roman empire and in medieval Europe, and persisted in limited use into the nineteenth century.
Due to Pope Sylvester II's reintroduction of the abacus with modifications, it became widely used in Europe once again during the 11th century This abacus used beads on wires, unlike the traditional Roman counting boards, which meant the abacus could be used much faster.
Writing in the 1st century BC, Horace refers to the wax abacus, a board covered with a thin layer of black wax on which columns and figures were inscribed using a stylus.
One example of archaeological evidence of the Roman abacus, shown here in reconstruction, dates to the 1st century AD.
It has eight long grooves containing up to five beads in each and eight shorter grooves having either one or no beads in each.
The groove marked I indicates units, X tens, and so on up to millions.
The beads in the shorter grooves denote fives –five units, five tens etc., essentially in a bi-quinary coded decimal system, related to the Roman numerals.
The short grooves on the right may have been used for marking Roman "ounces" (i.e. fractions).
The decimal number system invented in India replaced the abacus in Western Europe.
The "Abhidharmakośabhāṣya" of Vasubandhu (316-396), a Sanskrit work on Buddhist philosophy, says that the second-century CE philosopher Vasumitra said that "placing a wick (Sanskrit "vartikā") on the number one ("ekāṅka") means it is a one, while placing the wick on the number hundred means it is called a hundred, and on the number one thousand means it is a thousand".
It is unclear exactly what this arrangement may have been.
Around the 5th century, Indian clerks were already finding new ways of recording the contents of the Abacus.
Hindu texts used the term "śūnya" (zero) to indicate the empty column on the abacus.
In Japanese, the abacus is called "soroban" (, lit.
"Counting tray"), imported from China in the 14th century.
It was probably in use by the working class a century or more before the ruling class started, as the class structure did not allow for devices used by the lower class to be adopted or used by the ruling class.
The 1/4 abacus, which is suited to decimal calculation, appeared circa 1930, and became widespread as the Japanese abandoned hexadecimal weight calculation which was still common in China.
The abacus is still manufactured in Japan today even with the proliferation, practicality, and affordability of pocket electronic calculators.
The use of the soroban is still taught in Japanese primary schools as part of mathematics, primarily as an aid to faster mental calculation.
Using visual imagery of a soroban, one can arrive at the answer in the same time as, or even faster than, is possible with a physical instrument.
The Chinese abacus migrated from China to Korea around 1400 AD.
Koreans call it "jupan" (주판), "supan" (수판) or "jusan" (주산).
Some sources mention the use of an abacus called a "nepohualtzintzin" in ancient Aztec culture.
This Mesoamerican abacus used a 5-digit base-20 system.
The word Nepōhualtzintzin comes from Nahuatl and it is formed by the roots; "Ne" – personal -; "pōhual" or "pōhualli" – the account -; and "tzintzin" – small similar elements.
Its complete meaning was taken as: counting with small similar elements by somebody.
Its use was taught in the Calmecac to the "temalpouhqueh" , who were students dedicated to take the accounts of skies, from childhood.
The Nepōhualtzintzin was divided in two main parts separated by a bar or intermediate cord.
In the left part there were four beads, which in the first row have unitary values (1, 2, 3, and 4), and in the right side there are three beads with values of 5, 10, and 15 respectively.
In order to know the value of the respective beads of the upper rows, it is enough to multiply by 20 (by each row), the value of the corresponding account in the first row.
Altogether, there were 13 rows with 7 beads in each one, which made up 91 beads in each Nepōhualtzintzin.
This was a basic number to understand, 7 times 13, a close relation conceived between natural phenomena, the underworld and the cycles of the heavens.
One Nepōhualtzintzin (91) represented the number of days that a season of the year lasts, two Nepōhualtzitzin (182) is the number of days of the corn's cycle, from its sowing to its harvest, three Nepōhualtzintzin (273) is the number of days of a baby's gestation, and four Nepōhualtzintzin (364) completed a cycle and approximate a year (1 days short).
When translated into modern computer arithmetic, the Nepōhualtzintzin amounted to the rank from 10 to the 18 in floating point, which calculated stellar as well as infinitesimal amounts with absolute precision, meant that no round off was allowed.
The rediscovery of the Nepōhualtzintzin was due to the Mexican engineer David Esparza Hidalgo, who in his wanderings throughout Mexico found diverse engravings and paintings of this instrument and reconstructed several of them made in gold, jade, encrustations of shell, etc.
There have also been found very old Nepōhualtzintzin attributed to the Olmec culture, and even some bracelets of Mayan origin, as well as a diversity of forms and materials in other cultures.
George I. Sanchez, "Arithmetic in Maya", Austin-Texas, 1961 found another base 5, base 4 abacus in the Yucatán Peninsula that also computed calendar data.
This was a finger abacus, on one hand 0, 1, 2, 3, and 4 were used; and on the other hand 0, 1, 2 and 3 were used.
Note the use of zero at the beginning and end of the two cycles.
Sanchez worked with Sylvanus Morley, a noted Mayanist.
The quipu of the Incas was a system of colored knotted cords used to record numerical data, like advanced tally sticks – but not used to perform calculations.
Calculations were carried out using a yupana (Quechua for "counting tool"; see figure) which was still in use after the conquest of Peru.
The working principle of a yupana is unknown, but in 2001 an explanation of the mathematical basis of these instruments was proposed by Italian mathematician Nicolino De Pasquale.
By comparing the form of several yupanas, researchers found that calculations were based using the Fibonacci sequence 1, 1, 2, 3, 5 and powers of 10, 20 and 40 as place values for the different fields in the instrument.
Using the Fibonacci sequence would keep the number of grains within any one field at a minimum.
The Russian abacus, the "schoty" (счёты), usually has a single slanted deck, with ten beads on each wire (except one wire, usually positioned near the user, with four beads for quarter-ruble fractions).
Older models have another 4-bead wire for quarter-kopeks, which were minted until 1916.
The Russian abacus is often used vertically, with wires from left to right in the manner of a book.
The wires are usually bowed to bulge upward in the center, to keep the beads pinned to either of the two sides.
It is cleared when all the beads are moved to the right.
During manipulation, beads are moved to the left.
For easy viewing, the middle 2 beads on each wire (the 5th and 6th bead) usually are of a different color from the other eight beads.
Likewise, the left bead of the thousands wire (and the million wire, if present) may have a different color.
As a simple, cheap and reliable device, the Russian abacus was in use in all shops and markets throughout the former Soviet Union, and the usage of it was taught in most schools until the 1990s.
Even the 1874 invention of mechanical calculator, Odhner arithmometer, had not replaced them in Russia and likewise the mass production of Felix arithmometers since 1924 did not significantly reduce their use in the Soviet Union.
The Russian abacus began to lose popularity only after the mass production of microcalculators had started in the Soviet Union in 1974.
Today it is regarded as an archaism and replaced by the handheld calculator.
The Russian abacus was brought to France around 1820 by the mathematician Jean-Victor Poncelet, who served in Napoleon's army and had been a prisoner of war in Russia.
The abacus had fallen out of use in western Europe in the 16th century with the rise of decimal notation and algorismic methods.
To Poncelet's French contemporaries, it was something new.
Poncelet used it, not for any applied purpose, but as a teaching and demonstration aid.
The Turks and the Armenian people also used abacuses similar to the Russian schoty.
It was named a "coulba" by the Turks and a "choreb" by the Armenians.
Around the world, abacuses have been used in pre-schools and elementary schools as an aid in teaching the numeral system and arithmetic.
In Western countries, a bead frame similar to the Russian abacus but with straight wires and a vertical frame has been common (see image).
It is still often seen as a plastic or wooden toy.
The wire frame may be used either with positional notation like other abacuses (thus the 10-wire version may represent numbers up to 9,999,999,999), or each bead may represent one unit (so that e.g.
74 can be represented by shifting all beads on 7 wires and 4 beads on the 8th wire, so numbers up to 100 may be represented).
In the bead frame shown, the gap between the 5th and 6th wire, corresponding to the color change between the 5th and the 6th bead on each wire, suggests the latter use.
The red-and-white abacus is used in contemporary primary schools for a wide range of number-related lessons.
The twenty bead version, referred to by its Dutch name "rekenrek" ("calculating frame"), is often used, sometimes on a string of beads, sometimes on a rigid framework.
An adapted abacus, invented by Tim Cranmer, called a Cranmer abacus is still commonly used by individuals who are blind.
A piece of soft fabric or rubber is placed behind the beads so that they do not move inadvertently.
This keeps the beads in place while the users feel or manipulate them.
They use an abacus to perform the mathematical functions multiplication, division, addition, subtraction, square root and cube root.
Although blind students have benefited from talking calculators, the abacus is still very often taught to these students in early grades, both in public schools and state schools for the blind.
The abacus teaches mathematical skills that can never be replaced with talking calculators and is an important learning tool for blind students.
Blind students also complete mathematical assignments using a braille-writer and Nemeth code (a type of braille code for mathematics) but large multiplication and long division problems can be long and difficult.
The abacus gives blind and visually impaired students a tool to compute mathematical problems that equals the speed and mathematical knowledge required by their sighted peers using pencil and paper.
Many blind people find this number machine a very useful tool throughout life.
The binary abacus is used to explain how computers manipulate numbers.
The abacus shows how numbers, letters, and signs can be stored in a binary system on a computer, or via ASCII.
The device consists of a series of beads on parallel wires arranged in three separate rows.
The beads represent a switch on the computer in either an "on" or "off" position.
</doc>
<doc id="656" url="https://en.wikipedia.org/wiki?curid=656" title="Acid">
Acid

An acid is a molecule or ion capable of donating a hydron (proton or hydrogen ion H), or, alternatively, capable of forming a covalent bond with an electron pair (a Lewis acid).
The first category of acids is the proton donors or Brønsted acids.
In the special case of aqueous solutions, proton donors form the hydronium ion HO and are known as Arrhenius acids.
Brønsted and Lowry generalized the Arrhenius theory to include non-aqueous solvents.
A Brønsted or Arrhenius acid usually contains a hydrogen atom bonded to a chemical structure that is still energetically favorable after loss of H.

Aqueous Arrhenius acids have characteristic properties which provide a practical description of an acid.
Acids form aqueous solutions with a sour taste, can turn blue litmus red, and react with bases and certain metals (like calcium) to form salts.
The word "acid" is derived from the Latin "acidus/acēre" meaning "sour".
An aqueous solution of an acid has a pH less than 7 and is colloquially also referred to as 'acid' (as in 'dissolved in acid'), while the strict definition refers only to the solute.
A lower pH means a higher acidity, and thus a higher concentration of positive hydrogen ions in the solution.
Chemicals or substances having the property of an acid are said to be acidic.
Common aqueous acids include hydrochloric acid (a solution of hydrogen chloride which is found in gastric acid in the stomach and activates digestive enzymes), acetic acid (vinegar is a dilute aqueous solution of this liquid), sulfuric acid (used in car batteries), and citric acid (found in citrus fruits).
As these examples show, acids (in the colloquial sense) can be solutions or pure substances, and can be derived from acids (in the strict sense) that are solids, liquids, or gases.
Strong acids and some concentrated weak acids are corrosive, but there are exceptions such as carboranes and boric acid.
The second category of acids are Lewis acids, which form a covalent bond with an electron pair.
An example is boron trifluoride (BF), whose boron atom has a vacant orbital which can form a covalent bond by sharing a lone pair of electrons on an atom in a base, for example the nitrogen atom in ammonia (NH).
Lewis considered this as a generalization of the Brønsted definition, so that an acid is a chemical species that accepts electron pairs either directly "or" by releasing protons (H) into the solution, which then accept electron pairs.
However, hydrogen chloride, acetic acid, and most other Brønsted-Lowry acids cannot form a covalent bond with an electron pair and are therefore not Lewis acids.
Conversely, many Lewis acids are not Arrhenius or Brønsted-Lowry acids.
In modern terminology, an "acid" is implicitly a Brønsted acid and not a Lewis acid, since chemists almost always refer to a Lewis acid explicitly as "a Lewis acid".
Modern definitions are concerned with the fundamental chemical reactions common to all acids.
Most acids encountered in everyday life are aqueous solutions, or can be dissolved in water, so the Arrhenius and Brønsted-Lowry definitions are the most relevant.
The Brønsted-Lowry definition is the most widely used definition; unless otherwise specified, acid-base reactions are assumed to involve the transfer of a proton (H) from an acid to a base.
Hydronium ions are acids according to all three definitions.
Although alcohols and amines can be Brønsted-Lowry acids, they can also function as Lewis bases due to the lone pairs of electrons on their oxygen and nitrogen atoms.
The Swedish chemist Svante Arrhenius attributed the properties of acidity to hydrogen ions (H) or protons in 1884.
An Arrhenius acid is a substance that, when added to water, increases the concentration of H ions in the water.
Note that chemists often write H("aq") and refer to the hydrogen ion when describing acid-base reactions but the free hydrogen nucleus, a proton, does not exist alone in water, it exists as the hydronium ion, HO.
Thus, an Arrhenius acid can also be described as a substance that increases the concentration of hydronium ions when added to water.
Examples include molecular substances such as HCl and acetic acid.
An Arrhenius base, on the other hand, is a substance which increases the concentration of hydroxide (OH) ions when dissolved in water.
This decreases the concentration of hydronium because the ions react to form HO molecules:

HO + OH ⇌ HO + HO

Due to this equilibrium, any increase in the concentration of hydronium is accompanied by a decrease in the concentration of hydroxide.
Thus, an Arrhenius acid could also be said to be one that decreases hydroxide concentration, while an Arrhenius base increases it.
In an acidic solution, the concentration of hydronium ions is greater than 10 moles per liter.
Since pH is defined as the negative logarithm of the concentration of hydronium ions, acidic solutions thus have a pH of less than 7.
While the Arrhenius concept is useful for describing many reactions, it is also quite limited in its scope.
In 1923 chemists Johannes Nicolaus Brønsted and Thomas Martin Lowry independently recognized that acid-base reactions involve the transfer of a proton.
A Brønsted-Lowry acid (or simply Brønsted acid) is a species that donates a proton to a Brønsted-Lowry base.
Brønsted-Lowry acid-base theory has several advantages over Arrhenius theory.
Consider the following reactions of acetic acid (CHCOOH), the organic acid that gives vinegar its characteristic taste:

Both theories easily describe the first reaction: CHCOOH acts as an Arrhenius acid because it acts as a source of HO when dissolved in water, and it acts as a Brønsted acid by donating a proton to water.
In the second example CHCOOH undergoes the same transformation, in this case donating a proton to ammonia (NH), but does not relate to the Arrhenius definition of an acid because the reaction does not produce hydronium.
Nevertheless, CHCOOH is both an Arrhenius and a Brønsted-Lowry acid.
Brønsted-Lowry theory can be used to describe reactions of molecular compounds in nonaqueous solution or the gas phase.
Hydrogen chloride (HCl) and ammonia combine under several different conditions to form ammonium chloride, NHCl.
In aqueous solution HCl behaves as hydrochloric acid and exists as hydronium and chloride ions.
The following reactions illustrate the limitations of Arrhenius's definition:

As with the acetic acid reactions, both definitions work for the first example, where water is the solvent and hydronium ion is formed by the HCl solute.
The next two reactions do not involve the formation of ions but are still proton-transfer reactions.
In the second reaction hydrogen chloride and ammonia (dissolved in benzene) react to form solid ammonium chloride in a benzene solvent and in the third gaseous HCl and NH combine to form the solid.
A third, only marginally related concept was proposed in 1923 by Gilbert N. Lewis, which includes reactions with acid-base characteristics that do not involve a proton transfer.
A Lewis acid is a species that accepts a pair of electrons from another species; in other words, it is an electron pair acceptor.
Brønsted acid-base reactions are proton transfer reactions while Lewis acid-base reactions are electron pair transfers.
Many Lewis acids are not Brønsted-Lowry acids.
Contrast how the following reactions are described in terms of acid-base chemistry:
In the first reaction a fluoride ion, F, gives up an electron pair to boron trifluoride to form the product tetrafluoroborate.
Fluoride "loses" a pair of valence electrons because the electrons shared in the B—F bond are located in the region of space between the two atomic nuclei and are therefore more distant from the fluoride nucleus than they are in the lone fluoride ion.
BF is a Lewis acid because it accepts the electron pair from fluoride.
This reaction cannot be described in terms of Brønsted theory because there is no proton transfer.
The second reaction can be described using either theory.
A proton is transferred from an unspecified Brønsted acid to ammonia, a Brønsted base; alternatively, ammonia acts as a Lewis base and transfers a lone pair of electrons to form a bond with a hydrogen ion.
The species that gains the electron pair is the Lewis acid; for example, the oxygen atom in HO gains a pair of electrons when one of the H—O bonds is broken and the electrons shared in the bond become localized on oxygen.
Depending on the context, a Lewis acid may also be described as an oxidizer or an electrophile.
Organic Brønsted acids, such as acetic, citric, or oxalic acid, are not Lewis acids.
They dissociate in water to produce a Lewis acid, H, but at the same time also yield an equal amount of a Lewis base (acetate, citrate, or oxalate, respectively, for the acids mentioned).
Few, if any, of the acids discussed in the following are Lewis acids.
Reactions of acids are often generalized in the form HA H + A, where HA represents the acid and A is the conjugate base.
This reaction is referred to as protolysis.
The protonated form (HA) of an acid is also sometimes referred to as the free acid.
Acid-base conjugate pairs differ by one proton, and can be interconverted by the addition or removal of a proton (protonation and deprotonation, respectively).
Note that the acid can be the charged species and the conjugate base can be neutral in which case the generalized reaction scheme could be written as HA H + A. In solution there exists an equilibrium between the acid and its conjugate base.
The equilibrium constant "K" is an expression of the equilibrium concentrations of the molecules or the ions in solution.
Brackets indicate concentration, such that [HO] means "the concentration of HO".
The acid dissociation constant "K" is generally used in the context of acid-base reactions.
The numerical value of "K" is equal to the product of the concentrations of the products divided by the concentration of the reactants, where the reactant is the acid (HA) and the products are the conjugate base and H.
The stronger of two acids will have a higher "K" than the weaker acid; the ratio of hydrogen ions to acid will be higher for the stronger acid as the stronger acid has a greater tendency to lose its proton.
Because the range of possible values for "K" spans many orders of magnitude, a more manageable constant, p"K" is more frequently used, where p"K" = −log "K".
Stronger acids have a smaller p"K" than weaker acids.
Experimentally determined p"K" at 25 °C in aqueous solution are often quoted in textbooks and reference material.
In the classical naming system, acids are named according to their anions.
That ionic suffix is dropped and replaced with a new suffix (and sometimes prefix), according to the table below.
For example, HCl has chloride as its anion, so the -ide suffix makes it take the form hydrochloric acid.
In the IUPAC naming system, "aqueous" is simply added to the name of the ionic compound.
Thus, for hydrogen chloride, the IUPAC name would be aqueous hydrogen chloride.
The prefix "hydro-" is added only if the acid is made up of just hydrogen and one other element.
Classical naming system:
The strength of an acid refers to its ability or tendency to lose a proton.
A strong acid is one that completely dissociates in water; in other words, one mole of a strong acid HA dissolves in water yielding one mole of H and one mole of the conjugate base, A, and none of the protonated acid HA.
In contrast, a weak acid only partially dissociates and at equilibrium both the acid and the conjugate base are in solution.
Examples of strong acids are hydrochloric acid (HCl), hydroiodic acid (HI), hydrobromic acid (HBr), perchloric acid (HClO), nitric acid (HNO) and sulfuric acid (HSO).
In water each of these essentially ionizes 100%.
The stronger an acid is, the more easily it loses a proton, H. Two key factors that contribute to the ease of deprotonation are the polarity of the H—A bond and the size of atom A, which determines the strength of the H—A bond.
Acid strengths are also often discussed in terms of the stability of the conjugate base.
Stronger acids have a larger "K" and a more negative p"K" than weaker acids.
Sulfonic acids, which are organic oxyacids, are a class of strong acids.
A common example is toluenesulfonic acid (tosylic acid).
Unlike sulfuric acid itself, sulfonic acids can be solids.
In fact, polystyrene functionalized into polystyrene sulfonate is a solid strongly acidic plastic that is filterable.
Superacids are acids stronger than 100% sulfuric acid.
Examples of superacids are fluoroantimonic acid, magic acid and perchloric acid.
Superacids can permanently protonate water to give ionic, crystalline hydronium "salts".
They can also quantitatively stabilize carbocations.
While "K" measures the strength of an acid compound, the strength of an aqueous acid solution is measured by pH, which is an indication of the concentration of hydronium in the solution.
The pH of a simple solution of an acid compound in water is determined by the dilution of the compound and the compound's "K".
Monoprotic acids, also known as monobasic acids, are those acids that are able to donate one proton per molecule during the process of dissociation (sometimes called ionization) as shown below (symbolized by HA):

Common examples of monoprotic acids in mineral acids include hydrochloric acid (HCl) and nitric acid (HNO).
On the other hand, for organic acids the term mainly indicates the presence of one carboxylic acid group and sometimes these acids are known as monocarboxylic acid.
Examples in organic acids include formic acid (HCOOH), acetic acid (CHCOOH) and benzoic acid (CHCOOH).
Polyprotic acids, also known as polybasic acids, are able to donate more than one proton per acid molecule, in contrast to monoprotic acids that only donate one proton per molecule.
Specific types of polyprotic acids have more specific names, such as diprotic (or dibasic) acid (two potential protons to donate), and triprotic (or tribasic) acid (three potential protons to donate).
A diprotic acid (here symbolized by HA) can undergo one or two dissociations depending on the pH.
Each dissociation has its own dissociation constant, K and K.

The first dissociation constant is typically greater than the second; i.e., "K" > "K".
For example, sulfuric acid (HSO) can donate one proton to form the bisulfate anion (HSO), for which "K" is very large; then it can donate a second proton to form the sulfate anion (SO), wherein the "K" is intermediate strength.
The large "K" for the first dissociation makes sulfuric a strong acid.
In a similar manner, the weak unstable carbonic acid can lose one proton to form bicarbonate anion and lose a second to form carbonate anion (CO).
Both "K" values are small, but "K" > "K" .
A triprotic acid (HA) can undergo one, two, or three dissociations and has three dissociation constants, where "K" > "K" > "K".
An inorganic example of a triprotic acid is orthophosphoric acid (HPO), usually just called phosphoric acid.
All three protons can be successively lost to yield HPO, then HPO, and finally PO, the orthophosphate ion, usually just called phosphate.
Even though the positions of the three protons on the original phosphoric acid molecule are equivalent, the successive "K" values differ since it is energetically less favorable to lose a proton if the conjugate base is more negatively charged.
An organic example of a triprotic acid is citric acid, which can successively lose three protons to finally form the citrate ion.
Although the subsequent loss of each hydrogen ion is less favorable, all of the conjugate bases are present in solution.
The fractional concentration, "α" (alpha), for each species can be calculated.
For example, a generic diprotic acid will generate 3 species in solution: HA, HA, and A. The fractional concentrations can be calculated as below when given either the pH (which can be converted to the [H]) or the concentrations of the acid with all its conjugate bases:
A plot of these fractional concentrations against pH, for given "K" and "K", is known as a Bjerrum plot.
A pattern is observed in the above equations and can be expanded to the general "n" -protic acid that has been deprotonated "i" -times:
\alpha_{\ce H_{n-i} A^{i-} }= 


</doc>
<doc id="657" url="https://en.wikipedia.org/wiki?curid=657" title="Asphalt">
Asphalt

Asphalt, also known as bitumen (, ), is a sticky, black, and highly viscous liquid or semi-solid form of petroleum.
It may be found in natural deposits or may be a refined product, and is classed as a pitch.
Before the 20th century, the term asphaltum was also used.
The word is derived from the Ancient Greek ἄσφαλτος "ásphaltos".
The primary use (70%) of asphalt is in road construction, where it is used as the glue or binder mixed with aggregate particles to create asphalt concrete.
Its other main uses are for bituminous waterproofing products, including production of roofing felt and for sealing flat roofs.
The terms "asphalt" and "bitumen" are often used interchangeably to mean both natural and manufactured forms of the substance.
In American English, "asphalt" (or "asphalt cement") is commonly used for a refined residue from the distillation process of selected crude oils.
Outside the United States, the product is often called "bitumen", and geologists worldwide often prefer the term for the naturally occurring variety.
Common colloquial usage often refers to various forms of asphalt as "tar", as in the name of the La Brea Tar Pits.
Naturally occurring asphalt is sometimes specified by the term "crude bitumen".
Its viscosity is similar to that of cold molasses while the material obtained from the fractional distillation of crude oil boiling at is sometimes referred to as "refined bitumen".
The Canadian province of Alberta has most of the world's reserves of natural asphalt in the Athabasca oil sands, which cover , an area larger than England.
The word "asphalt" is derived from the late Middle English, in turn from French "asphalte", based on Late Latin "asphalton", "asphaltum", which is the latinisation of the Greek ἄσφαλτος ("ásphaltos", "ásphalton"), a word meaning "asphalt/bitumen/pitch", which perhaps derives from ἀ-, "without" and σφάλλω ("sfallō"), "make fall".
The first use of asphalt by the ancients was in the nature of a cement for securing or joining together various objects, and it thus seems likely that the name itself was expressive of this application.
Specifically, Herodotus mentioned that bitumen was brought to Babylon to build its gigantic fortification wall.
From the Greek, the word passed into late Latin, and thence into French ("asphalte") and English ("asphaltum" and "asphalt").
In French, the term "asphalte" is used for naturally occurring asphalt-soaked limestone deposits, and for specialised manufactured products with fewer voids or greater bitumen content than the "asphaltic concrete" used to pave roads.
The expression "bitumen" originated in the Sanskrit words "jatu", meaning "pitch", and "jatu-krit", meaning "pitch creating" or "pitch producing" (referring to coniferous or resinous trees).
The Latin equivalent is claimed by some to be originally "gwitu-men" (pertaining to pitch), and by others, "pixtumens" (exuding or bubbling pitch), which was subsequently shortened to "bitumen", thence passing via French into English.
From the same root is derived the Anglo-Saxon word "cwidu" (mastix), the German word "Kitt" (cement or mastic) and the old Norse word "kvada".
In British English, "bitumen" is used instead of "asphalt".
The word "asphalt" is instead used to refer to asphalt concrete, a mixture of construction aggregate and asphalt itself (also called "tarmac" in common parlance).
Bitumen mixed with clay was usually called "asphaltum", but the term is less commonly used today.
In Australian English, the word "asphalt" is used to describe a mix of construction aggregate.
"Bitumen" refers to the liquid derived from the heavy-residues from crude oil distillation.
In American English, "asphalt" is equivalent to the British "bitumen".
However, "asphalt" is also commonly used as a shortened form of "asphalt concrete" (therefore equivalent to the British "asphalt" or "tarmac").
In Canadian English, the word "bitumen" is used to refer to the vast Canadian deposits of extremely heavy crude oil, while "asphalt" is used for the oil refinery product.
Diluted bitumen (diluted with naphtha to make it flow in pipelines) is known as "dilbit" in the Canadian petroleum industry, while bitumen "upgraded" to synthetic crude oil is known as "syncrude", and syncrude blended with bitumen is called "synbit".
"Bitumen" is still the preferred geological term for naturally occurring deposits of the solid or semi-solid form of petroleum.
"Bituminous rock" is a form of sandstone impregnated with bitumen.
The oil sands of Alberta, Canada are a similar material.
Neither of the terms "asphalt" or "bitumen" should be confused with tar or coal tars.
The components of asphalt include four main classes of compounds: 

The naphthene aromatics and polar aromatics are typically the majority components.
Most natural bitumens also contain organosulfur compounds, resulting in an overall sulfur content of up to 4%.
Nickel and vanadium are found at <10 parts per million, as is typical of some petroleum.
The substance is soluble in carbon disulfide.
It is commonly modelled as a colloid, with asphaltenes as the dispersed phase and maltenes as the continuous phase.
"It is almost impossible to separate and identify all the different molecules of asphalt, because the number of molecules with different chemical structure is extremely large".
Asphalt may be confused with coal tar, which is a visually similar black, thermoplastic material produced by the destructive distillation of coal.
During the early and mid-20th century, when town gas was produced, coal tar was a readily available byproduct and extensively used as the binder for road aggregates.
The addition of coal tar to macadam roads led to the word "tarmac", which is now used in common parlance to refer to road-making materials.
However, since the 1970s, when natural gas succeeded town gas, asphalt has completely overtaken the use of coal tar in these applications.
Other examples of this confusion include the La Brea Tar Pits and the Canadian oil sands, both of which actually contain natural bitumen rather than tar.
"Pitch" is another term sometimes informally used at times to refer to asphalt, as in Pitch Lake.
For economic and other reasons, asphalt is sometimes sold combined with other materials, often without being labeled as anything other than simply "asphalt."
Of particular note, in the 21st century, is the use of re-refined engine oil bottoms -- "REOB" or "REOBs"—the residue of recycled autmotive engine oil, collected from the bottoms of re-refining vacuum distillation towers.
It contains the various non-refined elements and compounds in recycled engine oil, leftover from the re-refining process—both additives to the original oil, and materials accumulating from its circulation in the engine (typically iron and copper).
Some research has indicated a correlation between this contamination of asphalt and poorer-performing pavement.
The majority of asphalt used commercially is obtained from petroleum.
Nonetheless, large amounts of asphalt occur in concentrated form in nature.
Naturally occurring deposits of bitumen are formed from the remains of ancient, microscopic algae (diatoms) and other once-living things.
These remains were deposited in the mud on the bottom of the ocean or lake where the organisms lived.
Under the heat (above 50 °C) and pressure of burial deep in the earth, the remains were transformed into materials such as bitumen, kerogen, or petroleum.
Natural deposits of bitumen include lakes such as the Pitch Lake in Trinidad and Tobago and Lake Bermudez in Venezuela.
Natural seeps occur in the La Brea Tar Pits and in the Dead Sea.
Bitumen also occurs in unconsolidated sandstones known as "oil sands" in Alberta, Canada, and the similar "tar sands" in Utah, US.
The Canadian province of Alberta has most of the world's reserves, in three huge deposits covering , an area larger than England or New York state.
These bituminous sands contain of commercially established oil reserves, giving Canada the third largest oil reserves in the world.
Although historically it was used without refining to pave roads, nearly all of the output is now used as raw material for oil refineries in Canada and the United States.
The world's largest deposit of natural bitumen, known as the Athabasca oil sands, is located in the McMurray Formation of Northern Alberta.
This formation is from the early Cretaceous, and is composed of numerous lenses of oil-bearing sand with up to 20% oil.
Isotopic studies show the oil deposits to be about 110 million years old.
Two smaller but still very large formations occur in the Peace River oil sands and the Cold Lake oil sands, to the west and southeast of the Athabasca oil sands, respectively.
Of the Alberta deposits, only parts of the Athabasca oil sands are shallow enough to be suitable for surface mining.
The other 80% has to be produced by oil wells using enhanced oil recovery techniques like steam-assisted gravity drainage.
Much smaller heavy oil or bitumen deposits also occur in the Uinta Basin in Utah, US.
The Tar Sand Triangle deposit, for example, is roughly 6% bitumen.
Bitumen may occur in hydrothermal veins.
An example of this is within the Uinta Basin of Utah, in the US, where there is a swarm of laterally and vertically extensive veins composed of a solid hydrocarbon termed Gilsonite.
These veins formed by the polymerization and solidification of hydrocarbons that were mobilized from the deeper oil shales of the Green River Formation during burial and diagenesis.
Bitumen is similar to the organic matter in carbonaceous meteorites.
However, detailed studies have shown these materials to be distinct.
The vast Alberta bitumen resources are considered to have started out as living material from marine plants and animals, mainly algae, that died millions of years ago when an ancient ocean covered Alberta.
They were covered by mud, buried deeply over time, and gently cooked into oil by geothermal heat at a temperature of .
Due to pressure from the rising of the Rocky Mountains in southwestern Alberta, 80 to 55 million years ago, the oil was driven northeast hundreds of kilometres and trapped into underground sand deposits left behind by ancient river beds and ocean beaches, thus forming the oil sands.
The use of natural bitumen for waterproofing, and as an adhesive dates at least to the fifth millennium BC, with a crop storage basket discovered in Mehrgarh, of the Indus Valley Civilization, lined with it.
By the 3rd millennia BC refined rock asphalt was in use, in the region, and was used to waterproof the Great Bath, Mohenjo-daro.
In the ancient Middle East, the Sumerians used natural bitumen deposits for mortar between bricks and stones, to cement parts of carvings, such as eyes, into place, for ship caulking, and for waterproofing.
The Greek historian Herodotus said hot bitumen was used as mortar in the walls of Babylon.
The long Euphrates Tunnel beneath the river Euphrates at Babylon in the time of Queen Semiramis (ca.
800 BC) was reportedly constructed of burnt bricks covered with bitumen as a waterproofing agent.
Bitumen was used by ancient Egyptians to embalm mummies.
The Persian word for asphalt is "moom", which is related to the English word mummy.
The Egyptians' primary source of bitumen was the Dead Sea, which the Romans knew as "Palus Asphaltites" (Asphalt Lake).
Approximately 40 AD, Dioscorides described the Dead Sea material as "Judaicum bitumen", and noted other places in the region where it could be found.
The Sidon bitumen is thought to refer to material found at Hasbeya.
Pliny refers also to bitumen being found in Epirus.
It was a valuable strategic resource, the object of the first known battle for a hydrocarbon deposit—between the Seleucids and the Nabateans in 312 BC.
In the ancient Far East, natural bitumen was slowly boiled to get rid of the higher fractions, leaving a thermoplastic material of higher molecular weight that when layered on objects became quite hard upon cooling.
This was used to cover objects that needed waterproofing, such as scabbards and other items.
Statuettes of household deities were also cast with this type of material in Japan, and probably also in China.
In North America, archaeological recovery has indicated bitumen was sometimes used to adhere stone projectile points to wooden shafts.
In Canada, aboriginal people used bitumen seeping out of the banks of the Athabasca and other rivers to waterproof birch bark canoes, and also heated it in smudge pots to ward off mosquitoes in the summer.
In 1553, Pierre Belon described in his work "Observations" that "pissasphalto", a mixture of pitch and bitumen, was used in the Republic of Ragusa (now Dubrovnik, Croatia) for tarring of ships.
An 1838 edition of "Mechanics Magazine" cites an early use of asphalt in France.
A pamphlet dated 1621, by "a certain Monsieur d'Eyrinys, states that he had discovered the existence (of asphaltum) in large quantities in the vicinity of Neufchatel", and that he proposed to use it in a variety of ways – "principally in the construction of air-proof granaries, and in protecting, by means of the arches, the water-courses in the city of Paris from the intrusion of dirt and filth", which at that time made the water unusable.
"He expatiates also on the excellence of this material for forming level and durable terraces" in palaces, "the notion of forming such terraces in the streets not one likely to cross the brain of a Parisian of that generation".
But the substance was generally neglected in France until the revolution of 1830.
In the 1830s there was a surge of interest, and asphalt became widely used "for pavements, flat roofs, and the lining of cisterns, and in England, some use of it had been made of it for similar purposes".
Its rise in Europe was "a sudden phenomenon", after natural deposits were found "in France at Osbann (Bas-Rhin), the Parc (Ain) and the Puy-de-la-Poix (Puy-de-Dôme)", although it could also be made artificially.
One of the earliest uses in France was the laying of about 24,000 square yards of Seyssel asphalt at the Place de la Concorde in 1835.
Among the earlier uses of bitumen in the United Kingdom was for etching.
William Salmon's "Polygraphice" (1673) provides a recipe for varnish used in etching, consisting of three ounces of virgin wax, two ounces of mastic, and one ounce of asphaltum.
By the fifth edition in 1685, he had included more asphaltum recipes from other sources.
The first British patent for the use of asphalt was "Cassell's patent asphalte or bitumen" in 1834.
Then on 25 November 1837, Richard Tappin Claridge patented the use of Seyssel asphalt (patent #7849), for use in asphalte pavement, having seen it employed in France and Belgium when visiting with Frederick Walter Simms, who worked with him on the introduction of asphalt to Britain.
Dr T. Lamb Phipson writes that his father, Samuel Ryland Phipson, a friend of Claridge, was also "instrumental in introducing the asphalte pavement (in 1836)".
Indeed, mastic pavements had been previously employed at Vauxhall by a competitor of Claridge, but without success.
Claridge obtained a patent in Scotland on 27 March 1838, and obtained a patent in Ireland on 23 April 1838.
In 1851, extensions for the 1837 patent and for both 1838 patents were sought by the trustees of a company previously formed by Claridge.
"Claridge's Patent Asphalte Company"—formed in 1838 for the purpose of introducing to Britain "Asphalte in its natural state from the mine at Pyrimont Seysell in France",—"laid one of the first asphalt pavements in Whitehall".
Trials were made of the pavement in 1838 on the footway in Whitehall, the stable at Knightsbridge Barracks, "and subsequently on the space at the bottom of the steps leading from Waterloo Place to St.
James Park".
"The formation in 1838 of Claridge's Patent Asphalte Company (with a distinguished list of aristocratic patrons, and Marc and Isambard Brunel as, respectively, a trustee and consulting engineer), gave an enormous impetus to the development of a British asphalt industry".
"By the end of 1838, at least two other companies, Robinson's and the Bastenne company, were in production", with asphalt being laid as paving at Brighton, Herne Bay, Canterbury, Kensington, the Strand, and a large floor area in Bunhill-row, while meantime Claridge's Whitehall paving "continue(d) in good order".
In 1838, there was a flurry of entrepreneurial activity involving asphalt, which had uses beyond paving.
For example, asphalt could also be used for flooring, damp proofing in buildings, and for waterproofing of various types of pools and baths, both of which were also proliferating in the 19th century.
On the London stockmarket, there were various claims as to the exclusivity of asphalt quality from France, Germany and England.
And numerous patents were granted in France, with similar numbers of patent applications being denied in England due to their similarity to each other.
In England, "Claridge's was the type most used in the 1840s and 50s".
In 1914, Claridge's Company entered into a joint venture to produce tar-bound macadam, with materials manufactured through a subsidiary company called Clarmac Roads Ltd.
Two products resulted, namely "Clarmac", and "Clarphalte", with the former being manufactured by Clarmac Roads and the latter by Claridge's Patent Asphalte Co., although "Clarmac" was more widely used.
However, the First World War ruined the Clarmac Company, which entered into liquidation in 1915.
The failure of Clarmac Roads Ltd had a flow-on effect to Claridge's Company, which was itself compulsorily wound up, ceasing operations in 1917, having invested a substantial amount of funds into the new venture, both at the outset and in a subsequent attempt to save the Clarmac Company.
The first use of bitumen in the New World was by indigenous peoples.
On the west coast, as early as the 13th century, the Tongva, Luiseño and Chumash peoples collected the naturally occurring bitumen that seeped to the surface above underlying petroleum deposits.
All three groups used the substance as an adhesive.
It is found on many different artifacts of tools and ceremonial items.
For example, it was used on rattles to adhere gourds or turtle shells to rattle handles.
It was also used in decorations.
Small round shell beads were often set in asphaltum to provide decorations.
It was used as a sealant on baskets to make them watertight for carrying water, possibly poisoning those who drank the water.
Asphalt was used also to seal the planks on ocean-going canoes.
Asphalt was first used to pave streets in the 1870s.
At first naturally occurring "bituminous rock" was used, such as at Ritchie Mines in Macfarlan in Ritchie County, West Virginia from 1852 to 1873.
In 1876, asphalt-based paving was used to pave Pennsylvania Avenue in Washington DC, in time for the celebration of the national centennial.
In the horse-drawn era, streets were unpaved and covered with dirt or gravel.
However, that produced uneven wear, opened new hazards for pedestrians and made for dangerous potholes for bicycles and for motor vehicles.
Manhattan alone had 130,000 horses in 1900, pulling streetcars, wagons, and carriages, and leaving their waste behind.
They were not fast, and pedestrians could dodge and scramble their way across the crowded streets.
Small towns continued to rely on dirt and gravel, but larger cities wanted much better streets.
They looked to wood or granite blocks by the 1850s.
In 1890, a third of Chicago's 2000 miles of streets were paved, chiefly with wooden blocks, which gave better traction than mud.
Brick surfacing was a good compromise, but even better was asphalt paving, which was easy to install and to cut through to get at sewers.
With London and Paris serving as models, Washington laid 400,000 square yards of asphalt paving by 1882; it became the model for Buffalo, Philadelphia and elsewhere.
By the end of the century, American cities boasted 30 million square yards of asphalt paving, well ahead of brick.
The streets became faster and more dangerous so electric traffic lights were installed.
Electric trolleys (at 12 miles per hour) became the main transportation service for middle class shoppers and office workers until they bought automobiles after 1945 and commuted from more distant suburbs in privacy and comfort on asphalt highways.
Canada has the world's largest deposit of natural bitumen in the Athabasca oil sands, and Canadian First Nations along the Athabasca River had long used it to waterproof their canoes.
In 1719, a Cree named Wa-Pa-Su brought a sample for trade to Henry Kelsey of the Hudson’s Bay Company, who was the first recorded European to see it.
However, it wasn't until 1787 that fur trader and explorer Alexander MacKenzie saw the Athabasca oil sands and said, "At about 24 miles from the fork (of the Athabasca and Clearwater Rivers) are some bituminous fountains into which a pole of 20 feet long may be inserted without the least resistance."
The value of the deposit was obvious from the start, but the means of extracting the bitumen was not.
The nearest town, Fort McMurray, Alberta, was a small fur trading post, other markets were far away, and transportation costs were too high to ship the raw bituminous sand for paving.
In 1915, Sidney Ells of the Federal Mines Branch experimented with separation techniques and used the product to pave 600 feet of road in Edmonton, Alberta.
Other roads in Alberta were paved with material extracted from oil sands, but it was generally not economic.
During the 1920s Dr. Karl A. Clark of the Alberta Research Council patented a hot water oil separation process and entrepreneur Robert C. Fitzsimmons built the Bitumount oil separation plant, which between 1925 and 1958 produced up to per day of bitumen using Dr. Clark's method.
Most of the bitumen was used for waterproofing roofs, but other uses included fuels, lubrication oils, printers ink, medicines, rust- and acid-proof paints, fireproof roofing, street paving, patent leather, and fence post preservatives.
Eventually Fitzsimmons ran out of money and the plant was taken over by the Alberta government.
Today the Bitumount plant is a Provincial Historic Site.
Bitumen was used in early photographic technology.
In 1826 or 1827, it was used by French scientist Joseph Nicéphore Niépce to make the oldest surviving photograph from nature.
The bitumen was thinly coated onto a pewter plate which was then exposed in a camera.
Exposure to light hardened the bitumen and made it insoluble, so that when it was subsequently rinsed with a solvent only the sufficiently light-struck areas remained.
Many hours of exposure in the camera were required, making bitumen impractical for ordinary photography, but from the 1850s to the 1920s it was in common use as a photoresist in the production of printing plates for various photomechanical printing processes.
Bitumen was the nemesis of many artists during the 19th century.
Although widely used for a time, it ultimately proved unstable for use in oil painting, especially when mixed with the most common diluents, such as linseed oil, varnish and turpentine.
Unless thoroughly diluted, bitumen never fully solidifies and will in time corrupt the other pigments with which it comes into contact.
The use of bitumen as a glaze to set in shadow or mixed with other colors to render a darker tone resulted in the eventual deterioration of many paintings, for instance those of Delacroix.
Perhaps the most famous example of the destructiveness of bitumen is Théodore Géricault's Raft of the Medusa (1818–1819), where his use of bitumen caused the brilliant colors to degenerate into dark greens and blacks and the paint and canvas to buckle.
The vast majority of refined asphalt is used in construction: primarily as a constituent of products used in paving and roofing applications.
According to the requirements of the end use, asphalt is produced to specification.
This is achieved either by refining or blending.
It is estimated that the current world use of asphalt is approximately 102 million tonnes per year.
Approximately 85% of all the asphalt produced is used as the binder in asphalt concrete for roads.
It is also used in other paved areas such as airport runways, car parks and footways.
Typically, the production of asphalt concrete involves mixing fine and coarse aggregates such as sand, gravel and crushed rock with asphalt, which acts as the binding agent.
Other materials, such as recycled polymers (e.g., rubber tyres), may be added to the asphalt to modify its properties according to the application for which the asphalt is ultimately intended.
A further 10% of global asphalt production is used in roofing applications, where its waterproofing qualities are invaluable.
The remaining 5% of asphalt is used mainly for sealing and insulating purposes in a variety of building materials, such as pipe coatings, carpet tile backing and paint.
Asphalt is applied in the construction and maintenance of many structures, systems, and components, such as the following:

The largest use of asphalt is for making asphalt concrete for road surfaces; this accounts for approximately 85% of the asphalt consumed in the United States.
There are about 4,000 asphalt concrete mixing plants in the US, and a similar number in Europe.
Asphalt concrete pavement mixes are typically composed of 5% asphalt cement and 95% aggregates (stone, sand, and gravel).
Due to its highly viscous nature, asphalt cement must be heated so it can be mixed with the aggregates at the asphalt mixing facility.
The temperature required varies depending upon characteristics of the asphalt and the aggregates, but warm-mix asphalt technologies allow producers to reduce the temperature required.
The weight of an asphalt pavement depends upon the aggregate type, the asphalt, and the air void content.
An average example in the United States is about 112 pounds per square yard, per inch of pavement thickness.
When maintenance is performed on asphalt pavements, such as milling to remove a worn or damaged surface, the removed material can be returned to a facility for processing into new pavement mixtures.
The asphalt in the removed material can be reactivated and put back to use in new pavement mixes.
With some 95% of paved roads being constructed of or surfaced with asphalt, a substantial amount of asphalt pavement material is reclaimed each year.
According to industry surveys conducted annually by the Federal Highway Administration and the National Asphalt Pavement Association, more than 99% of the asphalt removed each year from road surfaces during widening and resurfacing projects is reused as part of new pavements, roadbeds, shoulders and embankments.
Asphalt concrete paving is widely used in airports around the world.
Due to the sturdiness and ability to be repaired quickly, it is widely used for runways.
Mastic asphalt is a type of asphalt that differs from dense graded asphalt (asphalt concrete) in that it has a higher asphalt (binder) content, usually around 7–10% of the whole aggregate mix, as opposed to rolled asphalt concrete, which has only around 5% asphalt.
This thermoplastic substance is widely used in the building industry for waterproofing flat roofs and tanking underground.
Mastic asphalt is heated to a temperature of and is spread in layers to form an impervious barrier about thick.
A number of technologies allow asphalt to be mixed at much lower temperatures.
These involve mixing with petroleum solvents to form "cutbacks" with reduced melting point or mixing with water to turn the asphalt into an emulsion.
Asphalt emulsions contain up to 70% asphalt and typically less than 1.5% chemical additives.
There are two main types of emulsions with different affinity for aggregates, cationic and anionic.
Asphalt emulsions are used in a wide variety of applications.
Chipseal involves spraying the road surface with asphalt emulsion followed by a layer of crushed rock, gravel or crushed slag.
Slurry seal involves the creation of a mixture of asphalt emulsion and fine crushed aggregate that is spread on the surface of a road.
Cold-mixed asphalt can also be made from asphalt emulsion to create pavements similar to hot-mixed asphalt, several inches in depth, and asphalt emulsions are also blended into recycled hot-mix asphalt to create low-cost pavements.
Synthetic crude oil, also known as syncrude, is the output from a bitumen upgrader facility used in connection with oil sand production in Canada.
Bituminous sands are mined using enormous (100 ton capacity) power shovels and loaded into even larger (400 ton capacity) dump trucks for movement to an upgrading facility.
The process used to extract the bitumen from the sand is a hot water process originally developed by Dr. Karl Clark of the University of Alberta during the 1920s.
After extraction from the sand, the bitumen is fed into a bitumen upgrader which converts it into a light crude oil equivalent.
This synthetic substance is fluid enough to be transferred through conventional oil pipelines and can be fed into conventional oil refineries without any further treatment.
By 2015 Canadian bitumen upgraders were producing over per day of synthetic crude oil, of which 75% was exported to oil refineries in the United States.
In Alberta, five bitumen upgraders produce synthetic crude oil and a variety of other products: The Suncor Energy upgrader near Fort McMurray, Alberta produces synthetic crude oil plus diesel fuel; the Syncrude Canada, Canadian Natural Resources, and Nexen upgraders near Fort McMurray produce synthetic crude oil; and the Shell Scotford Upgrader near Edmonton produces synthetic crude oil plus an intermediate feedstock for the nearby Shell Oil Refinery.
A sixth upgrader, under construction in 2015 near Redwater, Alberta, will upgrade half of its crude bitumen directly to diesel fuel, with the remainder of the output being sold as feedstock to nearby oil refineries and petrochemical plants.
Canadian bitumen does not differ substantially from oils such as Venezuelan extra-heavy and Mexican heavy oil in chemical composition, and the real difficulty is moving the extremely viscous bitumen through oil pipelines to the refinery.
Many modern oil refineries are extremely sophisticated and can process non-upgraded bitumen directly into products such as gasoline, diesel fuel, and refined asphalt without any preprocessing.
This is particularly common in areas such as the US Gulf coast, where refineries were designed to process Venezuelan and Mexican oil, and in areas such as the US Midwest where refineries were rebuilt to process heavy oil as domestic light oil production declined.
Given the choice, such heavy oil refineries usually prefer to buy bitumen rather than synthetic oil because the cost is lower, and in some cases because they prefer to produce more diesel fuel and less gasoline.
By 2015 Canadian production and exports of non-upgraded bitumen exceeded that of synthetic crude oil at over per day, of which about 65% was exported to the United States.
Because of the difficulty of moving crude bitumen through pipelines, non-upgraded bitumen is usually diluted with natural-gas condensate in a form called dilbit or with synthetic crude oil, called synbit.
However, to meet international competition, much non-upgraded bitumen is now sold as a blend of multiple grades of bitumen, conventional crude oil, synthetic crude oil, and condensate in a standardized benchmark product such as Western Canadian Select.
This sour, heavy crude oil blend is designed to have uniform refining characteristics to compete with internationally marketed heavy oils such as Mexican Mayan or Arabian Dubai Crude.
Asphalt was used starting in the 1960s as an hydrophobic matrix aiming to encapsulate radioactive waste such as medium-activity salts (mainly soluble sodium nitrate and sodium sulfate) produced by the reprocessing of spent nuclear fuels or radioactive sludges from sedimentation ponds.
Bituminised radioactive waste containing highly radiotoxic alpha-emitting transuranic elements from nuclear reprocessing plants have been produced at industrial scale in France, Belgium and Japan, but this type of waste conditioning has been abandoned because operational safety issues (risks of fire, as occurred in a bituminisation plant at Tokai Works in Japan) and long-term stability problems related to their geological disposal in deep rock formations.
One of the main problem is the swelling of asphalt exposed to radiation and to water.
Asphalt swelling is first induced by radiation because of the presence of hydrogen gas bubbles generated by alpha and gamma radiolysis.
A second mechanism is the matrix swelling when the encapsulated hygroscopic salts exposed to water or moisture start to rehydrate and to dissolve.
The high concentration of salt in the pore solution inside the bituminised matrix is then responsible for osmotic effects inside the bituminised matrix.
The water moves in the direction of the concentrated salts, the asphalt acting as a semi-permeable membrane.
This also causes the matrix to swell.
The swelling pressure due to osmotic effect under constant volume can be as high as 200 bar.
If not properly managed, this high pressure can cause fractures in the near field of a disposal gallery of bituminised medium-level waste.
When the bituminised matrix has been altered by swelling, encapsulated radionuclides are easily leached by the contact of ground water and released in the geosphere.
The high ionic strength of the concentrated saline solution also favours the migration of radionuclides in clay host rocks.
The presence of chemically reactive nitrate can also affect the redox conditions prevailing in the host rock by establishing oxidizing conditions, preventing the reduction of redox-sensitive radionuclides.
Under their higher valences, radionuclides of elements such as selenium, technetium, uranium, neptunium and plutonium have a higher solubility and are also often present in water as non-retarded anions.
This makes the disposal of medium-level bituminised waste very challenging.
Different type of asphalt have been used: blown bitumen (partly oxidized with air oxygen at high temperature after distillation, and harder) and direct distillation bitumen (softer).
Blown bitumens like Mexphalte, with a high content of saturated hydrocarbons, are more easily biodegraded by microorganisms than direct distillation bitumen, with a low content of saturated hydrocarbons and a high content of aromatic hydrocarbons.
Concrete encapsulation of radwaste is presently considered a safer alternative by the nuclear industry and the waste management organisations.
Roofing shingles account for most of the remaining asphalt consumption.
Other uses include cattle sprays, fence-post treatments, and waterproofing for fabrics.
Asphalt is used to make Japan black, a lacquer known especially for its use on iron and steel, and it is also used in paint and marker inks by some exterior paint supply companies to increase the weather resistance and permanence of the paint or ink, and to make the color darker.
Asphalt is also used to seal some alkaline batteries during the manufacturing process.
About 40,000,000 tons were produced in 1984.
It is obtained as the "heavy" (i.e., difficult to distill) fraction.
Material with a boiling point greater than around 500 °C is considered asphalt.
Vacuum distillation separates it from the other components in crude oil (such as naphtha, gasoline and diesel).
The resulting material is typically further treated to extract small but valuable amounts of lubricants and to adjust the properties of the material to suit applications.
In a de-asphalting unit, the crude asphalt is treated with either propane or butane in a supercritical phase to extract the lighter molecules, which are then separated.
Further processing is possible by "blowing" the product: namely reacting it with oxygen.
This step makes the product harder and more viscous.
Asphalt is typically stored and transported at temperatures around .
Sometimes diesel oil or kerosene are mixed in before shipping to retain liquidity; upon delivery, these lighter materials are separated out of the mixture.
This mixture is often called "bitumen feedstock", or BFS.
Some dump trucks route the hot engine exhaust through pipes in the dump body to keep the material warm.
The backs of tippers carrying asphalt, as well as some handling equipment, are also commonly sprayed with a releasing agent before filling to aid release.
Diesel oil is no longer used as a release agent due to environmental concerns.
Naturally occurring crude bitumen impregnated in sedimentary rock is the prime feed stock for petroleum production from "oil sands", currently under development in Alberta, Canada.
Canada has most of the world's supply of natural bitumen, covering 140,000 square kilometres (an area larger than England), giving it the second-largest proven oil reserves in the world.
The Athabasca oil sands are the largest bitumen deposit in Canada and the only one accessible to surface mining, although recent technological breakthroughs have resulted in deeper deposits becoming producible by "in situ" methods.
Because of oil price increases after 2003, producing bitumen became highly profitable, but as a result of the decline after 2014 it became uneconomic to build new plants again.
By 2014, Canadian crude bitumen production averaged about per day and was projected to rise to per day by 2020.
The total amount of crude bitumen in Alberta that could be extracted is estimated to be about , which at a rate of would last about 200 years.
Although uncompetitive economically, asphalt can be made from nonpetroleum-based renewable resources such as sugar, molasses and rice, corn and potato starches.
Asphalt can also be made from waste material by fractional distillation of used motor oil, which is sometimes otherwise disposed of by burning or dumping into landfills.
Use of motor oil may cause premature cracking in colder climates, resulting in roads that need to be repaved more frequently.
Nonpetroleum-based asphalt binders can be made light-colored.
Lighter-colored roads absorb less heat from solar radiation, reducing their contribution to the urban heat island effect.
Parking lots that use asphalt alternatives are called green parking lots.
Selenizza is a naturally occurring solid hydrocarbon bitumen found in native deposits in Selenice, in Albania, the only European asphalt mine still in use.
The bitumen is found in the form of veins, filling cracks in a more or less horizontal direction.
The bitumen content varies from 83% to 92% (soluble in carbon disulphide), with a penetration value near to zero and a softening point (ring and ball) around 120 °C.
The insoluble matter, consisting mainly of silica ore, ranges from 8% to 17%.
Albanian bitumen extraction has a long history and was practiced in an organized way by the Romans.
After centuries of silence, the first mentions of Albanian bitumen appeared only in 1868, when the Frenchman Coquand published the first geological description of the deposits of Albanian bitumen.
In 1875, the exploitation rights were granted to the Ottoman government and in 1912, they were transferred to the Italian company Simsa.
Since 1945, the mine was exploited by the Albanian government and from 2001 to date, the management passed to a French company, which organized the mining process for the manufacture of the natural bitumen on an industrial scale.
Today the mine is predominantly exploited in an open pit quarry but several of the many underground mines (deep and extending over several km) still remain viable.
Selenizza is produced primarily in granular form, after melting the bitumen pieces selected in the mine.
Selenizza is mainly used as an additive in the road construction sector.
It is mixed with traditional asphalt to improve both the viscoelastic properties and the resistance to ageing.
It may be blended with the hot asphalt in tanks, but its granular form allows it to be fed in the mixer or in the recycling ring of normal asphalt plants.
Other typical applications include the production of mastic asphalts for sidewalks, bridges, car-parks and urban roads as well as drilling fluid additives for the oil and gas industry.
Selenizza is available in powder or in granular material of various particle sizes and is packaged in sacks or in thermal fusible polyethylene bags.
A life-cycle assessment study of the natural selenizza compared with petroleum asphalt has shown that the environmental impact of the selenizza is about half the impact of the road asphalt produced in oil refineries in terms of carbon dioxide emission.
Although asphalt typically makes up only 4 to 5 percent (by weight) of the pavement mixture, as the pavement’s binder, it is also the most expensive part of the cost of the road-paving material.
During asphalt's early use in modern paving, oil refiners gave it away.
However, asphalt is, today, a highly traded commodity.
Its prices increased substantially in the early 21st Century.
A U.S.
government report states:

The report indicates that an "average" 1-mile (1.6-kilometer)-long, four-lane highway would include "300 tons of asphalt," which, "in 2002 would have cost around $48,000.
By 2006 this would have increased to $96,000 and by 2012 to $183,000... an increase of about $135,000 for every mile of highway in just 10 years."
People can be exposed to asphalt in the workplace by breathing in fumes or skin absorption.
The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit of 5 mg/m over a 15-minute period.
Asphalt is basically an inert material that must be heated or diluted to a point where it becomes workable for the production of materials for paving, roofing, and other applications.
In examining the potential health hazards associated with asphalt, the International Agency for Research on Cancer (IARC) determined that it is the application parameters, predominantly temperature, that affect occupational exposure and the potential bioavailable carcinogenic hazard/risk of the asphalt emissions.
In particular, temperatures greater than 199 °C (390 °F), were shown to produce a greater exposure risk than when asphalt was heated to lower temperatures, such as those typically used in asphalt pavement mix production and placement.
IARC has classified asphalt as a Class 2B possible carcinogen.
</doc>
<doc id="659" url="https://en.wikipedia.org/wiki?curid=659" title="American National Standards Institute">
American National Standards Institute

The American National Standards Institute (ANSI ) is a private non-profit organization that oversees the development of voluntary consensus standards for products, services, processes, systems, and personnel in the United States.
The organization also coordinates U.S.
standards with international standards so that American products can be used worldwide.
ANSI accredits standards that are developed by representatives of other standards organizations, government agencies, consumer groups, companies, and others.
These standards ensure that the characteristics and performance of products are consistent, that people use the same definitions and terms, and that products are tested the same way.
ANSI also accredits organizations that carry out product or personnel certification in accordance with requirements defined in international standards.
The organization's headquarters are in Washington, D.C.
ANSI's operations office is located in New York City.
The ANSI annual operating budget is funded by the sale of publications, membership dues and fees, accreditation services, fee-based programs, and international standards programs.
ANSI was originally formed in 1918, when five engineering societies and three government agencies founded the American Engineering Standards Committee (AESC).
In 1928, the AESC became the American Standards Association (ASA).
In 1966, the ASA was reorganized and became United States of America Standards Institute (USASI).
The present name was adopted in 1969.
Prior to 1918, these five founding engineering societies:
had been members of the United Engineering Society (UES).
At the behest of the AIEE, they invited the U.S.
government Departments of War, Navy (combined in 1947 to become the Department of Defense or DOD) and Commerce to join in founding a national standards organization.
According to Adam Stanton, the first permanent secretary and head of staff in 1919, AESC started as an ambitious program and little else.
Staff for the first year consisted of one executive, Clifford B. LePage, who was on loan from a founding member, ASME.
An annual budget of $7,500 was provided by the founding bodies.
In 1931, the organization (renamed ASA in 1928) became affiliated with the U.S.
National Committee of the International Electrotechnical Commission (IEC), which had been formed in 1904 to develop electrical and electronics standards.
ANSI's members are government agencies, organizations, academic and international bodies, and individuals.
In total, the Institute represents the interests of more than 270,000 companies and organizations and 30 million professionals worldwide.
Although ANSI itself does not develop standards, the Institute oversees the development and use of standards by accrediting the procedures of standards developing organizations.
ANSI accreditation signifies that the procedures used by standards developing organizations meet the Institute's requirements for openness, balance, consensus, and due process.
ANSI also designates specific standards as American National Standards, or ANS, when the Institute determines that the standards were developed in an environment that is equitable, accessible and responsive to the requirements of various stakeholders.
Voluntary consensus standards quicken the market acceptance of products while making clear how to improve the safety of those products for the protection of consumers.
There are approximately 9,500 American National Standards that carry the ANSI designation.
The American National Standards process involves:

In addition to facilitating the formation of standards in the United States, ANSI promotes the use of U.S.
standards internationally, advocates U.S.
policy and technical positions in international and regional standards organizations, and encourages the adoption of international standards as national standards where appropriate.
The Institute is the official U.S.
representative to the two major international standards organizations, the International Organization for Standardization (ISO), as a founding member, and the International Electrotechnical Commission (IEC), via the U.S.
National Committee (USNC).
ANSI participates in almost the entire technical program of both the ISO and the IEC, and administers many key committees and subgroups.
In many instances, U.S.
standards are taken forward to ISO and IEC, through ANSI or the USNC, where they are adopted in whole or in part as international standards.
Adoption of ISO and IEC standards as American standards increased from 0.2% in 1986 to 15.5% in May 2012.
The Institute administers nine standards panels:


Each of the panels works to identify, coordinate, and harmonize voluntary standards relevant to these areas.
In 2009, ANSI and the National Institute of Standards and Technology (NIST) formed the Nuclear Energy Standards Coordination Collaborative (NESCC).
NESCC is a joint initiative to identify and respond to the current need for standards in the nuclear industry.
</doc>
<doc id="661" url="https://en.wikipedia.org/wiki?curid=661" title="Argument (disambiguation)">
Argument (disambiguation)

In logic and philosophy, an argument is an attempt to persuade someone of something, or give evidence or reasons for accepting a particular conclusion.
Argument may also refer to: 






</doc>
<doc id="662" url="https://en.wikipedia.org/wiki?curid=662" title="Apollo 11">
Apollo 11

Apollo 11 was the spaceflight that landed the first two people on the Moon.
Mission commander Neil Armstrong and pilot Buzz Aldrin, both American, landed the lunar module "Eagle" on July 20, 1969, at 20:17 UTC.
Armstrong became the first person to step onto the lunar surface six hours after landing on July 21 at 02:56:15 UTC; Aldrin joined him about 20 minutes later.
They spent about two and a quarter hours together outside the spacecraft, and collected of lunar material to bring back to Earth.
Michael Collins piloted the command module "Columbia" alone in lunar orbit while they were on the Moon's surface.
Armstrong and Aldrin spent 21.5 hours on the lunar surface before rejoining "Columbia" in lunar orbit.
Apollo 11 was launched by a Saturn V rocket from Kennedy Space Center on Merritt Island, Florida, on July 16 at 13:32 UTC, and was the fifth crewed mission of NASA's Apollo program.
The Apollo spacecraft had three parts: a command module (CM) with a cabin for the three astronauts, and the only part that returned to Earth; a service module (SM), which supported the command module with propulsion, electrical power, oxygen, and water; and a lunar module (LM) that had two stages – a descent stage for landing on the Moon, and an ascent stage to place the astronauts back into lunar orbit.
After being sent to the Moon by the Saturn V's third stage, the astronauts separated the spacecraft from it and traveled for three days until they entered into lunar orbit.
Armstrong and Aldrin then moved into "Eagle" and landed in the Sea of Tranquillity.
The astronauts used "Eagle"s upper stage to lift off from the lunar surface and rejoin Collins in the command module.
They jettisoned "Eagle" before they performed the maneuvers that blasted them out of lunar orbit on a trajectory back to Earth.
They returned to Earth and splashed down in the Pacific Ocean on July 24 after more than eight days in space.
The landing was broadcast on live TV to a worldwide audience.
Armstrong stepped onto the lunar surface and described the event as "one small step for [a] man, one giant leap for mankind."
Apollo 11 effectively ended the Space Race and fulfilled a national goal proposed in 1961 by President John F. Kennedy: "before this decade is out, of landing a man on the Moon and returning him safely to the Earth."
In the late 1950s and early 1960s, the United States was engaged in the Cold War, a geopolitical rivalry with the Soviet Union.
On October 4, 1957, the Soviet Union launched Sputnik 1, the first artificial satellite.
This surprise success fired fears and imaginations around the world.
It not only demonstrated that the Soviet Union had the capability to deliver nuclear weapons over intercontinental distances, it challenged American claims of military, economic and technological superiority.
This precipitated the Sputnik crisis, and triggered the Space Race.
President Dwight D. Eisenhower responded by creating the National Aeronautics and Space Administration (NASA), and initiating Project Mercury, which aimed to launch a man into Earth orbit.
But on April 12, 1961, Soviet cosmonaut Yuri Gagarin became the first person in space, and the first to orbit the Earth.
It was another body blow to American pride.
Nearly a month later, on May 5, 1961, Alan Shepard became the first American in space, completing a 15-minute suborbital journey.
After being recovered from the Atlantic Ocean, he received a congratulatory telephone call from Eisenhower's successor, John F. Kennedy.
Kennedy cared about what people in other nations thought of the United States, and believed that not only was it in the national interest of the United States to be superior to other nations, but that the perception of American power was at least as important as the actuality.
It was therefore intolerable that the Soviet Union was more advanced in the field of space exploration.
He was determined that the United States should compete, and sought a challenge that maximized its chances of winning.
Since the Soviet Union had better booster rockets, he required a challenge that was beyond the capacity of the existing generation of rocketry, one where the US and Soviet Union would be starting from a position of equality.
Something spectacular, even if it could not be justified on military, economic or scientific grounds.
After consulting with his experts and advisors, he chose such a project.
On May 25, 1961, he addressed the United States Congress on "Urgent National Needs" and declared:

The effort to land a man on the Moon already had a name: Project Apollo.
An early and crucial decision was the adoption of lunar orbit rendezvous, under which a specialized spacecraft would land on the lunar surface.
The Apollo spacecraft therefore had three parts: a command module (CM) with a cabin for the three astronauts, and the only part that returned to Earth; a service module (SM), which supported the command module with propulsion, electrical power, oxygen, and water; and a lunar module (LM) that had two stages – a descent stage for landing on the Moon, and an ascent stage to place the astronauts back into lunar orbit.
This choice of mode meant that the spacecraft could be launched by the Saturn V rocket that was then under development.
Technologies and technics required for Apollo were developed by Project Gemini.
Project Apollo was abruptly halted by the Apollo 1 fire on January 27, 1967, in which three astronauts died, and the subsequent investigation.
In October 1968, Apollo 7 tested the command module in Earth orbit, and in December, Apollo 8 tested it in lunar orbit.
In March 1969, Apollo 9 tested the lunar module in Earth orbit, and then in May 1969, Apollo 10 conducted a "dress rehearsal", testing the lunar module in lunar orbit.
By July 1969, all was in readiness for Apollo 11 to take the final step onto the Moon.
The Soviet Union competed with the US, but were hampered by repeated failures in development of a launcher comparable to the Saturn V. Meanwhile, the Soviets tried to beat the US to return lunar material to the Earth by means of unmanned probes.
On July 13, three days before Apollo 11's launch, the Soviet Union launched Luna 15, which reached lunar orbit before Apollo 11.
During descent, a malfunction caused Luna 15 to crash in Mare Crisium about two hours before Armstrong and Aldrin took off from the Moon's surface to begin their voyage home.
The Nuffield Radio Astronomy Laboratories radio telescope in England recorded transmissions from Luna 15 during its descent, and these were released in July 2009 for the 40th anniversary of Apollo 11.
The initial crew assignment of Neil Armstrong as Commander, Jim Lovell as Command Module Pilot (CMP) and Buzz Aldrin as Lunar Module Pilot (LMP) on the backup crew for Apollo 9 was officially announced on November 20, 1967.
Lovell and Aldrin had previously flown together as the crew of Gemini 12.
Due to design and manufacturing delays in the Lunar Module (LM), Apollo 8 and Apollo 9 swapped prime and backup crews, and Armstrong's crew became the backup for Apollo 8.
Based on the normal crew rotation scheme, Armstrong was then expected to command Apollo 11.
There would be one change.
Michael Collins, the CMP on the Apollo 8 crew, began experiencing trouble with his legs.
Doctors diagnosed the problem as a bony growth between his fifth and sixth vertebrae, requiring surgery.
Lovell took his place on the Apollo 8 crew, and, when Collins recovered, he joined Armstrong's crew as CMP.
In the meantime, Fred Haise filled in as backup LMP, and Aldrin as backup CMP for Apollo 8.
Apollo 11 was the second all-veteran multi-person crew on an American mission, the first being that of Apollo 10.
An all-veteran crew would not be flown again until STS-26 in 1988.
The backup crew consisted of Lovell as Commander, William Anders as CMP, and Haise as LMP.
Anders had flown with Lovell on Apollo 8.
In early 1969, he accepted a job with the National Aeronautics and Space Council effective August 1969, and announced that he would retire as an astronaut at that time.
Ken Mattingly was moved from the support crew into parallel training with Anders as backup CMP in case Apollo 11 was delayed past its intended July launch date, at which point Anders would be unavailable.
Lovell, Haise, and Mattingly were later assigned as the prime crew of Apollo 13.
During Projects Mercury and Gemini, each mission had a prime and a backup crew.
For Apollo, a third crew of astronauts was added, known as the support crew.
The support crew maintained the flight plan, checklists and mission ground rules, and ensured that the prime and backup crews were apprised of any changes.
The support crew developed procedures in the simulators, especially those for emergency situations, so these were ready for when the prime and backup crews came to train in the simulators, allowing them to concentrate on practicing and mastering them.
For Apollo 11, the support crew consisted of Ken Mattingly, Ronald Evans and Bill Pogue.
The Capsule communicator (CAPCOM) was an astronaut at the Mission Control Center in Houston, Texas, who was the only person who communicated directly with the flight crew.
For Apollo 11, the CAPCOMs were: Charles Duke, Ronald Evans, Bruce McCandless II, James Lovell, William Anders, Ken Mattingly, Fred Haise, Don L. Lind, Owen K. Garriott and Harrison Schmitt.
The four shift flight directors for this mission were:

The Apollo 11 mission emblem was designed by Collins, who wanted a symbol for "peaceful lunar landing by the United States".
At Lovell's suggestion, he chose the bald eagle the national bird of the United States, as the symbol.
Tom Wilson, a simulator instructor, suggested that they put an olive branch in its beak.
Collins added a lunar background with the Earth in the distance.
The sunlight in the image was coming from the wrong direction; the shadow should have been in the lower part of the Earth instead of the left.
Aldrin, Armstrong and Collins decided that the Eagle and the Moon would be in their natural colours, and decided on a blue and gold border.
Armstrong was concerned that "eleven" would not be understood by non-English speakers, so they went with "Apollo 11", and they decided not to put their names on the patch, so it would "be representative of "everyone" who had worked toward a lunar landing".
An illustrator at the MSC did the artwork, which was then sent off to NASA officials for approval.
The design was rejected.
Bob Gilruth, the director of the MSC felt that the talons of the eagle looked "too warlike".
After some discussion, the olive branch was moved to the talons.
When the Eisenhower dollar coin was released in 1971, the patch design provided the eagle for its reverse side.
The design was also used for the smaller Susan B. Anthony dollar unveiled in 1979.
After the crew of Apollo 10 named their spacecraft "Charlie Brown" and "Snoopy", assistant manager for public affairs Julian Scheer wrote to George M. Low, the Manager of the Apollo Spacecraft Program Office at the Manned Spacecraft Center (MSC), to suggest the Apollo 11 crew be less flippant in naming their craft.
The name "Snowcone" was used for the Command Module and "Haystack" was used for the Lunar Module in both internal and external communications during early mission planning.
The Lunar Module was named "Eagle" after the motif which was featured prominently on the mission insignia.
At Scheer's suggestion, the Command Module was named "Columbia" after the "Columbiad", the giant cannon shell spacecraft fired by a giant cannon (also from Florida) in Jules Verne's 1865 novel "From the Earth to the Moon".
It also referenced Columbia, a historical name of the United States.
Each astronaut had a personal preference kit (PPK), a small bag containing personal items of significance that they wanted to take with them on the mission.
Neil Armstrong's PPK contained a piece of wood from the Wright brothers' 1903 airplane's left propeller and a piece of fabric from its wing, along with a diamond-studded astronaut pin originally given to Deke Slayton by the widows of the Apollo 1 crew.
This pin had been intended to be flown on that mission and given to Slayton afterwards; but following the disastrous launch pad fire and subsequent funerals, the widows gave the pin to Slayton.
Armstrong took it with him on Apollo 11.
NASA's Apollo Site Selection Board announced five potential landing sites on February 8, 1968.
These were the result of two years' worth of studies based on high-resolution photography of the lunar surface by the five unmanned probes of the Lunar Orbiter program and information about surface conditions provided by the Surveyor program.
The best Earth-bound telescopes could not resolve features with the resolution Project Apollo required.
Areas that appeared to be clear and promising on photographs taken on Earth were often found to be totally unacceptable.
The original requirement that the site be free of craters had to be relaxed, as no such site was found.
Five sites were considered: Sites 1 and 2 were in the Sea of Tranquility ("Mare Tranquilitatis"); Site 3 was in the Central Bay ("Sinus Medii"); and Sites 4 and 5 were in the Ocean of Storms ("Oceanus Procellarum").
The final site selection was based on seven criteria:

The requirement for the Sun angle was particularly restrictive, limiting the launch date to one day per month.
The Apollo Site Selection Board selected Site 2, with Sites 3 and 5 as backups in the event of the launch being delayed.
In May 1969, Apollo 10 flew to within of Site 2, and reported that it was acceptable.
The ascent stage of lunar module LM-5 arrived at the Kennedy Space Center on January 8, 1969, followed by the descent stage four days later, and Command and Service Module CM-107 on January, 23.
There were several differences between LM-5 and Apollo 10's LM-4; LM-5 had a VHF radio antenna to facilitate communication with the astronauts during their EVA on the lunar surface; a lighter ascent engine; more thermal protection on the landing gear; and a package of scientific experiments known as the Early Apollo Scientific Experiments Package (EASEP).
The only change in the configuration of the command module was the removal of some insulation from the forward hatch.
The command and service modules were mated on January 29, and shipped from the Operations and Checkout Building to the Vehicle Assembly Building on April 14.
Meanwhile, the S-IVB third stage of Saturn V AS-506 had arrived on January 18, followed by the S-II second stage on February 6, S-IC first stage on February 20, and the Saturn V Instrument Unit on February 27.
At 1230 on May 20, the assembly departed the Vehicle Assembly Building atop the crawler-transporter, bound for Launch Pad 39A, part of Launch Complex 39, while Apollo 10 was still on its way to the Moon.
A countdown test commenced on June 26, and concluded on July 2.
The launch complex was floodlit on the night of July 15, when the crawler-transporter carried the mobile service structure back to its parking area.
In the early hours of the morning, the fuel tanks of the S-II and S-IVB stages were filled with liquid hydrogen.
Fueling was completed by three hours before launch.
Launch operations were partly automated, with 43 programs written in the ATOLL programming language.
Haise entered "Columbia" about three hours and ten minutes before launch time.
Along with a technician, he helped Armstrong into the left hand couch at 06:54.
Five minutes later, Collins joined him, taking up his position on the right hand couch.
Finally, Aldrin entered, taking the center couch.
Haise left around two hours and ten minutes before launch.
The closeout crew sealed the hatch, and the cabin was purged and pressurized.
The closeout crew then left the launch complex about an hour before launch time.
The countdown became automated at three minutes and twenty seconds before launch time.
Over 450 personnel were at the consoles in the firing room.
An estimated one million spectators watched the launch of Apollo 11 from the highways and beaches vicinity of the launch site.
Dignitaries included the Chief of Staff of the United States Army, General William Westmoreland, four cabinet members, 19 state governors, 40 mayors, 60 ambassadors and 200 congressmen.
Vice President Spiro Agnew viewed the launch with the former president, Lyndon B. Johnson and his wife Lady Bird Johnson.
Around 3,500 media representatives were present.
About two-thirds were from the United States; the rest came from 55 other countries.
The launch was televised live in 33 countries, with an estimated 25 million viewers in the United States alone.
Millions more around the world listened to radio broadcasts.
President Richard Nixon viewed the launch from his office in the White House with his NASA liaison officer, Apollo astronaut Frank Borman.
Saturn V AS-506 launched Apollo 11 on July 16, 1969, at 13:32:00 UTC (9:32:00 EDT).
It entered Earth orbit at an altitude of by , twelve minutes later.
After one and a half orbits, the S-IVB third-stage engine pushed the spacecraft onto its trajectory toward the Moon with the trans-lunar injection (TLI) burn at 16:22:13 UTC.
About 30 minutes later, the transposition, docking, and extraction maneuver was performed: this involved separating "Columbia" from the spent S-IVB stage, turning around, and docking with "Eagle" still attached to the stage.
After the Lunar Module was extracted, the combined spacecraft headed for the Moon, while the rocket stage flew on a trajectory past the Moon.
The was done to avoid colliding with the spacecraft, the Earth or the Moon.
A slingshot effect from passing around the Moon threw it into an orbit around the Sun.
On July 19 at 17:21:50 UTC, Apollo 11 passed behind the Moon and fired its service propulsion engine to enter lunar orbit.
In the thirty orbits that followed, the crew saw passing views of their landing site in the southern Sea of Tranquillity about southwest of the crater Sabine D. The site was selected in part because it had been characterized as relatively flat and smooth by the automated Ranger 8 and Surveyor 5 landers and the Lunar Orbiter mapping spacecraft and unlikely to present major landing or EVA challenges.
It lay about southeast of the Surveyor 5 landing site, and southwest of Ranger 8's crash site.
At 12:52:00 UTC on July 20, Aldrin and Armstrong entered "Eagle", and began the final preparations for lunar descent.
At 17:44:00 "Eagle" separated from the "Columbia".
Collins, alone aboard "Columbia", inspected "Eagle" as it pirouetted before him to ensure the craft was not damaged, and that the landing gear was correctly deployed.
Armstrong exclaimed: "The "Eagle" has wings!"
As the descent began, Armstrong and Aldrin found that they were passing landmarks on the surface two or three seconds early, and reported that they were "long"; they would land miles west of their target point.
"Eagle" was traveling too fast.
The problem could have been mascons—concentrations of high mass that could have altered the trajectory.
Flight Director Gene Kranz speculated that it could have resulted from extra air pressure in the docking tunnel.
Or it could have been the result of "Eagle"s pirouette maneuver.
Five minutes into the descent burn, and above the surface of the Moon, the LM guidance computer (LGC) distracted the crew with the first of several unexpected 1201 and 1202 program alarms.
Inside Mission Control Center, computer engineer Jack Garman told guidance officer Steve Bales it was safe to continue the descent, and this was relayed to the crew.
The program alarms indicated "executive overflows", meaning the guidance computer could not complete all of its tasks in real time and had to postpone some of them.
Margaret Hamilton, the Director of Apollo Flight Computer Programming at the MIT Charles Stark Draper Laboratory later recalled:

During the mission, the cause was diagnosed as the rendezvous radar switch being in the wrong position, causing the computer to process data from both the rendezvous and landing radars at the same time.
However, software engineer Don Eyles concluded in a 2005 Guidance and Control Conference paper that the problem was actually due to a hardware design bug previously seen during testing of the first unmanned LM in Apollo 5.
Having the rendezvous radar on (so that it was warmed up in case of an emergency landing abort) should have been irrelevant to the computer, but an electrical phasing mismatch between two parts of the rendezvous radar system could cause the stationary antenna to appear to the computer as dithering back and forth between two positions, depending upon how the hardware randomly powered up.
The extra spurious cycle stealing, as the rendezvous radar updated an involuntary counter, caused the computer alarms.
When Armstrong again looked outside, he saw that the computer's landing target was in a boulder-strewn area just north and east of a diameter crater (later determined to be West crater, named for its location in the western part of the originally planned landing ellipse).
Armstrong took semi-automatic control.
Throughout the descent, Aldrin called out navigation data to Armstrong, who was busy piloting the "Eagle".
A few moments before the landing, a light informed Aldrin that at least one of the probes hanging from "Eagle" footpads had touched the surface, and he said: "Contact light!"
Three seconds later, "Eagle" landed and Armstrong said "Shutdown."
Aldrin immediately said "Okay, engine stop.
ACA – out of detent."
Armstrong acknowledged: "Out of detent.
Auto."
Aldrin continued: "Mode control – both auto.
Descent engine command override off.
Engine arm – off.
413 is in."
ACA was the attitude control assembly, the LM's control stick.
Output went to the LGC to command the reaction control system (RCS) jets to fire.
"Out of Detent" meant that the stick had moved away from its centered position; it was spring-centered like the turn indicator in a car.
LGC address 413 contained the variable that indicated that the LM had landed.
The "Eagle" landed at 20:17:40 UTC on Sunday July 20 with about 25 seconds of fuel left.
Apollo 11 landed with less fuel than subsequent missions, and the astronauts encountered a premature low fuel warning.
This was later found to be the result of greater propellant 'slosh' than expected, uncovering a fuel sensor.
On subsequent missions, extra anti-slosh baffles were added to the tanks to prevent this.
Armstrong acknowledged Aldrin's completion of the post landing checklist with "Engine arm is off", before responding to the CAPCOM, Charles Duke, with the words, "Houston, Tranquillity Base here.
The "Eagle" has landed."
Armstrong's unrehearsed change of call sign from "Eagle" to "Tranquillity Base" emphasized to listeners that landing was complete and successful.
Duke mispronounced his reply as he expressed the relief at Mission Control: "Roger, Twan— Tranquillity, we copy you on the ground.
You got a bunch of guys about to turn blue.
We're breathing again.
Thanks a lot."
Two and a half hours after landing, before preparations began for the EVA, Aldrin radioed to Earth:

He then took communion privately.
At this time NASA was still fighting a lawsuit brought by atheist Madalyn Murray O'Hair (who had objected to the Apollo 8 crew reading from the Book of Genesis) demanding that their astronauts refrain from broadcasting religious activities while in space.
As such, Aldrin chose to refrain from directly mentioning taking communion on the Moon.
Aldrin was an elder at the Webster Presbyterian Church, and his communion kit was prepared by the pastor of the church, Dean Woodruff.
Webster Presbyterian possesses the chalice used on the Moon and commemorates the event each year on the Sunday closest to July 20.
The schedule for the mission called for the astronauts to follow the landing with a five-hour sleep period.
However, they elected to forgo the sleep period and begin the preparations for the EVA early, thinking that they would be unable to sleep.
Preparations for the EVA began at 23:43.
These took longer than expected; three and a half hours instead of two.
During training on Earth, everything required had been neatly laid out in advance, but on the Moon the cabin contained a large number of other items as well, such as checklists, food packets and tools.
Once Armstrong and Aldrin were ready to go outside, "Eagle" was depressurized.
The hatch was opened at 02:39:33.
Armstrong initially had some difficulties squeezing through the hatch with his Portable Life Support System (PLSS).
Some of the highest heart rates recorded from Apollo astronauts occurred during LM egress and ingress.
At 02:51 Armstrong began his descent to the lunar surface.
The remote control unit controls on his chest kept him from seeing his feet.
Climbing down the nine-rung ladder, Armstrong pulled a D-ring to deploy the Modular Equipment Stowage Assembly (MESA) folded against "Eagle" side to activate the TV camera.
Apollo 11 used slow-scan television (TV) incompatible with broadcast TV, so it was displayed on a special monitor and a conventional TV camera viewed this monitor, significantly reducing the quality of the picture.
The signal was received at Goldstone in the United States, but with better fidelity by Honeysuckle Creek Tracking Station near Canberra in Australia.
Minutes later the feed was switched to the more sensitive Parkes radio telescope in Australia.
Despite some technical and weather difficulties, ghostly black and white images of the first lunar EVA were received and broadcast to at least 600 million people on Earth.
Although copies of this video in broadcast format were saved and are widely available, recordings of the original slow scan source transmission from the lunar surface were likely destroyed during routine magnetic tape re-use at NASA.
While still on the ladder, Armstrong uncovered a plaque mounted on the LM descent stage bearing two drawings of Earth (of the Western and Eastern Hemispheres), an inscription, and signatures of the astronauts and President Nixon.
The inscription read:
After describing the surface dust as "very fine-grained" and "almost like a powder", at 02:56:15, six and a half hours after landing, Armstrong stepped off "Eagle" footpad and declared: "That's one small step for [a] man, one giant leap for mankind."
Armstrong intended to say "That's one small step for a man", but the word "a" is not audible in the transmission, and thus was not initially reported by most observers of the live broadcast.
When later asked about his quote, Armstrong said he believed he said "for a man", and subsequent printed versions of the quote included the "a" in square brackets.
One explanation for the absence may be that his accent caused him to slur the words "for a" together; another is the intermittent nature of the audio and video links to Earth, partly because of storms near Parkes Observatory.
More recent digital analysis of the tape claims to reveal the "a" may have been spoken but obscured by static.
About seven minutes after stepping onto the Moon's surface, Armstrong collected a contingency soil sample using a sample bag on a stick.
He then folded the bag and tucked it into a pocket on his right thigh.
This was to guarantee there would be some lunar soil brought back in case an emergency required the astronauts to abandon the EVA and return to the LM.
Twelve minutes after the sample was collected, he removed the TV camera from the MESA and made a panoramic sweep, then mounted it on a tripod.
The TV camera cable remained partly coiled and presented a tripping hazard throughout the EVA.
Still photography was accomplished with a Hasselblad camera which could be operated hand held or mounted on Armstrong's Apollo/Skylab A7L space suit.
Aldrin joined Armstrong on the surface.
He described the view with the simple phrase: "Magnificent desolation."
Armstrong said that moving in the lunar gravity, one-sixth of Earth's, was "even perhaps easier than the simulations ... It's absolutely no trouble to walk around."
Aldrin joined him on the surface and tested methods for moving around, including two-footed kangaroo hops.
The PLSS backpack created a tendency to tip backward, but neither astronaut had serious problems maintaining balance.
Loping became the preferred method of movement.
The astronauts reported that they needed to plan their movements six or seven steps ahead.
The fine soil was quite slippery.
Aldrin remarked that moving from sunlight into "Eagle" shadow produced no temperature change inside the suit, though the helmet was warmer in sunlight, so he felt cooler in shadow.
The MESA failed to provide a stable work platform and was in shadow, slowing work somewhat.
As they worked, the moonwalkers kicked up gray dust which soiled the outer part of their suits.
The astronauts planted a specially designed U.S.
flag on the lunar surface, in clear view of the TV camera.
Aldrin remembered, "Of all the jobs I had to do on the Moon the one I wanted to go the smoothest was the flag raising."
But the astronauts struggled with the telescoping rod and could only jam the pole a couple of inches into the hard lunar surface.
Aldrin was afraid it might topple in front of TV viewers.
But he gave "a crisp West Point salute".
Before Aldrin could take a photo of Armstrong with the flag, President Richard Nixon spoke to them through a telephone-radio transmission which Nixon called "the most historic phone call ever made from the White House."
Nixon originally had a long speech prepared to read during the phone call, but Frank Borman, who was at the White House as a NASA liaison during Apollo 11, convinced Nixon to keep his words brief.
They deployed the EASEP, which included a Passive Seismic Experiment Package used to measure moonquakes and a retroreflector array used for the Lunar Laser Ranging experiment.
Then Armstrong walked from the LM to snap photos at the rim of Little West Crater while Aldrin collected two core samples.
He used the geologist's hammer to pound in the tubes – the only time the hammer was used on Apollo 11, but was unable to penetrate more than six inches deep.
The astronauts then collected rock samples using scoops and tongs on extension handles.
Many of the surface activities took longer than expected, so they had to stop documenting sample collection halfway through the allotted 34 minutes.
Aldrin shoveled of soil into the box of rocks in order to pack them in tightly.
Three new minerals were discovered in the rock samples collected by the astronauts: armalcolite, tranquillityite, and pyroxferroite.
Armalcolite was named after Armstrong, Aldrin, and Collins.
All have subsequently been found on Earth.
Mission Control used a coded phrase to warn Armstrong that his metabolic rates were high, and that he should slow down.
He was moving rapidly from task to task as time ran out.
However, as metabolic rates remained generally lower than expected for both astronauts throughout the walk, Mission Control granted the astronauts a 15-minute extension.
In a 2010 interview, Armstrong, who had walked a maximum of from the LM, explained that NASA limited the first moonwalk's time and distance because there was no empirical proof of how much cooling water the astronauts' PLSS backpacks would consume to handle their body heat generation while working on the Moon.
Aldrin entered "Eagle" first.
With some difficulty the astronauts lifted film and two sample boxes containing of lunar surface material to the LM hatch using a flat cable pulley device called the Lunar Equipment Conveyor (LEC).
This proved to be an inefficient tool, and later missions preferred to carry equipment and samples up to the LM by hand.
Armstrong reminded Aldrin of a bag of memorial items in his sleeve pocket, and Aldrin tossed the bag down.
Armstrong then jumped onto the ladder's third rung, and climbed into the LM.
After transferring to LM life support, the explorers lightened the ascent stage for the return to lunar orbit by tossing out their PLSS backpacks, lunar overshoes, an empty Hasselblad camera, and other equipment.
The hatch was closed again at 05:01.
They then pressurized the LM and settled down to sleep.
Nixon's speech writer William Safire had prepared "In Event of Moon Disaster" for the President to read on television in the event the Apollo 11 astronauts were stranded on the Moon.
The contingency plan originated in a memo from Safire to Nixon's White House Chief of Staff H. R. Haldeman, in which Safire suggested a protocol the administration might follow in reaction to such a disaster.
According to the plan, Mission Control would "close down communications" with the LM, and a clergyman would "commend their souls to the deepest of the deep" in a public ritual likened to burial at sea.
The last line of the prepared text contained an allusion to Rupert Brooke's First World War poem, "The Soldier".
While moving inside the cabin, Aldrin accidentally damaged the circuit breaker that would arm the main engine for lift off from the Moon.
There was a concern this would prevent firing the engine, stranding them on the Moon.
However, a felt-tip pen was sufficient to activate the switch.
Had this not worked, the Lunar Module circuitry could have been reconfigured to allow firing the ascent engine.
After more than hours on the lunar surface, in addition to the scientific instruments, the astronauts left behind an Apollo 1 mission patch and a memorial bag containing a gold replica of an olive branch as a traditional symbol of peace and a silicon message disk.
The disk carries the goodwill statements by Presidents Eisenhower, Kennedy, Johnson, and Nixon and messages from leaders of 73 countries around the world.
The disc also carries a listing of the leadership of the US Congress, a listing of members of the four committees of the House and Senate responsible for the NASA legislation, and the names of NASA's past and present top management.
After about seven hours of rest, the crew was awakened by Houston to prepare for the return flight.
Two and a half hours later, at 17:54:00 UTC, they lifted off in "Eagle" ascent stage to rejoin Collins aboard "Columbia" in lunar orbit.
Film taken from the LM Ascent Stage upon liftoff from the Moon reveals the American flag, planted some from the descent stage, whipping violently in the exhaust of the ascent stage engine.
Aldrin looked up in time to witness the flag topple: "The ascent stage of the LM separated ... I was concentrating on the computers, and Neil was studying the attitude indicator, but I looked up long enough to see the flag fall over."
Subsequent Apollo missions usually planted the American flags further from the LM to prevent them being blown over by the ascent engine exhaust.
After rendezvous with "Columbia", "Eagle"s ascent stage was jettisoned into lunar orbit on July 21, 1969, at 23:41 UTC.
Just before the Apollo 12 flight, it was noted that "Eagle" was still likely to be orbiting the Moon.
Later NASA reports mentioned that "Eagle" orbit had decayed, resulting in it impacting in an "uncertain location" on the lunar surface.
The location is uncertain because the "Eagle" ascent stage was not tracked after it was jettisoned, and the lunar gravity field is sufficiently non-uniform to make the orbit of the spacecraft unpredictable after a short time.
On July 23, the last night before splashdown, the three astronauts made a television broadcast in which Collins commented: 
Aldrin added: 

Armstrong concluded: 

On the return to Earth, a bearing at the Guam tracking station failed, potentially preventing communication on the last segment of the Earth return.
A regular repair was not possible in the available time but the station director, Charles Force, had his ten-year-old son Greg use his small hands to reach into the housing and pack it with grease.
Greg was later thanked by Armstrong.
The aircraft carrier , under the command of Captain Carl J. Seiberlich, was selected as the primary recovery ship (PRS) for Apollo 11 on June 5, replacing its sister ship, the LPH , which had recovered Apollo 10 on May 26.
The "Hornet" was then at her home port of Long Beach, California.
On reaching Pearl Harbor on July 5, "Hornet" embarked the Sikorsky SH-3 Sea King helicopters of HS-4, a unit which specialized in recovery of Apollo spacecraft, specialized divers of UDT Detachment Apollo, a 35-man NASA recovery team, and about 120 media representatives.
To make room, most of "Hornet"s air wing was left behind in Long Beach.
Special recovery equipment was also loaded, including a boilerplate command module used for training.
On July 12, with Apollo 11 still on the launch pad, "Hornet" departed Pearl Harbor for the recovery area in the central Pacific, in the vicinity of .
A presidential party consisting of Nixon, Borman, Secretary of State William P. Rogers and National Security Advisor Henry Kissinger flew to Johnston Atoll on Air Force One, then to the command ship in Marine One.
After a night on board, they would fly to "Hornet" in Marine One for a few hours of ceremonies.
On arrival on the "Hornet", the party was greeted by the Commander-in-Chief, Pacific Command (CINCPAC), Admiral John S. McCain Jr., and NASA Administrator Thomas O. Paine, who flew to "Hornet" from Pago Pago in one of "Hornet"s carrier onboard delivery aircraft.
Weather satellites were not yet common, but US Air Force Captain Hank Brandli had access to top secret spy satellite images.
He realized that a storm front was headed for the Apollo recovery area.
Poor visibility was a serious threat to the mission; if the helicopters could not locate "Columbia", the spacecraft, its crew, and its priceless cargo of Moon rocks might be lost.
Brandli alerted Navy Captain Willard S. Houston Jr., the commander of the Fleet Weather Center at Pearl Harbor, who had the required security clearance.
On their recommendation, Rear Admiral Donald C. Davis, the commander of Manned Spaceflight Recovery Forces, Pacific, advised NASA to change the recovery area.
This was done; a new one was designated, northeast of the original.
This altered the flight plan.
A different sequence of computer programs was used, one never before attempted.
In a conventional entry, P64 was followed by P67.
For a skip-out re-entry, P65 and P66 were employed to handle the exit and entry parts of the skip.
In this case, because they were extending the re-entry but not actually skipping out, P66 was not invoked and instead P65 led directly to P67.
The crew were also warned that they would not be in a full-lift (heads-down) attitude when the entered P67.
The first program's acceleration subjected the astronauts to ; the second, to .
Before dawn on July 24, "Hornet" launched four Sea King helicopters and three Grumman E-1 Tracers.
Two of the E-1s were designated as "air boss" while the third acted as a communications relay aircraft.
Two of the Sea Kings carried divers and recovery equipment.
The third carried photographic equipment, and the fourth carried the decontamination swimmer and the flight surgeon.
At 16:44 UTC (05:44 local time) "Columbia"s drogue parachutes were deployed.
This was observed by the helicopters.
Seven minutes later "Columbia" struck the water forcefully east of Wake Island, south of Johnston Atoll, and from "Hornet", at .
During splashdown, "Columbia" landed upside down but was righted within ten minutes by flotation bags activated by the astronauts.
A diver from the Navy helicopter hovering above attached a sea anchor to prevent it from drifting.
Additional divers attached flotation collars to stabilize the module and positioned rafts for astronaut extraction.
The divers then passed biological isolation garments (BIGs) to the astronauts, and assisted them into the life raft.
Though the chance of bringing back pathogens from the lunar surface was considered remote, it was a possibility, and NASA took precautions at the recovery site.
Additionally, they were rubbed down with a sodium hypochlorite solution and "Columbia" wiped with Betadine to remove any lunar dust that might be present.
The astronauts were winched on board the recovery helicopter.
BIGs were worn until they reached isolation facilities on board the "Hornet".
The raft containing decontamination materials was intentionally sunk.
After touchdown on the "Hornet" at 17:53 UTC, the helicopter was lowered by the elevator into the hangar bay, where the astronauts walked the to the Mobile Quarantine Facility (MQF), where they would begin the Earth-based portion of their 21 days of quarantine.
This practice would continue for two more Apollo missions, Apollo 12 and Apollo 14, before the Moon was proven to be barren of life, and the quarantine process dropped.
Nixon welcomed the astronauts back to Earth.
He told them: "As a result of what you've done, the world has never been closer together before."
After Nixon departed, the "Hornet" was brought alongside the "Columbia", which was lifted aboard by the ship's crane, placed on a dolly and moved next to the MQF.
It was then attached to the MQF with a flexible tunnel, allowing the lunar samples, film, data tapes and other items to be removed.
"Hornet" returned to Pearl Harbor, where the MQF was loaded onto a Lockheed C-141 Starlifter and airlifted to the Manned Spacecraft Center.
The astronauts arrived at the Lunar Receiving Laboratory at 10:00 UTC on July 28.
"Columbia" was taken to Ford Island for deactivation, and its pyrotechnics made safe.
It was then taken to Hickham Air Force Base, from whence it was flown to Houston in a Douglas C-133 Cargomaster, reaching the Lunar Receiving Laboratory on July 30.
In accordance with the Extra-Terrestrial Exposure Law, a set of regulations promulgated by NASA on July 16 to codify its quarantine protocol, the astronauts continued in quarantine.
However, after three weeks in confinement (first in the Apollo spacecraft, then in their trailer on the "Hornet", and finally in the Lunar Receiving Laboratory), the astronauts were given a clean bill of health.
On August 10, 1969, the Interagency Committee on Back Contamination met in Atlanta and lifted the quarantine on the astronauts, on those who had joined them in quarantine (NASA physician William Carpentier and MQF project engineer John Hirasaki), and on "Columbia" itself.
Loose equipment from the spacecraft remained in isolation until the lunar samples were released for study.
On August 13, the three astronauts rode in parades in their honor in New York, Chicago, and Los Angeles.
On the same evening in Los Angeles there was an official state dinner to celebrate the flight, attended by members of Congress, 44 governors, the Chief Justice of the United States, and ambassadors from 83 nations at the Century Plaza Hotel.
Nixon and Agnew honored each astronaut with a presentation of the Presidential Medal of Freedom.
This celebration was the beginning of a 45-day "Giant Leap" tour that brought the astronauts to 25 foreign countries and included visits with prominent leaders such as Queen Elizabeth II of the United Kingdom.
Many nations honored the first human Moon landing with special features in magazines or by issuing Apollo 11 commemorative postage stamps or coins.
On September 16, 1969, the three astronauts spoke before a joint session of Congress.
They presented two US flags, one to the House of Representatives and the other to the Senate, that had been carried to the surface of the Moon with them.
The flag of American Samoa which was taken to the Moon by Apollo 11 is on display at the Jean P. Haydon Museum in Pago Pago, the capital of American Samoa.
The Command Module "Columbia" went on a tour of the United States, visiting 49 state capitals, the District of Columbia, and Anchorage, Alaska.
In 1971, it was transferred to the Smithsonian Institute, and was displayed at the National Air and Space Museum (NASM) in Washington, DC.
It was in the central "Milestones of Flight" exhibition hall in front of the Jefferson Drive entrance, sharing the main hall with other pioneering flight vehicles such as the "Wright Flyer", the "Spirit of St.
Louis", the Bell X-1, the North American X-15, and Mercury spacecraft "Friendship 7".
"Columbia" was moved in 2017 to the NASM Mary Baker Engen Restoration Hangar at the Steven F. Udvar-Hazy Center in Chantilly, Virginia, to be readied for a four-city tour titled "Destination Moon: The Apollo 11 Mission".
This included Space Center Houston from October 14, 2017 to March 18, 2018, the Saint Louis Science Center from April 14 to September 3, 2018, the Senator John Heinz History Center in Pittsburgh from September 29, 2018 to February 18, 2019, and the Seattle Museum of Flight from March 16 to September 2, 2019.
Armstrong's and Aldrin's space suits are displayed in the museum's "Apollo to the Moon" exhibit.
The quarantine trailer, the flotation collar and the flotation bags are in the Smithsonian's Steven F. Udvar-Hazy Center annex near Washington Dulles International Airport in Chantilly, Virginia, where they are on display along with a test lunar module.
The descent stage of the Lunar Module "Eagle" remains on the Moon.
In 2009, the Lunar Reconnaissance Orbiter (LRO) imaged the various Apollo landing sites on the surface of the Moon, for the first time with sufficient resolution to see the descent stages of the lunar modules, scientific instruments, and foot trails made by the astronauts.
The remains of the ascent stage lie at an unknown location on the lunar surface, after being abandoned and impacting the Moon.
In March 2012 a team of specialists financed by Amazon founder Jeff Bezos located the F-1 engines from the S-IC stage that launched Apollo 11 into space.
They were found below the Atlantic Ocean's surface through the use of advanced sonar scanning.
His team brought parts of two of the five engines to the surface.
In July 2013, a conservator discovered a serial number under the rust on one of the engines raised from the Atlantic, which NASA confirmed was from Apollo 11.
The S-IVB third stage which performed Apollo 11's trans-lunar injection remains in a solar orbit near to that of Earth.
The main repository for the Apollo Moon rocks is the Lunar Sample Laboratory Facility at the Lyndon B. Johnson Space Center in Houston, Texas.
For safekeeping, there is also a smaller collection stored at White Sands Test Facility near Las Cruces, New Mexico.
Most of the rocks are stored in nitrogen to keep them free of moisture.
They are handled only indirectly, using special tools.
Over 100 research laboratories around the world conduct studies of the samples, and approximately 500 samples are prepared and sent to investigators every year.
In November 1969, Nixon asked NASA to make up about 250 presentation Apollo 11 lunar sample displays for 135 nations, the fifty states of the United States and its possessions, and the United Nations.
Each display included Moon dust from Apollo 11.
The rice-sized particles were four small pieces of Moon soil weighing about 50 mg and were enveloped in a clear acrylic button about as big as a United States half dollar coin.
This acrylic button magnified the grains of lunar dust.
The Apollo 11 lunar sample displays were given out as goodwill gifts by Nixon in 1970.
The Passive Seismic Experiment ran until the command uplink failed on August 25, 1969.
The downlink failed on December 14, 1969. , the Lunar Laser Ranging experiment remains operational.
On July 15, 2009, Life.com released a photo gallery of previously unpublished photos of the astronauts taken by "Life" photographer Ralph Morse prior to the Apollo 11 launch.
From July 16 to 24, 2009, NASA streamed the original mission audio on its website in real time 40 years to the minute after the events occurred.
In addition, it is in the process of restoring the video footage and has released a preview of key moments.
In July 2010, air-to-ground voice recordings and film footage shot in Mission Control during the Apollo 11 powered descent and landing was re-synchronized and released for the first time.
The John F. Kennedy Presidential Library and Museum set up an Adobe Flash website that rebroadcasts the transmissions of Apollo 11 from launch to landing on the Moon.
On July 20, 2009, the crew of Armstrong, Aldrin, and Collins met with U.S.
President Barack Obama at the White House.
"We expect that there is, as we speak, another generation of kids out there who are looking up at the sky and are going to be the next Armstrong, Collins, and Aldrin", Obama said.
"We want to make sure that NASA is going to be there for them when they want to take their journey."
On August 7, 2009, an act of Congress awarded the three astronauts a Congressional Gold Medal, the highest civilian award in the United States.
The bill was sponsored by Florida Senator Bill Nelson and Florida Representative Alan Grayson.
A group of British scientists interviewed as part of the anniversary events reflected on the significance of the Moon landing:





</doc>
<doc id="663" url="https://en.wikipedia.org/wiki?curid=663" title="Apollo 8">
Apollo 8

Apollo 8, the second manned spaceflight mission in the United States Apollo space program, was launched on December 21, 1968, and became the first manned spacecraft to leave low Earth orbit, reach the Earth's Moon, orbit it and return safely to Earth.
The three-astronaut crew — Commander Frank Borman, Command Module Pilot James Lovell, and Lunar Module Pilot William Anders — became the first humans to travel beyond low Earth orbit; see Earth as a whole planet; enter the gravity well of another celestial body (Earth's moon); orbit another celestial body (Earth's moon); directly see the far side of the Moon with their own eyes; witness an Earthrise; escape the gravity of another celestial body (Earth's moon); and re-enter the gravitational well of Earth.
The 1968 mission, the third flight of the Saturn V rocket and that rocket's first crewed launch, was also the first human spaceflight launch from the Kennedy Space Center, Florida, located adjacent to Cape Canaveral Air Force Station.
Originally planned as a second Lunar Module/Command Module test in an elliptical medium Earth orbit in early 1969, the mission profile was changed in August 1968 to a more ambitious Command Module-only lunar orbital flight to be flown in December, because the Lunar Module was not yet ready to make its first flight.
This meant Borman's crew was scheduled to fly two to three months sooner than originally planned, leaving them a shorter time for training and preparation, thus placing more demands than usual on their time and discipline.
Apollo 8 took 68 hours (2.8 days) to travel the distance to the Moon.
It orbited ten times over the course of 20 hours, during which the crew made a Christmas Eve television broadcast where they read the first 10 verses from the Book of Genesis.
At the time, the broadcast was the most watched TV program ever.
Apollo 8's successful mission paved the way for Apollo 11 to fulfill U.S.
President John F. Kennedy's goal of landing a man on the Moon before the end of the 1960s.
The Apollo 8 astronauts returned to Earth on December 27, 1968, when their spacecraft splashed down in the Northern Pacific Ocean.
The crew members were named "Time" magazine's "Men of the Year" for 1968 upon their return.
Lovell was originally the CMP on the back-up crew, with Michael Collins as the prime crew's CMP.
However, Collins was replaced in July 1968, after suffering a cervical disc herniation that required surgery to repair.
This crew was unique among pre-shuttle era missions in that the commander was not the most experienced member of the crew, as Lovell had flown twice before, on Gemini VII and Gemini XII.
This was also the first case of the rarity of an astronaut who had commanded a spaceflight mission subsequently flying as a non-commander, as Lovell had previously commanded Gemini XII.
On a lunar mission, the Command Module Pilot (CMP) was assigned the role of navigator, while the Lunar Module Pilot (LMP) was assigned the role of flight engineer, responsible for monitoring all spacecraft systems, even if the flight didn't include a Lunar Module.
Edwin "Buzz" Aldrin was originally the backup LMP.
When Lovell was rotated to the prime crew, no one with experience on CSM-103 (the specific spacecraft used for the mission) was available, so Aldrin was moved to CMP and Fred Haise brought in as backup LMP.
Neil Armstrong went on to command Apollo 11, where Aldrin was returned to the LMP position and Collins was assigned as CMP.
Haise was rotated out of the crew and onto the backup crew of Apollo 11 as LMP.
The Earth-based mission control teams for Apollo 8 consisted of astronauts assigned to the support crew, as well as non-astronaut flight directors and their staffs.
The support crew members were not trained to fly the mission, but were able to stand in for astronauts in meetings and be involved in the minutiae of mission planning, while the prime and backup crews trained.
They also served as CAPCOMs during the mission.
For Apollo 8, these crew members included astronauts Michael Collins, John S. Bull, Vance D. Brand, Gerald P. Carr, and Ken Mattingly.
The mission control teams on Earth rotated in three shifts, each led by a flight director.
The directors for Apollo 8 included Clifford E. Charlesworth (Green team), Glynn Lunney (Black team), and Milton Windler (Maroon team).
The triangular shape of the insignia symbolizes the shape of the Apollo Command Module (CM).
It shows a red figure-8 looping around the Earth and Moon representing the mission number as well as the circumlunar nature of the mission.
On the red number 8 are the names of the three astronauts.
The initial design of the insignia was developed by Jim Lovell.
Lovell reportedly sketched the initial design while riding in the backseat of a T-38 flight from California to Houston, shortly after learning of the re-designation of the flight to become a lunar-orbital mission.
The graphic design of the insignia was done by Houston artist and animator William Bradley.
Apollo 4 and Apollo 6 had been "A" missions, unmanned tests of the Saturn V launch vehicle using an unmanned Block I production model of the Apollo Command and Service Module in Earth orbit.
, scheduled for October 1968, would be a manned Earth-orbit flight of the CSM, completing the objectives for Mission "C".
Further missions depended on the readiness of the Lunar Module.
Apollo 8 was planned as the "D" mission, to test the LM in a low Earth orbit in December 1968 by James McDivitt, David Scott and Russell Schweickart, while Borman's crew would fly the "E" mission, a more rigorous LM test in an elliptical medium Earth orbit as Apollo 9, in early 1969.
But production of the LM fell behind schedule, and when Apollo 8's LM arrived at the Kennedy Space Center in June 1968, significant defects were discovered, leading Grumman, the lead contractor for the LM, to predict that the first mission-ready LM would not be ready until at least February 1969.
This would mean delaying the "D" and subsequent missions, endangering the program's goal of a lunar landing before the end of 1969.
George Low, the Manager of the Apollo Spacecraft Program Office, proposed a solution in August to keep the program on track despite the LM delay.
Since the Command/Service Module (CSM) would be ready three months before the Lunar Module, a CSM-only mission could be flown in December 1968.
Instead of just repeating the "C" mission flight of Apollo 7, this CSM could be sent all the way to the Moon, with the possibility of entering a lunar orbit.
The new mission would also allow NASA to test lunar landing procedures that would otherwise have to wait until Apollo 10, the scheduled "F" mission.
This also meant that the medium Earth orbit "E" mission could be dispensed with.
The net result was that only the "D" mission had to be delayed.
Almost every senior manager at NASA agreed with this new mission, citing both confidence in the hardware and personnel, and the potential for a significant morale boost provided by a circumlunar flight.
The only person who needed some convincing was James E. Webb, the NASA administrator.
With the rest of his agency in support of the new mission, Webb eventually approved the mission change.
The mission was officially changed from a "D" mission to a "C-Prime" lunar-orbit mission, but was still referred to in press releases as an Earth-orbit mission at Webb's direction.
No public announcement was made about the change in mission until November 12, three weeks after Apollo 7's successful Earth-orbit mission and less than 40 days before launch.
With the change in mission for Apollo 8, Director of Flight Crew Operations Deke Slayton decided to swap the crews of the D and E missions.
This swap also meant a swap of spacecraft, requiring Borman's crew to use CSM-103, while McDivitt's crew would use CSM-104.
On September 9, the crew entered the simulators to begin their preparation for the flight.
By the time the mission flew, the crew had spent seven hours training for every actual hour of flight.
Although all crew members were trained in all aspects of the mission, it was necessary to specialize.
Borman, as commander, was given training on controlling the spacecraft during the re-entry.
Lovell was trained on navigating the spacecraft in case communication was lost with the Earth.
Anders was placed in charge of checking that the spacecraft was in working order.
Added pressure on the Apollo program to make its 1969 landing goal was provided by the Soviet Union's flight of some living creatures, including Russian tortoises, in a cislunar loop around the Moon on Zond 5 and return to Earth on September 21.
There was speculation within NASA and the press that they might be preparing to launch cosmonauts on a similar circumlunar mission before the end of 1968.
The Apollo 8 crew, now living in the crew quarters at Kennedy Space Center, received a visit from Charles Lindbergh and his wife, Anne Morrow Lindbergh, the night before the launch.
They talked about how, before his 1927 flight, Lindbergh had used a piece of string to measure the distance from New York City to Paris on a globe and from that calculated the fuel needed for the flight.
The total was a tenth of the amount that the Saturn V would burn every second.
The next day, the Lindberghs watched the launch of Apollo 8 from a nearby dune.
The Saturn V rocket used by Apollo 8 was designated SA-503, or the "03rd" model of the Saturn V ("5") Rocket to be used in the Saturn-Apollo ("SA") program.
When it was erected in the Vertical Assembly Building on December 20, 1967, it was thought that the rocket would be used for an unmanned Earth-orbit test flight carrying a boilerplate Command/Service Module.
Apollo 6 had suffered several major problems during its April 1968 flight, including severe pogo oscillation during its first stage, two second stage engine failures, and a third stage that failed to reignite in orbit.
Without assurances that these problems had been rectified, NASA administrators could not justify risking a manned mission until additional unmanned test flights proved that the Saturn V was ready.
Teams from the Marshall Space Flight Center (MSFC) went to work on the problems.
Of primary concern was the pogo oscillation, which would not only hamper engine performance, but could exert significant g-forces on a crew.
A task force of contractors, NASA agency representatives, and MSFC researchers concluded that the engines vibrated at a frequency similar to the frequency at which the spacecraft itself vibrated, causing a resonance effect that induced oscillations in the rocket.
A system using helium gas to absorb some of these vibrations was installed.
Of equal importance was the failure of three engines during flight.
Researchers quickly determined that a leaking hydrogen fuel line ruptured when exposed to vacuum, causing a loss of fuel pressure in engine two.
When an automatic shutoff attempted to close the liquid hydrogen valve and shut down engine two, it accidentally shut down engine three's liquid oxygen due to a miswired connection.
As a result, engine three failed within one second of engine two's shutdown.
Further investigation revealed the same problem for the third-stage engine—a faulty igniter line.
The team modified the igniter lines and fuel conduits, hoping to avoid similar problems on future launches.
The teams tested their solutions in August 1968 at the Marshall Space Flight Center.
A Saturn stage IC was equipped with shock absorbing devices to demonstrate the team's solution to the problem of pogo oscillation, while a Saturn Stage II was retrofitted with modified fuel lines to demonstrate their resistance to leaks and ruptures in vacuum conditions.
Once NASA administrators were convinced that the problems were solved, they gave their approval for a manned mission using SA-503.
The Apollo 8 spacecraft was placed on top of the rocket on September 21 and the rocket made the slow 3-mile (5 km) journey to the launch pad on October 9.
Testing continued all through December until the day before launch, including various levels of readiness testing from December 5 through 11.
Final testing of modifications to address the problems of pogo oscillation, ruptured fuel lines, and bad igniter lines took place on December 18, a mere three days before the scheduled launch.
As the first manned spacecraft to orbit more than one celestial body, Apollo 8's profile had two different sets of orbital parameters, separated by a translunar injection maneuver.
Apollo lunar missions would begin with a nominal circular Earth parking orbit.
Apollo 8 was launched into an initial orbit with an apogee of and a perigee of , with an inclination of 32.51° to the Equator, and an orbital period of 88.19 minutes.
Propellant venting increased the apogee by over the 2 hours, 44 minutes and 30 seconds spent in the parking orbit.
This was followed by a Trans-Lunar Injection (TLI) burn of the S-IVB third stage for 318 seconds, accelerating the Command/Service Module and LM test article from an orbital velocity of to the injection velocity of , which set a record for the highest speed, relative to Earth, that humans had ever traveled.
This speed was slightly less than the Earth's escape velocity of , but put Apollo 8 into an elongated elliptical Earth orbit, to a point where the Moon's gravity would capture it.
The standard lunar orbit for Apollo missions was planned as a nominal circular orbit above the Moon's surface.
Initial lunar orbit insertion was an ellipse with a perilune of and an apolune of , at an inclination of 12° from the lunar equator.
This was then circularized at by , with an orbital period of 128.7 minutes.
The effect of lunar mass concentrations ("masscons") on the orbit was found to be greater than initially predicted; over the course of the ten lunar orbits lasting twenty hours, the orbital distance was perturbated to by .
Apollo 8 achieved a maximum distance from Earth of .
Apollo 8 launched at 7:51:00 a.m.
Eastern Standard Time on December 21, 1968, using the Saturn V's three stages to achieve Earth orbit.
The S-IC first stage impacted the Atlantic Ocean at and the S-II second stage at .
The S-IVB third stage injected the craft into Earth orbit, but remained attached to later perform the trans-lunar injection (TLI) burn that put the spacecraft on a trajectory to the Moon.
Once the vehicle reached Earth orbit, both the crew and Houston flight controllers spent the next 2 hours and 38 minutes checking that the spacecraft was in proper working order and ready for TLI.
The proper operation of the S-IVB third stage of the rocket was crucial: in the last unmanned test, it had failed to re-ignite for TLI.
During the flight, three fellow astronauts served on the ground as Capsule Communicators (usually referred to as "CAPCOMs") on a rotating schedule.
The CAPCOMs were the only people who regularly communicated with the crew.
Michael Collins was the first CAPCOM on duty and at 2 hours, 27 minutes and 22 seconds after launch radioed, "Apollo 8.
You are Go for TLI."
This communication signified that Mission Control had given official permission for Apollo 8 to go to the Moon.
Over the next 12 minutes before the TLI burn, the Apollo 8 crew continued to monitor the spacecraft and the S-IVB.
The engine ignited on time and performed the TLI burn perfectly.
After the S-IVB had performed its required tasks, it was jettisoned.
The crew then rotated the spacecraft to take some photographs of the spent stage and then practiced flying in formation with it.
As the crew rotated the spacecraft, they had their first views of the Earth as they moved away from it.
This marked the first time humans could view the whole Earth at once.
Borman became worried that the S-IVB was staying too close to the Command/Service Module and suggested to Mission Control that the crew perform a separation maneuver.
Mission Control first suggested pointing the spacecraft towards Earth and using the Reaction Control System (RCS) thrusters on the Service Module (SM) to add away from the Earth, but Borman did not want to lose sight of the S-IVB.
After discussion, the crew and Mission Control decided to burn in this direction, but at instead.
These discussions put the crew an hour behind their flight plan.
Five hours after launch, Mission Control sent a command to the S-IVB booster to vent its remaining fuel through its engine bell to change the booster's trajectory.
This S-IVB would then pass the Moon and enter into a solar orbit, posing no further hazard to Apollo 8.
The S-IVB subsequently went into a solar orbit with an inclination of 23.47° from the plane of the ecliptic, and an orbital period of 340.80 days.
After the insertion into trans-Lunar orbit, the Saturn IVB third stage became a object.
It will continue to orbit the Sun for many years.
The Apollo 8 crew were the first humans to pass through the Van Allen radiation belts, which extend up to from Earth.
Scientists predicted that passing through the belts quickly at the spacecraft's high speed would cause a radiation dosage of no more than a chest X-ray, or 1 milligray (during a year, the average human receives a dose of 2 to 3 mGy).
To record the actual radiation dosages, each crew member wore a Personal Radiation Dosimeter that transmitted data to Earth as well as three passive film dosimeters that showed the cumulative radiation experienced by the crew.
By the end of the mission, the crew experienced an average radiation dose of 1.6 mGy.
Jim Lovell's main job as Command Module Pilot was as navigator.
Although Mission Control performed all the actual navigation calculations, it was necessary to have a crew member serving as navigator so that the crew could return to Earth in case of loss of communication with Mission Control.
Lovell navigated by star sightings using a sextant built into the spacecraft, measuring the angle between a star and the Earth's (or the Moon's) horizon.
This task was difficult, because a large cloud of debris around the spacecraft, formed by the venting S-IVB, made it hard to distinguish the stars.
By seven hours into the mission, the crew was about one hour and 40 minutes behind flight plan, because of the problems in moving away from the S-IVB and Lovell's obscured star sightings.
The crew now placed the spacecraft into Passive Thermal Control (PTC), also called "barbecue roll", in which the spacecraft rotated about once per hour around its long axis to ensure even heat distribution across the surface of the spacecraft.
In direct sunlight, the spacecraft could be heated to over while the parts in shadow would be .
These temperatures could cause the heat shield to crack and propellant lines to burst.
Because it was impossible to get a perfect roll, the spacecraft swept out a cone as it rotated.
The crew had to make minor adjustments every half hour as the cone pattern got larger and larger.
The first mid-course correction came 11 hours into the flight.
Testing on the ground had shown that the Service Propulsion System (SPS) engine had a small chance of exploding when burned for long periods unless its combustion chamber was "coated" first.
Burning the engine for a short period would accomplish coating.
This first correction burn was only 2.4 seconds and added about velocity prograde (in the direction of travel).
This change was less than the planned , because of a bubble of helium in the oxidizer lines, which caused unexpectedly low propellant pressure.
The crew had to use the small RCS thrusters to make up the shortfall.
Two later planned mid-course corrections were canceled because the Apollo 8 trajectory was found to be perfect.
Eleven hours into the flight, the crew had been awake for more than 16 hours.
Before launch, NASA had decided that at least one crew member should be awake at all times to deal with problems that might arise.
Borman started the first sleep shift, but found sleeping difficult because of the constant radio chatter and mechanical noises.
About an hour after starting his sleep shift, Borman obtained permission from ground control to take a Seconal sleeping pill.
The pill had little effect.
Borman eventually fell asleep, and then awoke feeling ill.
He vomited twice and had a bout of diarrhea; this left the spacecraft full of small globules of vomit and feces, which the crew cleaned up as well as they could.
Borman initially did not want everyone to know about his medical problems, but Lovell and Anders wanted to inform Mission Control.
The crew decided to use the Data Storage Equipment (DSE), which could tape voice recordings and telemetry and dump them to Mission Control at high speed.
After recording a description of Borman's illness they asked Mission Control to check the recording, stating that they "would like an evaluation of the voice comments".
The Apollo 8 crew and Mission Control medical personnel held a conference using an unoccupied second-floor control room (there were two identical control rooms in Houston, on the second and third floors, only one of which was used during a mission).
The conference participants concluded that there was little to worry about and that Borman's illness was either a 24-hour flu, as Borman thought, or a reaction to the sleeping pill.
Researchers now believe that he was suffering from space-adaptation syndrome, which affects about a third of astronauts during their first day in space as their vestibular system adapts to weightlessness.
Space-adaptation syndrome had not occurred on previous spacecraft (Mercury and Gemini), because those astronauts couldn't move freely in the small cabins of those spacecraft.
The increased cabin space in the Apollo Command Module afforded astronauts greater freedom of movement, contributing to symptoms of space sickness for Borman and, later, astronaut Russell Schweickart during Apollo 9.
The cruise phase was a relatively uneventful part of the flight, except for the crew checking that the spacecraft was in working order and that they were on course.
During this time, NASA scheduled a television broadcast at 31 hours after launch.
The Apollo 8 crew used a 2 kg camera that broadcast in black-and-white only, using a Vidicon tube.
The camera had two lenses, a very wide-angle (160°) lens, and a telephoto (9°) lens.
During this first broadcast, the crew gave a tour of the spacecraft and attempted to show how the Earth appeared from space.
However, difficulties aiming the narrow-angle lens without the aid of a monitor to show what it was looking at made showing the Earth impossible.
Additionally, the Earth image became saturated by any bright source without proper filters.
In the end, all the crew could show the people watching back on Earth was a bright blob.
After broadcasting for 17 minutes, the rotation of the spacecraft took the high-gain antenna out of view of the receiving stations on Earth and they ended the transmission with Lovell wishing his mother a happy birthday.
By this time, the crew had completely abandoned the planned sleep shifts.
Lovell went to sleep 32½ hours into the flight—3½ hours before he had planned to.
A short while later, Anders also went to sleep after taking a sleeping pill.
The crew was unable to see the Moon for much of the outward cruise.
Two factors made the Moon almost impossible to see from inside the spacecraft: three of the five windows fogging up due to out-gassed oils from the silicone sealant, and the attitude required for the PTC.
It was not until the crew had gone behind the Moon that they would be able to see it for the first time.
Apollo 8 made a second television broadcast at 55 hours into the flight.
This time, the crew rigged up filters meant for the still cameras so they could acquire images of the Earth through the telephoto lens.
Although difficult to aim, as they had to maneuver the entire spacecraft, the crew was able to broadcast back to Earth the first television pictures of the Earth.
The crew spent the transmission describing the Earth and what was visible and the colors they could see.
The transmission lasted 23 minutes.
At about 55 hours and 40 minutes into the flight, the crew of Apollo 8 became the first humans to enter the gravitational sphere of influence of another celestial body.
In other words, the effect of the Moon's gravitational force on Apollo 8 became stronger than that of the Earth.
At the time it happened, Apollo 8 was from the Moon and had a speed of relative to the Moon.
This historic moment was of little interest to the crew since they were still calculating their trajectory with respect to the launch pad at Kennedy Space Center.
They would continue to do so until they performed their last mid-course correction, switching to a reference frame based on ideal orientation for the second engine burn they would make in lunar orbit.
It was only 13 hours until they would be in lunar orbit.
The last major event before Lunar Orbit Insertion (LOI) was a second mid-course correction.
It was in retrograde (against direction of travel) and slowed the spacecraft down by , effectively lowering the closest distance that the spacecraft would pass the moon.
At exactly 61 hours after launch, about from the Moon, the crew burned the RCS for 11 seconds.
They would now pass from the lunar surface.
At 64 hours into the flight, the crew began to prepare for Lunar Orbit Insertion-1 (LOI-1).
This maneuver had to be performed perfectly, and due to orbital mechanics had to be on the far side of the Moon, out of contact with the Earth.
After Mission Control was polled for a "go/no go" decision, the crew was told at 68 hours, they were Go and "riding the best bird we can find".
Lovell replied, "We'll see you on the other side", and for the first time in history, humans travelled behind the Moon and out of radio contact with the Earth.
With 10 minutes before the LOI-1, the crew began one last check of the spacecraft systems and made sure that every switch was in the correct place.
At that time, they finally got their first glimpses of the Moon.
They had been flying over the unlit side, and it was Lovell who saw the first shafts of sunlight obliquely illuminating the lunar surface.
The LOI burn was only two minutes away, so the crew had little time to appreciate the view.
The SPS ignited at 69 hours, 8 minutes, and 16 seconds after launch and burned for 4 minutes and 7 seconds, placing the Apollo 8 spacecraft in orbit around the Moon.
The crew described the burn as being the longest four minutes of their lives.
If the burn had not lasted exactly the correct amount of time, the spacecraft could have ended up in a highly elliptical lunar orbit or even flung off into space.
If it lasted too long they could have struck the Moon.
After making sure the spacecraft was working, they finally had a chance to look at the Moon, which they would orbit for the next 20 hours.
On Earth, Mission Control continued to wait.
If the crew had not burned the engine or the burn had not lasted the planned length of time, the crew would appear early from behind the Moon.
However, this time came and went without Apollo 8 reappearing.
Exactly at the calculated moment, the signal was received from the spacecraft, indicating it was in a orbit about the Moon.
After reporting on the status of the spacecraft, Lovell gave the first description of what the lunar surface looked like:

Lovell continued to describe the terrain they were passing over.
One of the crew's major tasks was reconnaissance of planned future landing sites on the Moon, especially one in Mare Tranquillitatis that would be the Apollo 11 landing site.
The launch time of Apollo 8 had been chosen to give the best lighting conditions for examining the site.
A film camera had been set up in one of the spacecraft windows to record a frame every second of the Moon below.
Bill Anders spent much of the next 20 hours taking as many photographs as possible of targets of interest.
By the end of the mission the crew had taken 700 photographs of the Moon and 150 of the Earth.
Throughout the hour that the spacecraft was in contact with Earth, Borman kept asking how the data for the SPS looked.
He wanted to make sure that the engine was working and could be used to return early to the Earth if necessary.
He also asked that they receive a "go/no go" decision before they passed behind the Moon on each orbit.
As they reappeared for their second pass in front of the Moon, the crew set up the equipment to broadcast a view of the lunar surface.
Anders described the craters that they were passing over.
At the end of this second orbit they performed the 11-second LOI-2 burn of the SPS to circularize the orbit to .
Through the next two orbits, the crew continued to keep check of the spacecraft and to observe and photograph the Moon.
During the third pass, Borman read a small prayer for his church.
He had been scheduled to participate in a service at St.
Christopher's Episcopal Church near Seabrook, Texas, but due to the Apollo 8 flight he was unable to.
A fellow parishioner and engineer at Mission Control, Rod Rose, suggested that Borman read the prayer which could be recorded and then replayed during the service.
When the spacecraft came out from behind the Moon for its fourth pass across the front, the crew witnessed "Earthrise" for the first time in human history (NASA's Lunar Orbiter 1 took the very first picture of an Earthrise from the vicinity of the Moon, on August 23, 1966).
Anders saw the Earth emerging from behind the lunar horizon, and then called in excitement to the others, taking a black-and-white photograph as he did so.
Anders asked Lovell for a color film and then took "Earthrise", a more famous color photo, later picked by "Life" magazine as one of its hundred photos of the century.
Due to the synchronous rotation of the Moon about the Earth, Earthrise is not generally visible from the lunar surface.
Earthrise is generally only visible when orbiting the Moon, other than at selected places near the Moon's limb, where libration carries the Earth slightly above and below the lunar horizon.
Anders continued to take photographs while Lovell assumed control of the spacecraft so Borman could rest.
Despite the difficulty resting in the cramped and noisy spacecraft, Borman was able to sleep for two orbits, awakening periodically to ask questions about their status.
Borman awoke fully, however, when he started to hear his fellow crew members make mistakes.
They were beginning to not understand questions and would have to ask for the answers to be repeated.
Borman realized that everyone was extremely tired from not having a good night's sleep in over three days.
He ordered Anders and Lovell to get some sleep and that the rest of the flight plan regarding observing the Moon be scrubbed.
At first Anders protested saying that he was fine, but Borman would not be swayed.
At last Anders agreed as long as Borman would set up the camera to continue to take automatic shots of the Moon.
Borman also remembered that there was a second television broadcast planned, and with so many people expected to be watching he wanted the crew to be alert.
For the next two orbits Anders and Lovell slept while Borman sat at the helm.
On subsequent Apollo missions, crews would avoid this situation by sleeping on the same schedule.
As they rounded the Moon for the ninth time, the second television transmission began.
Borman introduced the crew, followed by each man giving his impression of the lunar surface and what it was like to be orbiting the Moon.
Borman described it as being "a vast, lonely, forbidding expanse of nothing".
Then, after talking about what they were flying over, Anders said that the crew had a message for all those on Earth.
Each man on board read a section from the Biblical creation story from the Book of Genesis.
Borman finished the broadcast by wishing a Merry Christmas to everyone on Earth.
His message appeared to sum up the feelings that all three crewmen had from their vantage point in lunar orbit.
Borman said, "And from the crew of Apollo 8, we close with good night, good luck, a Merry Christmas and God bless all of you—all of you on the good Earth."
The only task left for the crew at this point was to perform the Trans-Earth Injection (TEI), which was scheduled for 2½ hours after the end of the television transmission.
The TEI was the most critical burn of the flight, as any failure of the SPS to ignite would strand the crew in lunar orbit, with little hope of escape.
As with the previous burn, the crew had to perform the maneuver above the far side of the Moon, out of contact with Earth.
The burn occurred exactly on time.
The spacecraft telemetry was reacquired as it re-emerged from behind the Moon at 89 hours, 28 minutes, and 39 seconds, the exact time calculated.
When voice contact was regained, Lovell announced, "Please be informed, there is a Santa Claus", to which Ken Mattingly, the current CAPCOM, replied, "That's affirmative, you are the best ones to know."
The spacecraft began its journey back to Earth on December 25, Christmas Day.
Later, Lovell used some otherwise idle time to do some navigational sightings, maneuvering the module to view various stars by using the computer keyboard.
However, he accidentally erased some of the computer's memory, which caused the Inertial Measurement Unit (IMU) to think the module was in the same relative position it had been in before lift-off and fire the thrusters to "correct" the module's attitude.
Once the crew realized why the computer had changed the module's attitude, they realized they would have to re-enter data that would tell the computer its real position.
It took Lovell ten minutes to figure out the right numbers, using the thrusters to get the stars Rigel and Sirius aligned, and another 15 minutes to enter the corrected data into the computer.
Sixteen months later, Lovell would once again have to perform a similar manual re-alignment, under more critical conditions, during the Apollo 13 mission, after that module's IMU had to be turned off to conserve energy.
In his 1994 book, "Lost Moon: The Perilous Voyage of Apollo 13", Lovell wrote, "My training [on Apollo 8] came in handy!"
In that book he dismissed the incident as a "planned experiment", requested by the ground crew.
In subsequent interviews Lovell has acknowledged that the incident was an accident, caused by his mistake.
The cruise back to Earth was mostly a time for the crew to relax and monitor the spacecraft.
As long as the trajectory specialists had calculated everything correctly, the spacecraft would re-enter two-and-half days after TEI and splash down in the Pacific.
On Christmas afternoon, the crew made their fifth television broadcast.
This time they gave a tour of the spacecraft, showing how an astronaut lived in space.
When they finished broadcasting they found a small present from Deke Slayton in the food locker: a real turkey dinner with stuffing, in the same kind of pack that the troops in Vietnam received.
Another Slayton surprise was a gift of three miniature bottles of brandy, that Borman ordered the crew to leave alone until after they landed.
They remained unopened, even years after the flight.
There were also small presents to the crew from their wives.
The next day, at about 124 hours into the mission, the sixth and final TV transmission showed the mission's best video images of the earth, in a four-minute broadcast.
After two uneventful days the crew prepared for re-entry.
The computer would control the re-entry and all the crew had to do was put the spacecraft in the correct attitude, blunt end forward.
If the computer broke down, Borman would take over.
Once the Command Module was separated from the Service Module, the astronauts were committed to re-entry.
Six minutes before they hit the top of the atmosphere, the crew saw the Moon rising above the Earth's horizon, just as had been predicted by the trajectory specialists.
As they hit the thin outer atmosphere they noticed it was becoming hazy outside as glowing plasma formed around the spacecraft.
The spacecraft started slowing down and the deceleration peaked at 6 g (59 m/s).
With the computer controlling the descent by changing the attitude of the spacecraft, Apollo 8 rose briefly like a skipping stone before descending to the ocean.
At the drogue parachute stabilized the spacecraft and was followed at by the three main parachutes.
The spacecraft splashdown position was officially reported as in the North Pacific Ocean south of Hawaii.
When it hit the water, the parachutes dragged the spacecraft over and left it upside down, in what was termed Stable 2 position.
About six minutes later the Command Module was righted into its normal apex-up splashdown orientation by the inflatable bag uprighting system.
As they were buffeted by a swell, Borman was sick, waiting for the three flotation balloons to right the spacecraft.
It was 43 minutes after splashdown before the first frogman from arrived, as the spacecraft had landed before sunrise.
Forty-five minutes later, the crew was safe on the deck of the aircraft carrier.
Apollo 8 came at the end of 1968, a year that had seen much upheaval in the United States and most of the world.
Even though the year saw political assassinations, political unrest in the streets of Europe and America, and the Prague Spring, "Time" magazine chose the crew of Apollo 8 as its Men of the Year for 1968, recognizing them as the people who most influenced events in the preceding year.
They had been the first people ever to leave the gravitational influence of the Earth and orbit another celestial body.
They had survived a mission that even the crew themselves had rated as only having a fifty-fifty chance of fully succeeding.
The effect of Apollo 8 can be summed up by a telegram from a stranger, received by Borman after the mission, that simply stated, "Thank you Apollo 8.
You saved 1968."
One of the most famous aspects of the flight was the Earthrise picture that was taken as they came around for their fourth orbit of the Moon.
This was the first time that humans had taken such a picture while actually behind the camera, and it has been credited with a role in inspiring the first Earth Day in 1970.
It was selected as the first of "Life" magazine's "100 Photographs That Changed the World".
Apollo 11 astronaut Michael Collins said, "Eight's momentous historic significance was foremost"; while space historian Robert K. Poole saw Apollo 8 as the most historically significant of all the Apollo missions.
The mission was the most widely covered by the media since the first American orbital flight, Mercury-Atlas 6 by John Glenn in 1962.
There were 1200 journalists covering the mission, with the BBC coverage being broadcast in 54 countries in 15 different languages.
The Soviet newspaper "Pravda" featured a quote from Boris Nikolaevich Petrov, Chairman of the Soviet Interkosmos program, who described the flight as an "outstanding achievement of American space sciences and technology".
It is estimated that a quarter of the people alive at the time saw—either live or delayed—the Christmas Eve transmission during the ninth orbit of the Moon.
The Apollo 8 broadcasts won an Emmy Award, the highest honor given by the Academy of Television Arts & Sciences.
Madalyn Murray O'Hair, an atheist, later caused controversy by bringing a lawsuit against NASA over the reading from Genesis.
O'Hair wished the courts to ban American astronauts—who were all government employees—from public prayer in space.
Though the case was rejected by the Supreme Court of the United States for lack of jurisdiction, it caused NASA to be skittish about the issue of religion throughout the rest of the Apollo program.
Buzz Aldrin, on Apollo 11, self-communicated Presbyterian Communion on the surface of the Moon after landing; he refrained from mentioning this publicly for several years, and only obliquely referred to it at the time.
In 1969, the United States Postal Service issued a postage stamp (Scott catalogue #1371) commemorating the Apollo 8 flight around the Moon.
The stamp featured a detail of the famous photograph of the Earthrise over the Moon taken by Anders on Christmas Eve, and the words, "In the beginning God ..." Just 18 days after the crew's return to Earth, they were featured during the 1969 Super Bowl pre-game show reciting the Pledge of Allegiance, before the national anthem was performed by Anita Bryant.
In January 1970, the spacecraft was delivered to Osaka, Japan, for display in the U.S.
pavilion at Expo '70.
It is now displayed at the Chicago Museum of Science and Industry, along with a collection of personal items from the flight donated by Lovell and the space suit worn by Frank Borman.
Jim Lovell's Apollo 8 space suit is on public display in the Visitor Center at NASA's Glenn Research Center.
Bill Anders's space suit is on display at the Science Museum in London, United Kingdom.
Apollo 8's historic mission has been shown and referred to in several forms, both documentary and fiction.
The various television transmissions and 16 mm footage shot by the crew of Apollo 8 were compiled and released by NASA in the 1969 documentary "Debrief: Apollo 8", hosted by Burgess Meredith.
In addition, Spacecraft Films released, in 2003, a three-disc DVD set containing all of NASA's TV and 16 mm film footage related to the mission, including all TV transmissions from space, training and launch footage, and motion pictures taken in flight.
Parts of the Apollo 8 mission can be seen in the 1989 documentary "For All Mankind", which won the Grand Jury Prize Documentary at the Sundance Film Festival.
The television series "American Experience" aired a documentary, "Race to the Moon", in 2005 during season 18.
The Apollo 8 mission was well covered in the 2007 British documentary "In the Shadow of the Moon".
Parts of the mission are dramatized in the 1998 miniseries "From the Earth to the Moon" episode "1968".
The S-IVB stage of Apollo 8 was also portrayed as the location of an alien device in the 1970 "UFO" episode "Conflict".
Apollo 8's Lunar Orbit Insertion One was chronicled with actual recordings in the song "The Other Side", on the album The Race for Space, by the band Public Service Broadcasting.
</doc>
<doc id="664" url="https://en.wikipedia.org/wiki?curid=664" title="Astronaut">
Astronaut

An astronaut or cosmonaut is a person trained by a human spaceflight program to command, pilot, or serve as a crew member of a spacecraft.
Although generally reserved for professional space travelers, the terms are sometimes applied to anyone who travels into space, including scientists, politicians, journalists, and tourists.
Until 2002, astronauts were sponsored and trained exclusively by governments, either by the military or by civilian space agencies.
With the suborbital flight of the privately funded SpaceShipOne in 2004, a new category of astronaut was created: the commercial astronaut.
The criteria for what constitutes human spaceflight vary.
The Fédération Aéronautique Internationale (FAI) Sporting Code for astronautics recognizes only flights that exceed an altitude of.
In the United States, professional, military, and commercial astronauts who travel above an altitude of are awarded astronaut wings.
, a total of 552 people from 36 countries have reached or more in altitude, of which 549 reached low Earth orbit or beyond.
Of these, 24 people have traveled beyond low Earth orbit, either to lunar orbit, the lunar surface, or, in one case, a loop around the Moon.
Three of the 24–Jim Lovell, John Young and Eugene Cernan–did so twice.
The three current astronauts who have flown without reaching low Earth orbit are spaceplane pilots Joe Walker, Mike Melvill, and Brian Binnie, who participated in suborbital missions.
, under the U.S.
definition, 558 people qualify as having reached space, above altitude.
Of eight X-15 pilots who exceeded in altitude, only one exceeded 100 kilometers (about 62 miles).
Space travelers have spent over 41,790 man-days (114.5 man-years) in space, including over 100 astronaut-days of spacewalks.
, the man with the longest cumulative time in space is Gennady Padalka, who has spent 879 days in space.
Peggy A. Whitson holds the record for the most time in space by a woman, 377 days.
In 1959, when both the United States and Soviet Union were planning, but had yet to launch humans into space, NASA Administrator T. Keith Glennan and his Deputy Administrator, Dr. Hugh Dryden, discussed whether spacecraft crew members should be called "astronauts" or "cosmonauts".
Dryden preferred "cosmonaut", on the grounds that flights would occur in the "cosmos" (near space), while the "astro" prefix suggested flight to the stars.
Most NASA Space Task Group members preferred "astronaut", which survived by common usage as the preferred American term.
When the Soviet Union launched the first man into space, Yuri Gagarin in 1961, they chose a term which anglicizes to "cosmonaut".
In English-speaking nations, a professional space traveler is called an "astronaut".
The term derives from the Greek words "ástron" (ἄστρον), meaning "star", and "nautes" (ναύτης), meaning "sailor".
The first known use of the term "astronaut" in the modern sense was by Neil R. Jones in his 1930 short story "The Death's Head Meteor".
The word itself had been known earlier; for example, in Percy Greg's 1880 book "Across the Zodiac", "astronaut" referred to a spacecraft.
In "Les Navigateurs de l'Infini" (1925) by J.-H.
Rosny aîné, the word "astronautique" (astronautic) was used.
The word may have been inspired by "aeronaut", an older term for an air traveler first applied in 1784 to balloonists.
An early use of "astronaut" in a non-fiction publication is Eric Frank Russell's poem "The Astronaut", appearing in the November 1934 "Bulletin of the British Interplanetary Society".
The first known formal use of the term astronautics in the scientific community was the establishment of the annual International Astronautical Congress in 1950, and the subsequent founding of the International Astronautical Federation the following year.
NASA applies the term astronaut to any crew member aboard NASA spacecraft bound for Earth orbit or beyond.
NASA also uses the term as a title for those selected to join its Astronaut Corps.
The European Space Agency similarly uses the term astronaut for members of its Astronaut Corps.
By convention, an astronaut employed by the Russian Federal Space Agency (or its Soviet predecessor) is called a "cosmonaut" in English texts.
The word is an anglicisation of the Russian word "kosmonavt" (, ), one who works in space outside the Earth's atmosphere, a space traveler, which derives from the Greek words "kosmos" (κόσμος), meaning "universe", and "nautes" (ναύτης), meaning "sailor".
Other countries of the former Eastern Bloc use variations of the Russian word "kosmonavt", such as the Polish "kosmonauta".
Coinage of the term "kosmonavt" has been credited to Soviet aeronautics pioneer Mikhail Tikhonravov (1900–1974).
The first cosmonaut was Soviet Air Force pilot Yuri Gagarin, also the first person in space.
Valentina Tereshkova, a Russian factory worker, was the first woman in space, as well as the first civilian among the Soviet cosmonaut or NASA astronaut corps to make a spaceflight.
On March 14, 1995, Norman Thagard became the first American to ride to space on board a Russian launch vehicle, and thus became the first "American cosmonaut".
"Yǔ háng yuán" (, "Space-universe navigating personnel") is used for astronauts and cosmonauts in general, while "Hángtiān yuán" (, "navigating outer space personnel") is used for Chinese astronauts.
Here, "Hángtiān" () is strictly defined as the navigation of outer space within the local star system, i.e. Solar system.
The phrase "tài kōng rén" (, "spaceman") is often used in Hong Kong and Taiwan.
The term "taikonaut" is used by some English-language news media organizations for professional space travelers from China.
The word has featured in the Longman and Oxford English dictionaries, the latter of which describes it as "a hybrid of the Chinese term "taikong" (space) and the Greek "naut" (sailor)"; the term became more common in 2003 when China sent its first astronaut Yang Liwei into space aboard the "Shenzhou 5" spacecraft.
This is the term used by Xinhua News Agency in the English version of the Chinese "People's Daily" since the advent of the Chinese space program.
The origin of the term is unclear; as early as May 1998, Chiew Lee Yih () from Malaysia, used it in newsgroups.
With the rise of space tourism, NASA and the Russian Federal Space Agency agreed to use the term "spaceflight participant" to distinguish those space travelers from professional astronauts on missions coordinated by those two agencies.
While no nation other than the Russian Federation (and previously the former Soviet Union), the United States, and China have launched a manned spacecraft, several other nations have sent people into space in cooperation with one of these countries.
Inspired partly by these missions, other synonyms for astronaut have entered occasional English usage.
For example, the term "spationaut" (French spelling: "spationaute") is sometimes used to describe French space travelers, from the Latin word "spatium" for "space", the Malay term "angkasawan" was used to describe participants in the Angkasawan program, and the Indian Space Research Organisation hope to launch a spacecraft in 2022 that would carry "vyomanauts", coined from the Sanskrit word for space.
In Finland, the NASA astronaut Timothy Kopra, a Finnish American, has sometimes been referred to as "sisunautti", from the Finnish word "sisu".
The first human in space was Soviet Yuri Gagarin, who was launched on April 12, 1961, aboard Vostok 1 and orbited around the Earth for 108 minutes.
The first woman in space was Soviet Valentina Tereshkova, who launched on June 16, 1963, aboard Vostok 6 and orbited Earth for almost three days.
Alan Shepard became the first American and second person in space on May 5, 1961, on a 15-minute sub-orbital flight.
The first American to orbit the Earth was John Glenn, aboard Friendship 7 on February 20, 1962.
The first American woman in space was Sally Ride, during Space Shuttle Challenger's mission STS-7, on June 18, 1983.
In 1992 Mae Jemison became the first African American woman to travel in space aboard STS-47.
Cosmonaut Alexei Leonov was the first person to conduct an extravehicular activity (EVA), (commonly called a "spacewalk"), on March 18, 1965, on the Soviet Union's Voskhod 2 mission.
This was followed two and a half months later by astronaut Ed White who made the first American EVA on NASA's Gemini 4 mission.
The first manned mission to orbit the Moon, "Apollo 8", included American William Anders who was born in Hong Kong, making him the first Asian-born astronaut in 1968.
The Soviet Union, through its Intercosmos program, allowed people from other "socialist" (i.e. Warsaw Pact and other Soviet-allied) countries to fly on its missions, with the notable exception of France participating in Soyuz TM-7.
An example is Czechoslovak Vladimír Remek, the first cosmonaut from a country other than the Soviet Union or the United States, who flew to space in 1978 on a Soyuz-U rocket.
On July 23, 1980, Pham Tuan of Vietnam became the first Asian in space when he flew aboard Soyuz 37.
Also in 1980, Cuban Arnaldo Tamayo Méndez became the first person of Hispanic and black African descent to fly in space, and in 1983, Guion Bluford became the first African American to fly into space.
In April 1985, Taylor Wang became the first ethnic Chinese person in space.
The first person born in Africa to fly in space was Patrick Baudry (France), in 1985.
In 1985, Saudi Arabian Prince Sultan Bin Salman Bin AbdulAziz Al-Saud became the first Arab Muslim astronaut in space.
In 1988, Abdul Ahad Mohmand became the first Afghan to reach space, spending nine days aboard the Mir space station.
With the increase of seats on the Space Shuttle, the U.S.
began taking international astronauts.
In 1983, Ulf Merbold of West Germany became the first non-US citizen to fly in a US spacecraft.
In 1984, Marc Garneau became the first of 8 Canadian astronauts to fly in space (through 2010).
In 1985, Rodolfo Neri Vela became the first Mexican-born person in space.
In 1991, Helen Sharman became the first Briton to fly in space.
In 2002, Mark Shuttleworth became the first citizen of an African country to fly in space, as a paying spaceflight participant.
In 2003, Ilan Ramon became the first Israeli to fly in space, although he died during a re-entry accident.
On October 15, 2003, Yang Liwei became China's first astronaut on the Shenzhou 5 spacecraft.
The youngest person to fly in space is Gherman Titov, who was 25 years old when he flew Vostok 2.
(Titov was also the first person to suffer space sickness).
The oldest person who has flown in space is John Glenn, who was 77 when he flew on STS-95.
438 days is the longest time spent in space, by Russian Valeri Polyakov.
As of 2006, the most spaceflights by an individual astronaut is seven, a record held by both Jerry L. Ross and Franklin Chang-Diaz.
The farthest distance from Earth an astronaut has traveled was , when Jim Lovell, Jack Swigert, and Fred Haise went around the Moon during the Apollo 13 emergency.
The first civilian in space was Valentina Tereshkova aboard Vostok 6 (she also became the first woman in space on that mission).
Tereshkova was only honorarily inducted into the USSR's Air Force, which did not accept female pilots at that time.
A month later, Joseph Albert Walker became the first American civilian in space when his X-15 Flight 90 crossed the line, qualifying him by the international definition of spaceflight.
Walker had joined the US Army Air Force but was not a member during his flight.
The first people in space who had never been a member of any country's armed forces were both Konstantin Feoktistov and Boris Yegorov aboard Voskhod 1.
The first non-governmental space traveler was Byron K. Lichtenberg, a researcher from the Massachusetts Institute of Technology who flew on STS-9 in 1983.
In December 1990, Toyohiro Akiyama became the first paying space traveler as a reporter for Tokyo Broadcasting System, a visit to Mir as part of an estimated $12 million (USD) deal with a Japanese TV station, although at the time, the term used to refer to Akiyama was "Research Cosmonaut".
Akiyama suffered severe space sickness during his mission, which affected his productivity.
The first self-funded space tourist was Dennis Tito on board the Russian spacecraft Soyuz TM-3 on April 28, 2001.
The first person to fly on an entirely privately funded mission was Mike Melvill, piloting SpaceShipOne flight 15P on a suborbital journey, although he was a test pilot employed by Scaled Composites and not an actual paying space tourist.
Seven others have paid the Russian Space Agency to fly into space:


The first NASA astronauts were selected for training in 1959.
Early in the space program, military jet test piloting and engineering training were often cited as prerequisites for selection as an astronaut at NASA, although neither John Glenn nor Scott Carpenter (of the Mercury Seven) had any university degree, in engineering or any other discipline at the time of their selection.
Selection was initially limited to military pilots.
The earliest astronauts for both America and the USSR tended to be jet fighter pilots, and were often test pilots.
Once selected, NASA astronauts go through twenty months of training in a variety of areas, including training for extravehicular activity in a facility such as NASA's Neutral Buoyancy Laboratory.
Astronauts-in-training (astronaut candidates) may also experience short periods of weightlessness (microgravity) in an aircraft called the "Vomit Comet," the nickname given to a pair of modified KC-135s (retired in 2000 and 2004, respectively, and replaced in 2005 with a C-9) which perform parabolic flights.
Astronauts are also required to accumulate a number of flight hours in high-performance jet aircraft.
This is mostly done in T-38 jet aircraft out of Ellington Field, due to its proximity to the Johnson Space Center.
Ellington Field is also where the Shuttle Training Aircraft is maintained and developed, although most flights of the aircraft are conducted from Edwards Air Force Base.
Astronauts is training must learn how to control and fly the Space Shuttle and, it is vital that they are familiar with the International Space Station so they know what they must do when they get there.
Mission Specialist Educators, or "Educator Astronauts", were first selected in 2004, and as of 2007, there are three NASA Educator astronauts: Joseph M. Acaba, Richard R. Arnold, and Dorothy Metcalf-Lindenburger.
Barbara Morgan, selected as back-up teacher to Christa McAuliffe in 1985, is considered to be the first Educator astronaut by the media, but she trained as a mission specialist.
The Educator Astronaut program is a successor to the Teacher in Space program from the 1980s.
Astronauts are susceptible to a variety of health risks including decompression sickness, barotrauma, immunodeficiencies, loss of bone and muscle, loss of eyesight, orthostatic intolerance, sleep disturbances, and radiation injury.
A variety of large scale medical studies are being conducted in space via the National Space and Biomedical Research Institute (NSBRI) to address these issues.
Prominent among these is the Advanced Diagnostic Ultrasound in Microgravity Study in which astronauts (including former ISS commanders Leroy Chiao and Gennady Padalka) perform ultrasound scans under the guidance of remote experts to diagnose and potentially treat hundreds of medical conditions in space.
This study's techniques are now being applied to cover professional and Olympic sports injuries as well as ultrasound performed by non-expert operators in medical and high school students.
It is anticipated that remote guided ultrasound will have application on Earth in emergency and rural care situations, where access to a trained physician is often rare.
A 2006 Space Shuttle experiment found that "Salmonella typhimurium", a bacterium that can cause food poisoning, became more virulent when cultivated in space.
More recently, in 2017, bacteria were found to be more resistant to antibiotics and to thrive in the near-weightlessness of space.
Microorganisms have been observed to survive the vacuum of outer space.
On December 31, 2012, a NASA-supported study reported that manned spaceflight may harm the brain and accelerate the onset of Alzheimer's disease.
In October 2015, the NASA Office of Inspector General issued a health hazards report related to space exploration, including a human mission to Mars.
Over the last decade, flight surgeons and scientists at NASA have seen a pattern of vision problems in astronauts on long-duration space missions.
The syndrome, known as visual impairment intracranial pressure (VIIP), has been reported in nearly two-thirds of space explorers after long periods spent aboard the International Space Station (ISS).
On November 2, 2017, scientists reported that significant changes in the position and structure of the brain have been found in astronauts who have taken trips in space, based on MRI studies.
Astronauts who took longer space trips were associated with greater brain changes.
Being in space can be physiologically deconditioning on the body.
It can affect the otolith organs and adaptive capabilities of the central nervous system.
Zero gravity and cosmic rays can cause many implications for astronauts.
In October 2018, NASA-funded researchers found that lengthy journeys into outer space, including travel to the planet Mars, may substantially damage the gastrointestinal tissues of astronauts.
The studies support earlier work that found such journeys could significantly damage the brains of astronauts, and age them prematurely.
An astronaut on the International Space Station requires about 0.83 kilograms (1.83 pounds) weight of food inclusive of food packaging per meal each day.
(The packaging for each meal weighs around 0.12 kilograms - 0.27 pounds) Longer-duration missions require more food.
Shuttle astronauts worked with nutritionists to select menus that appeal to their individual tastes.
Five months before flight, menus are selected and analyzed for nutritional content by the shuttle dietician.
Foods are tested to see how they will react in a reduced gravity environment.
Caloric requirements are determined using a basal energy expenditure (BEE) formula.
On Earth, the average American uses about 35 gallons (132 liters) of water every day.
On board the ISS astronauts limit water use to only about three gallons (11 liters) per day.
In Russia, cosmonauts are awarded Pilot-Cosmonaut of the Russian Federation upon completion of their missions, often accompanied with the award of Hero of the Russian Federation.
This follows the practice established in the USSR where cosmonauts were usually awarded the title Hero of the Soviet Union.
At NASA, those who complete astronaut candidate training receive a silver lapel pin.
Once they have flown in space, they receive a gold pin.
U.S.
astronauts who also have active-duty military status receive a special qualification badge, known as the Astronaut Badge, after participation on a spaceflight.
The United States Air Force also presents an Astronaut Badge to its pilots who exceed in altitude.
Eighteen astronauts (fourteen men and four women) have lost their lives during four space flights.
By nationality, thirteen were American (including one born in India), four were Russian (Soviet Union), and one was Israeli.
Eleven people (all men) have lost their lives training for spaceflight: eight Americans and three Russians.
Six of these were in crashes of training jet aircraft, one drowned during water recovery training, and four were due to fires in pure oxygen environments.
The Space Mirror Memorial, which stands on the grounds of the John F. Kennedy Space Center Visitor Complex, commemorates the lives of the men and women who have died during spaceflight and during training in the space programs of the United States.
In addition to twenty NASA career astronauts, the memorial includes the names of a U.S.
Air Force X-15 test pilot, a U.S.
Air Force officer who died while training for a then-classified military space program, and a civilian spaceflight participant.
</doc>
<doc id="665" url="https://en.wikipedia.org/wiki?curid=665" title="A Modest Proposal">
A Modest Proposal

A Modest Proposal For preventing the Children of Poor People From being a Burthen to Their Parents or Country, and For making them Beneficial to the Publick, commonly referred to as A Modest Proposal, is a Juvenalian satirical essay written and published anonymously by Jonathan Swift in 1729.
The essay suggests that the impoverished Irish might ease their economic troubles by selling their children as food for rich gentlemen and ladies.
This satirical hyperbole mocked heartless attitudes towards the poor, as well as British policy toward the Irish in general.
The primary target of Swift's satire was the rationalism of modern economics, and the growth of rationalistic modes of thinking in modern life at the expense of more traditional human values.
In English writing, the phrase "a modest proposal" is now conventionally an allusion to this style of straight-faced satire.
This essay is widely held to be one of the greatest examples of sustained irony in the history of the English language.
Much of its shock value derives from the fact that the first portion of the essay describes the plight of starving beggars in Ireland, so that the reader is unprepared for the surprise of Swift's solution when he states: "A young healthy child well nursed, is, at a year old, a most delicious nourishing and wholesome food, whether stewed, roasted, baked, or boiled; and I make no doubt that it will equally serve in a fricassee, or a ragout."
Swift goes to great lengths to support his argument, including a list of possible preparation styles for the children, and calculations showing the financial benefits of his suggestion.
He uses methods of argument throughout his essay which lampoon the then-influential William Petty and the social engineering popular among followers of Francis Bacon.
These lampoons include appealing to the authority of "a very knowing American of my acquaintance in London" and "the famous Psalmanazar, a native of the island Formosa" (who had already confessed to "not" being from Formosa in 1706).
In the tradition of Roman satire, Swift introduces the reforms he is actually suggesting by paralipsis:
George Wittkowsky argued that Swift's main target in "A Modest Proposal" was not the conditions in Ireland, but rather the can-do spirit of the times that led people to devise a number of illogical schemes that would purportedly solve social and economic ills.
Swift was especially attacking projects that tried to fix population and labour issues with a simple cure-all solution.
A memorable example of these sorts of schemes "involved the idea of running the poor through a joint-stock company".
In response, Swift's "Modest Proposal" was "a burlesque of projects concerning the poor" that were in vogue during the early 18th century.
"A Modest Proposal" also targets the calculating way people perceived the poor in designing their projects.
The pamphlet targets reformers who "regard people as commodities".
In the piece, Swift adopts the "technique of a political arithmetician" to show the utter ridiculousness of trying to prove any proposal with dispassionate statistics.
Critics differ about Swift's intentions in using this faux-mathematical philosophy.
Edmund Wilson argues that statistically "the logic of the 'Modest proposal' can be compared with defence of crime (arrogated to Marx) in which he argues that crime takes care of the superfluous population".
Wittkowsky counters that Swift's satiric use of statistical analysis is an effort to enhance his satire that "springs from a spirit of bitter mockery, not from the delight in calculations for their own sake".
Charles K. Smith argues that Swift's rhetorical style persuades the reader to detest the speaker and pity the Irish.
Swift's specific strategy is twofold, using a "trap" to create sympathy for the Irish and a dislike of the narrator who, in the span of one sentence, "details vividly and with rhetorical emphasis the grinding poverty" but feels emotion solely for members of his own class.
Swift's use of gripping details of poverty and his narrator's cool approach towards them create "two opposing points of view" that "alienate the reader, perhaps unconsciously, from a narrator who can view with 'melancholy' detachment a subject that Swift has directed us, rhetorically, to see in a much less detached way."
Swift has his proposer further degrade the Irish by using language ordinarily reserved for animals.
Lewis argues that the speaker uses "the vocabulary of animal husbandry" to describe the Irish.
Once the children have been commodified, Swift's rhetoric can easily turn "people into animals, then meat, and from meat, logically, into tonnage worth a price per pound".
Swift uses the proposer's serious tone to highlight the absurdity of his proposal.
In making his argument, the speaker uses the conventional, textbook-approved order of argument from Swift's time (which was derived from the Latin rhetorician Quintilian).
The contrast between the "careful control against the almost inconceivable perversion of his scheme" and "the ridiculousness of the proposal" create a situation in which the reader has "to consider just what perverted values and assumptions would allow such a diligent, thoughtful, and conventional man to propose so perverse a plan".
Scholars have speculated about which earlier works Swift may have had in mind when he wrote "A Modest Proposal".
James Johnson argued that "A Modest Proposal" was largely influenced and inspired by Tertullian's "Apology": a satirical attack against early Roman persecution of Christianity.
James William Johnson believes that Swift saw major similarities between the two situations.
Johnson notes Swift's obvious affinity for Tertullian and the bold stylistic and structural similarities between the works "A Modest Proposal" and "Apology".
In structure, Johnson points out the same central theme, that of cannibalism and the eating of babies as well as the same final argument, that "human depravity is such that men will attempt to justify their own cruelty by accusing their victims of being lower than human".
Stylistically, Swift and Tertullian share the same command of sarcasm and language.
In agreement with Johnson, Donald C. Baker points out the similarity between both authors' tones and use of irony.
Baker notes the uncanny way that both authors imply an ironic "justification by ownership" over the subject of sacrificing children—Tertullian while attacking pagan parents, and Swift while attacking the English mistreatment of the Irish poor.
It has also been argued that "A Modest Proposal" was, at least in part, a response to the 1728 essay "The Generous Projector or, A Friendly Proposal to Prevent Murder and Other Enormous Abuses, By Erecting an Hospital for Foundlings and Bastard Children" by Swift's rival Daniel Defoe.
Bernard Mandeville's "Modest Defence of Publick Stews" asked to introduce public and state controlled bordellos.
The 1726 paper acknowledges women's interests andwhile not being a complete satirical texthas also been discussed as an inspiration for Jonathan Swift's title.
Mandeville had by 1705 already become famous for the Fable of The Bees and deliberations on private vices and public benefits.
Locke commented: "Be it then as Sir Robert says, that Anciently, it was usual for Men to sell and Castrate their Children.
Let it be, that they exposed them; Add to it, if you please, for this is still greater Power, "that they begat them for their Tables to fat and eat them": If this proves a right to do so, we may, by the same Argument, justifie Adultery, Incest and Sodomy, for there are examples of these too, both Ancient and Modern; Sins, which I suppose, have the Principle Aggravation from this, that they cross the main intention of Nature, which willeth the increase of Mankind, and the continuation of the Species in the highest perfection, and the distinction of Families, with the Security of the Marriage Bed, as necessary thereunto".
(First Treatise, sec.
59).
Robert Phiddian's article "Have you eaten yet?
The Reader in A Modest Proposal" focuses on two aspects of "A Modest Proposal": the voice of Swift and the voice of the Proposer.
Phiddian stresses that a reader of the pamphlet must learn to distinguish between the satirical voice of Jonathan Swift and the apparent economic projections of the Proposer.
He reminds readers that "there is a gap between the narrator's meaning and the text's, and that a moral-political argument is being carried out by means of parody".
While Swift's proposal is obviously not a serious economic proposal, George Wittkowsky, author of "Swift's Modest Proposal: The Biography of an Early Georgian Pamphlet", argues that to understand the piece fully it is important to understand the economics of Swift's time.
Wittowsky argues that not enough critics have taken the time to focus directly on the mercantilism and theories of labour in 18th century England.
"[I]f one regards the "Modest Proposal" simply as a criticism of condition, about all one can say is that conditions were bad and that Swift's irony brilliantly underscored this fact".
At the start of a new industrial age in the 18th century, it was believed that "people are the riches of the nation", and there was a general faith in an economy that paid its workers low wages because high wages meant workers would work less.
Furthermore, "in the mercantilist view no child was too young to go into industry".
In those times, the "somewhat more humane attitudes of an earlier day had all but disappeared and the laborer had come to be regarded as a commodity".
Landa composed a conducive analysis when he noted that it would have been healthier for the Irish economy to more appropriately utilize their human assets by giving the people an opportunity to “become a source of wealth to the nation” or else they “must turn to begging and thievery”.
This opportunity may have included giving the farmers more coin to work for, diversifying their professions, or even consider enslaving their people to lower coin usage and build up financial stock in Ireland.
Landa wrote that, "Swift is maintaining that the maxim—people are the riches of a nation—applies to Ireland only if Ireland is permitted slavery or cannibalism" 

Louis A. Landa presents Swift's "A Modest Proposal" as a critique of the popular and unjustified maxim of mercantilism in the 18th century that "people are the riches of a nation".
Swift presents the dire state of Ireland and shows that mere population itself, in Ireland's case, did not always mean greater wealth and economy.
The uncontrolled maxim fails to take into account that a person who does not produce in an economic or political way makes a country poorer, not richer.
Swift also recognises the implications of this fact in making mercantilist philosophy a paradox: the wealth of a country is based on the poverty of the majority of its citizens.
Swift however, Landa argues, is not merely criticising economic maxims but also addressing the fact that England was denying Irish citizens their natural rights and dehumanising them by viewing them as a mere commodity.
Swift's writings created a backlash within the community after its publication.
The work was aimed at the aristocracy, and they responded in turn.
Several members of society wrote to Swift regarding the work.
Lord Bathurst's letter intimated that he certainly understood the message, and interpreted it as a work of comedy:

February 12, 1729-30:"I did immediately propose it to Lady Bathurst, as your advice, particularly for her last boy, which was born the plumpest, finest thing, that could be seen; but she fell in a passion, and bid me send you word, that she would not follow your direction, but that she would breed him up to be a parson, and he should live upon the fat of the land; or a lawyer, and then, instead of being eat himself, he should devour others.
You know women in passion never mind what they say; but, as she is a very reasonable woman, I have almost brought her over now to your opinion; and having convinced her, that as matters stood, we could not possibly maintain all the nine, she does begin to think it reasonable the youngest should raise fortunes for the eldest: and upon that foot a man may perforin family duty with more courage and zeal; for, if he should happen to get twins, the selling of one might provide for the other.
Or if, by any accident, while his wife lies in with one child, he should get a second upon the body of another woman, he might dispose of the fattest of the two, and that would help to breed up the other.The more I think upon this scheme, the more reasonable it appears to me; and it ought by no means to be confined to Ireland; for, in all probability, we shall, in a very little time, be altogether as poor here as you are there.
I believe, indeed, we shall carry it farther, and not confine our luxury only to the eating of children; for I happened to peep the other day into a large assembly [Parliament] not far from Westminster-hall, and I found them roasting a great fat fellow, [Walpole again] For my own part, I had not the least inclination to a slice of him; but, if I guessed right, four or five of the company had a devilish mind to be at him.
Well, adieu, you begin now to wish I had ended, when I might have done it so conveniently".
"A Modest Proposal" is included in many literature courses as an example of early modern western satire.
It also serves as an exceptional introduction to the concept and use of argumentative language, lending itself well to secondary and post-secondary essay courses.
Outside of the realm of English studies, "A Modest Proposal" is included in many comparative and global literature and history courses, as well as those of numerous other disciplines in the arts, humanities, and even the social sciences.
The essay's approach has been copied many times.
In his book "A Modest Proposal" (1984), the evangelical author Frank Schaeffer emulated Swift's work in a social conservative polemic against abortion and euthanasia, imagining a future dystopia that advocates recycling of aborted embryos, fetuses, and some disabled infants with compound intellectual, physical and physiological difficulties.
(Such Baby Doe Rules cases were then a major concern of the US pro-life movement of the early 1980s, which viewed selective treatment of those infants as disability discrimination).
In his book "A Modest Proposal for America" (2013), statistician Howard Friedman opens with a satirical reflection of the extreme drive to fiscal stability by ultra-conservatives.
In the 1998 edition of A Handmaid's Tale by Margaret Atwood there is a quote from "A Modest Proposal" before the introduction.
"A Modest Video Game Proposal" is the title of an open letter sent by activist/former attorney Jack Thompson on 10 October 2005.
He proposed that someone should "create, manufacture, distribute, and sell a video game" that would allow players to act out a scenario in which the game character kills video game developers.
Hunter S. Thompson's "Fear and Loathing in America: The Brutal Odyssey of an Outlaw Journalist includes" a letter in which he uses Swift's approach in connection with the Vietnam War.
Thompson writes a letter to a local Aspen newspaper informing them that, on Christmas Eve, he was going to use napalm to burn a number of dogs and hopefully any humans they find.
The letter protests against the burning of Vietnamese people occurring overseas.
The 2012 film "Butcher Boys," written by Kim Henkel, is said to be loosely based on Jonathan Swift's "A Modest Proposal."
The film's opening scene takes place in a restaurant named "J. Swift's".
On November 30, 2017, Jonathan Swift's 350th birthday, The Washington Post published a column entitled 'Why Alabamians should consider eating Democrats' babies", by the humorous columnist Alexandra Petri.
</doc>
<doc id="666" url="https://en.wikipedia.org/wiki?curid=666" title="Alkali metal">
Alkali metal

The alkali metals are a group (column) in the periodic table consisting of the chemical elements lithium (Li), sodium (Na), potassium (K), rubidium (Rb), caesium (Cs), and francium (Fr).
This group lies in the s-block of the periodic table of elements as all alkali metals have their outermost electron in an s-orbital: this shared electron configuration results in their having very similar characteristic properties.
Indeed, the alkali metals provide the best example of group trends in properties in the periodic table, with elements exhibiting well-characterised homologous behaviour.
The alkali metals are all shiny, soft, highly reactive metals at standard temperature and pressure and readily lose their outermost electron to form cations with charge +1.
They can all be cut easily with a knife due to their softness, exposing a shiny surface that tarnishes rapidly in air due to oxidation by atmospheric moisture and oxygen (and in the case of lithium, nitrogen).
Because of their high reactivity, they must be stored under oil to prevent reaction with air, and are found naturally only in salts and never as the free elements.
Caesium, the fifth alkali metal, is the most reactive of all the metals.
In the modern IUPAC nomenclature, the alkali metals comprise the group 1 elements, excluding hydrogen (H), which is nominally a group 1 element but not normally considered to be an alkali metal as it rarely exhibits behaviour comparable to that of the alkali metals.
All the alkali metals react with water, with the heavier alkali metals reacting more vigorously than the lighter ones.
All of the discovered alkali metals occur in nature as their compounds: in order of abundance, sodium is the most abundant, followed by potassium, lithium, rubidium, caesium, and finally francium, which is very rare due to its extremely high radioactivity; francium occurs only in the minutest traces in nature as an intermediate step in some obscure side branches of the natural decay chains.
Experiments have been conducted to attempt the synthesis of ununennium (Uue), which is likely to be the next member of the group, but they have all met with failure.
However, ununennium may not be an alkali metal due to relativistic effects, which are predicted to have a large influence on the chemical properties of superheavy elements; even if it does turn out to be an alkali metal, it is predicted to have some differences in physical and chemical properties from its lighter homologues.
Most alkali metals have many different applications.
One of the best-known applications of the pure elements is the use of rubidium and caesium in atomic clocks, of which caesium atomic clocks are the most accurate and precise representation of time.
A common application of the compounds of sodium is the sodium-vapour lamp, which emits light very efficiently.
Table salt, or sodium chloride, has been used since antiquity.
Sodium and potassium are also essential elements, having major biological roles as electrolytes, and although the other alkali metals are not essential, they also have various effects on the body, both beneficial and harmful.
Sodium compounds have been known since ancient times; salt (sodium chloride) has been an important commodity in human activities, as testified by the English word "salary", referring to "salarium", money paid to Roman soldiers for the purchase of salt.
While potash has been used since ancient times, it was not understood for most of its history to be a fundamentally different substance from sodium mineral salts.
Georg Ernst Stahl obtained experimental evidence which led him to suggest the fundamental difference of sodium and potassium salts in 1702, and Henri-Louis Duhamel du Monceau was able to prove this difference in 1736.
The exact chemical composition of potassium and sodium compounds, and the status as chemical element of potassium and sodium, was not known then, and thus Antoine Lavoisier did not include either alkali in his list of chemical elements in 1789.
Pure potassium was first isolated in 1807 in England by Sir Humphry Davy, who derived it from caustic potash (KOH, potassium hydroxide) by the use of electrolysis of the molten salt with the newly invented voltaic pile.
Previous attempts at electrolysis of the aqueous salt were unsuccessful due to potassium's extreme reactivity.
Potassium was the first metal that was isolated by electrolysis.
Later that same year, Davy reported extraction of sodium from the similar substance caustic soda (NaOH, lye) by a similar technique, demonstrating the elements, and thus the salts, to be different.
Petalite (Li Al SiO) was discovered in 1800 by the Brazilian chemist José Bonifácio de Andrada in a mine on the island of Utö, Sweden.
However, it was not until 1817 that Johan August Arfwedson, then working in the laboratory of the chemist Jöns Jacob Berzelius, detected the presence of a new element while analysing petalite ore.
This new element was noted by him to form compounds similar to those of sodium and potassium, though its carbonate and hydroxide were less soluble in water and more alkaline than the other alkali metals.
Berzelius gave the unknown material the name ""lithion"/"lithina"", from the Greek word "λιθoς" (transliterated as "lithos", meaning "stone"), to reflect its discovery in a solid mineral, as opposed to potassium, which had been discovered in plant ashes, and sodium, which was known partly for its high abundance in animal blood.
He named the metal inside the material ""lithium"".
Lithium, sodium, and potassium were part of the discovery of periodicity, as they are among a series of triads of elements in the same group that were noted by Johann Wolfgang Döbereiner in 1850 as having similar properties.
Rubidium and caesium were the first elements to be discovered using the spectroscope, invented in 1859 by Robert Bunsen and Gustav Kirchhoff.
The next year, they discovered caesium in the mineral water from Bad Dürkheim, Germany.
Their discovery of rubidium came the following year in Heidelberg, Germany, finding it in the mineral lepidolite.
The names of rubidium and caesium come from the most prominent lines in their emission spectra: a bright red line for rubidium (from the Latin word "rubidus", meaning dark red or bright red), and a sky-blue line for caesium (derived from the Latin word "caesius", meaning sky-blue).
Around 1865 John Newlands produced a series of papers where he listed the elements in order of increasing atomic weight and similar physical and chemical properties that recurred at intervals of eight; he likened such periodicity to the octaves of music, where notes an octave apart have similar musical functions.
His version put all the alkali metals then known (lithium to caesium), as well as copper, silver, and thallium (which show the +1 oxidation state characteristic of the alkali metals), together into a group.
His table placed hydrogen with the halogens.
After 1869, Dmitri Mendeleev proposed his periodic table placing lithium at the top of a group with sodium, potassium, rubidium, caesium, and thallium.
Two years later, Mendeleev revised his table, placing hydrogen in group 1 above lithium, and also moving thallium to the boron group.
In this 1871 version, copper, silver, and gold were placed twice, once as part of group IB, and once as part of a "group VIII" encompassing today's groups 8 to 11.
After the introduction of the 18-column table, the group IB elements were moved to their current position in the d-block, while alkali metals were left in "group IA".
Later the group's name was changed to "group 1" in 1988.
The trivial name "alkali metals" comes from the fact that the hydroxides of the group 1 elements are all strong alkalis when dissolved in water.
There were at least four erroneous and incomplete discoveries before Marguerite Perey of the Curie Institute in Paris, France discovered francium in 1939 by purifying a sample of actinium-227, which had been reported to have a decay energy of 220 keV.
However, Perey noticed decay particles with an energy level below 80 keV.
Perey thought this decay activity might have been caused by a previously unidentified decay product, one that was separated during purification, but emerged again out of the pure actinium-227.
Various tests eliminated the possibility of the unknown element being thorium, radium, lead, bismuth, or thallium.
The new product exhibited chemical properties of an alkali metal (such as coprecipitating with caesium salts), which led Perey to believe that it was element 87, caused by the alpha decay of actinium-227.
Perey then attempted to determine the proportion of beta decay to alpha decay in actinium-227.
Her first test put the alpha branching at 0.6%, a figure that she later revised to 1%.
The next element below francium (eka-francium) in the periodic table would be ununennium (Uue), element 119.
The synthesis of ununennium was first attempted in 1985 by bombarding a target of einsteinium-254 with calcium-48 ions at the superHILAC accelerator at Berkeley, California.
No atoms were identified, leading to a limiting yield of 300 nb.
It is highly unlikely that this reaction will be able to create any atoms of ununennium in the near future, given the extremely difficult task of making sufficient amounts of einsteinium-254, which is favoured for production of ultraheavy elements because of its large mass, relatively long half-life of 270 days, and availability in significant amounts of several micrograms, to make a large enough target to increase the sensitivity of the experiment to the required level; einsteinium has not been found in nature and has only been produced in laboratories, and in quantities smaller than those needed for effective synthesis of superheavy elements.
However, given that ununennium is only the first period 8 element on the extended periodic table, it may well be discovered in the near future through other reactions, and indeed an attempt to synthesise it is currently ongoing in Japan.
Currently, none of the period 8 elements have been discovered yet, and it is also possible, due to drip instabilities, that only the lower period 8 elements, up to around element 128, are physically possible.
No attempts at synthesis have been made for any heavier alkali metals: due to their extremely high atomic number, they would require new, more powerful methods and technology to make.
The Oddo–Harkins rule holds that elements with even atomic numbers are more common that those with odd atomic numbers, with the exception of hydrogen.
This rule argues that elements with odd atomic numbers have one unpaired proton and are more likely to capture another, thus increasing their atomic number.
In elements with even atomic numbers, protons are paired, with each member of the pair offsetting the spin of the other, enhancing stability.
All the alkali metals have odd atomic numbers and they are not as common as the elements with even atomic numbers adjacent to them (the noble gases and the alkaline earth metals) in the Solar System.
The heavier alkali metals are also less abundant than the lighter ones as the alkali metals from rubidium onward can only be synthesised in supernovae and not in stellar nucleosynthesis.
Lithium is also much less abundant than sodium and potassium as it is poorly synthesised in both Big Bang nucleosynthesis and in stars: the Big Bang could only produce trace quantities of lithium, beryllium and boron due to the absence of a stable nucleus with 5 or 8 nucleons, and stellar nucleosynthesis could only pass this bottleneck by the triple-alpha process, fusing three helium nuclei to form carbon, and skipping over those three elements.
The Earth formed from the same cloud of matter that formed the Sun, but the planets acquired different compositions during the formation and evolution of the solar system.
In turn, the natural history of the Earth caused parts of this planet to have differing concentrations of the elements.
The mass of the Earth is approximately 5.98 kg.
It is composed mostly of iron (32.1%), oxygen (30.1%), silicon (15.1%), magnesium (13.9%), sulfur (2.9%), nickel (1.8%), calcium (1.5%), and aluminium (1.4%); with the remaining 1.2% consisting of trace amounts of other elements.
Due to planetary differentiation, the core region is believed to be primarily composed of iron (88.8%), with smaller amounts of nickel (5.8%), sulfur (4.5%), and less than 1% trace elements.
The alkali metals, due to their high reactivity, do not occur naturally in pure form in nature.
They are lithophiles and therefore remain close to the Earth's surface because they combine readily with oxygen and so associate strongly with silica, forming relatively low-density minerals that do not sink down into the Earth's core.
Potassium, rubidium and caesium are also incompatible elements due to their large ionic radii.
Sodium and potassium are very abundant in earth, both being among the ten most common elements in Earth's crust; sodium makes up approximately 2.6% of the Earth's crust measured by weight, making it the sixth most abundant element overall and the most abundant alkali metal.
Potassium makes up approximately 1.5% of the Earth's crust and is the seventh most abundant element.
Sodium is found in many different minerals, of which the most common is ordinary salt (sodium chloride), which occurs in vast quantities dissolved in seawater.
Other solid deposits include halite, amphibole, cryolite, nitratine, and zeolite.
Many of these solid deposits occur as a result of ancient seas evaporating, which still occurs now in places such as Utah's Great Salt Lake and the Dead Sea.
Despite their near-equal abundance in Earth's crust, sodium is far more common than potassium in the ocean, both because potassium's larger size makes its salts less soluble, and because potassium is bound by silicates in soil and what potassium leaches is absorbed far more readily by plant life than sodium.
Despite its chemical similarity, lithium typically does not occur together with sodium or potassium due to its smaller size.
Due to its relatively low reactivity, it can be found in seawater in large amounts; it is estimated that seawater is approximately 0.14 to 0.25 parts per million (ppm) or 25 micromolar.
Its diagonal relationship with magnesium often allows it to replace magnesium in ferromagnesium minerals, where its crustal concentration is about 18 ppm, comparable to that of gallium and niobium.
Commercially, the most important lithium mineral is spodumene, which occurs in large deposits worldwide.
Rubidium is approximately as abundant as zinc and more abundant than copper.
It occurs naturally in the minerals leucite, pollucite, carnallite, zinnwaldite, and lepidolite, although none of these contain only rubidium and no other alkali metals.
Caesium is more abundant than some commonly known elements, such as antimony, cadmium, tin, and tungsten, but is much less abundant than rubidium.
Francium-223, the only naturally occurring isotope of francium, is the product of the alpha decay of actinium-227 and can be found in trace amounts in uranium minerals.
In a given sample of uranium, there is estimated to be only one francium atom for every 10 uranium atoms.
It has been calculated that there is at most 30 g of francium in the earth's crust at any time, due to its extremely short half-life of 22 minutes.
The physical and chemical properties of the alkali metals can be readily explained by their having an ns valence electron configuration, which results in weak metallic bonding.
Hence, all the alkali metals are soft and have low densities, melting and boiling points, as well as heats of sublimation, vaporisation, and dissociation.
They all crystallise in the body-centered cubic crystal structure, and have distinctive flame colours because their outer s electron is very easily excited.
The ns configuration also results in the alkali metals having very large atomic and ionic radii, as well as very high thermal and electrical conductivity.
Their chemistry is dominated by the loss of their lone valence electron in the outermost s-orbital to form the +1 oxidation state, due to the ease of ionising this electron and the very high second ionisation energy.
Most of the chemistry has been observed only for the first five members of the group.
The chemistry of francium is not well established due to its extreme radioactivity; thus, the presentation of its properties here is limited.
What little is known about francium shows that it is very close in behaviour to caesium, as expected.
The physical properties of francium are even sketchier because the bulk element has never been observed; hence any data that may be found in the literature are certainly speculative extrapolations.
The alkali metals are more similar to each other than the elements in any other group are to each other.
Indeed, the similarity is so great that it is quite difficult to separate potassium, rubidium, and caesium, due to their similar ionic radii; lithium and sodium are more distinct.
For instance, when moving down the table, all known alkali metals show increasing atomic radius, decreasing electronegativity, increasing reactivity, and decreasing melting and boiling points as well as heats of fusion and vaporisation.
In general, their densities increase when moving down the table, with the exception that potassium is less dense than sodium.
One of the very few properties of the alkali metals that does not display a very smooth trend is their reduction potentials: lithium's value is anomalous, being more negative than the others.
This is because the Li ion has a very high hydration energy in the gas phase: though the lithium ion disrupts the structure of water significantly, causing a higher change in entropy, this high hydration energy is enough to make the reduction potentials indicate it as being the most electropositive alkali metal, despite the difficulty of ionising it in the gas phase.
The stable alkali metals are all silver-coloured metals except for caesium, which has a pale golden tint: it is one of only three metals that are clearly coloured (the other two being copper and gold).
Additionally, the heavy alkaline earth metals calcium, strontium, and barium, as well as the divalent lanthanides europium and ytterbium, are pale yellow, though the colour is much less prominent than it is for caesium.
Their lustre tarnishes rapidly in air due to oxidation.
They all crystallise in the body-centered cubic crystal structure, and have distinctive flame colours because their outer s electron is very easily excited.
Indeed, these flame test colours are the most common way of identifying them since all their salts with common ions are soluble.
All the alkali metals are highly reactive and are never found in elemental forms in nature.
Because of this, they are usually stored in mineral oil or kerosene (paraffin oil).
They react aggressively with the halogens to form the alkali metal halides, which are white ionic crystalline compounds that are all soluble in water except lithium fluoride (Li F).
The alkali metals also react with water to form strongly alkaline hydroxides and thus should be handled with great care.
The heavier alkali metals react more vigorously than the lighter ones; for example, when dropped into water, caesium produces a larger explosion than potassium if the same number of moles of each metal is used.
The alkali metals have the lowest first ionisation energies in their respective periods of the periodic table because of their low effective nuclear charge and the ability to attain a noble gas configuration by losing just one electron.
Not only do the alkali metals react with water, but also with proton donors like alcohols and phenols, gaseous ammonia, and alkynes, the last demonstrating the phenomenal degree of their reactivity.
Their great power as reducing agents makes them very useful in liberating other metals from their oxides or halides.
The second ionisation energy of all of the alkali metals is very high as it is in a full shell that is also closer to the nucleus; thus, they almost always lose a single electron, forming cations.
The alkalides are an exception: they are unstable compounds which contain alkali metals in a −1 oxidation state, which is very unusual as before the discovery of the alkalides, the alkali metals were not expected to be able to form anions and were thought to be able to appear in salts only as cations.
The alkalide anions have filled s-subshells, which gives them enough stability to exist.
All the stable alkali metals except lithium are known to be able to form alkalides, and the alkalides have much theoretical interest due to their unusual stoichiometry and low ionisation potentials.
Alkalides are chemically similar to the electrides, which are salts with trapped electrons acting as anions.
A particularly striking example of an alkalide is "inverse sodium hydride", HNa (both ions being complexed), as opposed to the usual sodium hydride, NaH: it is unstable in isolation, due to its high energy resulting from the displacement of two electrons from hydrogen to sodium, although several derivatives are predicted to be metastable or stable.
In aqueous solution, the alkali metal ions form aqua ions of the formula [M(HO)], where "n" is the solvation number.
Their coordination numbers and shapes agree well with those expected from their ionic radii.
In aqueous solution the water molecules directly attached to the metal ion are said to belong to the first coordination sphere, also known as the first, or primary, solvation shell.
The bond between a water molecule and the metal ion is a dative covalent bond, with the oxygen atom donating both electrons to the bond.
Each coordinated water molecule may be attached by hydrogen bonds to other water molecules.
The latter are said to reside in the second coordination sphere.
However, for the alkali metal cations, the second coordination sphere is not well-defined as the +1 charge on the cation is not high enough to polarise the water molecules in the primary solvation shell enough for them to form strong hydrogen bonds with those in the second coordination sphere, producing a more stable entity.
The solvation number for Li has been experimentally determined to be 4, forming the tetrahedral [Li(HO)]: while solvation numbers of 3 to 6 have been found for lithium aqua ions, solvation numbers less than 4 may be the result of the formation of contact ion pairs, and the higher solvation numbers may be interpreted in terms of water molecules that approach [Li(HO)] through a face of the tetrahedron, though molecular dynamic simulations may indicate the existence of an octahedral hexaaqua ion.
There are also probably six water molecules in the primary solvation sphere of the sodium ion, forming the octahedral [Na(HO)] ion.
While it was previously thought that the heavier alkali metals also formed octahedral hexaaqua ions, it has since been found that potassium and rubidium probably form the [K(HO)] and [Rb(HO)] ions, which have the square antiprismatic structure, and that caesium forms the 12-coordinate [Cs(HO)] ion.
The chemistry of lithium shows several differences from that of the rest of the group as the small Li cation polarises anions and gives its compounds a more covalent character.
Lithium and magnesium have a diagonal relationship due to their similar atomic radii, so that they show some similarities.
For example, lithium forms a stable nitride, a property common among all the alkaline earth metals (magnesium's group) but unique among the alkali metals.
In addition, among their respective groups, only lithium and magnesium form organometallic compounds with significant covalent character (e.g.
LiMe and MgMe).
Lithium fluoride is the only alkali metal halide that is poorly soluble in water, and lithium hydroxide is the only alkali metal hydroxide that is not deliquescent.
Conversely, lithium perchlorate and other lithium salts with large anions that cannot be polarised are much more stable than the analogous compounds of the other alkali metals, probably because Li has a high solvation energy.
This effect also means that most simple lithium salts are commonly encountered in hydrated form, because the anhydrous forms are extremely hygroscopic: this allows salts like lithium chloride and lithium bromide to be used in dehumidifiers and air-conditioners.
Francium is also predicted to show some differences due to its high atomic weight, causing its electrons to travel at considerable fractions of the speed of light and thus making relativistic effects more prominent.
In contrast to the trend of decreasing electronegativities and ionisation energies of the alkali metals, francium's electronegativity and ionisation energy are predicted to be higher than caesium's due to the relativistic stabilisation of the 7s electrons; also, its atomic radius is expected to be abnormally low.
Thus, contrary to expectation, caesium is the most reactive of the alkali metals, not francium.
All known physical properties of francium also deviate from the clear trends going from lithium to caesium, such as the first ionisation energy, electron affinity, and anion polarisability, though due to the paucity of known data about francium many sources give extrapolated values, ignoring that relativistic effects make the trend from lithium to caesium become inapplicable at francium.
Some of the few properties of francium that have been predicted taking relativity into account are the electron affinity (47.2 kJ/mol) and the enthalpy of dissociation of the Fr molecule (42.1 kJ/mol).
The CsFr molecule is polarised as CsFr, showing that the 7s subshell of francium is much more strongly affected by relativistic effects than the 6s subshell of caesium.
Additionally, francium superoxide (FrO) is expected to have significant covalent character, unlike the other alkali metal superoxides, because of bonding contributions from the 6p electrons of francium.
All the alkali metals have odd atomic numbers; hence, their isotopes must be either odd–odd (both proton and neutron number are odd) or odd–even (proton number is odd, but neutron number is even).
Odd–odd nuclei have even mass numbers, whereas odd–even nuclei have odd mass numbers.
Odd–odd primordial nuclides are rare because most odd–odd nuclei are highly unstable with respect to beta decay, because the decay products are even–even, and are therefore more strongly bound, due to nuclear pairing effects.
Due to the great rarity of odd–odd nuclei, almost all the primordial isotopes of the alkali metals are odd–even (the exceptions being the light stable isotope lithium-6 and the long-lived radioisotope potassium-40).
For a given odd mass number, there can be only a single beta-stable nuclide, since there is not a difference in binding energy between even–odd and odd–even comparable to that between even–even and odd–odd, leaving other nuclides of the same mass number (isobars) free to beta decay toward the lowest-mass nuclide.
An effect of the instability of an odd number of either type of nucleons is that odd-numbered elements, such as the alkali metals, tend to have fewer stable isotopes than even-numbered elements.
Of the 26 monoisotopic elements that have only a single stable isotope, all but one have an odd atomic number and all but one also have an even number of neutrons.
Beryllium is the single exception to both rules, due to its low atomic number.
All of the alkali metals except lithium and caesium have at least one naturally occurring radioisotope: sodium-22 and sodium-24 are trace radioisotopes produced cosmogenically, potassium-40 and rubidium-87 have very long half-lives and thus occur naturally, and all isotopes of francium are radioactive.
Caesium was also thought to be radioactive in the early 20th century, although it has no naturally occurring radioisotopes.
(Francium had not been discovered yet at that time.)
The natural long-lived radioisotope of potassium, potassium-40, makes up about 0.012% of natural potassium, and thus natural potassium is weakly radioactive.
This natural radioactivity became a basis for a mistaken claim of the discovery for element 87 (the next alkali metal after caesium) in 1925.
Natural rubidium is similarly slightly radioactive, with 27.83% being the long-lived radioisotope rubidium-87.
Caesium-137, with a half-life of 30.17 years, is one of the two principal medium-lived fission products, along with strontium-90, which are responsible for most of the radioactivity of spent nuclear fuel after several years of cooling, up to several hundred years after use.
It constitutes most of the radioactivity still left from the Chernobyl accident.
Caesium-137 undergoes high-energy beta decay and eventually becomes stable barium-137.
It is a strong emitter of gamma radiation.
Caesium-137 has a very low rate of neutron capture and cannot be feasibly disposed of in this way, but must be allowed to decay.
Caesium-137 has been used as a tracer in hydrologic studies, analogous to the use of tritium.
Small amounts of caesium-134 and caesium-137 were released into the environment during nearly all nuclear weapon tests and some nuclear accidents, most notably the Goiânia accident and the Chernobyl disaster.
As of 2005, caesium-137 is the principal source of radiation in the zone of alienation around the Chernobyl nuclear power plant.
Its chemical properties as one of the alkali metals make it one of most problematic of the short-to-medium-lifetime fission products because it easily moves and spreads in nature due to the high water solubility of its salts, and is taken up by the body, which mistakes it for its essential congeners sodium and potassium.
The alkali metals are more similar to each other than the elements in any other group are to each other.
For instance, when moving down the table, all known alkali metals show increasing atomic radius, decreasing electronegativity, increasing reactivity, and decreasing melting and boiling points as well as heats of fusion and vaporisation.
In general, their densities increase when moving down the table, with the exception that potassium is less dense than sodium.
The atomic radii of the alkali metals increase going down the group.
Because of the shielding effect, when an atom has more than one electron shell, each electron feels electric repulsion from the other electrons as well as electric attraction from the nucleus.
In the alkali metals, the outermost electron only feels a net charge of +1, as some of the nuclear charge (which is equal to the atomic number) is cancelled by the inner electrons; the number of inner electrons of an alkali metal is always one less than the nuclear charge.
Therefore, the only factor which affects the atomic radius of the alkali metals is the number of electron shells.
Since this number increases down the group, the atomic radius must also increase down the group.
The ionic radii of the alkali metals are much smaller than their atomic radii.
This is because the outermost electron of the alkali metals is in a different electron shell than the inner electrons, and thus when it is removed the resulting atom has one fewer electron shell and is smaller.
Additionally, the effective nuclear charge has increased, and thus the electrons are attracted more strongly towards the nucleus and the ionic radius decreases.
The first ionisation energy of an element or molecule is the energy required to move the most loosely held electron from one mole of gaseous atoms of the element or molecules to form one mole of gaseous ions with electric charge +1.
The factors affecting the first ionisation energy are the nuclear charge, the amount of shielding by the inner electrons and the distance from the most loosely held electron from the nucleus, which is always an outer electron in main group elements.
The first two factors change the effective nuclear charge the most loosely held electron feels.
Since the outermost electron of alkali metals always feels the same effective nuclear charge (+1), the only factor which affects the first ionisation energy is the distance from the outermost electron to the nucleus.
Since this distance increases down the group, the outermost electron feels less attraction from the nucleus and thus the first ionisation energy decreases.
(This trend is broken in francium due to the relativistic stabilisation and contraction of the 7s orbital, bringing francium's valence electron closer to the nucleus than would be expected from non-relativistic calculations.
This makes francium's outermost electron feel more attraction from the nucleus, increasing its first ionisation energy slightly beyond that of caesium.)
The second ionisation energy of the alkali metals is much higher than the first as the second-most loosely held electron is part of a fully filled electron shell and is thus difficult to remove.
The reactivities of the alkali metals increase going down the group.
This is the result of a combination of two factors: the first ionisation energies and atomisation energies of the alkali metals.
Because the first ionisation energy of the alkali metals decreases down the group, it is easier for the outermost electron to be removed from the atom and participate in chemical reactions, thus increasing reactivity down the group.
The atomisation energy measures the strength of the metallic bond of an element, which falls down the group as the atoms increase in radius and thus the metallic bond must increase in length, making the delocalised electrons further away from the attraction of the nuclei of the heavier alkali metals.
Adding the atomisation and first ionisation energies gives a quantity closely related to (but not equal to) the activation energy of the reaction of an alkali metal with another substance.
This quantity decreases going down the group, and so does the activation energy; thus, chemical reactions can occur faster and the reactivity increases down the group.
Electronegativity is a chemical property that describes the tendency of an atom or a functional group to attract electrons (or electron density) towards itself.
If the bond between sodium and chlorine in sodium chloride were covalent, the pair of shared electrons would be attracted to the chlorine because the effective nuclear charge on the outer electrons is +7 in chlorine but is only +1 in sodium.
The electron pair is attracted so close to the chlorine atom that they are practically transferred to the chlorine atom (an ionic bond).
However, if the sodium atom was replaced by a lithium atom, the electrons will not be attracted as close to the chlorine atom as before because the lithium atom is smaller, making the electron pair more strongly attracted to the closer effective nuclear charge from lithium.
Hence, the larger alkali metal atoms (further down the group) will be less electronegative as the bonding pair is less strongly attracted towards them.
As mentioned previously, francium is expected to be an exception.
Because of the higher electronegativity of lithium, some of its compounds have a more covalent character.
For example, lithium iodide (Li I) will dissolve in organic solvents, a property of most covalent compounds.
Lithium fluoride (LiF) is the only alkali halide that is not soluble in water, and lithium hydroxide (LiOH) is the only alkali metal hydroxide that is not deliquescent.
The melting point of a substance is the point where it changes state from solid to liquid while the boiling point of a substance (in liquid state) is the point where the vapour pressure of the liquid equals the environmental pressure surrounding the liquid and all the liquid changes state to gas.
As a metal is heated to its melting point, the metallic bonds keeping the atoms in place weaken so that the atoms can move around, and the metallic bonds eventually break completely at the metal's boiling point.
Therefore, the falling melting and boiling points of the alkali metals indicate that the strength of the metallic bonds of the alkali metals decreases down the group.
This is because metal atoms are held together by the electromagnetic attraction from the positive ions to the delocalised electrons.
As the atoms increase in size going down the group (because their atomic radius increases), the nuclei of the ions move further away from the delocalised electrons and hence the metallic bond becomes weaker so that the metal can more easily melt and boil, thus lowering the melting and boiling points.
(The increased nuclear charge is not a relevant factor due to the shielding effect.)
The alkali metals all have the same crystal structure (body-centred cubic) and thus the only relevant factors are the number of atoms that can fit into a certain volume and the mass of one of the atoms, since density is defined as mass per unit volume.
The first factor depends on the volume of the atom and thus the atomic radius, which increases going down the group; thus, the volume of an alkali metal atom increases going down the group.
The mass of an alkali metal atom also increases going down the group.
Thus, the trend for the densities of the alkali metals depends on their atomic weights and atomic radii; if figures for these two factors are known, the ratios between the densities of the alkali metals can then be calculated.
The resultant trend is that the densities of the alkali metals increase down the table, with an exception at potassium.
Due to having the lowest atomic weight and the largest atomic radius of all the elements in their periods, the alkali metals are the least dense metals in the periodic table.
Lithium, sodium, and potassium are the only three metals in the periodic table that are less dense than water: in fact, lithium is the least dense known solid at room temperature.
The alkali metals form complete series of compounds with all usually encountered anions, which well illustrate group trends.
These compounds can be described as involving the alkali metals losing electrons to acceptor species and forming monopositive ions.
This description is most accurate for alkali halides and becomes less and less accurate as cationic and anionic charge increase, and as the anion becomes larger and more polarisable.
For instance, ionic bonding gives way to metallic bonding along the series NaCl, NaO, NaS, NaP, NaAs, NaSb, NaBi, Na.
All the alkali metals react vigorously or explosively with cold water, producing an aqueous solution of a strongly basic alkali metal hydroxide and releasing hydrogen gas.
This reaction becomes more vigorous going down the group: lithium reacts steadily with effervescence, but sodium and potassium can ignite and rubidium and caesium sink in water and generate hydrogen gas so rapidly that shock waves form in the water that may shatter glass containers.
When an alkali metal is dropped into water, it produces an explosion, of which there are two separate stages.
The metal reacts with the water first, breaking the hydrogen bonds in the water and producing hydrogen gas; this takes place faster for the more reactive heavier alkali metals.
Second, the heat generated by the first part of the reaction often ignites the hydrogen gas, causing it to burn explosively into the surrounding air.
This secondary hydrogen gas explosion produces the visible flame above the bowl of water, lake or other body of water, not the initial reaction of the metal with water (which tends to happen mostly under water).
The alkali metal hydroxides are the most basic known hydroxides.
Recent research has suggested that the explosive behavior of alkali metals in water is driven by a Coulomb explosion rather than solely by rapid generation of hydrogen itself.
All alkali metals melt as a part of the reaction with water.
Water molecules ionise the bare metallic surface of the liquid metal, leaving a positively charged metal surface and negatively charged water ions.
The attraction between the charged metal and water ions will rapidly increase the surface area, causing an exponential increase of ionisation.
When the repulsive forces within the liquid metal surface exceeds the forces of the surface tension, it vigorously explodes.
The hydroxides themselves are the most basic hydroxides known, reacting with acids to give salts and with alcohols to give oligomeric alkoxides.
They easily react with carbon dioxide to form carbonates or bicarbonates, or with hydrogen sulfide to form sulfides or bisulfides, and may be used to separate thiols from petroleum.
They react with amphoteric oxides: for example, the oxides of aluminium, zinc, tin, and lead react with the alkali metal hydroxides to give aluminates, zincates, stannates, and plumbates.
Silicon dioxide is acidic, and thus the alkali metal hydroxides can also attack silicate glass.
The alkali metals form many intermetallic compounds with each other and the elements from groups 2 to 13 in the periodic table of varying stoichiometries, such as the sodium amalgams with mercury, including NaHg and NaHg.
Some of these have ionic characteristics: taking the alloys with gold, the most electronegative of metals, as an example, NaAu and KAu are metallic, but RbAu and CsAu are semiconductors.
NaK is an alloy of sodium and potassium that is very useful because it is liquid at room temperature, although precautions must be taken due to its extreme reactivity towards water and air.
The eutectic mixture melts at −12.6 °C.
An alloy of 41% caesium, 47% sodium, and 12% potassium has the lowest known melting point of any metal or alloy, −78 °C.
The intermetallic compounds of the alkali metals with the heavier group 13 elements (aluminium, gallium, indium, and thallium), such as NaTl, are poor conductors or semiconductors, unlike the normal alloys with the preceding elements, implying that the alkali metal involved has lost an electron to the Zintl anions involved.
Nevertheless, while the elements in group 14 and beyond tend to form discrete anionic clusters, group 13 elements tend to form polymeric ions with the alkali metal cations located between the giant ionic lattice.
For example, NaTl consists of a polymeric anion (—Tl—) with a covalent diamond cubic structure with Na ions located between the anionic lattice.
The larger alkali metals cannot fit similarly into an anionic lattice and tend to force the heavier group 13 elements to form anionic clusters.
Boron is a special case, being the only nonmetal in group 13.
The alkali metal borides tend to be boron-rich, involving appreciable boron–boron bonding involving deltahedral structures, and are thermally unstable due to the alkali metals having a very high vapour pressure at elevated temperatures.
This makes direct synthesis problematic because the alkali metals do not react with boron below 700 °C, and thus this must be accomplished in sealed containers with the alkali metal in excess.
Furthermore, exceptionally in this group, reactivity with boron decreases down the group: lithium reacts completely at 700 °C, but sodium at 900 °C and potassium not until 1200 °C, and the reaction is instantaneous for lithium but takes hours for potassium.
Rubidium and caesium borides have not even been characterised.
Various phases are known, such as LiB, NaB, NaB, and KB.
Under high pressure the boron–boron bonding in the lithium borides changes from following Wade's rules to forming Zintl anions like the rest of group 13.
Lithium and sodium react with carbon to form acetylides, LiC and NaC, which can also be obtained by reaction of the metal with acetylene.
Potassium, rubidium, and caesium react with graphite; their atoms are intercalated between the hexagonal graphite layers, forming graphite intercalation compounds of formulae MC (dark grey, almost black), MC (dark grey, almost black), MC (blue), MC (steel blue), and MC (bronze) (M = K, Rb, or Cs).
These compounds are over 200 times more electrically conductive than pure graphite, suggesting that the valence electron of the alkali metal is transferred to the graphite layers (e.g.
).
Upon heating of KC, the elimination of potassium atoms results in the conversion in sequence to KC, KC, KC and finally KC.
KC is a very strong reducing agent and is pyrophoric and explodes on contact with water.
While the larger alkali metals (K, Rb, and Cs) initially form MC, the smaller ones initially form MC, and indeed they require reaction of the metals with graphite at high temperatures around 500 °C to form.
Apart from this, the alkali metals are such strong reducing agents that they can even reduce buckminsterfullerene to produce solid fullerides MC; sodium, potassium, rubidium, and caesium can form fullerides where "n" = 2, 3, 4, or 6, and rubidium and caesium additionally can achieve "n" = 1.
When the alkali metals react with the heavier elements in the carbon group (silicon, germanium, tin, and lead), ionic substances with cage-like structures are formed, such as the silicides MSi (M = K, Rb, or Cs), which contains M and tetrahedral ions.
The chemistry of alkali metal germanides, involving the germanide ion Ge and other cluster (Zintl) ions such as , , , and [(Ge)], is largely analogous to that of the corresponding silicides.
Alkali metal stannides are mostly ionic, sometimes with the stannide ion (Sn), and sometimes with more complex Zintl ions such as , which appears in tetrapotassium nonastannide (KSn).
The monatomic plumbide ion (Pb) is unknown, and indeed its formation is predicted to be energetically unfavourable; alkali metal plumbides have complex Zintl ions, such as .
These alkali metal germanides, stannides, and plumbides may be produced by reducing germanium, tin, and lead with sodium metal in liquid ammonia.
Lithium, the lightest of the alkali metals, is the only alkali metal which reacts with nitrogen at standard conditions, and its nitride is the only stable alkali metal nitride.
Nitrogen is an unreactive gas because breaking the strong triple bond in the dinitrogen molecule (N) requires a lot of energy.
The formation of an alkali metal nitride would consume the ionisation energy of the alkali metal (forming M ions), the energy required to break the triple bond in N and the formation of N ions, and all the energy released from the formation of an alkali metal nitride is from the lattice energy of the alkali metal nitride.
The lattice energy is maximised with small, highly charged ions; the alkali metals do not form highly charged ions, only forming ions with a charge of +1, so only lithium, the smallest alkali metal, can release enough lattice energy to make the reaction with nitrogen exothermic, forming lithium nitride.
The reactions of the other alkali metals with nitrogen would not release enough lattice energy and would thus be endothermic, so they do not form nitrides at standard conditions.
Sodium nitride (NaN) and potassium nitride (KN), while existing, are extremely unstable, being prone to decomposing back into their constituent elements, and cannot be produced by reacting the elements with each other at standard conditions.
Steric hindrance forbids the existence of rubidium or caesium nitride.
However, sodium and potassium form colourless azide salts involving the linear anion; due to the large size of the alkali metal cations, they are thermally stable enough to be able to melt before decomposing.
All the alkali metals react readily with phosphorus and arsenic to form phosphides and arsenides with the formula MPn (where M represents an alkali metal and Pn represents a pnictogen – phosphorus, arsenic, antimony, or bismuth).
This is due to the greater size of the P and As ions, so that less lattice energy needs to be released for the salts to form.
These are not the only phosphides and arsenides of the alkali metals: for example, potassium has nine different known phosphides, with formulae KP, KP, KP, KP, KP, KP, KP, KP, and KP.
While most metals form arsenides, only the alkali and alkaline earth metals form mostly ionic arsenides.
The structure of NaAs is complex with unusually short Na–Na distances of 328–330 pm which are shorter than in sodium metal, and this indicates that even with these electropositive metals the bonding cannot be straightforwardly ionic.
Other alkali metal arsenides not conforming to the formula MAs are known, such as LiAs, which has a metallic lustre and electrical conductivity indicating the presence of some metallic bonding.
The antimonides are unstable and reactive as the Sb ion is a strong reducing agent; reaction of them with acids form the toxic and unstable gas stibine (SbH).
Indeed, they have some metallic properties, and the alkali metal antimonides of stoichiometry MSb involve antimony atoms bonded in a spiral Zintl structure.
Bismuthides are not even wholly ionic; they are intermetallic compounds containing partially metallic and partially ionic bonds.
All the alkali metals react vigorously with oxygen at standard conditions.
They form various types of oxides, such as simple oxides (containing the O ion), peroxides (containing the ion, where there is a single bond between the two oxygen atoms), superoxides (containing the ion), and many others.
Lithium burns in air to form lithium oxide, but sodium reacts with oxygen to form a mixture of sodium oxide and sodium peroxide.
Potassium forms a mixture of potassium peroxide and potassium superoxide, while rubidium and caesium form the superoxide exclusively.
Their reactivity increases going down the group: while lithium, sodium and potassium merely burn in air, rubidium and caesium are pyrophoric (spontaneously catch fire in air).
The smaller alkali metals tend to polarise the larger anions (the peroxide and superoxide) due to their small size.
This attracts the electrons in the more complex anions towards one of its constituent oxygen atoms, forming an oxide ion and an oxygen atom.
This causes lithium to form the oxide exclusively on reaction with oxygen at room temperature.
This effect becomes drastically weaker for the larger sodium and potassium, allowing them to form the less stable peroxides.
Rubidium and caesium, at the bottom of the group, are so large that even the least stable superoxides can form.
Because the superoxide releases the most energy when formed, the superoxide is preferentially formed for the larger alkali metals where the more complex anions are not polarised.
(The oxides and peroxides for these alkali metals do exist, but do not form upon direct reaction of the metal with oxygen at standard conditions.)
In addition, the small size of the Li and O ions contributes to their forming a stable ionic lattice structure.
Under controlled conditions, however, all the alkali metals, with the exception of francium, are known to form their oxides, peroxides, and superoxides.
The alkali metal peroxides and superoxides are powerful oxidising agents.
Sodium peroxide and potassium superoxide react with carbon dioxide to form the alkali metal carbonate and oxygen gas, which allows them to be used in submarine air purifiers; the presence of water vapour, naturally present in breath, makes the removal of carbon dioxide by potassium superoxide even more efficient.
All the stable alkali metals except lithium can form red ozonides (MO) through low-temperature reaction of the powdered anhydrous hydroxide with ozone: the ozonides may be then extracted using liquid ammonia.
They slowly decompose at standard conditions to the superoxides and oxygen, and hydrolyse immediately to the hydroxides when in contact with water.
Potassium, rubidium, and caesium also form sesquioxides MO, which may be better considered peroxide disuperoxides, .
Rubidium and caesium can form a great variety of suboxides with the metals in formal oxidation states below +1.
Rubidium can form RbO and RbO (copper-coloured) upon oxidation in air, while caesium forms an immense variety of oxides, such as the ozonide CsO and several brightly coloured suboxides, such as CsO (bronze), CsO (red-violet), CsO (violet), CsO (dark green), CsO, CsO, as well as CsO.
The last of these may be heated under vacuum to generate CsO.
The alkali metals can also react analogously with the heavier chalcogens (sulfur, selenium, tellurium, and polonium), and all the alkali metal chalcogenides are known (with the exception of francium's).
Reaction with an excess of the chalcogen can similarly result in lower chalcogenides, with chalcogen ions containing chains of the chalcogen atoms in question.
For example, sodium can react with sulfur to form the sulfide (NaS) and various polysulfides with the formula NaS ("x" from 2 to 6), containing the ions.
Due to the basicity of the Se and Te ions, the alkali metal selenides and tellurides are alkaline in solution; when reacted directly with selenium and tellurium, alkali metal polyselenides and polytellurides are formed along with the selenides and tellurides with the and ions.
They may be obtained directly from the elements in liquid ammonia or when air is not present, and are colourless, water-soluble compounds that air oxidises quickly back to selenium or tellurium.
The alkali metal polonides are all ionic compounds containing the Po ion; they are very chemically stable and can be produced by direct reaction of the elements at around 300–400 °C.
The alkali metals are among the most electropositive elements on the periodic table and thus tend to bond ionically to the most electronegative elements on the periodic table, the halogens (fluorine, chlorine, bromine, iodine, and astatine), forming salts known as the alkali metal halides.
The reaction is very vigorous and can sometimes result in explosions.
All twenty stable alkali metal halides are known; the unstable ones are not known, with the exception of sodium astatide, because of the great instability and rarity of astatine and francium.
The most well-known of the twenty is certainly sodium chloride, otherwise known as common salt.
All of the stable alkali metal halides have the formula MX where M is an alkali metal and X is a halogen.
They are all white ionic crystalline solids that have high melting points.
All the alkali metal halides are soluble in water except for lithium fluoride (LiF), which is insoluble in water due to its very high lattice enthalpy.
The high lattice enthalpy of lithium fluoride is due to the small sizes of the Li and F ions, causing the electrostatic interactions between them to be strong: a similar effect occurs for magnesium fluoride, consistent with the diagonal relationship between lithium and magnesium.
The alkali metals also react similarly with hydrogen to form ionic alkali metal hydrides, where the hydride anion acts as a pseudohalide: these are often used as reducing agents, producing hydrides, complex metal hydrides, or hydrogen gas.
Other pseudohalides are also known, notably the cyanides.
These are isostructural to the respective halides except for lithium cyanide, indicating that the cyanide ions may rotate freely.
Ternary alkali metal halide oxides, such as NaClO, KBrO (yellow), NaBrO, NaIO, and KBrO, are also known.
The polyhalides are rather unstable, although those of rubidium and caesium are greatly stabilised by the feeble polarising power of these extremely large cations.
Alkali metal cations do not usually form coordination complexes with simple Lewis bases due to their low charge of just +1 and their relatively large size; thus the Li ion forms most complexes and the heavier alkali metal ions form less and less (though exceptions occur for weak complexes).
Lithium in particular has a very rich coordination chemistry in which it exhibits coordination numbers from 1 to 12, although octahedral hexacoordination is its preferred mode.
In aqueous solution, the alkali metal ions exist as octahedral hexahydrate complexes ([M(HO))]), with the exception of the lithium ion, which due to its small size forms tetrahedral tetrahydrate complexes ([Li(HO))]); the alkali metals form these complexes because their ions are attracted by electrostatic forces of attraction to the polar water molecules.
Because of this, anhydrous salts containing alkali metal cations are often used as desiccants.
Alkali metals also readily form complexes with crown ethers (e.g.
12-crown-4 for Li, 15-crown-5 for Na, 18-crown-6 for K, and 21-crown-7 for Rb) and cryptands due to electrostatic attraction.
The alkali metals dissolve slowly in liquid ammonia, forming ammoniacal solutions of solvated M and e, which react to form hydrogen gas and the alkali metal amide (MNH, where M represents an alkali metal): this was first noted by Humphry Davy in 1809 and rediscovered by W. Weyl in 1864.
The process may be speeded up by a catalyst.
Similar solutions are formed by the heavy divalent alkaline earth metals calcium, strontium, barium, as well as the divalent lanthanides, europium and ytterbium.
The amide salt is quite insoluble and readily precipitates out of solution, leaving intensely coloured ammonia solutions of the alkali metals.
In 1907, Charles Krause identified the colour as being due to the presence of solvated electrons, which contribute to the high electrical conductivity of these solutions.
At low concentrations (below 3 M), the solution is dark blue and has ten times the conductivity of aqueous sodium chloride; at higher concentrations (above 3 M), the solution is copper-coloured and has approximately the conductivity of liquid metals like mercury.
In addition to the alkali metal amide salt and solvated electrons, such ammonia solutions also contain the alkali metal cation (M), the neutral alkali metal atom (M), diatomic alkali metal molecules (M) and alkali metal anions (M).
These are unstable and eventually become the more thermodynamically stable alkali metal amide and hydrogen gas.
Solvated electrons are powerful reducing agents and are often used in chemical synthesis.
Being the smallest alkali metal, lithium forms the widest variety of and most stable organometallic compounds, which are bonded covalently.
Organolithium compounds are electrically non-conducting volatile solids or liquids that melt at low temperatures, and tend to form oligomers with the structure (RLi) where R is the organic group.
As the electropositive nature of lithium puts most of the charge density of the bond on the carbon atom, effectively creating a carbanion, organolithium compounds are extremely powerful bases and nucleophiles.
For use as bases, butyllithiums are often used and are commercially available.
An example of an organolithium compound is methyllithium ((CHLi)), which exists in tetrameric ("x" = 4, tetrahedral) and hexameric ("x" = 6, octahedral) forms.
Organolithium compounds, especially "n"-butyllithium, are useful reagents in organic synthesis, as might be expected given lithium's diagonal relationship with magnesium, which plays an important role in the Grignard reaction.
For example, alkyllithiums and aryllithiums may be used to synthesise aldehydes and ketones by reaction with metal carbonyls.
The reaction with nickel tetracarbonyl, for example, proceeds through an unstable acyl nickel carbonyl complex which then undergoes electrophilic substitution to give the desired aldehyde (using H as the electrophile) or ketone (using an alkyl halide) product.
Alkyllithiums and aryllithiums may also react with "N","N"-disubstituted amides to give aldehydes and ketones, and symmetrical ketones by reacting with carbon monoxide.
They thermally decompose to eliminate a β-hydrogen, producing alkenes and lithium hydride: another route is the reaction of ethers with alkyl- and aryllithiums that act as strong bases.
In non-polar solvents, aryllithiums react as the carbanions they effectively are, turning carbon dioxide to aromatic carboxylic acids (ArCOH) and aryl ketones to tertiary carbinols (Ar'C(Ar)OH).
Finally, they may be used to synthesise other organometallic compounds through metal-halogen exchange.
Unlike the organolithium compounds, the organometallic compounds of the heavier alkali metals are predominantly ionic.
The application of organosodium compounds in chemistry is limited in part due to competition from organolithium compounds, which are commercially available and exhibit more convenient reactivity.
The principal organosodium compound of commercial importance is sodium cyclopentadienide.
Sodium tetraphenylborate can also be classified as an organosodium compound since in the solid state sodium is bound to the aryl groups.
Organometallic compounds of the higher alkali metals are even more reactive than organosodium compounds and of limited utility.
A notable reagent is Schlosser's base, a mixture of "n"-butyllithium and potassium "tert"-butoxide.
This reagent reacts with propene to form the compound allylpotassium (KCHCHCH).
"cis"-2-Butene and "trans"-2-butene equilibrate when in contact with alkali metals.
Whereas isomerisation is fast with lithium and sodium, it is slow with the heavier alkali metals.
The heavier alkali metals also favour the sterically congested conformation.
Several crystal structures of organopotassium compounds have been reported, establishing that they, like the sodium compounds, are polymeric.
Organosodium, organopotassium, organorubidium and organocaesium compounds are all mostly ionic and are insoluble (or nearly so) in nonpolar solvents.
Alkyl and aryl derivatives of sodium and potassium tend to react with air.
They cause the cleavage of ethers, generating alkoxides.
Unlike alkyllithium compounds, alkylsodiums and alkylpotassiums cannot be made by reacting the metals with alkyl halides because Wurtz coupling occurs:

As such, they have to be made by reacting alkylmercury compounds with sodium or potassium metal in inert hydrocarbon solvents.
While methylsodium forms tetramers like methyllithium, methylpotassium is more ionic and has the nickel arsenide structure with discrete methyl anions and potassium cations.
The alkali metals and their hydrides react with acidic hydrocarbons, for example cyclopentadienes and terminal alkynes, to give salts.
Liquid ammonia, ether, or hydrocarbon solvents are used, the most common of which being tetrahydrofuran.
The most important of these compounds is sodium cyclopentadienide, NaCH, an important precursor to many transition metal cyclopentadienyl derivatives.
Similarly, the alkali metals react with cyclooctatetraene in tetrahydrofuran to give alkali metal cyclooctatetraenides; for example, dipotassium cyclooctatetraenide (KCH) is an important precursor to many metal cyclooctatetraenyl derivatives, such as uranocene.
The large and very weakly polarising alkali metal cations can stabilise large, aromatic, polarisable radical anions, such as the dark-green sodium naphthalenide, Na[CH•], a strong reducing agent.
Although francium is the heaviest alkali metal that has been discovered, there has been some theoretical work predicting the physical and chemical characteristics of the hypothetical heavier alkali metals.
Being the first period 8 element, the undiscovered element ununennium (element 119) is predicted to be the next alkali metal after francium and behave much like their lighter congeners; however, it is also predicted to differ from the lighter alkali metals in some properties.
Its chemistry is predicted to be closer to that of potassium or rubidium instead of caesium or francium.
This is unusual as periodic trends, ignoring relativistic effects would predict ununennium to be even more reactive than caesium and francium.
This lowered reactivity is due to the relativistic stabilisation of ununennium's valence electron, increasing ununennium's first ionisation energy and decreasing the metallic and ionic radii; this effect is already seen for francium.
This assumes that ununennium will behave chemically as an alkali metal, which, although likely, may not be true due to relativistic effects.
The relativistic stabilisation of the 8s orbital also increases ununennium's electron affinity far beyond that of caesium and francium; indeed, ununennium is expected to have an electron affinity higher than all the alkali metals lighter than it.
Relativistic effects also cause a very large drop in the polarisability of ununennium.
On the other hand, ununennium is predicted to continue the trend of melting points decreasing going down the group, being expected to have a melting point between 0 °C and 30 °C.
The stabilisation of ununennium's valence electron and thus the contraction of the 8s orbital cause its atomic radius to be lowered to 240 pm, very close to that of rubidium (247 pm), so that the chemistry of ununennium in the +1 oxidation state should be more similar to the chemistry of rubidium than to that of francium.
On the other hand, the ionic radius of the Uue ion is predicted to be larger than that of Rb, because the 7p orbitals are destabilised and are thus larger than the p-orbitals of the lower shells.
Ununennium may also show the +3 oxidation state, which is not seen in any other alkali metal, in addition to the +1 oxidation state that is characteristic of the other alkali metals and is also the main oxidation state of all the known alkali metals: this is because of the destabilisation and expansion of the 7p spinor, causing its outermost electrons to have a lower ionisation energy than what would otherwise be expected.
Indeed, many ununennium compounds are expected to have a large covalent character, due to the involvement of the 7p electrons in the bonding.
Not as much work has been done predicting the properties of the alkali metals beyond ununennium.
Although a simple extrapolation of the periodic table would put element 169, unhexennium, under ununennium, Dirac-Fock calculations predict that the next alkali metal after ununennium may actually be element 165, unhexpentium, which is predicted to have the electron configuration [Og] 5g 6f 7d 8s 8p 9s.
Furthermore, this element would be intermediate in properties between an alkali metal and a group 11 element, and while its physical and atomic properties would be closer to the former, its chemistry may be closer to that of the latter.
Further calculations show that unhexpentium would follow the trend of increasing ionisation energy beyond caesium, having an ionisation energy comparable to that of sodium, and that it should also continue the trend of decreasing atomic radii beyond caesium, having an atomic radius comparable to that of potassium.
However, the 7d electrons of unhexpentium may also be able to participate in chemical reactions along with the 9s electron, possibly allowing oxidation states beyond +1, whence the likely transition metal behaviour of unhexpentium.
Due to the alkali and alkaline earth metals both being s-block elements, these predictions for the trends and properties of ununennium and unhexpentium also mostly hold quite similarly for the corresponding alkaline earth metals unbinilium (Ubn) and unhexhexium (Uhh).
The probable properties of further alkali metals beyond unhexpentium have not been explored yet as of 2015; in fact, it is suspected that they may not be able to exist.
In periods 8 and above of the periodic table, relativistic and shell-structure effects become so strong that extrapolations from lighter congeners become completely inaccurate.
In addition, the relativistic and shell-structure effects (which stabilise the s-orbitals and destabilise and expand the d-, f-, and g-orbitals of higher shells) have opposite effects, causing even larger difference between relativistic and non-relativistic calculations of the properties of elements with such high atomic numbers.
Interest in the chemical properties of ununennium and unhexpentium stems from the fact that both elements are located close to the expected locations of islands of stabilities, centered at elements 122 (Ubb) and 164 (Uhq).
Many other substances are similar to the alkali metals in their tendency to form monopositive cations.
Analogously to the pseudohalogens, they have sometimes been called "pseudo-alkali metals".
These substances include some elements and many more polyatomic ions; the polyatomic ions are especially similar to the alkali metals in their large size and weak polarising power.
The element hydrogen, with one electron per neutral atom, is usually placed at the top of Group 1 of the periodic table for convenience, but hydrogen is not normally considered to be an alkali metal; when it is considered to be an alkali metal, it is because of its atomic properties and not its chemical properties.
Under typical conditions, pure hydrogen exists as a diatomic gas consisting of two atoms per molecule (H); however, the alkali metals only form diatomic molecules (such as dilithium, Li) at high temperatures, when they are in the gaseous state.
Hydrogen, like the alkali metals, has one valence electron and reacts easily with the halogens, but the similarities end there because of the small size of a bare proton H compared to the alkali metal cations.
Its placement above lithium is primarily due to its electron configuration.
It is sometimes placed above carbon due to their similar electronegativities or fluorine due to their similar chemical properties.
The first ionisation energy of hydrogen (1312.0 kJ/mol) is much higher than that of the alkali metals.
As only one additional electron is required to fill in the outermost shell of the hydrogen atom, hydrogen often behaves like a halogen, forming the negative hydride ion, and is very occasionally considered to be a halogen on that basis.
(The alkali metals can also form negative ions, known as alkalides, but these are little more than laboratory curiosities, being unstable.)
An argument against this placement is that formation of hydride from hydrogen is endothermic, unlike the exothermic formation of halides from halogens.
The radius of the H anion also does not fit the trend of increasing size going down the halogens: indeed, H is very diffuse because its single proton cannot easily control both electrons.
It was expected for some time that liquid hydrogen would show metallic properties; while this has been shown to not be the case, under extremely high pressures, such as those found at the cores of Jupiter and Saturn, hydrogen does become metallic and behaves like an alkali metal; in this phase, it is known as metallic hydrogen.
The electrical resistivity of liquid metallic hydrogen at 3000 K is approximately equal to that of liquid rubidium and caesium at 2000 K at the respective pressures when they undergo a nonmetal-to-metal transition.
The 1s electron configuration of hydrogen, while superficially similar to that of the alkali metals (ns), is unique because there is no 1p subshell.
Hence it can lose an electron to form the hydron H, or gain one to form the hydride ion H. In the former case it resembles superficially the alkali metals; in the latter case, the halogens, but the differences due to the lack of a 1p subshell are important enough that neither group fits the properties of hydrogen well.
Group 14 is also a good fit in terms of thermodynamic properties such as ionisation energy and electron affinity, but makes chemical nonsense because hydrogen cannot be tetravalent.
Thus none of the three placements are entirely satisfactory, although group 1 is the most common placement (if one is chosen) because the hydron is by far the most important of all monatomic hydrogen species, being the foundation of acid-base chemistry.
As an example of hydrogen's unorthodox properties stemming from its unusual electron configuration and small size, the hydrogen ion is very small (radius around 150 fm compared to the 50–220 pm size of most other atoms and ions) and so is nonexistent in condensed systems other than in association with other atoms or molecules.
Indeed, transferring of protons between chemicals is the basis of acid-base chemistry.
Also unique is hydrogen's ability to form hydrogen bonds, which are an effect of charge-transfer, electrostatic, and electron correlative contributing phenomena.
While analogous lithium bonds are also known, they are mostly electrostatic.
Nevertheless, hydrogen can take on the same structural role as the alkali metals in some molecular crystals, and has a close relationship with the lightest alkali metals (especially lithium).
The ammonium ion () has very similar properties to the heavier alkali metals, acting as an alkali metal intermediate between potassium and rubidium, and is often considered a close relative.
For example, most alkali metal salts are soluble in water, a property which ammonium salts share.
Ammonium is expected to behave stably as a metal ( ions in a sea of delocalised electrons) at very high pressures (though less than the typical pressure where transitions from insulating to metallic behaviour occur around, 100 GPa), and could possibly occur inside the ice giants Uranus and Neptune, which may have significant impacts on their interior magnetic fields.
It has been estimated that the transition from a mixture of ammonia and dihydrogen molecules to metallic ammonium may occur at pressures just below 25 GPa.
Under standard conditions, ammonium can form a metallic amalgam with mercury.
Other "pseudo-alkali metals" include the alkylammonium cations, in which some of the hydrogen atoms in the ammonium cation are replaced by alkyl or aryl groups.
In particular, the quaternary ammonium cations () are very useful since they are permanently charged, and they are often used as an alternative to the expensive Cs to stabilise very large and very easily polarisable anions such as .
Tetraalkylammonium hydroxides, like alkali metal hydroxides, are very strong bases that react with atmospheric carbon dioxide to form carbonates.
Furthermore, the nitrogen atom may be replaced by a phosphorus, arsenic, or antimony atom (the heavier nonmetallic pnictogens), creating a phosphonium () or arsonium () cation that can itself be substituted similarly; while stibonium () itself is not known, some of its organic derivatives are characterised.
Cobaltocene, Co(CH), is a metallocene, the cobalt analogue of ferrocene.
It is a dark purple solid.
Cobaltocene has 19 valence electrons, one more than usually found in organotransition metal complexes, such as its very stable relative, ferrocene, in accordance with the 18-electron rule.
This additional electron occupies an orbital that is antibonding with respect to the Co–C bonds.
Consequently, many chemical reactions of Co(CH) are characterized by its tendency to lose this "extra" electron, yielding a very stable 18-electron cation known as cobaltocenium.
Many cobaltocenium salts coprecipitate with caesium salts, and cobaltocenium hydroxide is a strong base that absorbs atmospheric carbon dioxide to form cobaltocenium carbonate.
Like the alkali metals, cobaltocene is a strong reducing agent, and decamethylcobaltocene is stronger still due to the combined inductive effect of the ten methyl groups.
Cobalt may be substituted by its heavier congener rhodium to give rhodocene, an even stronger reducing agent.
Iridocene (involving iridium) would presumably be still more potent, but is not very well-studied due to its instability.
Thallium is the heaviest stable element in group 13 of the periodic table.
At the bottom of the periodic table, the inert pair effect is quite strong, because of the relativistic stabilisation of the 6s orbital and the decreasing bond energy as the atoms increase in size so that the amount of energy released in forming two more bonds is not worth the high ionisation energies of the 6s electrons.
It displays the +1 oxidation state that all the known alkali metals display, and thallium compounds with thallium in its +1 oxidation state closely resemble the corresponding potassium or silver compounds stoichiometrically due to the similar ionic radii of the Tl (164 pm), K (152 pm) and Ag (129 pm) ions.
It was sometimes considered an alkali metal in continental Europe (but not in England) in the years immediately following its discovery, and was placed just after caesium as the sixth alkali metal in Dmitri Mendeleev's 1869 periodic table and Julius Lothar Meyer's 1868 periodic table.
(Mendeleev's 1871 periodic table and Meyer's 1870 periodic table put thallium in its current position in the boron group and left the space below caesium blank.)
However, thallium also displays the oxidation state +3, which no known alkali metal displays (although ununennium, the undiscovered seventh alkali metal, is predicted to possibly display the +3 oxidation state).
The sixth alkali metal is now considered to be francium.
While Tl is stabilised by the inert pair effect, this inert pair of 6s electrons is still able to participate chemically, so that these electrons are stereochemically active in aqueous solution.
Additionally, the thallium halides (except TlF) are quite insoluble in water, and TlI has an unusual structure because of the presence of the stereochemically active inert pair in thallium.
The group 11 metals (or coinage metals), copper, silver, and gold, are typically categorised as transition metals given they can form ions with incomplete d-shells.
Physically, they have the relatively low melting points and high electronegativity values associated with post-transition metals.
"The filled "d" subshell and free "s" electron of Cu, Ag, and Au contribute to their high electrical and thermal conductivity.
Transition metals to the left of group 11 experience interactions between "s" electrons and the partially filled "d" subshell that lower electron mobility."
Chemically, the group 11 metals behave like main-group metals in their +1 valence states, and are hence somewhat related to the alkali metals: this is one reason for their previously being labelled as "group IB", paralleling the alkali metals' "group IA".
They are occasionally classified as post-transition metals.
Their spectra are analogous to those of the alkali metals.
Their monopositive ions are paramagnetic and contribute no colour to their salts, like those of the alkali metals.
In Mendeleev's 1871 periodic table, copper, silver, and gold are listed twice, once under group VIII (with the iron triad and platinum group metals), and once under group IB.
Group IB was nonetheless parenthesised to note that it was tentative.
Mendeleev's main criterion for group assignment was the maximum oxidation state of an element: on that basis, the group 11 elements could not be classified in group IB, due to the existence of copper(II) and gold(III) compounds being known at that time.
However, eliminating group IB would make group I the only main group (group VIII was labelled a transition group) to lack an A–B bifurcation.
Soon afterward, a majority of chemists chose to classify these elements in group IB and remove them from group VIII for the resulting symmetry: this was the predominant classification until the rise of the modern medium-long 18-column periodic table, which separated the alkali metals and group 11 metals.
The coinage metals were traditionally regarded as a subdivision of the alkali metal group, due to them sharing the characteristic s electron configuration of the alkali metals (group 1: ps; group 11: ds).
However, the similarities are largely confined to the stoichiometries of the +1 compounds of both groups, and not their chemical properties.
This stems from the filled d subshell providing a much weaker shielding effect on the outermost s electron than the filled p subshell, so that the coinage metals have much higher first ionisation energies and smaller ionic radii than do the corresponding alkali metals.
Furthermore, they have higher melting points, hardnesses, and densities, and lower reactivities and solubilities in liquid ammonia, as well as having more covalent character in their compounds.
Finally, the alkali metals are at the top of the electrochemical series, whereas the coinage metals are almost at the very bottom.
The coinage metals' filled d shell is much more easily disrupted than the alkali metals' filled p shell, so that the second and third ionisation energies are lower, enabling higher oxidation states than +1 and a richer coordination chemistry, thus giving the group 11 metals clear transition metal character.
Particularly noteworthy is gold forming ionic compounds with rubidium and caesium, in which it forms the auride ion (Au) which also occurs in solvated form in liquid ammonia solution: here gold behaves as a pseudohalogen because its 5d6s configuration has one electron less than the quasi-closed shell 5d6s configuration of mercury.
The production of pure alkali metals is somewhat complicated due to their extreme reactivity with commonly used substances, such as water.
From their silicate ores, all the stable alkali metals may be obtained the same way: sulfuric acid is first used to dissolve the desired alkali metal ion and aluminium(III) ions from the ore (leaching), whereupon basic precipitation removes aluminium ions from the mixture by precipitating it as the hydroxide.
The remaining insoluble alkali metal carbonate is then precipitated selectively; the salt is then dissolved in hydrochloric acid to produce the chloride.
The result is then left to evaporate and the alkali metal can then be isolated.
Lithium and sodium are typically isolated through electrolysis from their liquid chlorides, with calcium chloride typically added to lower the melting point of the mixture.
The heavier alkali metals, however, is more typically isolated in a different way, where a reducing agent (typically sodium for potassium and magnesium or calcium for the heaviest alkali metals) is used to reduce the alkali metal chloride.
The liquid or gaseous product (the alkali metal) then undergoes fractional distillation for purification.
Most routes to the pure alkali metals require the use of electrolysis due to their high reactivity; one of the few which does not is the pyrolysis of the corresponding alkali metal azide, which yields the metal for sodium, potassium, rubidium, and caesium and the nitride for lithium.
Lithium salts have to be extracted from the water of mineral springs, brine pools, and brine deposits.
The metal is produced electrolytically from a mixture of fused lithium chloride and potassium chloride.
Sodium occurs mostly in seawater and dried seabed, but is now produced through electrolysis of sodium chloride by lowering the melting point of the substance to below 700 °C through the use of a Downs cell.
Extremely pure sodium can be produced through the thermal decomposition of sodium azide.
Potassium occurs in many minerals, such as sylvite (potassium chloride).
Previously, potassium was generally made from the electrolysis of potassium chloride or potassium hydroxide, found extensively in places such as Canada, Russia, Belarus, Germany, Israel, United States, and Jordan, in a method similar to how sodium was produced in the late 1800s and early 1900s.
It can also be produced from seawater.
However, these methods are problematic because the potassium metal tends to dissolve in its molten chloride and vaporises significantly at the operating temperatures, potentially forming the explosive superoxide.
As a result, pure potassium metal is now produced by reducing molten potassium chloride with sodium metal at 850 °C.
Although sodium is less reactive than potassium, this process works because at such high temperatures potassium is more volatile than sodium and can easily be distilled off, so that the equilibrium shifts towards the right to produce more potassium gas and proceeds almost to completion.
For several years in the 1950s and 1960s, a by-product of the potassium production called Alkarb was a main source for rubidium.
Alkarb contained 21% rubidium while the rest was potassium and a small fraction of caesium.
Today the largest producers of caesium, for example the Tanco Mine in Manitoba, Canada, produce rubidium as by-product from pollucite.
Today, a common method for separating rubidium from potassium and caesium is the fractional crystallisation of a rubidium and caesium alum (Cs,Rb)Al(SO)·12HO, which yields pure rubidium alum after approximately 30 recrystallisations.
The limited applications and the lack of a mineral rich in rubidium limit the production of rubidium compounds to 2 to 4 tonnes per year.
Caesium, however, is not produced from the above reaction.
Instead, the mining of pollucite ore is the main method of obtaining pure caesium, extracted from the ore mainly by three methods: acid digestion, alkaline decomposition, and direct reduction.
Both metals are produced as by-products of lithium production: after 1958, when interest in lithium's thermonuclear properties increased sharply, the production of rubidium and caesium also increased correspondingly.
Pure rubidium and caesium metals are produced by reducing their chlorides with calcium metal at 750 °C and low pressure.
As a result of its extreme rarity in nature, most francium is synthesised in the nuclear reaction Au + O → Fr + 5 n, yielding francium-209, francium-210, and francium-211.
The greatest quantity of francium ever assembled to date is about 300,000 neutral atoms, which were synthesised using the nuclear reaction given above.
When the only natural isotope francium-223 is specifically required, it is produced as the alpha daughter of actinium-227, itself produced synthetically from the neutron irradiation of natural radium-226, one of the daughters of natural uranium-238.
Lithium, sodium, and potassium have many applications, while rubidium and caesium are very useful in academic contexts but do not have many applications yet.
Lithium is often used in lithium-ion batteries, and lithium oxide can help process silica.
Lithium stearate is a thickener and can be used to make lubricating greases; it is produced from lithium hydroxide, which is also used to absorb carbon dioxide in space capsules and submarines.
Lithium chloride is used as a brazing alloy for aluminium parts.
Metallic lithium is used in alloys with magnesium and aluminium to give very tough and light alloys.
Sodium compounds have many applications, the most well-known being sodium chloride as table salt.
Sodium salts of fatty acids are used as soap.
Pure sodium metal also has many applications, including use in sodium-vapour lamps, which produce very efficient light compared to other types of lighting, and can help smooth the surface of other metals.
Being a strong reducing agent, it is often used to reduce many other metals, such as titanium and zirconium, from their chlorides.
Furthermore, it is very useful as a heat-exchange liquid in fast breeder nuclear reactors due to its low melting point, viscosity, and cross-section towards neutron absorption.
Potassium compounds are often used as fertilisers as potassium is an important element for plant nutrition.
Potassium hydroxide is a very strong base, and is used to control the pH of various substances.
Potassium nitrate and potassium permanganate are often used as powerful oxidising agents.
Potassium superoxide is used in breathing masks, as it reacts with carbon dioxide to give potassium carbonate and oxygen gas.
Pure potassium metal is not often used, but its alloys with sodium may substitute for pure sodium in fast breeder nuclear reactors.
Rubidium and caesium are often used in atomic clocks.
Caesium atomic clocks are extraordinarily accurate; if a clock had been made at the time of the dinosaurs, it would be off by less than four seconds (after 80 million years).
For that reason, caesium atoms are used as the definition of the second.
Rubidium ions are often used in purple fireworks, and caesium is often used in drilling fluids in the petroleum industry.
Francium has no commercial applications, but because of francium's relatively simple atomic structure, among other things, it has been used in spectroscopy experiments, leading to more information regarding energy levels and the coupling constants between subatomic particles.
Studies on the light emitted by laser-trapped francium-210 ions have provided accurate data on transitions between atomic energy levels, similar to those predicted by quantum theory.
Pure alkali metals are dangerously reactive with air and water and must be kept away from heat, fire, oxidising agents, acids, most organic compounds, halocarbons, plastics, and moisture.
They also react with carbon dioxide and carbon tetrachloride, so that normal fire extinguishers are counterproductive when used on alkali metal fires.
Some Class D dry powder extinguishers designed for metal fires are effective, depriving the fire of oxygen and cooling the alkali metal.
Experiments are usually conducted using only small quantities of a few grams in a fume hood.
Small quantities of lithium may be disposed of by reaction with cool water, but the heavier alkali metals should be dissolved in the less reactive isopropanol.
The alkali metals must be stored under mineral oil or an inert atmosphere.
The inert atmosphere used may be argon or nitrogen gas, except for lithium, which reacts with nitrogen.
Rubidium and caesium must be kept away from air, even under oil, because even a small amount of air diffused into the oil may trigger formation of the dangerously explosive peroxide; for the same reason, potassium should not be stored under oil in an oxygen-containing atmosphere for longer than 6 months.
The bioinorganic chemistry of the alkali metal ions has been extensively reviewed.
Solid state crystal structures have been determined for many complexes of alkali metal ions in small peptides, nucleic acid constituents, carbohydrates and ionophore complexes.
Lithium naturally only occurs in traces in biological systems and has no known biological role, but does have effects on the body when ingested.
Lithium carbonate is used as a mood stabiliser in psychiatry to treat bipolar disorder (manic-depression) in daily doses of about 0.5 to 2 grams, although there are side-effects.
Excessive ingestion of lithium causes drowsiness, slurred speech and vomiting, among other symptoms, and poisons the central nervous system, which is dangerous as the required dosage of lithium to treat bipolar disorder is only slightly lower than the toxic dosage.
Its biochemistry, the way it is handled by the human body and studies using rats and goats suggest that it is an essential trace element, although the natural biological function of lithium in humans has yet to be identified.
Sodium and potassium occur in all known biological systems, generally functioning as electrolytes inside and outside cells.
Sodium is an essential nutrient that regulates blood volume, blood pressure, osmotic equilibrium and pH; the minimum physiological requirement for sodium is 500 milligrams per day.
Sodium chloride (also known as common salt) is the principal source of sodium in the diet, and is used as seasoning and preservative, such as for pickling and jerky; most of it comes from processed foods.
The Dietary Reference Intake for sodium is 1.5 grams per day, but most people in the United States consume more than 2.3 grams per day, the minimum amount that promotes hypertension; this in turn causes 7.6 million premature deaths worldwide.
Potassium is the major cation (positive ion) inside animal cells, while sodium is the major cation outside animal cells.
The concentration differences of these charged particles causes a difference in electric potential between the inside and outside of cells, known as the membrane potential.
The balance between potassium and sodium is maintained by ion transporter proteins in the cell membrane.
The cell membrane potential created by potassium and sodium ions allows the cell to generate an action potential—a "spike" of electrical discharge.
The ability of cells to produce electrical discharge is critical for body functions such as neurotransmission, muscle contraction, and heart function.
Disruption of this balance may thus be fatal: for example, ingestion of large amounts of potassium compounds can lead to hyperkalemia strongly influencing the cardiovascular system.
Potassium chloride is used in the United States for lethal injection executions.
Due to their similar atomic radii, rubidium and caesium in the body mimic potassium and are taken up similarly.
Rubidium has no known biological role, but may help stimulate metabolism, and, similarly to caesium, replace potassium in the body causing potassium deficiency.
Partial substitution is quite possible and rather non-toxic: a 70 kg person contains on average 0.36 g of rubidium, and an increase in this value by 50 to 100 times did not show negative effects in test persons.
Rats can survive up to 50% substitution of potassium by rubidium.
Rubidium (and to a much lesser extent caesium) can function as temporary cures for hypokalemia; while rubidium can adequately physiologically substitute potassium in some systems, caesium is never able to do so.
There is only very limited evidence in the form of deficiency symptoms for rubidium being possibly essential in goats; even if this is true, the trace amounts usually present in food are more than enough.
Caesium compounds are rarely encountered by most people, but most caesium compounds are mildly toxic.
Like rubidium, caesium tends to substitute potassium in the body, but is significantly larger and is therefore a poorer substitute.
Excess caesium can lead to hypokalemia, arrythmia, and acute cardiac arrest, but such amounts would not ordinarily be encountered in natural sources.
As such, caesium is not a major chemical environmental pollutant.
The median lethal dose (LD) value for caesium chloride in mice is 2.3 g per kilogram, which is comparable to the LD values of potassium chloride and sodium chloride.
Caesium chloride has been promoted as an alternative cancer therapy, but has been linked to the deaths of over 50 patients, on whom it was used as part of a scientifically unvalidated cancer treatment.
Radioisotopes of caesium require special precautions: the improper handling of caesium-137 gamma ray sources can lead to release of this radioisotope and radiation injuries.
Perhaps the best-known case is the Goiânia accident of 1987, in which an improperly-disposed-of radiation therapy system from an abandoned clinic in the city of Goiânia, Brazil, was scavenged from a junkyard, and the glowing caesium salt sold to curious, uneducated buyers.
This led to four deaths and serious injuries from radiation exposure.
Together with caesium-134, iodine-131, and strontium-90, caesium-137 was among the isotopes distributed by the Chernobyl disaster which constitute the greatest risk to health.
Radioisotopes of francium would presumably be dangerous as well due to their high decay energy and short half-life, but none have been produced in large enough amounts to pose any serious risk.
</doc>
<doc id="670" url="https://en.wikipedia.org/wiki?curid=670" title="Alphabet">
Alphabet

An alphabet is a standard set of letters (basic written symbols or graphemes) that represent the phonemes (basic significant sounds) of any spoken language it is used to write.
This is in contrast to other types of writing systems, such as syllabaries (in which each character represents a syllable) and logographic systems (in which each character represents a word, morpheme, or semantic unit).
The first fully phonemic script, the Proto-Canaanite script, later known as the Phoenician alphabet, is considered to be the first alphabet, and is the ancestor of most modern alphabets, including Arabic, Greek, Latin, Cyrillic, Hebrew, and possibly Brahmic.
Peter T. Daniels, however, distinguishes an abugida or alphasyllabary, a set of graphemes that represent consonantal base letters which diacritics modify to represent vowels (as in Devanagari and other South Asian scripts), an abjad, in which letters predominantly or exclusively represent consonants (as in the original Phoenician, Hebrew or Arabic), and an "alphabet," a set of graphemes that represent both vowels and consonants.
In this narrow sense of the word the first "true" alphabet was the Greek alphabet, which was developed on the basis of the earlier Phoenician alphabet.
Of the dozens of alphabets in use today, the most popular is the Latin alphabet, which was derived from the Greek, and which many languages modify by adding letters formed using diacritical marks.
While most alphabets have letters composed of lines (linear writing), there are also exceptions such as the alphabets used in Braille.
The Khmer alphabet (for Cambodian) is the longest, with 74 letters.
Alphabets are usually associated with a standard ordering of letters.
This makes them useful for purposes of collation, specifically by allowing words to be sorted in alphabetical order.
It also means that their letters can be used as an alternative method of "numbering" ordered items, in such contexts as numbered lists and number placements.
The English word "alphabet" came into Middle English from the Late Latin word "alphabetum", which in turn originated in the Greek ἀλφάβητος ("alphabētos").
The Greek word was made from the first two letters, "alpha"(α) and "beta"(β).
The names for the Greek letters came from the first two letters of the Phoenician alphabet; "aleph", which also meant "ox", and "bet", which also meant "house".
Sometimes, like in the alphabet song in English, the term "ABCs" is used instead of the word "alphabet" ("Now I know my ABCs"...).
"Knowing one's ABCs", in general, can be used as a metaphor for knowing the basics about anything.
The history of the alphabet started in ancient Egypt.
Egyptian writing had a set of some 24 hieroglyphs that are called uniliterals, to represent syllables that begin with a single consonant of their language, plus a vowel (or no vowel) to be supplied by the native speaker.
These glyphs were used as pronunciation guides for logograms, to write grammatical inflections, and, later, to transcribe loan words and foreign names.
In the Middle Bronze Age, an apparently "alphabetic" system known as the Proto-Sinaitic script appears in Egyptian turquoise mines in the Sinai peninsula dated to circa the 15th century BC, apparently left by Canaanite workers.
In 1999, John and Deborah Darnell discovered an even earlier version of this first alphabet at Wadi el-Hol dated to circa 1800 BC and showing evidence of having been adapted from specific forms of Egyptian hieroglyphs that could be dated to circa 2000 BC, strongly suggesting that the first alphabet had been developed about that time.
Based on letter appearances and names, it is believed to be based on Egyptian hieroglyphs.
This script had no characters representing vowels, although originally it probably was a syllabary, but unneeded symbols were discarded.
An alphabetic cuneiform script with 30 signs including three that indicate the following vowel was invented in Ugarit before the 15th century BC.
This script was not used after the destruction of Ugarit.
The Proto-Sinaitic script eventually developed into the Phoenician alphabet, which is conventionally called "Proto-Canaanite" before ca.
1050 BC.
The oldest text in Phoenician script is an inscription on the sarcophagus of King Ahiram.
This script is the parent script of all western alphabets.
By the tenth century, two other forms can be distinguished, namely Canaanite and Aramaic.
The Aramaic gave rise to the Hebrew script.
The South Arabian alphabet, a sister script to the Phoenician alphabet, is the script from which the Ge'ez alphabet (an abugida) is descended.
Vowelless alphabets, which are not true alphabets, are called abjads, currently exemplified in scripts including Arabic, Hebrew, and Syriac.
The omission of vowels was not always a satisfactory solution and some "weak" consonants are sometimes used to indicate the vowel quality of a syllable (matres lectionis).
These letters have a dual function since they are also used as pure consonants.
The Proto-Sinaitic or Proto-Canaanite script and the Ugaritic script were the first scripts with a limited number of signs, in contrast to the other widely used writing systems at the time, Cuneiform, Egyptian hieroglyphs, and Linear B. The Phoenician script was probably the first phonemic script and it contained only about two dozen distinct letters, making it a script simple enough for common traders to learn.
Another advantage of Phoenician was that it could be used to write down many different languages, since it recorded words phonemically.
The script was spread by the Phoenicians across the Mediterranean.
In Greece, the script was modified to add vowels, giving rise to the ancestor of all alphabets in the West.
The vowels have independent letter forms separate from those of consonants; therefore it was the first true alphabet.
The Greeks chose letters representing sounds that did not exist in Greek to represent vowels.
Vowels are significant in the Greek language, and the syllabical Linear B script that was used by the Mycenaean Greeks from the 16th century BC had 87 symbols, including 5 vowels.
In its early years, there were many variants of the Greek alphabet, a situation that caused many different alphabets to evolve from it.
The Greek alphabet, in its Euboean form, was carried over by Greek colonists to the Italian peninsula, where it gave rise to a variety of alphabets used to write the Italic languages.
One of these became the Latin alphabet, which was spread across Europe as the Romans expanded their empire.
Even after the fall of the Roman state, the alphabet survived in intellectual and religious works.
It eventually became used for the descendant languages of Latin (the Romance languages) and then for most of the other languages of Europe.
Some adaptations of the Latin alphabet are augmented with ligatures, such as æ in Danish and Icelandic and Ȣ in Algonquian; by borrowings from other alphabets, such as the thorn þ in Old English and Icelandic, which came from the Futhark runes; and by modifying existing letters, such as the eth ð of Old English and Icelandic, which is a modified "d".
Other alphabets only use a subset of the Latin alphabet, such as Hawaiian, and Italian, which uses the letters "j, k, x, y" and "w" only in foreign words.
Another notable script is Elder Futhark, which is believed to have evolved out of one of the Old Italic alphabets.
Elder Futhark gave rise to a variety of alphabets known collectively as the Runic alphabets.
The Runic alphabets were used for Germanic languages from AD 100 to the late Middle Ages.
Its usage is mostly restricted to engravings on stone and jewelry, although inscriptions have also been found on bone and wood.
These alphabets have since been replaced with the Latin alphabet, except for decorative usage for which the runes remained in use until the 20th century.
The Old Hungarian script is a contemporary writing system of the Hungarians.
It was in use during the entire history of Hungary, albeit not as an official writing system.
From the 19th century it once again became more and more popular.
The Glagolitic alphabet was the initial script of the liturgical language Old Church Slavonic and became, together with the Greek uncial script, the basis of the Cyrillic script.
Cyrillic is one of the most widely used modern alphabetic scripts, and is notable for its use in Slavic languages and also for other languages within the former Soviet Union.
Cyrillic alphabets include the Serbian, Macedonian, Bulgarian, Russian, Belarusian and Ukrainian.
The Glagolitic alphabet is believed to have been created by Saints Cyril and Methodius, while the Cyrillic alphabet was invented by Clement of Ohrid, who was their disciple.
They feature many letters that appear to have been borrowed from or influenced by the Greek alphabet and the Hebrew alphabet.
The longest European alphabet is the Latin-derived Slovak alphabet which has 46 letters.
Beyond the logographic Chinese writing, many phonetic scripts are in existence in Asia.
The Arabic alphabet, Hebrew alphabet, Syriac alphabet, and other abjads of the Middle East are developments of the Aramaic alphabet, but because these writing systems are largely consonant-based they are often not considered true alphabets.
Most alphabetic scripts of India and Eastern Asia are descended from the Brahmi script, which is often believed to be a descendant of Aramaic.
In Korea, the Hangul alphabet was created by Sejong the Great.
Hangul is a unique alphabet: it is a featural alphabet, where many of the letters are designed from a sound's place of articulation (P to look like the widened mouth, L to look like the tongue pulled in, etc.
); its design was planned by the government of the day; and it places individual letters in syllable clusters with equal dimensions, in the same way as Chinese characters, to allow for mixed-script writing (one syllable always takes up one type-space no matter how many letters get stacked into building that one sound-block).
Zhuyin (sometimes called "Bopomofo") is a semi-syllabary used to phonetically transcribe Mandarin Chinese in the Republic of China.
After the later establishment of the People's Republic of China and its adoption of Hanyu Pinyin, the use of Zhuyin today is limited, but it is still widely used in Taiwan where the Republic of China still governs.
Zhuyin developed out of a form of Chinese shorthand based on Chinese characters in the early 1900s and has elements of both an alphabet and a syllabary.
Like an alphabet the phonemes of syllable initials are represented by individual symbols, but like a syllabary the phonemes of the syllable finals are not; rather, each possible final (excluding the medial glide) is represented by its own symbol.
For example, "luan" is represented as ㄌㄨㄢ ("l-u-an"), where the last symbol ㄢ represents the entire final "-an".
While Zhuyin is not used as a mainstream writing system, it is still often used in ways similar to a romanization system—that is, for aiding in pronunciation and as an input method for Chinese characters on computers and cellphones.
European alphabets, especially Latin and Cyrillic, have been adapted for many languages of Asia.
Arabic is also widely used, sometimes as an abjad (as with Urdu and Persian) and sometimes as a complete alphabet (as with Kurdish and Uyghur).
The term "alphabet" is used by linguists and paleographers in both a wide and a narrow sense.
In the wider sense, an alphabet is a script that is "segmental" at the phoneme level—that is, it has separate glyphs for individual sounds and not for larger units such as syllables or words.
In the narrower sense, some scholars distinguish "true" alphabets from two other types of segmental script, abjads and abugidas.
These three differ from each other in the way they treat vowels: abjads have letters for consonants and leave most vowels unexpressed; abugidas are also consonant-based, but indicate vowels with diacritics to or a systematic graphic modification of the consonants.
In alphabets in the narrow sense, on the other hand, consonants and vowels are written as independent letters.
The earliest known alphabet in the wider sense is the Wadi el-Hol script, believed to be an abjad, which through its successor Phoenician is the ancestor of modern alphabets, including Arabic, Greek, Latin (via the Old Italic alphabet), Cyrillic (via the Greek alphabet) and Hebrew (via Aramaic).
Examples of present-day abjads are the Arabic and Hebrew scripts; true alphabets include Latin, Cyrillic, and Korean hangul; and abugidas are used to write Tigrinya, Amharic, Hindi, and Thai.
The Canadian Aboriginal syllabics are also an abugida rather than a syllabary as their name would imply, since each glyph stands for a consonant that is modified by rotation to represent the following vowel.
(In a true syllabary, each consonant-vowel combination would be represented by a separate glyph.)
All three types may be augmented with syllabic glyphs.
Ugaritic, for example, is basically an abjad, but has syllabic letters for .
(These are the only time vowels are indicated.)
Cyrillic is basically a true alphabet, but has syllabic letters for (я, е, ю); Coptic has a letter for .
Devanagari is typically an abugida augmented with dedicated letters for initial vowels, though some traditions use अ as a zero consonant as the graphic base for such vowels.
The boundaries between the three types of segmental scripts are not always clear-cut.
For example, Sorani Kurdish is written in the Arabic script, which is normally an abjad.
However, in Kurdish, writing the vowels is mandatory, and full letters are used, so the script is a true alphabet.
Other languages may use a Semitic abjad with mandatory vowel diacritics, effectively making them abugidas.
On the other hand, the Phagspa script of the Mongol Empire was based closely on the Tibetan abugida, but all vowel marks were written after the preceding consonant rather than as diacritic marks.
Although short "a" was not written, as in the Indic abugidas, one could argue that the linear arrangement made this a true alphabet.
Conversely, the vowel marks of the Tigrinya abugida and the Amharic abugida (ironically, the original source of the term "abugida") have been so completely assimilated into their consonants that the modifications are no longer systematic and have to be learned as a syllabary rather than as a segmental script.
Even more extreme, the Pahlavi abjad eventually became logographic.
(See below.)
Thus the primary classification of alphabets reflects how they treat vowels.
For tonal languages, further classification can be based on their treatment of tone, though names do not yet exist to distinguish the various types.
Some alphabets disregard tone entirely, especially when it does not carry a heavy functional load, as in Somali and many other languages of Africa and the Americas.
Such scripts are to tone what abjads are to vowels.
Most commonly, tones are indicated with diacritics, the way vowels are treated in abugidas.
This is the case for Vietnamese (a true alphabet) and Thai (an abugida).
In Thai, tone is determined primarily by the choice of consonant, with diacritics for disambiguation.
In the Pollard script, an abugida, vowels are indicated by diacritics, but the placement of the diacritic relative to the consonant is modified to indicate the tone.
More rarely, a script may have separate letters for tones, as is the case for Hmong and Zhuang.
For most of these scripts, regardless of whether letters or diacritics are used, the most common tone is not marked, just as the most common vowel is not marked in Indic abugidas; in Zhuyin not only is one of the tones unmarked, but there is a diacritic to indicate lack of tone, like the virama of Indic.
The number of letters in an alphabet can be quite small.
The Book Pahlavi script, an abjad, had only twelve letters at one point, and may have had even fewer later on.
Today the Rotokas alphabet has only twelve letters.
(The Hawaiian alphabet is sometimes claimed to be as small, but it actually consists of 18 letters, including the ʻokina and five long vowels.
However, Hawaiian Braille has only 13 letters.)
While Rotokas has a small alphabet because it has few phonemes to represent (just eleven), Book Pahlavi was small because many letters had been "conflated"—that is, the graphic distinctions had been lost over time, and diacritics were not developed to compensate for this as they were in Arabic, another script that lost many of its distinct letter shapes.
For example, a comma-shaped letter represented "g", "d", "y", "k", or "j".
However, such apparent simplifications can perversely make a script more complicated.
In later Pahlavi papyri, up to half of the remaining graphic distinctions of these twelve letters were lost, and the script could no longer be read as a sequence of letters at all, but instead each word had to be learned as a whole—that is, they had become logograms as in Egyptian Demotic.
The largest segmental script is probably an abugida, Devanagari.
When written in Devanagari, Vedic Sanskrit has an alphabet of 53 letters, including the "visarga" mark for final aspiration and special letters for "kš" and "jñ," though one of the letters is theoretical and not actually used.
The Hindi alphabet must represent both Sanskrit and modern vocabulary, and so has been expanded to 58 with the "khutma" letters (letters with a dot added) to represent sounds from Persian and English.
Thai has a total of 59 symbols, consisting of 44 consonants, 13 vowels and 2 syllabics, not including 4 diacritics for tone marks and one for vowel length.
The largest known abjad is Sindhi, with 51 letters.
The largest alphabets in the narrow sense include Kabardian and Abkhaz (for Cyrillic), with 58 and 56 letters, respectively, and Slovak (for the Latin script), with 46.
However, these scripts either count di- and tri-graphs as separate letters, as Spanish did with "ch" and "ll" until recently, or uses diacritics like Slovak "č".
The Georgian alphabet ( "") is alphabetical writing system.
It is the largest true alphabet where each letter is graphically independent with 33 letters.
Original Georgian alphabet had 38 letters but 5 letters were removed in 19th century by Ilia Chavchavadze.
The Georgian alphabet is much closer to Greek than the other Caucasian alphabets.
The numeric value runs parallel to the Greek one, the consonants without a Greek equivalent are organized at the end of the alphabet.
Origins of the alphabet are still unknown, some Armenian and Western scholars believe it was created by Mesrop Mashtots (Armenian: Մեսրոպ Մաշտոց Mesrop Maštoc') also known as Mesrob the Vartabed, who was an early medieval Armenian linguist, theologian, statesman and hymnologist, best known for inventing the Armenian alphabet c.
405 AD, other Georgian and Western, scholars are against this theory.
Syllabaries typically contain 50 to 400 glyphs, and the glyphs of logographic systems typically number from the many hundreds into the thousands.
Thus a simple count of the number of distinct symbols is an important clue to the nature of an unknown script.
The Armenian alphabet ( ' or ') is a graphically unique alphabetical writing system that has been used to write the Armenian language.
It was introduced by Mesrob Mashdots around 405 AD, an Armenian linguist and ecclesiastical leader, and originally contained 36 letters.
Two more letters, օ (o) and ֆ (f), were added in the Middle Ages.
During the 1920s orthography reform, a new letter և (capital ԵՎ) was added, which was a ligature before ե+ւ, while the letter Ւ ւ was discarded and reintroduced as part of a new letter ՈՒ ու (which was a digraph before).
The Armenian word for "alphabet" is "" (), named after the first two letters of the Armenian alphabet Ա այբ ayb and Բ բեն ben.
The Armenian script's directionality is horizontal left-to-right, like the Latin and Greek alphabets.
Alphabets often come to be associated with a standard ordering of their letters, which can then be used for purposes of collation—namely for the listing of words and other items in what is called "alphabetical order".
The basic ordering of the Latin alphabet (A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
P
Q
R
S
T
U
V
W
X
Y
Z), which is derived from the Northwest Semitic "Abgad" order, is well established, although languages using this alphabet have different conventions for their treatment of modified letters (such as the French "é", "à", and "ô") and of certain combinations of letters (multigraphs).
In French, these are not considered to be additional letters for the purposes of collation.
However, in Icelandic, the accented letters such as "á", "í", and "ö" are considered distinct letters representing different vowel sounds from the sounds represented by their unaccented counterparts.
In Spanish, "ñ" is considered a separate letter, but accented vowels such as "á" and "é" are not.
The "ll" and "ch" were also considered single letters, but in 1994 the Real Academia Española changed the collating order so that "ll" is between "lk" and "lm" in the dictionary and "ch" is between "cg" and "ci", and in 2010 the tenth congress of the Association of Spanish Language Academies changed it so they were no longer letters at all.
In German, words starting with "sch-" (which spells the German phoneme ) are inserted between words with initial "sca-" and "sci-" (all incidentally loanwords) instead of appearing after initial "sz", as though it were a single letter—in contrast to several languages such as Albanian, in which "dh-", "ë-", "gj-", "ll-", "rr-", "th-", "xh-" and "zh-" (all representing phonemes and considered separate single letters) would follow the letters "d", "e", "g", "l", "n", "r", "t", "x" and "z" respectively, as well as Hungarian and Welsh.
Further, German words with umlaut are collated ignoring the umlaut—contrary to Turkish that adopted the graphemes ö and ü, and where a word like "tüfek", would come after "tuz", in the dictionary.
An exception is the German telephone directory where umlauts are sorted like "ä" = "ae" since names as "Jäger" appear also with the spelling "Jaeger", and are not distinguished in the spoken language.
The Danish and Norwegian alphabets end with "æ"—"ø"—"å", whereas the Swedish and Finnish ones conventionally put "å"—"ä"—"ö" at the end.
It is unknown whether the earliest alphabets had a defined sequence.
Some alphabets today, such as the Hanuno'o script, are learned one letter at a time, in no particular order, and are not used for collation where a definite order is required.
However, a dozen Ugaritic tablets from the fourteenth century BC preserve the alphabet in two sequences.
One, the "ABCDE" order later used in Phoenician, has continued with minor changes in Hebrew, Greek, Armenian, Gothic, Cyrillic, and Latin; the other, "HMĦLQ," was used in southern Arabia and is preserved today in Ethiopic.
Both orders have therefore been stable for at least 3000 years.
Runic used an unrelated Futhark sequence, which was later simplified.
Arabic uses its own sequence, although Arabic retains the traditional abjadi order for numbering.
The Brahmic family of alphabets used in India use a unique order based on phonology: The letters are arranged according to how and where they are produced in the mouth.
This organization is used in Southeast Asia, Tibet, Korean hangul, and even Japanese kana, which is not an alphabet.
The Phoenician letter names, in which each letter was associated with a word that begins with that sound (acrophony), continue to be used to varying degrees in Samaritan, Aramaic, Syriac, Hebrew, Greek and Arabic.
The names were abandoned in Latin, which instead referred to the letters by adding a vowel (usually e) before or after the consonant; the two exceptions were Y and Z, which were borrowed from the Greek alphabet rather than Etruscan, and were known as "Y Graeca" "Greek Y" (pronounced "I Graeca" "Greek I") and "zeta" (from Greek)—this discrepancy was inherited by many European languages, as in the term "zed" for Z in all forms of English other than American English.
Over time names sometimes shifted or were added, as in "double U" for W ("double V" in French), the English name for Y, and American "zee" for Z. Comparing names in English and French gives a clear reflection of the Great Vowel Shift: A, B, C and D are pronounced /eɪ, biː, siː, diː/ in today's English, but in contemporary French they are /a, be, se, de/.
The French names (from which the English names are derived) preserve the qualities of the English vowels from before the Great Vowel Shift.
By contrast, the names of F, L, M, N and S (/ɛf, ɛl, ɛm, ɛn, ɛs/) remain the same in both languages, because "short" vowels were largely unaffected by the Shift.
In Cyrillic originally the letters were given names based on Slavic words; this was later abandoned as well in favor of a system similar to that used in Latin.
When an alphabet is adopted or developed to represent a given language, an orthography generally comes into being, providing rules for the spelling of words in that language.
In accordance with the principle on which alphabets are based, these rules will generally map letters of the alphabet to the phonemes (significant sounds) of the spoken language.
In a perfectly phonemic orthography there would be a consistent one-to-one correspondence between the letters and the phonemes, so that a writer could predict the spelling of a word given its pronunciation, and a speaker would always know the pronunciation of a word given its spelling, and vice versa.
However this ideal is not usually achieved in practice; some languages (such as Spanish and Finnish) come close to it, while others (such as English) deviate from it to a much larger degree.
The pronunciation of a language often evolves independently of its writing system, and writing systems have been borrowed for languages they were not designed for, so the degree to which letters of an alphabet correspond to phonemes of a language varies greatly from one language to another and even within a single language.
Languages may fail to achieve a one-to-one correspondence between letters and sounds in any of several ways:

National languages sometimes elect to address the problem of dialects by simply associating the alphabet with the national standard.
Some national languages like Finnish, Turkish, Russian, Serbo-Croatian (Serbian, Croatian and Bosnian) and Bulgarian have a very regular spelling system with a nearly one-to-one correspondence between letters and phonemes.
Strictly speaking, these national languages lack a word corresponding to the verb "to spell" (meaning to split a word into its letters), the closest match being a verb meaning to split a word into its syllables.
Similarly, the Italian verb corresponding to 'spell (out)', "compitare", is unknown to many Italians because spelling is usually trivial, as Italian spelling is highly phonemic.
In standard Spanish, one can tell the pronunciation of a word from its spelling, but not vice versa, as certain phonemes can be represented in more than one way, but a given letter is consistently pronounced.
French, with its silent letters and its heavy use of nasal vowels and elision, may seem to lack much correspondence between spelling and pronunciation, but its rules on pronunciation, though complex, are actually consistent and predictable with a fair degree of accuracy.
At the other extreme are languages such as English, where the pronunciations of many words simply have to be memorized as they do not correspond to the spelling in a consistent way.
For English, this is partly because the Great Vowel Shift occurred after the orthography was established, and because English has acquired a large number of loanwords at different times, retaining their original spelling at varying levels.
Even English has general, albeit complex, rules that predict pronunciation from spelling, and these rules are successful most of the time; rules to predict spelling from the pronunciation have a higher failure rate.
Sometimes, countries have the written language undergo a spelling reform to realign the writing with the contemporary spoken language.
These can range from simple spelling changes and word forms to switching the entire writing system itself, as when Turkey switched from the Arabic alphabet to a Latin-based Turkish alphabet.
The standard system of symbols used by linguists to represent sounds in any language, independently of orthography, is called the International Phonetic Alphabet.
</doc>
<doc id="673" url="https://en.wikipedia.org/wiki?curid=673" title="Atomic number">
Atomic number

The atomic number or proton number (symbol "Z") of a chemical element is the number of protons found in the nucleus of an atom.
It is identical to the charge number of the nucleus.
The atomic number uniquely identifies a chemical element.
In an uncharged atom, the atomic number is also equal to the number of electrons.
The sum of the atomic number "Z" and the number of neutrons, "N", gives the mass number "A" of an atom.
Since protons and neutrons have approximately the same mass (and the mass of the electrons is negligible for many purposes) and the mass defect of nucleon binding is always small compared to the nucleon mass, the atomic mass of any atom, when expressed in unified atomic mass units (making a quantity called the "relative isotopic mass"), is within 1% of the whole number "A".
Atoms with the same atomic number "Z" but different neutron numbers "N", and hence different atomic masses, are known as isotopes.
A little more than three-quarters of naturally occurring elements exist as a mixture of isotopes (see monoisotopic elements), and the average isotopic mass of an isotopic mixture for an element (called the relative atomic mass) in a defined environment on Earth, determines the element's standard atomic weight.
Historically, it was these atomic weights of elements (in comparison to hydrogen) that were the quantities measurable by chemists in the 19th century.
The conventional symbol "Z" comes from the German word meaning "number", which, before the modern synthesis of ideas from chemistry and physics, merely denoted an element's numerical place in the periodic table, whose order is approximately, but not completely, consistent with the order of the elements by atomic weights.
Only after 1915, with the suggestion and evidence that this "Z" number was also the nuclear charge and a physical characteristic of atoms, did the word (and its English equivalent "atomic number") come into common use in this context.
Loosely speaking, the existence or construction of a periodic table of elements creates an ordering of the elements, and so they can be numbered in order.
Dmitri Mendeleev claimed that he arranged his first periodic tables (first published on March 6th, 1869) in order of atomic weight ("Atomgewicht").
However, in consideration of the elements' observed chemical properties, he changed the order slightly and placed tellurium (atomic weight 127.6) ahead of iodine (atomic weight 126.9).
This placement is consistent with the modern practice of ordering the elements by proton number, "Z", but that number was not known or suspected at the time.
A simple numbering based on periodic table position was never entirely satisfactory, however.
Besides the case of iodine and tellurium, later several other pairs of elements (such as argon and potassium, cobalt and nickel) were known to have nearly identical or reversed atomic weights, thus requiring their placement in the periodic table to be determined by their chemical properties.
However the gradual identification of more and more chemically similar lanthanide elements, whose atomic number was not obvious, led to inconsistency and uncertainty in the periodic numbering of elements at least from lutetium (element 71) onward (hafnium was not known at this time).
In 1911, Ernest Rutherford gave a model of the atom in which a central core held most of the atom's mass and a positive charge which, in units of the electron's charge, was to be approximately equal to half of the atom's atomic weight, expressed in numbers of hydrogen atoms.
This central charge would thus be approximately half the atomic weight (though it was almost 25% different from the atomic number of gold , ), the single element from which Rutherford made his guess).
Nevertheless, in spite of Rutherford's estimation that gold had a central charge of about 100 (but was element on the periodic table), a month after Rutherford's paper appeared, Antonius van den Broek first formally suggested that the central charge and number of electrons in an atom was "exactly" equal to its place in the periodic table (also known as element number, atomic number, and symbolized "Z").
This proved eventually to be the case.
The experimental position improved dramatically after research by Henry Moseley in 1913.
Moseley, after discussions with Bohr who was at the same lab (and who had used Van den Broek's hypothesis in his Bohr model of the atom), decided to test Van den Broek's and Bohr's hypothesis directly, by seeing if spectral lines emitted from excited atoms fitted the Bohr theory's postulation that the frequency of the spectral lines be proportional to the square of "Z".
To do this, Moseley measured the wavelengths of the innermost photon transitions (K and L lines) produced by the elements from aluminum ("Z" = 13) to gold ("Z" = 79) used as a series of movable anodic targets inside an x-ray tube.
The square root of the frequency of these photons increased from one target to the next in an arithmetic progression.
This led to the conclusion (Moseley's law) that the atomic number does closely correspond (with an offset of one unit for K-lines, in Moseley's work) to the calculated electric charge of the nucleus, i.e. the element number "Z".
Among other things, Moseley demonstrated that the lanthanide series (from lanthanum to lutetium inclusive) must have 15 members—no fewer and no more—which was far from obvious from the chemistry at that time.
After Moseley's death in 1915, the atomic numbers of all known elements from hydrogen to uranium ("Z" = 92) were examined by his method.
There were seven elements (with "Z" < 92) which were not found and therefore identified as still undiscovered, corresponding to atomic numbers 43, 61, 72, 75, 85, 87 and 91.
From 1918 to 1947, all seven of these missing elements were discovered.
By this time the first four transuranium elements had also been discovered, so that the periodic table was complete with no gaps as far as curium ("Z" = 96).
In 1915 the reason for nuclear charge being quantized in units of "Z", which were now recognized to be the same as the element number, was not understood.
An old idea called Prout's hypothesis had postulated that the elements were all made of residues (or "protyles") of the lightest element hydrogen, which in the Bohr-Rutherford model had a single electron and a nuclear charge of one.
However, as early as 1907 Rutherford and Thomas Royds had shown that alpha particles, which had a charge of +2, were the nuclei of helium atoms, which had a mass four times that of hydrogen, not two times.
If Prout's hypothesis were true, something had to be neutralizing some of the charge of the hydrogen nuclei present in the nuclei of heavier atoms.
In 1917 Rutherford succeeded in generating hydrogen nuclei from a nuclear reaction between alpha particles and nitrogen gas, and believed he had proven Prout's law.
He called the new heavy nuclear particles protons in 1920 (alternate names being proutons and protyles).
It had been immediately apparent from the work of Moseley that the nuclei of heavy atoms have more than twice as much mass as would be expected from their being made of hydrogen nuclei, and thus there was required a hypothesis for the neutralization of the extra protons presumed present in all heavy nuclei.
A helium nucleus was presumed to be composed of four protons plus two "nuclear electrons" (electrons bound inside the nucleus) to cancel two of the charges.
At the other end of the periodic table, a nucleus of gold with a mass 197 times that of hydrogen, was thought to contain 118 nuclear electrons in the nucleus to give it a residual charge of + 79, consistent with its atomic number.
All consideration of nuclear electrons ended with James Chadwick's discovery of the neutron in 1932.
An atom of gold now was seen as containing 118 neutrons rather than 118 nuclear electrons, and its positive charge now was realized to come entirely from a content of 79 protons.
After 1932, therefore, an element's atomic number "Z" was also realized to be identical to the proton number of its nuclei.
The conventional symbol "Z" possibly comes from the German word (atomic number).
However, prior to 1915, the word "Zahl" (simply "number") was used for an element's assigned number in the periodic table.
Each element has a specific set of chemical properties as a consequence of the number of electrons present in the neutral atom, which is "Z" (the atomic number).
The configuration of these electrons follows from the principles of quantum mechanics.
The number of electrons in each element's electron shells, particularly the outermost valence shell, is the primary factor in determining its chemical bonding behavior.
Hence, it is the atomic number alone that determines the chemical properties of an element; and it is for this reason that an element can be defined as consisting of "any" mixture of atoms with a given atomic number.
The quest for new elements is usually described using atomic numbers.
As of 2010, all elements with atomic numbers 1 to 118 have been observed.
Synthesis of new elements is accomplished by bombarding target atoms of heavy elements with ions, such that the sum of the atomic numbers of the target and ion elements equals the atomic number of the element being created.
In general, the half-life becomes shorter as atomic number increases, though an "island of stability" may exist for undiscovered isotopes with certain numbers of protons and neutrons.
</doc>
<doc id="674" url="https://en.wikipedia.org/wiki?curid=674" title="Anatomy">
Anatomy

Anatomy (Greek anatomē, "dissection") is the branch of biology concerned with the study of the structure of organisms and their parts.
Anatomy is a branch of natural science which deals with the structural organization of living things.
It is an old science, having its beginnings in prehistoric times.
Anatomy is inherently tied to developmental biology, embryology, comparative anatomy, evolutionary biology, and phylogeny, as these are the processes by which anatomy is generated over immediate (embryology) and long (evolution) timescales.
Anatomy and physiology, which study (respectively) the structure and function of organisms and their parts, make a natural pair of related disciplines, and they are often studied together.
Human anatomy is one of the essential basic sciences that are applied in medicine.
The discipline of anatomy is divided into macroscopic and microscopic anatomy.
Macroscopic anatomy, or gross anatomy, is the examination of an animal's body parts using unaided eyesight.
Gross anatomy also includes the branch of superficial anatomy.
Microscopic anatomy involves the use of optical instruments in the study of the tissues of various structures, known as histology, and also in the study of cells.
The history of anatomy is characterized by a progressive understanding of the functions of the organs and structures of the human body.
Methods have also improved dramatically, advancing from the examination of animals by dissection of carcasses and cadavers (corpses) to 20th century medical imaging techniques including X-ray, ultrasound, and magnetic resonance imaging.
Derived from the Greek "anatomē" "dissection" (from "anatémnō" "I cut up, cut open" from ἀνά "aná" "up", and τέμνω "témnō" "I cut"), anatomy is the scientific study of the structure of organisms including their systems, organs and tissues.
It includes the appearance and position of the various parts, the materials from which they are composed, their locations and their relationships with other parts.
Anatomy is quite distinct from physiology and biochemistry, which deal respectively with the functions of those parts and the chemical processes involved.
For example, an anatomist is concerned with the shape, size, position, structure, blood supply and innervation of an organ such as the liver; while a physiologist is interested in the production of bile, the role of the liver in nutrition and the regulation of bodily functions.
The discipline of anatomy can be subdivided into a number of branches including gross or macroscopic anatomy and microscopic anatomy.
Gross anatomy is the study of structures large enough to be seen with the naked eye, and also includes superficial anatomy or surface anatomy, the study by sight of the external body features.
Microscopic anatomy is the study of structures on a microscopic scale, along with histology (the study of tissues), and embryology (the study of an organism in its immature condition).
Anatomy can be studied using both invasive and non-invasive methods with the goal of obtaining information about the structure and organization of organs and systems.
Methods used include dissection, in which a body is opened and its organs studied, and endoscopy, in which a video camera-equipped instrument is inserted through a small incision in the body wall and used to explore the internal organs and other structures.
Angiography using X-rays or magnetic resonance angiography are methods to visualize blood vessels.
The term "anatomy" is commonly taken to refer to human anatomy.
However, substantially the same structures and tissues are found throughout the rest of the animal kingdom and the term also includes the anatomy of other animals.
The term "zootomy" is also sometimes used to specifically refer to non-human animals.
The structure and tissues of plants are of a dissimilar nature and they are studied in plant anatomy.
The kingdom Animalia contains multicellular organisms that are heterotrophic and motile (although some have secondarily adopted a sessile lifestyle).
Most animals have bodies differentiated into separate tissues and these animals are also known as eumetazoans.
They have an internal digestive chamber, with one or two openings; the gametes are produced in multicellular sex organs, and the zygotes include a blastula stage in their embryonic development.
Metazoans do not include the sponges, which have undifferentiated cells.
Unlike plant cells, animal cells have neither a cell wall nor chloroplasts.
Vacuoles, when present, are more in number and much smaller than those in the plant cell.
The body tissues are composed of numerous types of cell, including those found in muscles, nerves and skin.
Each typically has a cell membrane formed of phospholipids, cytoplasm and a nucleus.
All of the different cells of an animal are derived from the embryonic germ layers.
Those simpler invertebrates which are formed from two germ layers of ectoderm and endoderm are called diploblastic and the more developed animals whose structures and organs are formed from three germ layers are called triploblastic.
All of a triploblastic animal's tissues and organs are derived from the three germ layers of the embryo, the ectoderm, mesoderm and endoderm.
Animal tissues can be grouped into four basic types: connective, epithelial, muscle and nervous tissue.
Connective tissues are fibrous and made up of cells scattered among inorganic material called the extracellular matrix.
Connective tissue gives shape to organs and holds them in place.
The main types are loose connective tissue, adipose tissue, fibrous connective tissue, cartilage and bone.
The extracellular matrix contains proteins, the chief and most abundant of which is collagen.
Collagen plays a major part in organizing and maintaining tissues.
The matrix can be modified to form a skeleton to support or protect the body.
An exoskeleton is a thickened, rigid cuticle which is stiffened by mineralization, as in crustaceans or by the cross-linking of its proteins as in insects.
An endoskeleton is internal and present in all developed animals, as well as in many of those less developed.
Epithelial tissue is composed of closely packed cells, bound to each other by cell adhesion molecules, with little intercellular space.
Epithelial cells can be squamous (flat), cuboidal or columnar and rest on a basal lamina, the upper layer of the basement membrane, the lower layer is the reticular lamina lying next to the connective tissue in the extracellular matrix secreted by the epithelial cells.
There are many different types of epithelium, modified to suit a particular function.
In the respiratory tract there is a type of ciliated epithelial lining; in the small intestine there are microvilli on the epithelial lining and in the large intestine there are intestinal villi.
Skin consists of an outer layer of keratinized stratified squamous epithelium that covers the exterior of the vertebrate body.
Keratinocytes make up to 95% of the cells in the skin.
The epithelial cells on the external surface of the body typically secrete an extracellular matrix in the form of a cuticle.
In simple animals this may just be a coat of glycoproteins.
In more advanced animals, many glands are formed of epithelial cells.
Muscle cells (myocytes) form the active contractile tissue of the body.
Muscle tissue functions to produce force and cause motion, either locomotion or movement within internal organs.
Muscle is formed of contractile filaments and is separated into three main types; smooth muscle, skeletal muscle and cardiac muscle.
Smooth muscle has no striations when examined microscopically.
It contracts slowly but maintains contractibility over a wide range of stretch lengths.
It is found in such organs as sea anemone tentacles and the body wall of sea cucumbers.
Skeletal muscle contracts rapidly but has a limited range of extension.
It is found in the movement of appendages and jaws.
Obliquely striated muscle is intermediate between the other two.
The filaments are staggered and this is the type of muscle found in earthworms that can extend slowly or make rapid contractions.
In higher animals striated muscles occur in bundles attached to bone to provide movement and are often arranged in antagonistic sets.
Smooth muscle is found in the walls of the uterus, bladder, intestines, stomach, oesophagus, respiratory airways, and blood vessels.
Cardiac muscle is found only in the heart, allowing it to contract and pump blood round the body.
Nervous tissue is composed of many nerve cells known as neurons which transmit information.
In some slow-moving radially symmetrical marine animals such as ctenophores and cnidarians (including sea anemones and jellyfish), the nerves form a nerve net, but in most animals they are organized longitudinally into bundles.
In simple animals, receptor neurons in the body wall cause a local reaction to a stimulus.
In more complex animals, specialized receptor cells such as chemoreceptors and photoreceptors are found in groups and send messages along neural networks to other parts of the organism.
Neurons can be connected together in ganglia.
In higher animals, specialized receptors are the basis of sense organs and there is a central nervous system (brain and spinal cord) and a peripheral nervous system.
The latter consists of sensory nerves that transmit information from sense organs and motor nerves that influence target organs.
The peripheral nervous system is divided into the somatic nervous system which conveys sensation and controls voluntary muscle, and the autonomic nervous system which involuntarily controls smooth muscle, certain glands and internal organs, including the stomach.
All vertebrates have a similar basic body plan and at some point in their lives, mostly in the embryonic stage, share the major chordate characteristics; a stiffening rod, the notochord; a dorsal hollow tube of nervous material, the neural tube; pharyngeal arches; and a tail posterior to the anus.
The spinal cord is protected by the vertebral column and is above the notochord and the gastrointestinal tract is below it.
Nervous tissue is derived from the ectoderm, connective tissues are derived from mesoderm, and gut is derived from the endoderm.
At the posterior end is a tail which continues the spinal cord and vertebrae but not the gut.
The mouth is found at the anterior end of the animal, and the anus at the base of the tail.
The defining characteristic of a vertebrate is the vertebral column, formed in the development of the segmented series of vertebrae.
In most vertebrates the notochord becomes the nucleus pulposus of the intervertebral discs.
However, a few vertebrates, such as the sturgeon and the coelacanth retain the notochord into adulthood.
Jawed vertebrates are typified by paired appendages, fins or legs, which may be secondarily lost.
The limbs of vertebrates are considered to be homologous because the same underlying skeletal structure was inherited from their last common ancestor.
This is one of the arguments put forward by Charles Darwin to support his theory of evolution.
The body of a fish is divided into a head, trunk and tail, although the divisions between the three are not always externally visible.
The skeleton, which forms the support structure inside the fish, is either made of cartilage, in cartilaginous fish, or bone in bony fish.
The main skeletal element is the vertebral column, composed of articulating vertebrae which are lightweight yet strong.
The ribs attach to the spine and there are no limbs or limb girdles.
The main external features of the fish, the fins, are composed of either bony or soft spines called rays, which with the exception of the caudal fins, have no direct connection with the spine.
They are supported by the muscles which compose the main part of the trunk.
The heart has two chambers and pumps the blood through the respiratory surfaces of the gills and on round the body in a single circulatory loop.
The eyes are adapted for seeing underwater and have only local vision.
There is an inner ear but no external or middle ear.
Low frequency vibrations are detected by the lateral line system of sense organs that run along the length of the sides of fish, and these respond to nearby movements and to changes in water pressure.
Sharks and rays are basal fish with numerous primitive anatomical features similar to those of ancient fish, including skeletons composed of cartilage.
Their bodies tend to be dorso-ventrally flattened, they usually have five pairs of gill slits and a large mouth set on the underside of the head.
The dermis is covered with separate dermal placoid scales.
They have a cloaca into which the urinary and genital passages open, but not a swim bladder.
Cartilaginous fish produce a small number of large, yolky eggs.
Some species are ovoviviparous and the young develop internally but others are oviparous and the larvae develop externally in egg cases.
The bony fish lineage shows more derived anatomical traits, often with major evolutionary changes from the features of ancient fish.
They have a bony skeleton, are generally laterally flattened, have five pairs of gills protected by an operculum, and a mouth at or near the tip of the snout.
The dermis is covered with overlapping scales.
Bony fish have a swim bladder which helps them maintain a constant depth in the water column, but not a cloaca.
They mostly spawn a large number of small eggs with little yolk which they broadcast into the water column.
Amphibians are a class of animals comprising frogs, salamanders and caecilians.
They are tetrapods, but the caecilians and a few species of salamander have either no limbs or their limbs are much reduced in size.
Their main bones are hollow and lightweight and are fully ossified and the vertebrae interlock with each other and have articular processes.
Their ribs are usually short and may be fused to the vertebrae.
Their skulls are mostly broad and short, and are often incompletely ossified.
Their skin contains little keratin and lacks scales, but contains many mucous glands and in some species, poison glands.
The hearts of amphibians have three chambers, two atria and one ventricle.
They have a urinary bladder and nitrogenous waste products are excreted primarily as urea.
Amphibians breathe by means of buccal pumping, a pump action in which air is first drawn into the buccopharyngeal region through the nostrils.
These are then closed and the air is forced into the lungs by contraction of the throat.
They supplement this with gas exchange through the skin which needs to be kept moist.
In frogs the pelvic girdle is robust and the hind legs are much longer and stronger than the forelimbs.
The feet have four or five digits and the toes are often webbed for swimming or have suction pads for climbing.
Frogs have large eyes and no tail.
Salamanders resemble lizards in appearance; their short legs project sideways, the belly is close to or in contact with the ground and they have a long tail.
Caecilians superficially resemble earthworms and are limbless.
They burrow by means of zones of muscle contractions which move along the body and they swim by undulating their body from side to side.
Reptiles are a class of animals comprising turtles, tuataras, lizards, snakes and crocodiles.
They are tetrapods, but the snakes and a few species of lizard either have no limbs or their limbs are much reduced in size.
Their bones are better ossified and their skeletons stronger than those of amphibians.
The teeth are conical and mostly uniform in size.
The surface cells of the epidermis are modified into horny scales which create a waterproof layer.
Reptiles are unable to use their skin for respiration as do amphibians and have a more efficient respiratory system drawing air into their lungs by expanding their chest walls.
The heart resembles that of the amphibian but there is a septum which more completely separates the oxygenated and deoxygenated bloodstreams.
The reproductive system has evolved for internal fertilization, with a copulatory organ present in most species.
The eggs are surrounded by amniotic membranes which prevents them from drying out and are laid on land, or develop internally in some species.
The bladder is small as nitrogenous waste is excreted as uric acid.
Turtles are notable for their protective shells.
They have an inflexible trunk encased in a horny carapace above and a plastron below.
These are formed from bony plates embedded in the dermis which are overlain by horny ones and are partially fused with the ribs and spine.
The neck is long and flexible and the head and the legs can be drawn back inside the shell.
Turtles are vegetarians and the typical reptile teeth have been replaced by sharp, horny plates.
In aquatic species, the front legs are modified into flippers.
Tuataras superficially resemble lizards but the lineages diverged in the Triassic period.
There is one living species, "Sphenodon punctatus".
The skull has two openings (fenestrae) on either side and the jaw is rigidly attached to the skull.
There is one row of teeth in the lower jaw and this fits between the two rows in the upper jaw when the animal chews.
The teeth are merely projections of bony material from the jaw and eventually wear down.
The brain and heart are more primitive than those of other reptiles, and the lungs have a single chamber and lack bronchi.
The tuatara has a well-developed parietal eye on its forehead.
Lizards have skulls with only one fenestra on each side, the lower bar of bone below the second fenestra having been lost.
This results in the jaws being less rigidly attached which allows the mouth to open wider.
Lizards are mostly quadrupeds, with the trunk held off the ground by short, sideways-facing legs, but a few species have no limbs and resemble snakes.
Lizards have moveable eyelids, eardrums are present and some species have a central parietal eye.
Snakes are closely related to lizards, having branched off from a common ancestral lineage during the Cretaceous period, and they share many of the same features.
The skeleton consists of a skull, a hyoid bone, spine and ribs though a few species retain a vestige of the pelvis and rear limbs in the form of pelvic spurs.
The bar under the second fenestra has also been lost and the jaws have extreme flexibility allowing the snake to swallow its prey whole.
Snakes lack moveable eyelids, the eyes being covered by transparent "spectacle" scales.
They do not have eardrums but can detect ground vibrations through the bones of their skull.
Their forked tongues are used as organs of taste and smell and some species have sensory pits on their heads enabling them to locate warm-blooded prey.
Crocodilians are large, low-slung aquatic reptiles with long snouts and large numbers of teeth.
The head and trunk are dorso-ventrally flattened and the tail is laterally compressed.
It undulates from side to side to force the animal through the water when swimming.
The tough keratinized scales provide body armour and some are fused to the skull.
The nostrils, eyes and ears are elevated above the top of the flat head enabling them to remain above the surface of the water when the animal is floating.
Valves seal the nostrils and ears when it is submerged.
Unlike other reptiles, crocodilians have hearts with four chambers allowing complete separation of oxygenated and deoxygenated blood.
Birds are tetrapods but though their hind limbs are used for walking or hopping, their front limbs are wings covered with feathers and adapted for flight.
Birds are endothermic, have a high metabolic rate, a light skeletal system and powerful muscles.
The long bones are thin, hollow and very light.
Air sac extensions from the lungs occupy the centre of some bones.
The sternum is wide and usually has a keel and the caudal vertebrae are fused.
There are no teeth and the narrow jaws are adapted into a horn-covered beak.
The eyes are relatively large, particularly in nocturnal species such as owls.
They face forwards in predators and sideways in ducks.
The feathers are outgrowths of the epidermis and are found in localized bands from where they fan out over the skin.
Large flight feathers are found on the wings and tail, contour feathers cover the bird's surface and fine down occurs on young birds and under the contour feathers of water birds.
The only cutaneous gland is the single uropygial gland near the base of the tail.
This produces an oily secretion that waterproofs the feathers when the bird preens.
There are scales on the legs, feet and claws on the tips of the toes.
Mammals are a diverse class of animals, mostly terrestrial but some are aquatic and others have evolved flapping or gliding flight.
They mostly have four limbs but some aquatic mammals have no limbs or limbs modified into fins and the forelimbs of bats are modified into wings.
The legs of most mammals are situated below the trunk, which is held well clear of the ground.
The bones of mammals are well ossified and their teeth, which are usually differentiated, are coated in a layer of prismatic enamel.
The teeth are shed once (milk teeth) during the animal's lifetime or not at all, as is the case in cetaceans.
Mammals have three bones in the middle ear and a cochlea in the inner ear.
They are clothed in hair and their skin contains glands which secrete sweat.
Some of these glands are specialized as mammary glands, producing milk to feed the young.
Mammals breathe with lungs and have a muscular diaphragm separating the thorax from the abdomen which helps them draw air into the lungs.
The mammalian heart has four chambers and oxygenated and deoxygenated blood are kept entirely separate.
Nitrogenous waste is excreted primarily as urea.
Mammals are amniotes, and most are viviparous, giving birth to live young.
The exception to this are the egg-laying monotremes, the platypus and the echidnas of Australia.
Most other mammals have a placenta through which the developing foetus obtains nourishment, but in marsupials, the foetal stage is very short and the immature young is born and finds its way to its mother's pouch where it latches on to a nipple and completes its development.
Humans have the overall body plan of a mammal.
Humans have a head, neck, trunk (which includes the thorax and abdomen), two arms and hands, and two legs and feet.
Generally, students of certain biological sciences, paramedics, prosthetists and orthotists, physiotherapists, occupational therapists, nurses, podiatrists, and medical students learn gross anatomy and microscopic anatomy from anatomical models, skeletons, textbooks, diagrams, photographs, lectures and tutorials, and in addition, medical students generally also learn gross anatomy through practical experience of dissection and inspection of cadavers.
The study of microscopic anatomy (or histology) can be aided by practical experience examining histological preparations (or slides) under a microscope.
Human anatomy, physiology and biochemistry are complementary basic medical sciences, which are generally taught to medical students in their first year at medical school.
Human anatomy can be taught regionally or systemically; that is, respectively, studying anatomy by bodily regions such as the head and chest, or studying by specific systems, such as the nervous or respiratory systems.
The major anatomy textbook, Gray's Anatomy, has been reorganized from a systems format to a regional format, in line with modern teaching methods.
A thorough working knowledge of anatomy is required by physicians, especially surgeons and doctors working in some diagnostic specialties, such as histopathology and radiology.
Academic anatomists are usually employed by universities, medical schools or teaching hospitals.
They are often involved in teaching anatomy, and research into certain systems, organs, tissues or cells.
Invertebrates constitute a vast array of living organisms ranging from the simplest unicellular eukaryotes such as "Paramecium" to such complex multicellular animals as the octopus, lobster and dragonfly.
They constitute about 95% of the animal species.
By definition, none of these creatures has a backbone.
The cells of single-cell protozoans have the same basic structure as those of multicellular animals but some parts are specialized into the equivalent of tissues and organs.
Locomotion is often provided by cilia or flagella or may proceed via the advance of pseudopodia, food may be gathered by phagocytosis, energy needs may be supplied by photosynthesis and the cell may be supported by an endoskeleton or an exoskeleton.
Some protozoans can form multicellular colonies.
Metazoans are multicellular organism, different groups of cells of which have separate functions.
The most basic types of metazoan tissues are epithelium and connective tissue, both of which are present in nearly all invertebrates.
The outer surface of the epidermis is normally formed of epithelial cells and secretes an extracellular matrix which provides support to the organism.
An endoskeleton derived from the mesoderm is present in echinoderms, sponges and some cephalopods.
Exoskeletons are derived from the epidermis and is composed of chitin in arthropods (insects, spiders, ticks, shrimps, crabs, lobsters).
Calcium carbonate constitutes the shells of molluscs, brachiopods and some tube-building polychaete worms and silica forms the exoskeleton of the microscopic diatoms and radiolaria.
Other invertebrates may have no rigid structures but the epidermis may secrete a variety of surface coatings such as the pinacoderm of sponges, the gelatinous cuticle of cnidarians (polyps, sea anemones, jellyfish) and the collagenous cuticle of annelids.
The outer epithelial layer may include cells of several types including sensory cells, gland cells and stinging cells.
There may also be protrusions such as microvilli, cilia, bristles, spines and tubercles.
Marcello Malpighi, the father of microscopical anatomy, discovered that plants had tubules similar to those he saw in insects like the silk worm.
He observed that when a ring-like portion of bark was removed on a trunk a swelling occurred in the tissues above the ring, and he unmistakably interpreted this as growth stimulated by food coming down from the leaves, and being captured above the ring.
Arthropods comprise the largest phylum in the animal kingdom with over a million known invertebrate species.
Insects possess segmented bodies supported by a hard-jointed outer covering, the exoskeleton, made mostly of chitin.
The segments of the body are organized into three distinct parts, a head, a thorax and an abdomen.
The head typically bears a pair of sensory antennae, a pair of compound eyes, one to three simple eyes (ocelli) and three sets of modified appendages that form the mouthparts.
The thorax has three pairs of segmented legs, one pair each for the three segments that compose the thorax and one or two pairs of wings.
The abdomen is composed of eleven segments, some of which may be fused and houses the digestive, respiratory, excretory and reproductive systems.
There is considerable variation between species and many adaptations to the body parts, especially wings, legs, antennae and mouthparts.
Spiders a class of arachnids have four pairs of legs; a body of two segments—a cephalothorax and an abdomen.
Spiders have no wings and no antennae.
They have mouthparts called chelicerae which are often connected to venom glands as most spiders are venomous.
They have a second pair of appendages called pedipalps attached to the cephalothorax.
These have similar segmentation to the legs and function as taste and smell organs.
At the end of each male pedipalp is a spoon-shaped cymbium that acts to support the copulatory organ.
In 1600 BCE, the Edwin Smith Papyrus, an Ancient Egyptian medical text, described the heart, its vessels, liver, spleen, kidneys, hypothalamus, uterus and bladder, and showed the blood vessels diverging from the heart.
The Ebers Papyrus (c.
1550 BCE) features a "treatise on the heart", with vessels carrying all the body's fluids to or from every member of the body.
Ancient Greek anatomy and physiology underwent great changes and advances throughout the early medieval world.
Over time, this medical practice expanded by a continually developing understanding of the functions of organs and structures in the body.
Phenomenal anatomical observations of the human body were made, which have contributed towards the understanding of the brain, eye, liver, reproductive organs and the nervous system.
The Hellenistic Egyptian city of Alexandria was the stepping-stone for Greek anatomy and physiology.
Alexandria not only housed the biggest library for medical records and books of the liberal arts in the world during the time of the Greeks, but was also home to many medical practitioners and philosophers.
Great patronage of the arts and sciences from the Ptolemy rulers helped raise Alexandria up, further rivalling the cultural and scientific achievements of other Greek states.
Some of the most striking advances in early anatomy and physiology took place in Hellenistic Alexandria.
Two of the most famous anatomists and physiologists of the third century were Herophilus and Erasistratus.
These two physicians helped pioneer human dissection for medical research.
They also conducted vivisections on the cadavers of condemned criminals, which was considered taboo until the Renaissance – Herophilus was recognized as the first person to perform systematic dissections.
Herophilus became known for his anatomical works making impressing contributions to many branches of anatomy and many other aspects of medicine.
Some of the works included classifying the system of the pulse, the discovery that human arteries had thicker walls then veins, and that the atria were parts of the heart.
Herophilus's knowledge of the human body has provided vital input towards understanding the brain, eye, liver, reproductive organs and nervous system, and characterizing the course of disease.
Erasistratus accurately described the structure of the brain, including the cavities and membranes, and made a distinction between its cerebrum and cerebellum During his study in Alexandria, Erasistratus was particularly concerned with studies of the circulatory and nervous systems.
He was able to distinguish the sensory and the motor nerves in the human body and believed that air entered the lungs and heart, which was then carried throughout the body.
His distinction between the arteries and veins – the arteries carrying the air through the body, while the veins carried the blood from the heart was a great anatomical discovery.
Erasistratus was also responsible for naming and describing the function of the epiglottis and the valves of the heart, including the tricuspid.
During the third century, Greek physicians were able to differentiate nerves from blood vessels and tendons and to realize that the nerves convey neural impulses.
It was Herophilus who made the point that damage to motor nerves induced paralysis.
Herophilus named the meninges and ventricles in the brain, appreciated the division between cerebellum and cerebrum and recognized that the brain was the "seat of intellect" and not a "cooling chamber" as propounded by Aristotle Herophilus is also credited with describing the optic, oculomotor, motor division of the trigeminal, facial, vestibulocochlear and hypoglossal nerves.
Great feats were made during the third century in both the digestive and reproductive systems.
Herophilus was able to discover and describe not only the salivary glands, but the small intestine and liver.
He showed that the uterus is a hollow organ and described the ovaries and uterine tubes.
He recognized that spermatozoa were produced by the testes and was the first to identify the prostate gland.
The anatomy of the muscles and skeleton is described in the "Hippocratic Corpus", an Ancient Greek medical work written by unknown authors.
Aristotle described vertebrate anatomy based on animal dissection.
Praxagoras identified the difference between arteries and veins.
Also in the 4th century BCE, Herophilos and Erasistratus produced more accurate anatomical descriptions based on vivisection of criminals in Alexandria during the Ptolemaic dynasty.
In the 2nd century, Galen of Pergamum, an anatomist, clinician, writer and philosopher, wrote the final and highly influential anatomy treatise of ancient times.
He compiled existing knowledge and studied anatomy through dissection of animals.
He was one of the first experimental physiologists through his vivisection experiments on animals.
Galen's drawings, based mostly on dog anatomy, became effectively the only anatomical textbook for the next thousand years.
His work was known to Renaissance doctors only through Islamic Golden Age medicine until it was translated from the Greek some time in the 15th century.
Anatomy developed little from classical times until the sixteenth century; as the historian Marie Boas writes, "Progress in anatomy before the sixteenth century is as mysteriously slow as its development after 1500 is startlingly rapid".
Between 1275 and 1326, the anatomists Mondino de Luzzi, Alessandro Achillini and Antonio Benivieni at Bologna carried out the first systematic human dissections since ancient times.
Mondino's "Anatomy" of 1316 was the first textbook in the medieval rediscovery of human anatomy.
It describes the body in the order followed in Mondino's dissections, starting with the abdomen, then the thorax, then the head and limbs.
It was the standard anatomy textbook for the next century.
Leonardo da Vinci (1452–1519) was trained in anatomy by Andrea del Verrocchio.
He made use of his anatomical knowledge in his artwork, making many sketches of skeletal structures, muscles and organs of humans and other vertebrates that he dissected.
Andreas Vesalius (1514–1564) (Latinized from Andries van Wezel), professor of anatomy at the University of Padua, is considered the founder of modern human anatomy.
Originally from Brabant, Vesalius published the influential book "De humani corporis fabrica" ("the structure of the human body"), a large format book in seven volumes, in 1543.
The accurate and intricately detailed illustrations, often in allegorical poses against Italianate landscapes, are thought to have been made by the artist Jan van Calcar, a pupil of Titian.
In England, anatomy was the subject of the first public lectures given in any science; these were given by the Company of Barbers and Surgeons in the 16th century, joined in 1583 by the Lumleian lectures in surgery at the Royal College of Physicians.
In the United States, medical schools began to be set up towards the end of the 18th century.
Classes in anatomy needed a continual stream of cadavers for dissection and these were difficult to obtain.
Philadelphia, Baltimore and New York were all renowned for body snatching activity as criminals raided graveyards at night, removing newly buried corpses from their coffins.
A similar problem existed in Britain where demand for bodies became so great that grave-raiding and even anatomy murder were practised to obtain cadavers.
Some graveyards were in consequence protected with watchtowers.
The practice was halted in Britain by the Anatomy Act of 1832, while in the United States, similar legislation was enacted after the physician William S. Forbes of Jefferson Medical College was found guilty in 1882 of "complicity with resurrectionists in the despoliation of graves in Lebanon Cemetery".
The teaching of anatomy in Britain was transformed by Sir John Struthers, Regius Professor of Anatomy at the University of Aberdeen from 1863 to 1889.
He was responsible for setting up the system of three years of "pre-clinical" academic teaching in the sciences underlying medicine, including especially anatomy.
This system lasted until the reform of medical training in 1993 and 2003.
As well as teaching, he collected many vertebrate skeletons for his museum of comparative anatomy, published over 70 research papers, and became famous for his public dissection of the Tay Whale.
From 1822 the Royal College of Surgeons regulated the teaching of anatomy in medical schools.
Medical museums provided examples in comparative anatomy, and were often used in teaching.
Ignaz Semmelweis investigated puerperal fever and he discovered how it was caused.
He noticed that the frequently fatal fever occurred more often in mothers examined by medical students than by midwives.
The students went from the dissecting room to the hospital ward and examined women in childbirth.
Semmelweis showed that when the trainees washed their hands in chlorinated lime before each clinical examination, the incidence of puerperal fever among the mothers could be reduced dramatically.
Before the modern medical era, the main means for studying the internal structures of the body were dissection of the dead and inspection, palpation and auscultation of the living.
It was the advent of microscopy that opened up an understanding of the building blocks that constituted living tissues.
Technical advances in the development of achromatic lenses increased the resolving power of the microscope and around 1839, Matthias Jakob Schleiden and Theodor Schwann identified that cells were the fundamental unit of organization of all living things.
Study of small structures involved passing light through them and the microtome was invented to provide sufficiently thin slices of tissue to examine.
Staining techniques using artificial dyes were established to help distinguish between different types of tissue.
Advances in the fields of histology and cytology began in the late 19th century along with advances in surgical techniques allowing for the painless and safe removal of biopsy specimens.
The invention of the electron microscope brought a great advance in resolution power and allowed research into the ultrastructure of cells and the organelles and other structures within them.
About the same time, in the 1950s, the use of X-ray diffraction for studying the crystal structures of proteins, nucleic acids and other biological molecules gave rise to a new field of molecular anatomy.
Equally important advances have occurred in "non-invasive" techniques for examining the interior structures of the body.
X-rays can be passed through the body and used in medical radiography and fluoroscopy to differentiate interior structures that have varying degrees of opaqueness.
Magnetic resonance imaging, computed tomography, and ultrasound imaging have all enabled examination of internal structures in unprecedented detail to a degree far beyond the imagination of earlier generations.
</doc>
<doc id="675" url="https://en.wikipedia.org/wiki?curid=675" title="Affirming the consequent">
Affirming the consequent

Affirming the consequent, sometimes called converse error, fallacy of the converse, or confusion of necessity and sufficiency, is a formal fallacy of taking a true conditional statement (e.g., "If the lamp were broken, then the room would be dark,") and invalidly inferring its converse ("The room is dark, so the lamp is broken,") even though the converse may not be true.
This arises when a consequent ("the room would be dark") has one or more "other" antecedents (for example, "the lamp is not plugged in" or "the lamp is in working order, but is switched off").
Converse errors are common in everyday thinking and communication and can result from, among other causes, communication issues, misconceptions about logic, and failure to consider other causes.
Affirming the consequent is the action of taking a true statement formula_1 and invalidly concluding its converse formula_2.
The name "affirming the consequent" derives from using the consequent, "Q," of formula_1, to conclude the antecedent P. This illogic can be summarized formally as formula_4 or, alternatively, formula_5.
The root cause of such a logic error is sometimes failure to realize that just because "P" is a "possible" condition for Q", P" may not be the "only" condition for "Q", i.e. "Q" may follow from another condition as well.
Affirming the consequent can also result from overgeneralizing the experience of many statements "having" true converses.
If P and Q are "equivalent" statements, i.e. formula_6, it "is" possible to infer P under the condition Q. For example, the statements "It is August 13, so it is my birthday" formula_1 and "It is my birthday, so it is August 13" formula_2are equivalent and both true consequences of the statement "August 13 is my birthday" (an abbreviated form of formula_6).
Using one statement to conclude the other is "not" an example of affirming the consequent, but some person misapply the approach.
It may be worth noting that formula_1"does" imply its contrapositive, formula_11, where formula_12symbolize the negations of Q and P, respectively.
For example, the statement "If the lamp were broken, then the room would be dark," (formula_1) does imply its contrapositive, "The room is "not" dark, so the lamp is "not" broken," (formula_11).
Example 1

One way to demonstrate the invalidity of this argument form is with a counterexample with true premises but an obviously false conclusion.
For example:

Owning Fort Knox is not the "only" way to be rich.
Any number of other ways to be rich exist.
However, one can affirm with certainty that "if someone is not rich" ("non-Q"), then "this person does not own Fort Knox" ("non-P").
This is the contrapositive of the first statement, and it must be true if and only if the original statement is true.
Example 2

Here is another useful, obviously-fallacious example, but one that does not require familiarity with who Bill Gates is and what Fort Knox is:

Here, it is immediately intuitive that any number of other antecedents ("If an animal is a deer...", "If an animal is an elephant...", "If an animal is a moose...", etc.)
can give rise to the consequent ("then it has four legs"), and that it is preposterous to suppose that having four legs must imply that the animal is a dog and nothing else.
This is useful as a teaching example since most people can immediately recognize that the conclusion reached must be wrong (intuitively, a cat cannot be a dog), and that the method by which it was reached must therefore be fallacious.
Example 3

Arguments of the same form can sometimes seem superficially convincing, as in the following example:

Being thrown off the top of the Eiffel Tower is not the "only" cause of death, since there exist numerous different causes of death.
Affirming the consequent is commonly used in rationalization, and thus appears as a coping mechanism in some people.
Example 4

In "Catch-22", the chaplain is interrogated for supposedly being 'Washington Irving'/'Irving Washington', who has been blocking out large portions of soldiers' letters home.
The colonel has found such a letter, but with the Chaplain's name signed.
"P" in this case is 'The chaplain signs his own name', and "Q" 'The chaplain's name is written'.
The chaplain's name may be written, but he did not necessarily write it, as the colonel falsely concludes."
</doc>
<doc id="676" url="https://en.wikipedia.org/wiki?curid=676" title="Andrei Tarkovsky">
Andrei Tarkovsky

Andrei Arsenyevich Tarkovsky (; 4 April 1932 – 29 December 1986) was a Russian filmmaker, writer, film editor, film theorist, theatre and opera director.
His work is characterized by long takes, unconventional dramatic structure, distinctly authored use of cinematography, and spiritual and metaphysical themes.
Director Ingmar Bergman said of him:

Tarkovsky for me is the greatest (director), the one who invented a new language, true to the nature of film, as it captures life as a reflection, life as a dream.
Tarkovsky's films include "Ivan's Childhood" (1962), "Andrei Rublev" (1966), "Solaris" (1972), "Mirror" (1975), and "Stalker" (1979).
He directed the first five of his seven feature films in the Soviet Union; his last two films, "Nostalghia" (1983) and "The Sacrifice" (1986), were produced in Italy and Sweden, respectively.
The films "Andrei Rublev", "Solaris", "Mirror", and "Stalker" are regularly listed among the greatest films of all time.
Andrei Tarkovsky was born in the village of Zavrazhye in the Yuryevetsky District of the Ivanovo Industrial Oblast (modern-day Kadyysky District of the Kostroma Oblast, Russia) to the poet and translator Arseny Alexandrovich Tarkovsky, a native of Yelisavetgrad, Kherson Governorate, and Maria Ivanova Vishnyakova, a graduate of the Maxim Gorky Literature Institute who later worked as a corrector; she was born in Moscow in the Dubasov family estate.
Andrei's paternal grandfather Aleksandr Karlovich Tarkovsky (in ) was a Polish nobleman who worked as a bank clerk.
His wife Maria Danilovna Rachkovskaya was a Romanian teacher who arrived from Iași.
Andrei's maternal grandmother Vera Nikolaevna Vishnyakova (née Dubasova) belonged to an old Dubasov family of Russian nobility that traces its history back to the 17th century; among her relatives was Admiral Fyodor Dubasov, a fact she had to conceal during the Soviet days.
She was married to Ivan Ivanovich Vishnyakov, a native of the Kaluga Governorate who studied law at the Moscow University and served as a judge in Kozelsk.
According to the family legend, Tarkovsky's ancestors on his father's side were princes from the Shamkhalate of Tarki, Dagestan, although his sister Marina Tarkovskaya who did a detailed research on their genealogy called it «a myth, even a prank of sorts», stressing that none of the documents confirms this version.
Tarkovsky spent his childhood in Yuryevets.
He was described by childhood friends as active and popular, having many friends and being typically in the center of action.
His father left the family in 1937, subsequently volunteering for the army in 1941.
Tarkovsky stayed with his mother, moving with her and his sister Marina to Moscow, where she worked as a proofreader at a printing press.
In 1939 Tarkovsky enrolled at the Moscow School No.
554.
During the war, the three evacuated to Yuryevets, living with his maternal grandmother.
In 1943 the family returned to Moscow.
Tarkovsky continued his studies at his old school, where the poet Andrey Voznesensky was one of his classmates.
He studied piano at a music school and attended classes at an art school.
The family lived on Shchipok Street in the Zamoskvorechye District in Moscow.
From November 1947 to spring 1948 he was in the hospital with tuberculosis.
Many themes of his childhood—the evacuation, his mother and her two children, the withdrawn father, the time in the hospital—feature prominently in his film "Mirror".
In his school years, Tarkovsky was a troublemaker and a poor student.
He still managed to graduate, and from 1951 to 1952 studied Arabic at the Oriental Institute in Moscow, a branch of the Academy of Sciences of the USSR.
Although he already spoke some Arabic and was a successful student in his first semesters, he did not finish his studies and dropped out to work as a prospector for the Academy of Science Institute for Non-Ferrous Metals and Gold.
He participated in a year-long research expedition to the river Kureikye near Turukhansk in the Krasnoyarsk Province.
During this time in the taiga, Tarkovsky decided to study film.
Upon returning from the research expedition in 1954, Tarkovsky applied at the State Institute of Cinematography (VGIK) and was admitted to the film-directing program.
He was in the same class as Irma Raush whom he married in April 1957.
The early Khrushchev era offered good opportunities for young film directors.
Before 1953, annual film production was low and most films were directed by veteran directors.
After 1953, more films were produced, many of them by young directors.
The Khrushchev Thaw relaxed Soviet social restrictions a bit and permitted a limited influx of European and North American literature, films and music.
This allowed Tarkovsky to see films of the Italian neorealists, French New Wave, and of directors such as Kurosawa, Buñuel, Bergman, Bresson, Andrzej Wajda (whose film "Ashes and Diamonds" influenced Tarkovsky) and Mizoguchi.
Tarkovsky's teacher and mentor was Mikhail Romm, who taught many film students who would later become influential film directors.
In 1956 Tarkovsky directed his first student short film, "The Killers", from a short story of Ernest Hemingway.
The short film "There Will Be No Leave Today" and the screenplay "Concentrate" followed in 1958 and 1959.
An important influence on Tarkovsky was the film director Grigori Chukhrai, who was teaching at the VGIK.
Impressed by the talent of his student, Chukhrai offered Tarkovsky a position as assistant director for his film "Clear Skies".
Tarkovsky initially showed interest but then decided to concentrate on his studies and his own projects.
During his third year at the VGIK, Tarkovsky met Andrei Konchalovsky.
They found much in common as they liked the same film directors and shared ideas on cinema and films.
In 1959 they wrote the script "Antarctica – Distant Country", which was later published in the "Moskovskij Komsomolets".
Tarkovsky submitted the script to Lenfilm, but it was rejected.
They were more successful with the script "The Steamroller and the Violin", which they sold to Mosfilm.
This became Tarkovsky's graduation project, earning him his diploma in 1960 and winning First Prize at the New York Student Film Festival in 1961.
Tarkovsky's first feature film was "Ivan's Childhood" in 1962.
He had inherited the film from director Eduard Abalov, who had to abort the project.
The film earned Tarkovsky international acclaim and won the Golden Lion award at the Venice Film Festival in the year 1962.
In the same year, on 30 September, his first son Arseny (called Senka in Tarkovsky's diaries) Tarkovsky was born.
In 1965, he directed the film "Andrei Rublev" about the life of Andrei Rublev, the fifteenth-century Russian icon painter.
"Andrei Rublev" was not, except for a single screening in Moscow in 1966, immediately released after completion due to problems with Soviet authorities.
Tarkovsky had to cut the film several times, resulting in several different versions of varying lengths.
A version of the film was presented at the Cannes Film Festival in 1969 and won the FIPRESCI prize.
The film was widely released in the Soviet Union in a cut version in 1971.
He divorced his wife, Irma Raush, in June 1970.
In the same year, he married Larissa Kizilova (née Egorkina), who had been a production assistant for the film "Andrei Rublev" (they had been living together since 1965).
Their son, Andrei Andreyevich Tarkovsky, was born in the same year on 7 August.
The film was presented an award at the Cannes Film Festival.
In 1972, he completed "Solaris", an adaptation of the novel "Solaris" by Stanisław Lem.
He had worked on this together with screenwriter Fridrikh Gorenshtein as early as 1968.
The film was presented at the Cannes Film Festival, won the Grand Prix Spécial du Jury and the FIPRESCI prize, and was nominated for the Palme d'Or.
From 1973 to 1974, he shot the film "Mirror", a highly autobiographical and unconventionally structured film drawing on his childhood and incorporating some of his father's poems.
In this film Tarkovsky portrayed the plight of childhood affected by war.
Tarkovsky had worked on the screenplay for this film since 1967, under the consecutive titles "Confession", "White day" and "A white, white day".
From the beginning the film was not well received by Soviet authorities due to its content and its perceived elitist nature.
Russian authorities placed the film in the "third category," a severely limited distribution, and only allowed it to be shown in third-class cinemas and workers' clubs.
Few prints were made and the film-makers received no returns.
Third category films also placed the film-makers in danger of being accused of wasting public funds, which could have serious effects on their future productivity.
These difficulties are presumed to have made Tarkovsky play with the idea of going abroad and producing a film outside the Soviet film industry.
During 1975, Tarkovsky also worked on the screenplay "Hoffmanniana", about the German writer and poet E. T. A. Hoffmann.
In December 1976, he directed "Hamlet", his only stage play, at the Lenkom Theatre in Moscow.
The main role was played by Anatoly Solonitsyn, who also acted in several of Tarkovsky's films.
At the end of 1978, he also wrote the screenplay "Sardor" together with the writer Aleksandr Misharin.
The last film Tarkovsky completed in the Soviet Union was "Stalker", inspired by the novel "Roadside Picnic" by the brothers Arkady and Boris Strugatsky.
Tarkovsky had met the brothers first in 1971 and was in contact with them until his death in 1986.
Initially he wanted to shoot a film based on their novel "Dead Mountaineer's Hotel" and he developed a raw script.
Influenced by a discussion with Arkady Strugatsky he changed his plan and began to work on the script based on "Roadside Picnic".
Work on this film began in 1976.
The production was mired in troubles; improper development of the negatives had ruined all the exterior shots.
Tarkovsky's relationship with cinematographer Georgy Rerberg deteriorated to the point where he hired Alexander Knyazhinsky as a new first cinematographer.
Furthermore, Tarkovsky suffered a heart attack in April 1978, resulting in further delay.
The film was completed in 1979 and won the Prize of the Ecumenical Jury at the Cannes Film Festival.
In the same year Tarkovsky also began the production of the film "The First Day" (Russian: Первый День "Pervyj Dyen′"), based on a script by his friend and long-term collaborator Andrei Konchalovsky.
The film was set in 18th-century Russia during the reign of Peter the Great and starred Natalya Bondarchuk and Anatoli Papanov.
To get the project approved by Goskino, Tarkovsky submitted a script that was different from the original script, omitting several scenes that were critical of the official atheism in the Soviet Union.
After shooting roughly half of the film the project was stopped by Goskino after it became apparent that the film differed from the script submitted to the censors.
Tarkovsky was reportedly infuriated by this interruption and destroyed most of the film.
During the summer of 1979, Tarkovsky traveled to Italy, where he shot the documentary "Voyage in Time" together with his long-time friend Tonino Guerra.
Tarkovsky returned to Italy in 1980 for an extended trip during which he and Guerra completed the script for the film "Nostalghia".
Tarkovsky returned to Italy in 1982 to start shooting "Nostalghia".
He did not return to his home country.
As Mosfilm withdrew from the project, he had to complete the film with financial support provided by the Italian RAI.
Tarkovsky completed the film in 1983.
"Nostalghia" was presented at the Cannes Film Festival and won the FIPRESCI prize and the Prize of the Ecumenical Jury.
Tarkovsky also shared a special prize called "Grand Prix du cinéma de creation" with Robert Bresson.
Soviet authorities prevented the film from winning the Palme d'Or, a fact that hardened Tarkovsky's resolve to never work in the Soviet Union again.
He also said: '"'I am not a Soviet dissident, I have no conflict with the Soviet Government.""
But if he returned home, he added, "<nowiki>"I would be unemployed.
"</nowiki>" In the same year, he also staged the opera "Boris Godunov" at the Royal Opera House in London under the musical direction of Claudio Abbado.
He spent most of 1984 preparing the film "The Sacrifice".
At a press conference in Milan on 10 July 1984, he announced that he would never return to the Soviet Union and would remain in Europe.
At that time, his son Andrei Jr.
was still in the Soviet Union and not allowed to leave the country.
On 28 August 1985, Tarkovsky arrived at Latina Refugee Camp in Latina, where he was registered with the serial number 13225/379.
During 1985, he shot the film "The Sacrifice" in Sweden.
At the end of the year he was diagnosed with terminal lung cancer.
In January 1986, he began treatment in Paris and was joined there by his son, who was finally allowed to leave the Soviet Union.
"The Sacrifice" was presented at the Cannes Film Festival and received the Grand Prix Spécial du Jury, the FIPRESCI prize and the Prize of the Ecumenical Jury.
As Tarkovsky was unable to attend due to his illness, the prizes were collected by his son, Andrei Jr.
In Tarkovsky's last entry (15 December 1986), he wrote: "But now I have no strength left – that is the problem".
The diaries are sometimes also known as "" and were published posthumously in 1989 and in English in 1991.
Tarkovsky died in Paris on 29 December 1986.
His funeral ceremony was held at the Alexander Nevsky Cathedral.
He was buried on 3 January 1987 in the Russian Cemetery in Sainte-Geneviève-des-Bois in France.
The inscription on his gravestone, which was conceived by Tarkovsky's wife, Larisa Tarkovskaya, reads: "To the man who saw the Angel".
A conspiracy theory emerged in Russia in the early 1990s when it was alleged that Tarkovsky did not die of natural causes but was assassinated by the KGB.
Evidence for this hypothesis includes testimonies by former KGB agents who claim that Viktor Chebrikov gave the order to eradicate Tarkovsky to curtail what the Soviet government and the KGB saw as anti-Soviet propaganda by Tarkovsky.
Other evidence includes several memoranda that surfaced after the 1991 coup and the claim by one of Tarkovsky's doctors that his cancer could not have developed from a natural cause.
As with Tarkovsky, his wife Larisa Tarkovskaya and actor Anatoli Solonitsyn all died from the very same type of lung cancer.
Vladimir Sharun, sound designer in "Stalker", is convinced that they were all poisoned by the chemical plant where they were shooting the film.
Tarkovsky is mainly known as a film director.
During his career he directed only seven feature films, as well as three shorts from his time at VGIK.
These include:
He also wrote several screenplays.
Furthermore, he directed the play "Hamlet" for the stage in Moscow, directed the opera "Boris Godunov" in London, and he directed a radio production of the short story "Turnabout" by William Faulkner.
He also wrote "Sculpting in Time", a book on film theory.
Tarkovsky's first feature film was "Ivan's Childhood" in 1962.
He then directed "Andrei Rublev" in 1966, "Solaris" in 1972, "Mirror" in 1975 and "Stalker" in 1979.
The documentary "Voyage in Time" was produced in Italy in 1982, as was "Nostalghia" in 1983.
His last film "The Sacrifice" was produced in Sweden in 1986.
Tarkovsky was personally involved in writing the screenplays for all his films, sometimes with a cowriter.
Tarkovsky once said that a director who realizes somebody else's screenplay without being involved in it becomes a mere illustrator, resulting in dead and monotonous films.
A book of 60 photos, "Instant Light, Tarkovsky Polaroids", taken by Tarkovsky in Russia and Italy between 1979 and 1984 was published in 2006.
The collection was selected by Italian photographer Giovanni Chiaramonte and Tarkovsky's son Andrey A. Tarkovsky.
Bibliography

Books written by Tarkovsky

Numerous awards were bestowed on Tarkovsky throughout his lifetime.
At the Venice Film Festival he was awarded the Golden Lion for "Ivan's Childhood".
At the Cannes Film Festival, he won the FIPRESCI prize four times, the Prize of the Ecumenical Jury three times (more than any other director), and the Grand Prix Spécial du Jury twice.
He was also nominated for the Palme d'Or two times.
In 1987, the British Academy of Film and Television Arts awarded the BAFTA Award for Best Foreign Language Film to "The Sacrifice".
Under the influence of Glasnost and Perestroika, Tarkovsky was finally recognized in the Soviet Union in the Autumn of 1986, shortly before his death, by a retrospective of his films in Moscow.
After his death, an entire issue of the film magazine "Iskusstvo Kino" was devoted to Tarkovsky.
In their obituaries, the film committee of the Council of Ministers of the USSR and the Union of Soviet Film Makers expressed their sorrow that Tarkovsky had to spend the last years of his life in exile.
Posthumously, he was awarded the Lenin Prize in 1990, one of the highest state honors in the Soviet Union.
In 1989 the "Andrei Tarkovsky Memorial Prize" was established, with its first recipient being the Russian animator Yuriy Norshteyn.
In three consecutive events, the Moscow International Film Festival awards the annual "Andrei Tarkovsky Award" in the years of 1993, 1995 and 1997.
In 1996 the Andrei Tarkovsky Museum opened in Yuryevets, his childhood town.
A minor planet, 3345 Tarkovskij, discovered by Soviet astronomer Lyudmila Georgievna Karachkina in 1982, has also been named after him.
Tarkovsky has been the subject of several documentaries.
Most notable is the 1988 documentary "Moscow Elegy", by Russian film director Alexander Sokurov.
Sokurov's own work has been heavily influenced by Tarkovsky.
The film consists mostly of narration over stock footage from Tarkovsky's films.
"Directed by Andrei Tarkovsky" is 1988 documentary film by Michal Leszczylowski, an editor of the film "The Sacrifice".
Film director Chris Marker produced the television documentary "One Day in the Life of Andrei Arsenevich" as an homage to Andrei Tarkovsky in 2000.
Ingmar Bergman was quoted as saying: "Tarkovsky for me is the greatest [of us all], the one who invented a new language, true to the nature of film, as it captures life as a reflection, life as a dream".
Film historian Steven Dillon says that much of subsequent film was deeply influenced by the films of Tarkovsky.
At the entrance to the Gerasimov Institute of Cinematography in Moscow, Russia there is a monument that includes statues of Tarkovsky, Gennady Shpalikov and Vasily Shukshin.
Concentrate (, "Konsentrat") is a never-filmed 1958 screenplay by Russian film director Andrei Tarkovsky.
The screenplay is based on Tarkovsky's year in the taiga as a member of a research expedition, prior to his enrollment in film school.
It's about the leader of a geological expedition, who waits for the boat that brings back the concentrates collected by the expedition.
The expedition is surrounded by mystery, and its purpose is a state secret.
Although some authors claim that the screenplay was filmed, according to Marina Tarkovskaya, Tarkovsky's sister (and wife of Aleksandr Gordon, a fellow student of Tarvosky during his film school years) the screenplay was never filmed.
Tarkovsky wrote the screenplay during his entrance examination at the State Institute of Cinematography (VGIK) in a single sitting.
He earned the highest possible grade, excellent () for this work.
In 1994 fragments of the "Concentrate" were filmed and used in the documentary "Andrei Tarkovsky's Taiga Summer" by Marina Tarkovskaya and Aleksandr Gordon.
Hoffmanniana () is a never-filmed 1974 screenplay by Russian film director Andrei Tarkovsky.
The screenplay is based on the life and work of German author E. T. A. Hoffmann.
In 1974 an acquaintance from Tallinnfilm approached Tarkovsky to write a screenplay on a German theme.
Tarkovsky considered Thomas Mann and E.T.A.
Hoffmann, and also thought about Ibsen's "Peer Gynt".
In the end Tarkovsky signed a contract for a script based on the life and work of Hoffmann.
Tarkovsky planned to write the script during the summer of 1974 at his dacha.
Writing was not without difficulty, less than a month before the deadline he had not written a single page.
He finally finished the project in late 1974 and submitted the final script to Tallinnfilm in October.
Although the script was well received by the officials at Tallinnfilm, it was the consensus that no one but Tarkovsky would be able to direct it.
The script was sent to Goskino in February 1976, and although approval was granted for proceeding with making the film the screenplay was never realized.
In 1984, during the time of his exile in the West, Tarkovsky revisited the screenplay and made a few changes.
He also considered to finally direct a film based on the screenplay but ultimately dropped this idea.
Tarkovsky became a film director during the mid and late 1950s, a period referred to as the Khrushchev Thaw, during which Soviet society opened to foreign films, literature and music, among other things.
This allowed Tarkovsky to see films of European, American and Japanese directors, an experience which influenced his own film making.
His teacher and mentor at the film school, Mikhail Romm, allowed his students considerable freedom and emphasized the independence of the film director.
Tarkovsky was, according to fellow student Shavkat Abdusalmov, fascinated by Japanese films.
He was amazed by how every character on the screen is exceptional and how everyday events such as a Samurai cutting bread with his sword are elevated to something special and put into the limelight.
Tarkovsky has also expressed interest in the art of Haiku and its ability to create "images in such a way that they mean nothing beyond themselves."
Tarkovsky perceived that the art of cinema has only been truly mastered by very few filmmakers, stating in a 1970 interview with Naum Abramov that "they can be counted on the fingers of one hand."
In 1972, Tarkovsky told film historian Leonid Kozlov his ten favorite films.
The list includes: "Diary of a Country Priest" and "Mouchette" by Robert Bresson; "Winter Light", "Wild Strawberries", and "Persona" by Ingmar Bergman; "Nazarín" by Luis Buñuel; "City Lights" by Charlie Chaplin; "Ugetsu" by Kenji Mizoguchi; "Seven Samurai" by Akira Kurosawa, and "Woman in the Dunes" by Hiroshi Teshigahara.
Among his favorite directors were Buñuel, Mizoguchi, Bergman, Bresson, Kurosawa, Michelangelo Antonioni, Jean Vigo, and Carl Theodor Dreyer.
With the exception of "City Lights", the list does not contain any films of the early silent era.
The reason is that Tarkovsky saw film as an art as only a relatively recent phenomenon, with the early film-making forming only a prelude.
The list has also no films or directors from Tarkovsky's native Russia, although he rated Soviet directors such as Boris Barnet, Sergei Parajanov and Alexander Dovzhenko highly.
He said of Dovzhenko's "Earth", "I have lived a lot among very simple farmers and met extraordinary people.
They spread calmness, had such tact, they conveyed a feeling of dignity and displayed wisdom that I have seldom come across on such a scale.
Dovzhenko had obviously understood wherein the sense of life resides.
[...] This trespassing of the border between nature and mankind is an ideal place for the existence of man.
Dovzhenko understood this."
Although strongly opposed to commercial cinema, in a famous exception Tarkovsky praised the blockbuster film "The Terminator", saying its "vision of the future and the relation between man and its destiny is pushing the frontier of cinema as an art".
He was critical of the "brutality and low acting skills", but was nevertheless impressed by the film.
In a 1962 interview, Tarkovsky argued, "All art, of course, is intellectual, but for me, all the arts, and cinema even more so, must above all be emotional and act upon the heart."
His films are characterized by metaphysical themes, extremely long takes, and images often considered by critics to be of exceptional beauty.
Recurring motifs are dreams, memory, childhood, running water accompanied by fire, rain indoors, reflections, levitation, and characters re-appearing in the foreground of long panning movements of the camera.
He once said, "Juxtaposing a person with an environment that is boundless, collating him with a countless number of people passing by close to him and far away, relating a person to the whole world, that is the meaning of cinema.”

Tarkovsky incorporated levitation scenes into several of his films, most notably "Solaris".
To him these scenes possess great power and are used for their photogenic value and magical inexplicability.
Water, clouds, and reflections were used by him for their surreal beauty and photogenic value, as well as their symbolism, such as waves or the forms of brooks or running water.
Bells and candles are also frequent symbols.
These are symbols of film, sight and sound, and Tarkovsky's film frequently has themes of self-reflection.
Tarkovsky developed a theory of cinema that he called "sculpting in time".
By this he meant that the unique characteristic of cinema as a medium was to take our experience of time and alter it.
Unedited movie footage transcribes time in real time.
By using long takes and few cuts in his films, he aimed to give the viewers a sense of time passing, time lost, and the relationship of one moment in time to another.
Up to, and including, his film "Mirror", Tarkovsky focused his cinematic works on exploring this theory.
After "Mirror", he announced that he would focus his work on exploring the dramatic unities proposed by Aristotle: a concentrated action, happening in one place, within the span of a single day.
Several of Tarkovsky's films have color or black and white sequences.
This first occurs in the otherwise monochrome "Andrei Rublev", which features a color epilogue of Rublev's authentic religious icon paintings.
All of his films afterwards contain monochrome, and in "Stalker's" case sepia sequences, while otherwise being in color.
In 1966, in an interview conducted shortly after finishing "Andrei Rublev", Tarkovsky dismissed color film as a "commercial gimmick" and cast doubt on the idea that contemporary films meaningfully use color.
He claimed that in everyday life one does not consciously notice colors most of the time, and that color should therefore be used in film mainly to emphasize certain moments, but not all the time, as this distracts the viewer.
To him, films in color were like moving paintings or photographs, which are too beautiful to be a realistic depiction of life.
Bergman on Tarkovsky

Ingmar Bergman, a renowned director, commented on Tarkovsky
Contrarily, however, Bergman conceded the truth in the claim made by a critic who wrote that, "with "Autumn Sonata" Bergman does Bergman," adding, "Tarkovsky began to make Tarkovsky films, and that Fellini began to make Fellini films [...] Buñuel nearly always made Buñuel films."
This pastiche of one's own work has been derogatorily termed as "self-karaoke."
Tarkovsky worked in close collaboration with cinematographer Vadim Yusov from 1958 to 1972, and much of the visual style of Tarkovsky's films can be attributed to this collaboration.
Tarkovsky would spend two days preparing for Yusov to film a single long take, and due to the preparation, usually only a single take was needed.
In his last film, "The Sacrifice", Tarkovsky worked with cinematographer Sven Nykvist, who had worked on many films with director Ingmar Bergman.
(Nykvist was not alone: several people involved in the production had previously collaborated with Bergman, notably lead actor Erland Josephson, who had also acted for Tarkovsky in "Nostalghia".)
Nykvist complained that Tarkovsky would frequently look through the camera and even direct actors through it.
Notes

Bibliography


</doc>
<doc id="677" url="https://en.wikipedia.org/wiki?curid=677" title="Ambiguity">
Ambiguity

Ambiguity is a type of meaning in which several interpretations are plausible.
A common aspect of ambiguity is uncertainty.
It is thus an attribute of any idea or statement whose intended meaning cannot be definitively resolved according to a rule or process with a finite number of steps.
(The "ambi-" part of the term reflects an idea of "two", as in "two meanings".)
The concept of ambiguity is generally contrasted with vagueness.
In ambiguity, specific and distinct interpretations are permitted (although some may not be immediately obvious), whereas with information that is vague, it is difficult to form any interpretation at the desired level of specificity.
Context may play a role in resolving ambiguity.
For example, the same piece of information may be ambiguous in one context and unambiguous in another.
The lexical ambiguity of a word or phrase pertains to its having more than one meaning in the language to which the word belongs.
"Meaning" here refers to whatever should be captured by a good dictionary.
For instance, the word "bank" has several distinct lexical definitions, including "financial institution" and "edge of a river".
Or consider "apothecary".
One could say "I bought herbs from the apothecary".
This could mean one actually spoke to the apothecary (pharmacist) or went to the apothecary (pharmacy).
The context in which an ambiguous word is used often makes it evident which of the meanings is intended.
If, for instance, someone says "I buried $100 in the bank", most people would not think someone used a shovel to dig in the mud.
However, some linguistic contexts do not provide sufficient information to disambiguate a used word.
Lexical ambiguity can be addressed by algorithmic methods that automatically associate the appropriate meaning with a word in context, a task referred to as word sense disambiguation.
The use of multi-defined words requires the author or speaker to clarify their context, and sometimes elaborate on their specific intended meaning (in which case, a less ambiguous term should have been used).
The goal of clear concise communication is that the receiver(s) have no misunderstanding about what was meant to be conveyed.
An exception to this could include a politician whose "weasel words" and obfuscation are necessary to gain support from multiple constituents with mutually exclusive conflicting desires from their candidate of choice.
Ambiguity is a powerful tool of political science.
More problematic are words whose senses express closely related concepts.
"Good", for example, can mean "useful" or "functional" ("That's a good hammer"), "exemplary" ("She's a good student"), "pleasing" ("This is good soup"), "moral" ("a good person" versus "the lesson to be learned from a story"), "righteous", etc. "
I have a good daughter" is not clear about which sense is intended.
The various ways to apply prefixes and suffixes can also create ambiguity ("unlockable" can mean "capable of being unlocked" or "impossible to lock").
Syntactic ambiguity arises when a sentence can have two (or more) different meanings because of the structure of the sentence—its syntax.
This is often due to a modifying expression, such as a prepositional phrase, the application of which is unclear.
"He ate the cookies on the couch", for example, could mean that he ate those cookies that were on the couch (as opposed to those that were on the table), or it could mean that he was sitting on the couch when he ate the cookies.
"To get in, you will need an entrance fee of $10 or your voucher and your drivers' license."
This could mean that you need EITHER ten dollars OR BOTH your voucher and your license.
Or it could mean that you need your license AND you need EITHER ten dollars OR a voucher.
Only rewriting the sentence, or placing appropriate punctuation can resolve a syntactic ambiguity.
For the notion of, and theoretic results about, syntactic ambiguity in artificial, formal languages (such as computer programming languages), see Ambiguous grammar.
Spoken language can contain many more types of ambiguities which are called phonological ambiguities, where there is more than one way to compose a set of sounds into words.
For example, "ice cream" and "I scream".
Such ambiguity is generally resolved according to the context.
A mishearing of such, based on incorrectly resolved ambiguity, is called a mondegreen.
Semantic ambiguity happens when a sentence contains an ambiguous word or phrase—a word or phrase that has more than one meaning.
In "We saw her duck" (example due to Richard Nordquist), the word "duck" can refer either

Lexical ambiguity is contrasted with semantic ambiguity.
The former represents a choice between a finite number of known and meaningful context-dependent interpretations.
The latter represents a choice between any number of possible interpretations, none of which may have a standard agreed-upon meaning.
This form of ambiguity is closely related to vagueness.
Linguistic ambiguity can be a problem in law, because the interpretation of written documents and oral agreements is often of paramount importance.
Philosophers (and other users of logic) spend a lot of time and effort searching for and removing (or intentionally adding) ambiguity in arguments because it can lead to incorrect conclusions and can be used to deliberately conceal bad arguments.
For example, a politician might say, "I oppose taxes which hinder economic growth", an example of a glittering generality.
Some will think he opposes taxes in general because they hinder economic growth.
Others may think he opposes only those taxes that he believes will hinder economic growth.
In writing, the sentence can be rewritten to reduce possible misinterpretation, either by adding a comma after "taxes" (to convey the first sense) or by changing "which" to "that" (to convey the second sense) or by rewriting it in other ways.
The devious politician hopes that each constituent will interpret the statement in the most desirable way, and think the politician supports everyone's opinion.
However, the opposite can also be true – an opponent can turn a positive statement into a bad one if the speaker uses ambiguity (intentionally or not).
The logical fallacies of amphiboly and equivocation rely heavily on the use of ambiguous words and phrases.
In continental philosophy (particularly phenomenology and existentialism), there is much greater tolerance of ambiguity, as it is generally seen as an integral part of the human condition.
Martin Heidegger argued that the relation between the subject and object is ambiguous, as is the relation of mind and body, and part and whole.
[3] In Heidegger's phenomenology, Dasein is always in a meaningful world, but there is always an underlying background for every instance of signification.
Thus, although some things may be certain, they have little to do with Dasein's sense of care and existential anxiety, e.g., in the face of death.
In calling his work Being and Nothingness an "essay in phenomenological ontology" Jean-Paul Sartre follows Heidegger in defining the human essence as ambiguous, or relating fundamentally to such ambiguity.
Simone de Beauvoir tries to base an ethics on Heidegger's and Sartre's writings (The Ethics of Ambiguity), where she highlights the need to grapple with ambiguity: "as long as philosophers and they [men] have thought, most of them have tried to mask it...And the ethics which they have proposed to their disciples have always pursued the same goal.
It has been a matter of eliminating the ambiguity by making oneself pure inwardness or pure externality, by escaping from the sensible world or being engulfed by it, by yielding to eternity or enclosing oneself in the pure moment."
Ethics cannot be based on the authoritative certainty given by mathematics and logic, or prescribed directly from the empirical findings of science.
She states: "Since we do not succeed in fleeing it, let us, therefore, try to look the truth in the face.
Let us try to assume our fundamental ambiguity.
It is in the knowledge of the genuine conditions of our life that we must draw our strength to live and our reason for acting".
Other continental philosophers suggest that concepts such as life, nature, and sex are ambiguous.
Corey Anton has argued that we cannot be certain what is separate from or unified with something else: language, he asserts, divides what is not, in fact, separate.
Following Ernest Becker, he argues that the desire to 'authoritatively disambiguate' the world and existence has led to numerous ideologies and historical events such as genocide.
On this basis, he argues that ethics must focus on 'dialectically integrating opposites' and balancing tension, rather than seeking a priori validation or certainty.
Like the existentialists and phenomenologists, he sees the ambiguity of life as the basis of creativity.
In literature and rhetoric, ambiguity can be a useful tool.
Groucho Marx's classic joke depends on a grammatical ambiguity for its humor, for example: "Last night I shot an elephant in my pajamas.
How he got in my pajamas, I'll never know".
Songs and poetry often rely on ambiguous words for artistic effect, as in the song title "Don't It Make My Brown Eyes Blue" (where "blue" can refer to the color, or to sadness).
In the narrative, ambiguity can be introduced in several ways: motive, plot, character.
F. Scott Fitzgerald uses the latter type of ambiguity with notable effect in his novel "The Great Gatsby".
Christianity and Judaism employ the concept of paradox synonymously with 'ambiguity'.
Many Christians and Jews endorse Rudolf Otto's description of the sacred as 'mysterium tremendum et fascinans', the awe-inspiring mystery which fascinates humans.
[dubious – discuss] The orthodox Catholic writer G. K. Chesterton regularly employed paradox to tease out the meanings in common concepts which he found ambiguous or to reveal meaning often overlooked or forgotten in common phrases.
(The title of one of his most famous books, Orthodoxy, itself employing such a paradox.)
Metonymy involves the use of the name of a subcomponent part as an abbreviation, or jargon, for the name of the whole object (for example "wheels" to refer to a car, or "flowers" to refer to beautiful offspring, an entire plant, or a collection of blooming plants).
In modern vocabulary, critical semiotics,[9] metonymy encompasses any potentially ambiguous word substitution that is based on contextual contiguity (located close together), or a function or process that an object performs, such as "sweet ride" to refer to a nice car.
Metonym miscommunication is considered a primary mechanism of linguistic humor.
In music, pieces or sections which confound expectations and may be or are interpreted simultaneously in different ways are ambiguous, such as some polytonality, polymeter, other ambiguous meters or rhythms, and ambiguous phrasing, or (Stein 2005, p.
79) any aspect of music.
The music of Africa is often purposely ambiguous.
To quote Sir Donald Francis Tovey (1935, p.
195), "Theorists are apt to vex themselves with vain efforts to remove uncertainty just where it has a high aesthetic value."
In visual art, certain images are visually ambiguous, such as the Necker cube, which can be interpreted in two ways.
Perceptions of such objects remain stable for a time, then may flip, a phenomenon called multistable perception.
The opposite of such ambiguous images are impossible objects.
Pictures or photographs may also be ambiguous at the semantic level: the visual image is unambiguous, but the meaning and narrative may be ambiguous: is a certain facial expression one of excitement or fear, for instance?
Some languages have been created with the intention of avoiding ambiguity, especially lexical ambiguity.
Lojban and Loglan are two related languages which have been created for this, focusing chiefly on syntactic ambiguity as well.
The languages can be both spoken and written.
These languages are intended to provide a greater technical precision over big natural languages, although historically, such attempts at language improvement have been criticized.
Languages composed from many diverse sources contain much ambiguity and inconsistency.
The many exceptions to syntax and semantic rules are time-consuming and difficult to learn.
In computer science, the SI prefixes kilo-, mega- and giga- were historically used in certain contexts to mean either the first three powers of 1024 (1024, 1024 and 1024) contrary to the metric system in which these units unambiguously mean one thousand, one million, and one billion.
This usage is particularly prevalent with electronic memory devices (e.g.
DRAM) addressed directly by a binary machine register where a decimal interpretation makes no practical sense.
Subsequently, the Ki, Mi, and Gi prefixes were introduced so that binary prefixes could be written explicitly, also rendering k, M, and G "unambiguous" in texts conforming to the new standard — this led to a "new" ambiguity in engineering documents lacking outward trace of the binary prefixes (necessarily indicating the new style) as to whether the usage of k, M, and G remains ambiguous (old style) or not (new style).
Note also that 1 M (where M is ambiguously 1,000,000 or 1,048,576) is "less" uncertain than the engineering value 1.0e6 (defined to designate the interval 950,000 to 1,050,000), and that as non-volatile storage devices began to commonly exceed 1 GB in capacity (where the ambiguity begins to routinely impact the second significant digit), GB and TB almost always mean 10 and 10 bytes.
Mathematical notation, widely used in physics and other sciences, avoids many ambiguities compared to expression in natural language.
However, for various reasons, several lexical, syntactic and semantic ambiguities remain.
The ambiguity in the style of writing a function should not be confused with a multivalued function, which can (and should) be defined in a deterministic and unambiguous way.
Several special functions still do not have established notations.
Usually, the conversion to another notation requires to scale the argument or the resulting value; sometimes, the same name of the function is used, causing confusions.
Examples of such underestablished functions:

Ambiguous expressions often appear in physical and mathematical texts.
It is common practice to omit multiplication signs in mathematical expressions.
Also, it is common to give the same name to a variable and a function, for example, formula_1.
Then, if one sees formula_2, there is no way to distinguish whether it means formula_1 multiplied by formula_4, or function formula_5 evaluated at argument equal to formula_4.
In each case of use of such notations, the reader is supposed to be able to perform the deduction and reveal the true meaning.
Creators of algorithmic languages try to avoid ambiguities.
Many algorithmic languages (C++ and Fortran) require the character * as symbol of multiplication.
The Wolfram Language used in Mathematica allows the user to omit the multiplication symbol, but requires square brackets to indicate the argument of a function; square brackets are not allowed for grouping of expressions.
Fortran, in addition, does not allow use of the same name (identifier) for different objects, for example, function and variable; in particular, the expression f=f(x) is qualified as an error.
The order of operations may depend on the context.
In most programming languages, the operations of division and multiplication have equal priority and are executed from left to right.
Until the last century, many editorials assumed that multiplication is performed first, for example, formula_7 is interpreted as formula_8; in this case, the insertion of parentheses is required when translating the formulas to an algorithmic language.
In addition, it is common to write an argument of a function without parenthesis, which also may lead to ambiguity.
Sometimes, one uses "italics" letters to denote elementary functions.
In the scientific journal style, the expression
formula_9
means
product of variables
formula_10,
formula_11,
formula_12 and
formula_13, although in a slideshow, it may mean formula_14.
A comma in subscripts and superscripts sometimes is omitted; it is also ambiguous notation.
If it is written formula_15, the reader should guess from the context, does it mean a single-index object, evaluated while the subscript is equal to product of variables
formula_16, formula_12 and formula_18, or it is indication to a trivalent tensor.
The writing of formula_15 instead of formula_20 may mean that the writer either is stretched in space (for example, to reduce the publication fees) or aims to increase number of publications without considering readers.
The same may apply to any other use of ambiguous notations.
Subscripts are also used to denote the argument to a function, as in formula_21.
formula_22, which could be understood to mean either formula_23 or formula_24.
In addition, formula_25 may mean formula_26, as formula_27 means formula_28 (see tetration).
formula_29, which by convention means formula_30, though it might be thought to mean formula_31, since formula_32 means formula_33.
formula_34, which arguably should mean formula_35 but would commonly be understood to mean formula_36 .
It is common to define the coherent states in quantum optics with formula_37 and states with fixed number of photons with formula_38.
Then, there is an "unwritten rule": the state is coherent if there are more Greek characters than Latin characters in the argument, and formula_39photon state if the Latin characters dominate.
The ambiguity becomes even worse, if formula_40 is used for the states with certain value of the coordinate, and formula_41 means the state with certain value of the momentum, which may be used in books on quantum mechanics.
Such ambiguities easily lead to confusions, especially if some normalized adimensional, dimensionless variables are used.
Expression formula_42 may mean a state with single photon, or the coherent state with mean amplitude equal to 1, or state with momentum equal to unity, and so on.
The reader is supposed to guess from the context.
Some physical quantities do not yet have established notations; their value (and sometimes even dimension, as in the case of the Einstein coefficients), depends on the system of notations.
Many terms are ambiguous.
Each use of an ambiguous term should be preceded by the definition, suitable for a specific case.
Just like Ludwig Wittgenstein states in Tractatus Logico-Philosophicus: "... Only in the context of a proposition has a name meaning."
A highly confusing term is "gain".
For example, the sentence "the gain of a system should be doubled", without context, means close to nothing.
It may mean that the ratio of the output voltage of an electric circuit to the input voltage should be doubled.
It may mean that the ratio of the output power of an electric or optical circuit to the input power should be doubled.
It may mean that the gain of the laser medium should be doubled, for example, doubling the population of the upper laser level in a quasi-two level system (assuming negligible absorption of the ground-state).
The term "intensity" is ambiguous when applied to light.
The term can refer to any of irradiance, luminous intensity, radiant intensity, or radiance, depending on the background of the person using the term.
Also, confusions may be related with the use of atomic percent as measure of concentration of a dopant, or resolution of an imaging system, as measure of the size of the smallest detail which still can be resolved at the background of statistical noise.
See also Accuracy and precision and its talk.
The Berry paradox arises as a result of systematic ambiguity in the meaning of terms such as "definable" or "nameable".
Terms of this kind give rise to vicious circle fallacies.
Other terms with this type of ambiguity are: satisfiable, true, false, function, property, class, relation, cardinal, and ordinal.
In mathematics and logic, ambiguity can be considered to be an instance of the logical concept of underdetermination—for example, formula_43 leaves open what the value of "X" is—while its opposite is a self-contradiction, also called inconsistency, paradoxicalness, or oxymoron, or in mathematics an inconsistent system—such as formula_44, which has no solution.
Logical ambiguity and self-contradiction is analogous to visual ambiguity and impossible objects, such as the Necker cube and impossible cube, or many of the drawings of M. C. Escher.
</doc>
<doc id="679" url="https://en.wikipedia.org/wiki?curid=679" title="Animal (disambiguation)">
Animal (disambiguation)

An animal is a multicellular, eukaryotic organism of the kingdom Animalia or Metazoa.
Animal or Animals or The Animal may also refer to:











</doc>
<doc id="680" url="https://en.wikipedia.org/wiki?curid=680" title="Aardvark">
Aardvark

The aardvark ( ; "Orycteropus afer") is a medium-sized, burrowing, nocturnal mammal native to Africa.
It is the only living species of the order Tubulidentata, although other prehistoric species and genera of Tubulidentata are known.
Unlike other insectivores, it has a long pig-like snout, which is used to sniff out food.
It roams over most of the southern two-thirds of the African continent, avoiding areas that are mainly rocky.
A nocturnal feeder, it subsists on ants and termites, which it will dig out of their hills using its sharp claws and powerful legs.
It also digs to create burrows in which to live and rear its young.
It receives a "least concern" rating from the IUCN, although its numbers seem to be decreasing.
The aardvark is sometimes colloquially called "African ant bear", "anteater" (not to be confused with the South American anteater), or the "Cape anteater" after the Cape of Good Hope.
The name "aardvark" () comes from earlier Afrikaans (erdvark) and means "earth pig" or "ground pig" ("aarde": earth/ground, "vark": pig), because of its burrowing habits.
The name "Orycteropus" means burrowing foot, and the name "afer" refers to Africa.
The name of the aardvarks's order, "Tubulidentata," comes from the tubule-style teeth.
The aardvark is not closely related to the pig; rather, it is the sole extant representative of the obscure mammalian order Tubulidentata, in which it is usually considered to form one variable species of the genus "Orycteropus", the sole surviving genus in the family Orycteropodidae.
The aardvark is not closely related to the South American anteater, despite sharing some characteristics and a superficial resemblance.
The similarities are based on convergent evolution.
The closest living relatives of the aardvark are the elephant shrews, tenrecs and golden moles.
Along with the sirenians, hyraxes, elephants, and their extinct relatives, these animals form the superorder Afrotheria.
Studies of the brain have shown the similarities with Condylarthra, and given the clade's status as a wastebasket taxon it may mean some species traditionally classified as "condylarths" are actually stem-aardvarks.
Based on fossils, Bryan Patterson has concluded that early relatives of the aardvark appeared in Africa around the end of the Paleocene.
The ptolemaiidans, a mysterious clade of mammals with uncertain affinities, may actually be stem-aardvarks, either as a sister clade to Tubulidentata or as a grade leading to true tubulidentates.
The first unambiguous tubulidentate was probably "Myorycteropus africanus" from Kenyan Miocene deposits.
The earliest example from the genus "Orycteropus" was "Orycteropus mauritanicus", found in Algeria in deposits from the middle Miocene, with an equally old version found in Kenya.
Fossils from the aardvark have been dated to 5 million years, and have been located throughout Europe and the Near East.
The mysterious Pleistocene "Plesiorycteropus" from Madagascar was originally thought to be a tubulidentate that was descended from ancestors that entered the island during the Eocene.
However, a number of subtle anatomical differences coupled with recent molecular evidence now lead researchers to believe that "Plesiorycteropus" is a relative of golden moles and tenrecs that achieved an aardvark-like appearance and ecological niche through convergent evolution.
The aardvark has seventeen poorly defined subspecies listed:


The 1911 Encyclopædia Britannica also mentions "O. a. capensis" or Cape ant-bear from South Africa.
The aardvark is vaguely pig-like in appearance.
Its body is stout with a prominently arched back and is sparsely covered with coarse hairs.
The limbs are of moderate length, with the rear legs being longer than the forelegs.
The front feet have lost the pollex (or 'thumb'), resulting in four toes, while the rear feet have all five toes.
Each toe bears a large, robust nail which is somewhat flattened and shovel-like, and appears to be intermediate between a claw and a hoof.
Whereas the aardvark is considered digitigrade, it appears at time to be plantigrade.
This confusion happens because when it squats it stands on its soles.
A contributing characteristic to the burrow digging capabilities of aardvarks is an endosteal tissue called compacted coarse cancellous bone (CCCB).
The stress and strain resistance provided by CCCB allows aardvarks to create their burrows, ultimately leading to a favorable environment for plants and a variety of animals.
An aardvark's weight is typically between .
An aardvark's length is usually between , and can reach lengths of when its tail (which can be up to ) is taken into account.
It is tall at the shoulder, and has a girth of about .
It is the largest member of the proposed clade Afroinsectiphilia.
The aardvark is pale yellowish-gray in color and often stained reddish-brown by soil.
The aardvark's coat is thin, and the animal's primary protection is its tough skin.
Its hair is short on its head and tail; however its legs tend to have longer hair.
The hair on the majority of its body is grouped in clusters of 3-4 hairs.
The hair surrounding its nostrils is dense to help filter particulate matter out as it digs.
Its tail is very thick at the base and gradually tapers.
The greatly elongated head is set on a short, thick neck, and the end of the snout bears a disc, which houses the nostrils.
It contains a thin but complete zygomatic arch.
The head of the aardvark contains many unique and different features.
One of the most distinctive characteristics of the Tubulidentata is their teeth.
Instead of having a pulp cavity, each tooth has a cluster of thin, hexagonal, upright, parallel tubes of vasodentin (a modified form of dentine), with individual pulp canals, held together by cementum.
The number of columns is dependent on the size of the tooth, with the largest having about 1,500.
The teeth have no enamel coating and are worn away and regrow continuously.
The aardvark is born with conventional incisors and canines at the front of the jaw, which fall out and are not replaced.
Adult aardvarks have only cheek teeth at the back of the jaw, and have a dental formula of: These remaining teeth are peg-like and rootless and are of unique composition.
The teeth consist of 14 upper and 12 lower jaw molars.
The nasal area of the aardvark is another unique area, as it contains ten nasal conchae, more than any other placental mammal.
The sides of the nostrils are thick with hair.
The tip of the snout is highly mobile and is moved by modified mimetic muscles.
The fleshy dividing tissue between its nostrils probably has sensory functions, but it is uncertain whether they are olfactory or vibratory in nature.
Its nose is made up of more turbinate bones than any other mammal, with between 9 and 11, compared to dogs with 4 to 5.
With a large quantity of turbinate bones, the aardvark has more space for the moist epithelium, which is the location of the olfactory bulb.
The nose contains nine olfactory bulbs, more than any other mammal.
Its keen sense of smell is not just from the quantity of bulbs in the nose but also in the development of the brain, as its olfactory lobe is very developed.
The snout resembles an elongated pig snout.
The mouth is small and tubular, typical of species that feed on ants and termites.
The aardvark has a long, thin, snakelike, protruding tongue (as much as long) and elaborate structures supporting a keen sense of smell.
The ears, which are very effective, are disproportionately long, about long.
The eyes are small for its head, and consist only of rods.
The aardvark's stomach has a muscular pyloric area that acts as a gizzard to grind swallowed food up, thereby rendering chewing unnecessary.
Its cecum is large.
Both sexes emit a strong smelling secretion from an anal gland.
Its salivary glands are highly developed and almost completely ring the neck; their output is what causes the tongue to maintain its tackiness.
The female has two pairs of teats in the inguinal region.
Genetically speaking, the aardvark is a living fossil, as its chromosomes are highly conserved, reflecting much of the early eutherian arrangement before the divergence of the major modern taxa.
Aardvarks are found in sub-Saharan Africa, where suitable habitat (savannas, grasslands, woodlands and bushland) and food (i.e., ants and termites) is available.
They spend the daylight hours in dark underground burrows to avoid the heat of the day.
The only major habitat that they are not present in is swamp forest, as the high water table precludes digging to a sufficient depth.
They also avoid terrain rocky enough to cause problems with digging.
They have been documented as high as in Ethiopia.
They are present throughout sub-Saharan Africa all the way to South Africa with few exceptions.
These exceptions include the coastal areas of Namibia, Ivory Coast, and Ghana.
They are not found in Madagascar.
Aardvarks live for up to 23 years in captivity.
Its keen hearing warns it of predators: lions, leopards, cheetahs, hunting dogs, hyenas, and pythons.
Some humans also hunt aardvarks for meat.
Aardvarks can dig fast or run in zigzag fashion to elude enemies, but if all else fails, they will strike with their claws, tail and shoulders, sometimes flipping onto their backs lying motionless except to lash out with all four feet.
They are capable of causing substantial damage to unprotected areas of an attacker.
They will also dig to escape as they can, when pressed, dig extremely quickly.
The aardvark is nocturnal and is a solitary creature that feeds almost exclusively on ants and termites (myrmecophagy); the only fruit eaten by aardvarks is the aardvark cucumber.
In fact, the cucumber and the aardvark have a symbiotic relationship as they eat the subterranean fruit, then defecate the seeds near their burrows, which then grow rapidly due to the loose soil and fertile nature of the area.
The time spent in the intestine of the aardvark helps the fertility of the seed, and the fruit provides needed moisture for the aardvark.
They avoid eating the African driver ant and red ants.
Due to their stringent diet requirements, they require a large range to survive.
An aardvark emerges from its burrow in the late afternoon or shortly after sunset, and forages over a considerable home range encompassing .
While foraging for food, the aardvark will keep its nose to the ground and its ears pointed forward, which indicates that both smell and hearing are involved in the search for food.
They zig-zag as they forage and will usually not repeat a route for 5–8 days as they appear to allow time for the termite nests to recover before feeding on it again.
During a foraging period, they will stop and dig a "V" shaped trench with their forefeet and then sniff it profusely as a means to explore their location.
When a concentration of ants or termites is detected, the aardvark digs into it with its powerful front legs, keeping its long ears upright to listen for predators, and takes up an astonishing number of insects with its long, sticky tongue—as many as 50,000 in one night have been recorded.
Its claws enable it to dig through the extremely hard crust of a termite or ant mound quickly.
It avoids inhaling the dust by sealing the nostrils.
When successful, the aardvark's long (up to ) tongue licks up the insects; the termites' biting, or the ants' stinging attacks are rendered futile by the tough skin.
After an aardvark visit at a termite mound, other animals will visit to pick up all the leftovers.
Termite mounds alone don't provide enough food for the aardvark, so they look for termites that are on the move.
When these insects move, they can form columns long and these tend to provide easy pickings with little effort exerted by the aardvark.
These columns are more common in areas of livestock or other hoofed animals.
The trampled grass and dung attract termites from Odontotermes, Microtermes, and Pseudacanthotermes genera.
On a nightly basis they tend to be more active during the first portion of the night time (20:00-00:00); however, they don't seem to prefer bright or dark nights over the other.
During adverse weather or if disturbed they will retreat to their burrow systems.
They cover between per night; however, some studies have shown that they may traverse as far as in a night.
The aardvark is a rather quiet animal.
However, it does make soft grunting sounds as it forages and loud grunts as it makes for its tunnel entrance.
It makes a bleating sound if frightened.
When it is threatened it will make for one of its burrows.
If one is not close it will dig a new one rapidly.
This new one will be short and require the aardvark to back out when the coast is clear.
The aardvark is known to be a good swimmer and has been witnessed successfully swimming in strong currents.
It can dig a yard of tunnel in about five minutes, but otherwise moves fairly slowly.
When leaving the burrow at night, they pause at the entrance for about ten minutes, sniffing and listening.
After this period of watchfulness, it will bound out and within seconds it will be away.
It will then pause, prick its ears, twisting its head to listen, then jump and move off to start foraging.
Aside from digging out ants and termites, the aardvark also excavates burrows in which to live; of which they generally fall into three categories: burrows made while foraging, refuge and resting location, and permanent homes.
Temporary sites are scattered around the home range and are used as refuges, while the main burrow is also used for breeding.
Main burrows can be deep and extensive, have several entrances and can be as long as .
These burrows can be large enough for a man to enter.
The aardvark changes the layout of its home burrow regularly, and periodically moves on and makes a new one.
The old burrows are an important part of the African wildlife scene.
As they are vacated, then they are inhabited by smaller animals like the African wild dog, ant-eating chat, "Nycteris thebaica" and warthogs.
Other animals that use them are hares, mongooses, hyenas, owls, pythons, and lizards.
Without these refuges many animals would die during wildfire season.
Only mothers and young share burrows; however, the aardvark is known to live in small family groups or as a solitary creature.
If attacked in the tunnel, it will escape by digging out of the tunnel thereby placing the fresh fill between it and its predator, or if it decides to fight it will roll onto its back, and attack with its claws.
The aardvark has been known to sleep in a recently excavated ant nest, which also serves as protection from its predators.
Aardvarks pair only during the breeding season; after a gestation period of seven months, one cub weighing around is born during May–July.
When born, the young has flaccid ears and many wrinkles.
When nursing, it will nurse off each teat in succession.
After two weeks, the folds of skin disappear and after three, the ears can be held upright.
After 5–6 weeks, body hair starts growing.
It is able to leave the burrow to accompany its mother after only two weeks and eats termites at 9 weeks, and is weaned between three months and 16 weeks.
At six months of age, it is able to dig its own burrows, but it will often remain with the mother until the next mating season, and is sexually mature from approximately two years of age.
Aardvarks were thought to have declining numbers, however, this is possibly due to the fact that they are not readily seen.
There are no definitive counts because of their nocturnal and secretive habits; however, their numbers seem to be stable overall.
They are not considered common anywhere in Africa, but due to their large range, they maintain sufficient numbers.
There may be a slight decrease in numbers in eastern, northern, and western Africa.
Southern African numbers are not decreasing.
It receives an official designation from the IUCN as least concern.
However, they are a species in a precarious situation, as they are so dependent on such specific food; therefore if a problem arises with the abundance of termites, the species as a whole would be affected drastically.
Aardvarks handle captivity well.
The first zoo to have one was London Zoo in 1869, which had an animal from South Africa.
In African folklore, the aardvark is much admired because of its diligent quest for food and its fearless response to soldier ants.
Hausa magicians make a charm from the heart, skin, forehead, and nails of the aardvark, which they then proceed to pound together with the root of a certain tree.
Wrapped in a piece of skin and worn on the chest, the charm is said to give the owner the ability to pass through walls or roofs at night.
The charm is said to be used by burglars and those seeking to visit young girls without their parents' permission.
Also, some tribes, such as the Margbetu, Ayanda, and Logo, will use aardvark teeth to make bracelets, which are regarded as good luck charms.
The meat, which has a resemblance to pork, is eaten in certain cultures.
The Egyptian god Set is usually depicted with the head of an unidentified animal, whose similarity to an aardvark has been noted in scholarship.
The titular character of "Arthur", an animated television series for children based on a book series and produced by WGBH, shown in more than 180 countries, is an aardvark.
Otis the Aardvark was a puppet character used on Children's BBC programming.
An aardvark features as the antagonist in the cartoon "The Ant and the Aardvark" as well as in the Canadian animated series "The Raccoons".
In the military, the Air Force supersonic fighter-bomber F-111/FB-111 was nicknamed the Aardvark because of its long nose resembling the animal.
It also had similarities with its nocturnal missions flown at a very low level employing ordnance that could penetrate deep into the ground.
In the US Navy, the squadron VF-114 was nicknamed the Aardvarks, flying F-4s and then F-14s.
The squadron mascot was adapted from the animal in the comic strip "B.C.
", which the F-4 was said to resemble.
Cerebus the Aardvark is a 300-issue comic book series by Dave Sim.
</doc>
<doc id="681" url="https://en.wikipedia.org/wiki?curid=681" title="Aardwolf">
Aardwolf

The aardwolf ("Proteles cristata") is a small, insectivorous mammal, native to East and Southern Africa.
Its name means "earth-wolf" in Afrikaans and Dutch.
It is also called "maanhaar-jackal" (Afrikaans for "mane-jackal") or "civet hyena", based on its habit of secreting substances from its anal gland, a characteristic shared with the African civet.
The aardwolf is in the same family as the hyena.
Unlike many of its relatives in the order Carnivora, the aardwolf does not hunt large animals.
It eats insects and their larvae, mainly termites; one aardwolf can lap up as many as 250,000 termites during a single night using its long, sticky tongue.
The aardwolf lives in the shrublands of eastern and southern Africa – open lands covered with stunted trees and shrubs.
It is nocturnal, resting in burrows during the day and emerging at night to seek food.
The aardwolf is generally classified with the hyena family Hyaenidae, though it was formerly placed in its own family Protelidae.
Early on, scientists felt that it was merely mimicking the striped hyena, which subsequently led to the creation of Protelidae.
Recent studies have suggested that the aardwolf probably broke away from the rest of the hyena family early on; how early is still unclear, as the fossil record and genetic studies disagree by 10 million years.
The aardwolf is the only surviving species in the subfamily Protelinae.
There is disagreement as to whether the species is monotypic.
or can be divided into subspecies "P. c. cristatus" of Southern Africa and "P. c. septentrionalis" of East Africa.
The generic name "proteles" comes from two words both of Greek origin, "protos" and "teleos" which combined means "complete in front" based on the fact that they have five toes on their front feet and four on the rear.
The specific name, "cristatus", comes from Latin and means "provided with a comb", relating to their mane.
The aardwolf resembles a very thin striped hyena, but with a more-slender muzzle, black vertical stripes on a coat of yellowish fur, and a long, distinct mane down the midline of the neck and back.
It also has one or two diagonal stripes down the fore- and hind-quarters, along with several stripes on its legs.
The mane is raised during confrontations to make the aardwolf appear larger.
It is missing the throat spot that others in the family have.
Its lower leg (from the knee down) is all black, and its tail is bushy with a black tip.
The aardwolf is about long, excluding its bushy tail, which is about long, and stands about tall at the shoulders.
An adult aardwolf weighs approximately , sometimes reaching .
The aardwolves in the south of the continent tend to be smaller (about )than the eastern version (around ).
The front feet have five toes each, unlike the four-toed hyena.
The teeth and skull are similar to those of other hyenas, though smaller, and its cheek teeth are specialised for eating insects.
It does still have canines, but, unlike other hyenas, these teeth are used primarily for fighting and defense.
Its ears, which are large, are very similar to those of the striped hyena.
As an aardwolf ages, it will normally lose some of its teeth, though this has little impact on its feeding habits due to the softness of the insects that it eats.
Aardwolves live in open, dry plains and bushland, avoiding mountainous areas.
Due to their specific food requirements, they are only found in regions where termites of the family Hodotermitidae occur.
Termites of this family depend on dead and withered grass and are most populous in heavily grazed grasslands and savannahs, including farmland.
For most of the year, aardwolves spend time in shared territories consisting of up to a dozen dens, which are occupied for six weeks at a time.
There are two distinct populations: one in Southern Africa, and another in East and Northeast Africa.
The species does not occur in the intermediary miombo forests.
An adult pair, along with their most-recent offspring, occupies a territory of .
Aardwolves are shy and nocturnal, sleeping in underground burrows by day.
They will, on occasion during the winter, become diurnal feeders.
This happens during the coldest periods as they then stay in at night to conserve heat.
They have often been mistaken for solitary animals.
In fact, they live as monogamous pairs with their young.
If their territory is infringed upon, they will chase the intruder up to or to the border.
If the intruder is caught, which rarely happens, a fight will occur, which is accompanied by soft clucking, hoarse barking, and a type of roar.
The majority of incursions occur during mating season, when they can occur once or twice per week.
When food is scarce, the stringent territorial system may be abandoned and as many as three pairs may occupy a "single territory".
The territory is marked by both sexes, as they both have developed anal glands from which they extrude a black substance that is smeared on rocks or grass stalks in -long streaks.
Aardwolves also have scent glands on the forefoot and penile pad.
They often mark near termite mounds within their territory every 20 minutes or so.
If they are patrolling their territorial boundaries, the marking frequency increases drastically, to once every .
At this rate, an individual may mark 60 marks per hour, and upwards of 200 per night.
An aardwolf pair may have up to 10 dens, and numerous feces middens, within their territory.
When they deposit excreta at their middens, they dig a small hole and cover it with sand.
Their dens are usually abandoned aardvark, springhare, or porcupine dens, or on occasion they are crevices in rocks.
They will also dig their own dens, or enlarge dens started by springhares.
They typically will only use one or two dens at a time, rotating through all of their dens every six months.
During the summer, they may rest outside their den during the night, and sleep underground during the heat of the day.
Aardwolves are not fast runners nor are they particularly adept at fighting off predators.
Therefore, when threatened, the aardwolf may attempt to mislead its foe by doubling back on its tracks.
If confronted, it may raise its mane in an attempt to appear more menacing.
It also emits a foul-smelling liquid from its anal glands.
The aardwolf feeds primarily on termites and more specifically on "Trinervitermes".
This genus of termites has different species throughout the aardwolf's range.
In East Africa, they eat "Trinervitermes bettonianus", and in central Africa, they eat "Trinervitermes rhodesiensis", and finally in southern Africa, they eat "T. trinervoides".
Their technique consists of licking them off the ground as opposed to the aardvark, which digs into the mound.
They locate their food by sound and also from the scent secreted by the soldier termites.
An aardwolf may consume up to 250,000 termites per night using its sticky, long tongue.
They do not destroy the termite mound or consume the entire colony, thus ensuring that the termites can rebuild and provide a continuous supply of food.
They often memorize the location of such nests and return to them every few months.
During certain seasonal events, such as the onset of the rainy season and the cold of midwinter, the primary termites become scarce, so the need for other foods becomes pronounced.
During these times, the southern aardwolf will seek out "Hodotermes mossambicus", a type of harvester termite active in the afternoon, which explains some of their diurnal behavior in the winter.
The eastern aardwolf, during the rainy season, subsists on termites from the genera "Odontotermes" and "Macrotermes".
They are also known to feed on other insects, larvae, eggs, and, some sources say, occasionally small mammals and birds, but these constitute a very small percentage of their total diet.
Unlike other hyenas, aardwolves do not scavenge or kill larger animals.
Contrary to popular myths, aardwolves do not eat carrion, and if they are seen eating while hunched over a dead carcass, they are actually eating larvae and beetles.
Also, contrary to some sources, they do not like meat, unless it is finely ground or cooked for them.
The adult aardwolf was formerly assumed to forage in small groups, but more recent research has shown that they are primarily solitary foragers, necessary because of the scarcity of their insect prey.
Their primary source, "Trinervitermes", forages in small but dense patches of .
While foraging, the aardwolf can cover about per hour, which translates to per summer night and per winter night.
The breeding season varies depending on location, but normally takes place during autumn or spring.
In South Africa, breeding occurs in early July.
During the breeding season, unpaired male aardwolves search their own territory, as well as others, for a female to mate with.
Dominant males also mate opportunistically with the females of less dominant neighboring aardwolves, which can result in conflict between rival males.
Dominant males even go a step further and as the breeding season approaches, they make increasingly greater and greater incursions onto weaker males' territories.
As the female comes into oestrus, they add pasting to their tricks inside of the other territories, sometimes doing so more in rivals' territories than their own.
Females will also, when given the opportunity, mate with the dominant male, which increases the chances of the dominant male guarding "his" cubs with her.
Copulation lasts between 1 and 4.5 hours.
Gestation lasts between 89 and 92 days, producing two to five cubs (most often two or three) during the rainy season (November–December), when termites are more active.
They are born with their eyes open, but initially are helpless, and weigh around .
The first six to eight weeks are spent in the den with their parents.
The male may spend up to six hours a night watching over the cubs while the mother is out looking for food.
After three months, they begin supervised foraging, and by four months are normally independent, though they often share a den with their mother until the next breeding season.
By the time the next set of cubs is born, the older cubs have moved on.
Aardwolves generally achieve sexual maturity at one and a half to two years of age.
The aardwolf has not seen decreasing numbers and they are relatively widespread throughout eastern Africa.
They are not common throughout their range, as they maintain a density of no more than 1 per square kilometer, if the food is good.
Because of these factors, the IUCN has rated the aardwolf as least concern.
In some areas, they are persecuted by man because of the mistaken belief that they prey on livestock; however, they are actually beneficial to the farmers because they eat termites that are detrimental.
In other areas, the farmers have recognized this, but they are still killed, on occasion, for their fur.
Dogs and insecticides are also common killers of the aardwolf.
Aardwolfs are common sights at zoos.
Frankfurt Zoo in Germany was home to the oldest recorded aardwolf in captivity at 18 years and 11 months.
</doc>
<doc id="682" url="https://en.wikipedia.org/wiki?curid=682" title="Adobe">
Adobe

Adobe (, ; ) (")" is a building material made from earth and organic materials.
Adobe is Spanish for mudbrick, but in some English-speaking regions of Spanish heritage, the term is used to refer to any kind of earth construction.
Most adobe buildings are similar in appearance to cob and rammed earth buildings.
Adobe is among the earliest building materials, and is used throughout the world.
Adobe bricks are rectangular prisms small enough that they can quickly air dry individually without cracking.
They can be subsequently assembled, with the application of adobe mud to bond the individual bricks into a structure.
There is no standard size, with substantial variations over the years and in different regions.
In some areas a popular size measured 8" inches x 4" inches x 12" inches weighing about 25 pounds; in other contexts the size is 10" x 4" x 14" weighing about 35 pounds.
The maximum sizes can reach up to 100 pound; above this weight it becomes difficult to move the pieces, and it is preferred to ram the mud "in situ", resulting in a different typology known as rammed earth.
In dry climates, adobe structures are extremely durable, and account for some of the oldest existing buildings in the world.
Adobe buildings offer significant advantages due to their greater thermal mass, but they are known to be particularly susceptible to earthquake damage if they are not somehow reinforced.
Cases where adobe structures were widely damaged during earthquakes include the 1976 Guatemala earthquake, the 2003 Bam earthquake, and the 2010 Chile earthquake.
Buildings made of sun-dried earth are common throughout the world (Middle East, Western Asia, North Africa, West Africa, South America, southwestern North America, Spain, and Eastern Europe.)
Adobe had been in use by indigenous peoples of the Americas in the Southwestern United States, Mesoamerica, and the Andes for several thousand years.
Puebloan peoples built their adobe structures with handsful or basketsful of adobe, until the Spanish introduced them to making bricks.
Adobe bricks were used in Spain from the Late Bronze and Iron Ages (eighth century BCE onwards).
Its wide use can be attributed to its simplicity of design and manufacture, and economics.
A distinction is sometimes made between the smaller "adobes", which are about the size of ordinary baked bricks, and the larger "adobines", some of which may be one to two yards (1–2 m) long.
The word "adobe" has existed for around 4000 years with relatively little change in either pronunciation or meaning.
The word can be traced from the Middle Egyptian (c.
2000 BC) word "ɟbt" "mudbrick."
Middle Egyptian evolved into Late Egyptian, Demotic or "pre-Coptic", and finally to Coptic (c.
600 BC), where it appeared as τωωβε .
This was adopted into Arabic as "aṭ-ṭawbu" or "aṭ-ṭūbu", with the definite article "al-" attached.
"tuba", This was assimilated into the Old Spanish language as "adobe" , probably via Mozarabic.
English borrowed the word from Spanish in the early 18th century, still referring to mudbrick construction.
In more modern English usage, the term "adobe" has come to include a style of architecture popular in the desert climates of North America, especially in New Mexico, regardless of the construction method.
An adobe brick is a composite material made of earth mixed with water and an organic material such as straw or dung.
The soil composition typically contains sand, silt and clay.
Straw is useful in binding the brick together and allowing the brick to dry evenly, thereby preventing cracking due to uneven shrinkage rates through the brick.
Dung offers the same advantage.
The most desirable soil texture for producing the mud of adobe is 15% clay, 10–30% silt, and 55–75% fine sand.
Another source quotes 15–25% clay and the remainder sand and coarser particles up to cobbles , with no deleterious effect.
Modern adobe is stabilized with either emulsified asphalt or Portland cement up to 10% by weight.
No more than half the clay content should be expansive clays, with the remainder non-expansive illite or kaolinite.
Too much expansive clay results in uneven drying through the brick, resulting in cracking, while too much kaolinite will make a weak brick.
Typically the soils of the Southwest United States, where such construction has been widely used, are an adequate composition.
Adobe walls are load bearing, i.e. they carry their own weight into the foundation rather than by another structure, hence the adobe must have sufficient compressive strength.
In the United States, most building codes call for a minimum compressive strength of 300 lbf/in (2.07 newton/mm) for the adobe block.
Adobe construction should be designed so as to avoid lateral structural loads that would cause bending loads.
The building codes require the building sustain a 1 g lateral acceleration earthquake load.
Such an acceleration will cause lateral loads on the walls, resulting in shear and bending and inducing tensile stresses.
To withstand such loads, the codes typically call for a tensile modulus of rupture strength of at least 50 lbf/in (0.345 newton/mm) for the finished block.
In addition to being an inexpensive material with a small resource cost, adobe can serve as a significant heat reservoir due to the thermal properties inherent in the massive walls typical in adobe construction.
In climates typified by hot days and cool nights, the high thermal mass of adobe mediates the high and low temperatures of the day, moderating the temperature of the living space.
The massive walls require a large and relatively long input of heat from the sun (radiation) and from the surrounding air (convection) before they warm through to the interior.
After the sun sets and the temperature drops, the warm wall will continue to transfer heat to the interior for several hours due to the time-lag effect.
Thus, a well-planned adobe wall of the appropriate thickness is very effective at controlling inside temperature through the wide daily fluctuations typical of desert climates, a factor which has contributed to its longevity as a building material.
Thermodynamic material properties are sparsely quoted.
The thermal resistance of adobe is quoted as having an R-value of R = 0.41 h ft °F/(Btu in) and a conductivity of 0.57 W/(m K) quoted from another source.
A third source provides the following properties: conductivity=0.30 Btu/(h ft °F); heat capacity=0.24 Btu/(lb °F); density=106 lb/ft (1700 kg/m).
To determine the total R-value of a wall for example, multiply R by the thickness of the wall.
From knowledge of the adobe density, heat capacity and a diffusivity value, the conductivity is found to be k = 0.20 Btu/(h ft °F) or 0.35 W/(m K).
The heat capacity is commonly quoted as c = 0.20 Btu/(lb F) or 840 joules/(kg K).
The density is 95 lb/ft or 1520 kg/m.
The thermal diffusivity is calculated to be 0.0105 ft/h or 2.72x10 m/s.
Poured and puddled adobe (puddled clay, piled earth), today called "cob", is made by placing soft adobe in layers, rather than by making individual dried bricks or using a form.
"Puddle" is a general term for a clay or clay and sand-based material worked into a dense, plastic state.
These are the oldest methods of building with adobe in the Americas until holes in the ground were used as forms, and later wooden forms used to make individual bricks were introduced by the Spanish.
Bricks made from adobe are usually made by pressing the mud mixture into an open timber frame.
In North America, the brick is typically about in size.
The mixture is molded into the frame, which is removed after initial setting.
After drying for a few hours, the bricks are turned on edge to finish drying.
Slow drying in shade reduces cracking.
The same mixture, without straw, is used to make mortar and often plaster on interior and exterior walls.
Some cultures used lime-based cement for the plaster to protect against rain damage.
Depending on the form into which the mixture is pressed, adobe can encompass nearly any shape or size, provided drying is even and the mixture includes reinforcement for larger bricks.
Reinforcement can include manure, straw, cement, rebar or wooden posts.
Experience has shown straw, cement, or manure added to a standard adobe mixture can all produce a stronger, more crack-resistant brick.
A test is done on the soil content first.
To do so, a sample of the soil is mixed into a clear container with some water, creating an almost completely saturated liquid.
The container is shaken vigorously for one minute.
It is then allowed to settle for a day until the soil has settled into layers.
Heavier particles settle out first, sand above, silt above that and very fine clay and organic matter will stay in suspension for days.
After the water has cleared, percentages of the various particles can be determined.
Fifty to 60 percent sand and 35 to 40 percent clay will yield strong bricks.
The Cooperative State Research, Education, and Extension Service at New Mexico State University recommends a mix of not more than 1/3 clay, not less than 1/2 sand, and never more than 1/3 silt.
The ground supporting an adobe structure should be compressed, as the weight of adobe wall is significant and foundation settling may cause cracking of the wall.
Footing depth is to below the ground frost level.
The footing and stem wall are commonly 24 and 14 inches thick, respectively.
Modern construction codes call for the use of reinforcing steel in the footing and stem wall.
Adobe bricks are laid by course.
Adobe walls usually never rise above two stories as they are load bearing and adobe has low structural strength.
When creating window and door openings, a lintel is placed on top of the opening to support the bricks above.
Atop the last courses of brick, bond beams made of heavy wood beams or modern reinforced concrete are laid to provide a horizontal bearing plate for the roof beams and to redistribute lateral earthquake loads to shear walls more able to carry the forces.
To protect the interior and exterior adobe walls, finishes such as mud plaster, whitewash or stucco can be applied.
These protect the adobe wall from water damage, but need to be reapplied periodically.
Alternatively, the walls can be finished with other nontraditional plasters that provide longer protection.
Bricks made with stabilized adobe generally do not need protection of plasters.
The traditional adobe roof has been constructed using a mixture of soil/clay, water, sand and organic materials.
The mixture was then formed and pressed into wood forms, producing rows of dried earth bricks that would then be laid across a support structure of wood and plastered into place with more adobe.
Depending on the materials available, a roof may be assembled using wood or metal beams to create a framework to begin layering adobe bricks.
Depending on the thickness of the adobe bricks, the framework has been preformed using a steel framing and a layering of a metal fencing or wiring over the framework to allow an even load as masses of adobe are spread across the metal fencing like cob and allowed to air dry accordingly.
This method was demonstrated with an adobe blend heavily impregnated with cement to allow even drying and prevent cracking.
The more traditional flat adobe roofs are functional only in dry climates that are not exposed to snow loads.
The heaviest wooden beams, called vigas, lie atop the wall.
Across the vigas lie smaller members called latillas and upon those brush is then laid.
Finally, the adobe layer is applied.
To construct a flat adobe roof, beams of wood were laid to span the building, the ends of which were attached to the tops of the walls.
Once the vigas, latillas and brush are laid, adobe bricks are placed.
An adobe roof is often laid with bricks slightly larger in width to ensure a greater expanse is covered when placing the bricks onto the roof.
Following each individual brick should be a layer of adobe mortar, recommended to be at least thick to make certain there is ample strength between the brick's edges and also to provide a relative moisture barrier during rain.
Roof design evolved around 1850 in the American Southwest.
Three inches of adobe mud was applied on top of the latillas, then 18 inches of dry adobe dirt applied to the roof.
The dirt was contoured into a low slope to a downspout aka a 'canal'.
When moisture was applied to the roof the clay particles expanded to create a waterproof membrane.
Once a year it was necessary to pull the weeds from the roof and reslope the dirt as needed.
Depending on the materials, adobe roofs can be inherently fire-proof.
The construction of a chimney can greatly influence the construction of the roof supports, creating an extra need for care in choosing the materials.
The builders can make an adobe chimney by stacking simple adobe bricks in a similar fashion as the surrounding walls.
The largest structure ever made from adobe is the Arg-é Bam built by the Achaemenid Empire.
Other large adobe structures are the Huaca del Sol in Peru, with 100 million signed bricks and the "ciudellas" of Chan Chan and Tambo Colorado, both in Peru.
</doc>
<doc id="683" url="https://en.wikipedia.org/wiki?curid=683" title="Adventure">
Adventure

An adventure is an exciting experience that is typically a bold, sometimes risky, undertaking.
Adventures may be activities with some potential for physical danger such as traveling, exploring, skydiving, mountain climbing, scuba diving, river rafting or participating in extreme sports.
Adventurous experiences create psychological arousal, which can be interpreted as negative (e.g.
fear) or positive (e.g.
flow).
For some people, adventure becomes a major pursuit in and of itself.
According to adventurer André Malraux, in his "La Condition Humaine" (1933), "If a man is not ready to risk his life, where is his dignity?".
Similarly, Helen Keller stated that "Life is either a daring adventure or nothing."
Outdoor adventurous activities are typically undertaken for the purposes of recreation or excitement: examples are adventure racing and adventure tourism.
Adventurous activities can also lead to gains in knowledge, such as those undertaken by explorers and pioneers – the British adventurer Jason Lewis, for example, uses adventures to draw global sustainability lessons from living within finite environmental constraints on expeditions to share with schoolchildren.
Adventure education intentionally uses challenging experiences for learning.
Some of the oldest and most widespread stories in the world are stories of adventure such as Homer's "The Odyssey".
The knight errant was the form the "adventure seeker" character took in the late Middle Ages.
The adventure novel exhibits these "protagonist on adventurous journey" characteristics as do many popular feature films, such as "Star Wars" and "Raiders of the Lost Ark".
Adventure books may have the theme of the hero or main character going to face the wilderness or Mother Nature.
Examples include books such as Hatchet or My Side of the Mountain.
These books are less about "questing", such as in mythology or other adventure novels, but more about surviving on their own, living off the land, gaining new experiences, and becoming closer to the natural world.
Many adventures are based on the idea of a quest: the hero goes off in pursuit of a reward, whether it be a skill, prize, or perhaps the safety of a person.
On the way, the hero must overcome various obstacles.
Mythologist Joseph Campbell discussed his notion of the monomyth in his book, "The Hero with a Thousand Faces".
Campbell proposed that the heroic mythological stories from culture to culture followed a similar underlying pattern, starting with the "call to adventure", followed by a hazardous journey, and eventual triumph.
Many video games are adventure games.
From ancient times, travelers and explorers have written about their adventures.
Journals which became best-sellers in their day were written, such as Marco Polo's journal "The Travels of Marco Polo" or Mark Twain's "Roughing It".
Others were personal journals, only later published, such as the journals of Lewis and Clark or Captain James Cook's journals.
There are also books written by those not directly a part of the adventure in question, such as The Right Stuff by Tom Wolfe, or books written by those participating in the adventure but in a format other than that of a journal, such as Conquistadors of the Useless by Lionel Terray.
Documentaries often use the theme of adventure as well.
There are many sports classified as adventure sports, due to their inherent danger and excitement.
Some of these include mountain climbing, skydiving, or other extreme sports.
</doc>
<doc id="689" url="https://en.wikipedia.org/wiki?curid=689" title="Asia">
Asia

Asia () is Earth's largest and most populous continent, located primarily in the Eastern and Northern Hemispheres.
It shares the continental landmass of Eurasia with the continent of Europe and the continental landmass of Afro-Eurasia with both Europe and Africa.
Asia covers an area of , about 30% of Earth's total land area and 8.7% of the Earth's total surface area.
The continent, which has long been home to the majority of the human population, was the site of many of the first civilizations.
Asia is notable for not only its overall large size and population, but also dense and large settlements, as well as vast barely populated regions.
Its 4.5 billion people () constitute roughly 60% of the world's population.
In general terms, Asia is bounded on the east by the Pacific Ocean, on the south by the Indian Ocean, and on the north by the Arctic Ocean.
The border of Asia with Europe is a historical and cultural construct, as there is no clear physical and geographical separation between them.
It is somewhat arbitrary and has moved since its first conception in classical antiquity.
The division of Eurasia into two continents reflects East-West cultural, linguistic, and ethnic differences, some of which vary on a spectrum rather than with a sharp dividing line.
The most commonly accepted boundaries place Asia to the east of the Suez Canal separating it from Africa; and to the east of the Turkish Straits, the Ural Mountains and Ural River, and to the south of the Caucasus Mountains and the Caspian and Black Seas, separating it from Europe.
China and India alternated in being the largest economies in the world from 1 to 1800 CE.
China was a major economic power and attracted many to the east, and for many the legendary wealth and prosperity of the ancient culture of India personified Asia, attracting European commerce, exploration and colonialism.
The accidental discovery of a trans-Atlantic route from Europe to America by Columbus while in search for a route to India demonstrates this deep fascination.
The Silk Road became the main East-West trading route in the Asian hinterlands while the Straits of Malacca stood as a major sea route.
Asia has exhibited economic dynamism (particularly East Asia) as well as robust population growth during the 20th century, but overall population growth has since fallen.
Asia was the birthplace of most of the world's mainstream religions including Christianity, Islam, Judaism, Hinduism, Buddhism, Confucianism, Taoism, Jainism, Sikhism, Zoroastrianism, as well as many other religions.
Given its size and diversity, the concept of Asia—a name dating back to classical antiquity—may actually have more to do with human geography than physical geography.
Asia varies greatly across and within its regions with regard to ethnic groups, cultures, environments, economics, historical ties and government systems.
It also has a mix of many different climates ranging from the equatorial south via the hot desert in the Middle East, temperate areas in the east and the continental centre to vast subarctic and polar areas in Siberia.
The boundary between Asia and Africa is the Red Sea, the Gulf of Suez, and the Suez Canal.
This makes Egypt a transcontinental country, with the Sinai peninsula in Asia and the remainder of the country in Africa.
The border between Asia and Europe was historically defined by European academics.
The Don River became unsatisfactory to northern Europeans when Peter the Great, king of the Tsardom of Russia, defeating rival claims of Sweden and the Ottoman Empire to the eastern lands, and armed resistance by the tribes of Siberia, synthesized a new Russian Empire extending to the Ural Mountains and beyond, founded in 1721.
The major geographical theorist of the empire was actually a former Swedish prisoner-of-war, taken at the Battle of Poltava in 1709 and assigned to Tobolsk, where he associated with Peter's Siberian official, Vasily Tatishchev, and was allowed freedom to conduct geographical and anthropological studies in preparation for a future book.
In Sweden, five years after Peter's death, in 1730 Philip Johan von Strahlenberg published a new atlas proposing the Urals as the border of Asia.
The Russians were enthusiastic about the concept, which allowed them to keep their European identity in geography.
Tatishchev announced that he had proposed the idea to von Strahlenberg.
The latter had suggested the Emba River as the lower boundary.
Over the next century various proposals were made until the Ural River prevailed in the mid-19th century.
The border had been moved perforce from the Black Sea to the Caspian Sea into which the Ural River projects.
The border between the Black Sea and the Caspian is usually placed along the crest of the Caucasus Mountains, although it is sometimes placed further north.
The border between Asia and the region of Oceania is usually placed somewhere in the Malay Archipelago.
The Maluku Islands in Indonesia are often considered to lie on the border of southeast Asia, with New Guinea, to the east of the islands, being wholly part of Oceania.
The terms Southeast Asia and Oceania, devised in the 19th century, have had several vastly different geographic meanings since their inception.
The chief factor in determining which islands of the Malay Archipelago are Asian has been the location of the colonial possessions of the various empires there (not all European).
Lewis and Wigen assert, "The narrowing of 'Southeast Asia' to its present boundaries was thus a gradual process."
Geographical Asia is a cultural artifact of European conceptions of the world, beginning with the Ancient Greeks, being imposed onto other cultures, an imprecise concept causing endemic contention about what it means.
Asia is larger and more culturally diverse than Europe.
It does not exactly correspond to the cultural borders of its various types of constituents.
From the time of Herodotus a minority of geographers have rejected the three-continent system (Europe, Africa, Asia) on the grounds that there is no substantial physical separation between them.
For example, Sir Barry Cunliffe, the emeritus professor of European archeology at Oxford, argues that Europe has been geographically and culturally merely "the western excrescence of the continent of Asia".
Geographically, Asia is the major eastern constituent of the continent of Eurasia with Europe being a northwestern peninsula of the landmass.
Asia, Europe and Africa make up a single continuous landmass – Afro-Eurasia (except for the Suez Canal) – and share a common continental shelf.
Almost all of Europe and the better part of Asia sit atop the Eurasian Plate, adjoined on the south by the Arabian and Indian Plate and with the easternmost part of Siberia (east of the Chersky Range) on the North American Plate.
The idea of a place called "Asia" was originally a concept of Greek civilization, though this might not correspond to the entire continent currently known by that name.
The English word comes from Latin literature, where it has the same form, "Asia".
Whether "Asia" in other languages comes from Latin of the Roman Empire is much less certain, and the ultimate source of the Latin word is uncertain, though several theories have been published.
One of the first classical writers to use Asia as a name of the whole continent was Pliny.
This metonymical change in meaning is common and can be observed in some other geographical names, such as Skandinavia (from Scania).
Before Greek poetry, the Aegean Sea area was in a Greek Dark Age, at the beginning of which syllabic writing was lost and alphabetic writing had not begun.
Prior to then in the Bronze Age the records of the Assyrian Empire, the Hittite Empire and the various Mycenaean states of Greece mention a region undoubtedly Asia, certainly in Anatolia, including if not identical to Lydia.
These records are administrative and do not include poetry.
The Mycenaean states were destroyed about 1200 BCE by unknown agents although one school of thought assigns the Dorian invasion to this time.
The burning of the palaces baked clay diurnal administrative records written in a Greek syllabic script called Linear B, deciphered by a number of interested parties, most notably by a young World War II cryptographer, Michael Ventris, subsequently assisted by the scholar, John Chadwick.
A major cache discovered by Carl Blegen at the site of ancient Pylos included hundreds of male and female names formed by different methods.
Some of these are of women held in servitude (as study of the society implied by the content reveals).
They were used in trades, such as cloth-making, and usually came with children.
The epithet "lawiaiai", "captives", associated with some of them identifies their origin.
Some are ethnic names.
One in particular, aswiai, identifies "women of Asia".
Perhaps they were captured in Asia, but some others, Milatiai, appear to have been of Miletus, a Greek colony, which would not have been raided for slaves by Greeks.
Chadwick suggests that the names record the locations where these foreign women were purchased.
The name is also in the singular, Aswia, which refers both to the name of a country and to a female of it.
There is a masculine form, aswios.
This Aswia appears to have been a remnant of a region known to the Hittites as Assuwa, centered on Lydia, or "Roman Asia".
This name, "Assuwa", has been suggested as the origin for the name of the continent "Asia".
The Assuwa league was a confederation of states in western Anatolia, defeated by the Hittites under Tudhaliya I around 1400 BCE.
Alternatively, the etymology of the term may be from the Akkadian word "(w)aṣû(m)", which means 'to go outside' or 'to ascend', referring to the direction of the sun at sunrise in the Middle East and also likely connected with the Phoenician word "asa" meaning east.
This may be contrasted to a similar etymology proposed for "Europe", as being from Akkadian "erēbu(m)" 'to enter' or 'set' (of the sun).
T.R.
Reid supports this alternative etymology, noting that the ancient Greek name must have derived from "asu", meaning 'east' in Assyrian ("ereb" for "Europe" meaning 'west').
The ideas of "Occidental" (form Latin "Occidens" 'setting') and "Oriental" (from Latin "Oriens" for 'rising') are also European invention, synonymous with "Western" and "Eastern".
Reid further emphasizes that it explains the Western point of view of placing all the peoples and cultures of Asia into a single classification, almost as if there were a need for setting the distinction between Western and Eastern civilizations on the Eurasian continent.
Ogura Kazuo and Tenshin Okakura are two outspoken Japanese figures on the subject.
Latin Asia and Greek Ἀσία appear to be the same word.
Roman authors translated Ἀσία as Asia.
The Romans named a province Asia, located in western Anatolia (in modern-day Turkey).
There was an Asia Minor and an Asia Major located in modern-day Iraq.
As the earliest evidence of the name is Greek, it is likely circumstantially that Asia came from Ἀσία, but ancient transitions, due to the lack of literary contexts, are difficult to catch in the act.
The most likely vehicles were the ancient geographers and historians, such as Herodotus, who were all Greek.
Ancient Greek certainly evidences early and rich uses of the name.
The first continental use of Asia is attributed to Herodotus (about 440 BCE), not because he innovated it, but because his "Histories" are the earliest surviving prose to describe it in any detail.
He defines it carefully, mentioning the previous geographers whom he had read, but whose works are now missing.
By it he means Anatolia and the Persian Empire, in contrast to Greece and Egypt.
Herodotus comments that he is puzzled as to why three women's names were "given to a tract which is in reality one" (Europa, Asia, and Libya, referring to Africa), stating that most Greeks assumed that Asia was named after the wife of Prometheus (i.e. Hesione), but that the Lydians say it was named after Asies, son of Cotys, who passed the name on to a tribe at Sardis.
In Greek mythology, "Asia" ("Ἀσία") or "Asie" ("Ἀσίη") was the name of a "Nymph or Titan goddess of Lydia".
In ancient Greek religion, places were under the care of female divinities, parallel to guardian angels.
The poets detailed their doings and generations in allegoric language salted with entertaining stories, which subsequently playwrights transformed into classical Greek drama and became "Greek mythology".
For example, Hesiod mentions the daughters of Tethys and Ocean, among whom are a "holy company", "who with the Lord Apollo and the Rivers have youths in their keeping".
Many of these are geographic: Doris, Rhodea, Europa, Asia.
Hesiod explains:

For there are three-thousand neat-ankled daughters of Ocean who are dispersed far and wide, and in every place alike serve the earth and the deep waters.
The Iliad (attributed by the ancient Greeks to Homer) mentions two Phrygians (the tribe that replaced the Luvians in Lydia) in the Trojan War named Asios (an adjective meaning "Asian"); and also a marsh or lowland containing a marsh in Lydia as .
The history of Asia can be seen as the distinct histories of several peripheral coastal regions: East Asia, South Asia, Southeast Asia and the Middle East, linked by the interior mass of the Central Asian steppes.
The coastal periphery was home to some of the world's earliest known civilizations, each of them developing around fertile river valleys.
The civilizations in Mesopotamia, the Indus Valley and the Yellow River shared many similarities.
These civilizations may well have exchanged technologies and ideas such as mathematics and the wheel.
Other innovations, such as writing, seem to have been developed individually in each area.
Cities, states and empires developed in these lowlands.
The central steppe region had long been inhabited by horse-mounted nomads who could reach all areas of Asia from the steppes.
The earliest postulated expansion out of the steppe is that of the Indo-Europeans, who spread their languages into the Middle East, South Asia, and the borders of China, where the Tocharians resided.
The northernmost part of Asia, including much of Siberia, was largely inaccessible to the steppe nomads, owing to the dense forests, climate and tundra.
These areas remained very sparsely populated.
The center and the peripheries were mostly kept separated by mountains and deserts.
The Caucasus and Himalaya mountains and the Karakum and Gobi deserts formed barriers that the steppe horsemen could cross only with difficulty.
While the urban city dwellers were more advanced technologically and socially, in many cases they could do little in a military aspect to defend against the mounted hordes of the steppe.
However, the lowlands did not have enough open grasslands to support a large horsebound force; for this and other reasons, the nomads who conquered states in China, India, and the Middle East often found themselves adapting to the local, more affluent societies.
The Islamic Caliphate's defeats of the Byzantine and Persian empires led to West Asia and southern parts of Central Asia and western parts of South Asia under its control during its conquests of the 7th century.
The Mongol Empire conquered a large part of Asia in the 13th century, an area extending from China to Europe.
Before the Mongol invasion, Song dynasty reportedly had approximately 120 million citizens; the 1300 census which followed the invasion reported roughly 60 million people.
The Black Death, one of the most devastating pandemics in human history, is thought to have originated in the arid plains of central Asia, where it then travelled along the Silk Road.
The Russian Empire began to expand into Asia from the 17th century, and would eventually take control of all of Siberia and most of Central Asia by the end of the 19th century.
The Ottoman Empire controlled Anatolia, most of the Middle East, North Africa and the Balkans from the mid 16th century onwards.
In the 17th century, the Manchu conquered China and established the Qing dynasty.
The Islamic Mughal Empire and the Hindu Maratha Empire controlled much of India in the 16th and 18th centuries respectively.
Asia is the largest continent on Earth.
It covers 9% of the Earth's total surface area (or 30% of its land area), and has the largest coastline, at .
Asia is generally defined as comprising the eastern four-fifths of Eurasia.
It is located to the east of the Suez Canal and the Ural Mountains, and south of the Caucasus Mountains (or the Kuma–Manych Depression) and the Caspian and Black Seas.
It is bounded on the east by the Pacific Ocean, on the south by the Indian Ocean and on the north by the Arctic Ocean.
Asia is subdivided into 48 countries, three of them (Russia, Kazakhstan and Turkey) having part of their land in Europe.
Asia has extremely diverse climates and geographic features.
Climates range from arctic and subarctic in Siberia to tropical in southern India and Southeast Asia.
It is moist across southeast sections, and dry across much of the interior.
Some of the largest daily temperature ranges on Earth occur in western sections of Asia.
The monsoon circulation dominates across southern and eastern sections, due to the presence of the Himalayas forcing the formation of a thermal low which draws in moisture during the summer.
Southwestern sections of the continent are hot.
Siberia is one of the coldest places in the Northern Hemisphere, and can act as a source of arctic air masses for North America.
The most active place on Earth for tropical cyclone activity lies northeast of the Philippines and south of Japan.
The Gobi Desert is in Mongolia and the Arabian Desert stretches across much of the Middle East.
The Yangtze River in China is the longest river in the continent.
The Himalayas between Nepal and China is the tallest mountain range in the world.
Tropical rainforests stretch across much of southern Asia and coniferous and deciduous forests lie farther north.
A survey carried out in 2010 by global risk analysis farm Maplecroft identified 16 countries that are extremely vulnerable to climate change.
Each nation's vulnerability was calculated using 42 socio, economic and environmental indicators, which identified the likely climate change impacts during the next 30 years.
The Asian countries of Bangladesh, India, Vietnam, Thailand, Pakistan and Sri Lanka were among the 16 countries facing extreme risk from climate change.
Some shifts are already occurring.
For example, in tropical parts of India with a semi-arid climate, the temperature increased by 0.4 °C between 1901 and 2003.
A 2013 study by the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) aimed to find science-based, pro-poor approaches and techniques that would enable Asia's agricultural systems to cope with climate change, while benefitting poor and vulnerable farmers.
The study's recommendations ranged from improving the use of climate information in local planning and strengthening weather-based agro-advisory services, to stimulating diversification of rural household incomes and providing incentives to farmers to adopt natural resource conservation measures to enhance forest cover, replenish groundwater and use renewable energy.
Asia has the largest continental economy by both GDP Nominal and PPP in the world, and is the fastest growing economic region.
, the largest economies in Asia are China, Japan, India, Russia, South Korea, Indonesia and Turkey based on GDP in both nominal and PPP.
Based on Global Office Locations 2011, Asia dominated the office locations with 4 of the top 5 being in Asia: Hong Kong, Singapore, Tokyo, Seoul and Shanghai.
Around 68 percent of international firms have office in Hong Kong.
In the late 1990s and early 2000s, the economies of China and India have been growing rapidly, both with an average annual growth rate of more than 8%.
Other recent very-high-growth nations in Asia include Israel, Malaysia, Indonesia, Bangladesh, Pakistan, Thailand, Vietnam, Mongolia, Uzbekistan, Cyprus and the Philippines, and mineral-rich nations such as Kazakhstan, Turkmenistan, Iran, Brunei, the United Arab Emirates, Qatar, Kuwait, Saudi Arabia, Bahrain and Oman.
According to economic historian Angus Maddison in his book "The World Economy: A Millennial Perspective", India had the world's largest economy during 0 BCE and 1000 BCE.
China was the largest and most advanced economy on earth for much of recorded history, until the British Empire (excluding India) overtook it in the mid-19th century.
For several decades in the late twentieth century Japan was the largest economy in Asia and second-largest of any single nation in the world, after surpassing the Soviet Union (measured in net material product) in 1986 and Germany in 1968.
(NB: A number of supernational economies are larger, such as the European Union (EU), the North American Free Trade Agreement (NAFTA) or APEC).
This ended in 2010 when China overtook Japan to become the world's second largest economy.
In the late 1980s and early 1990s, Japan's GDP was almost as large (current exchange rate method) as that of the rest of Asia combined.
In 1995, Japan's economy nearly equaled that of the US as the largest economy in the world for a day, after the Japanese currency reached a record high of 79 yen/US$.
Economic growth in Asia since World War II to the 1990s had been concentrated in Japan as well as the four regions of South Korea, Taiwan, Hong Kong and Singapore located in the Pacific Rim, known as the Asian tigers, which have now all received developed country status, having the highest GDP per capita in Asia.
It is forecasted that India will overtake Japan in terms of nominal GDP by 2020.
By 2027, according to Goldman Sachs, China will have the largest economy in the world.
Several trade blocs exist, with the most developed being the Association of Southeast Asian Nations.
Asia is the largest continent in the world by a considerable margin, and it is rich in natural resources, such as petroleum, forests, fish, water, rice, copper and silver.
Manufacturing in Asia has traditionally been strongest in East and Southeast Asia, particularly in China, Taiwan, South Korea, Japan, India, the Philippines, and Singapore.
Japan and South Korea continue to dominate in the area of multinational corporations, but increasingly the PRC and India are making significant inroads.
Many companies from Europe, North America, South Korea and Japan have operations in Asia's developing countries to take advantage of its abundant supply of cheap labour and relatively developed infrastructure.
According to Citigroup 9 of 11 Global Growth Generators countries came from Asia driven by population and income growth.
They are Bangladesh, China, India, Indonesia, Iraq, Mongolia, Philippines, Sri Lanka and Vietnam.
Asia has four main financial centers: Tokyo, Hong Kong, Singapore and Shanghai.
Call centers and business process outsourcing (BPOs) are becoming major employers in India and the Philippines due to the availability of a large pool of highly skilled, English-speaking workers.
The increased use of outsourcing has assisted the rise of India and the China as financial centers.
Due to its large and extremely competitive information technology industry, India has become a major hub for outsourcing.
In 2010, Asia had 3.3 million millionaires (people with net worth over US$1 million excluding their homes), slightly below North America with 3.4 million millionaires.
Last year Asia had toppled Europe.
Citigroup in The Wealth Report 2012 stated that Asian centa-millionaire overtook North America's wealth for the first time as the world's "economic center of gravity" continued moving east.
At the end of 2011, there were 18,000 Asian people mainly in Southeast Asia, China and Japan who have at least $100 million in disposable assets, while North America with 17,000 people and Western Europe with 14,000 people.
With growing Regional Tourism with domination of Chinese visitors, MasterCard has released Global Destination Cities Index 2013 with 10 of 20 are dominated by Asia and Pacific Region Cities and also for the first time a city of a country from Asia (Bangkok) set in the top-ranked with 15.98 international visitors.
East Asia had by far the strongest overall Human Development Index (HDI) improvement of any region in the world, nearly doubling average HDI attainment over the past 40 years, according to the report's analysis of health, education and income data.
China, the second highest achiever in the world in terms of HDI improvement since
1970, is the only country on the "Top 10 Movers" list due to income rather than health or education achievements.
Its per capita income increased a stunning 21-fold over the last four decades, also lifting hundreds of millions out of income poverty.
Yet it was not among the region's top performers in improving school enrollment and life expectancy.
<br>Nepal, a South Asian country, emerges as one of the world's fastest movers since 1970 mainly due to health and education achievements.
Its present life expectancy is 25 years longer than in the 1970s.
More than four of every five children of school age in Nepal now attend primary school, compared to just one in five 40 years ago.
<br> Japan and South Korea ranked highest among the countries grouped on the HDI (number 11 and 12 in the world, which are in the "very high human development" category), followed by Hong Kong (21) and Singapore (27).
Afghanistan (155) ranked lowest amongst Asian countries out of the 169 countries assessed.
Asia is home to several language families and many language isolates.
Most Asian countries have more than one language that is natively spoken.
For instance, according to Ethnologue, more than 600 languages are spoken in Indonesia, more than 800 languages spoken in India, and more than 100 are spoken in the Philippines.
China has many languages and dialects in different provinces.
Many of the world's major religions have their origins in Asia, including the five most practiced in the world (excluding irreligion), which are Christianity, Islam, Hinduism, Chinese folk religion (classified as Confucianism and Taoism), and Buddhism respectively.
Asian mythology is complex and diverse.
The story of the Great Flood for example, as presented to Jews in the Hebrew Bible in the narrative of Noah—and later to Christians in the Old Testament, and to Muslims in the Quran—is earliest found in Mesopotamian mythology, in the Enûma Eliš and "Epic of Gilgamesh".
Hindu mythology similarly tells about an avatar of Vishnu in the form of a fish who warned Manu of a terrible flood.
Ancient Chinese mythology also tells of a Great Flood spanning generations, one that required the combined efforts of emperors and divinities to control.
The Abrahamic religions including Judaism, Christianity, Islam and Bahá'í Faith originated in West Asia.
Judaism, the oldest of the Abrahamic faiths, is practiced primarily in Israel, the indigenous homeland and historical birthplace of the Hebrew nation: which today consists both of those Israelites who remained in Asia/North Africa and those who returned from diaspora in Europe, North America, and other regions; though various diaspora communities persist worldwide.
Jews are the predominant ethnic group in Israel (75.6%) numbering at about 6.1 million, although the levels of adherence to Jewish religion vary.
Outside of Israel there are small ancient Jewish communities in Turkey (17,400), Azerbaijan (9,100), Iran (8,756), India (5,000) and Uzbekistan (4,000), among many other places.
In total, there are 14.4–17.5 million (2016, est.)
Jews alive in the world today, making them one of the smallest Asian minorities, at roughly 0.3 to 0.4 percent of the total population of the continent.
Christianity is a widespread religion in Asia with more than 286 million adherents according to Pew Research Center in 2010, and nearly 364 million according to Britannica Book of the Year 2014.
Constituting around 12.6% of the total population of Asia.
In the Philippines and East Timor, Roman Catholicism is the predominant religion; it was introduced by the Spaniards and the Portuguese, respectively.
In Armenia, Cyprus, Georgia and Asian Russia, Eastern Orthodoxy is the predominant religion.
In the Middle East, such as in the Levant, Syriac Christianity (Church of the East) and Oriental Orthodoxy are prevalent minority denominations, which are both Eastern Christian sects mainly adhered to Assyrian people or Syriac Christians.
Saint Thomas Christians in India trace their origins to the evangelistic activity of Thomas the Apostle in the 1st century.
Islam, which originated in the Hejaz located in modern-day Saudi Arabia, is the second largest and most widely-spread religion in Asia with at least 1 billion Muslims constituting around 23.8% of the total population of Asia.
With 12.7% of the world Muslim population, the country currently with the largest Muslim population in the world is Indonesia, followed by Pakistan (11.5%), India (10%), Bangladesh, Iran and Turkey.
Mecca, Medina and Jerusalem are the three holiest cities for Islam in all the world.
The Hajj and Umrah attract large numbers of Muslim devotees from all over the world to Mecca and Medina.
Iran is the largest Shi'a country.
The Bahá'í Faith originated in Asia, in Iran (Persia), and spread from there to the Ottoman Empire, Central Asia, India, and Burma during the lifetime of Bahá'u'lláh.
Since the middle of the 20th century, growth has particularly occurred in other Asian countries, because Bahá'í activities in many Muslim countries has been severely suppressed by authorities.
Lotus Temple is a big Baha'i Temple in India.
Almost all Asian religions have philosophical character and Asian philosophical traditions cover a large spectrum of philosophical thoughts and writings.
Indian philosophy includes Hindu philosophy and Buddhist philosophy.
They include elements of nonmaterial pursuits, whereas another school of thought from India, Cārvāka, preached the enjoyment of the material world.
The religions of Hinduism, Buddhism, Jainism and Sikhism originated in India, South Asia.
In East Asia, particularly in China and Japan, Confucianism, Taoism and Zen Buddhism took shape.
, Hinduism has around 1.1 billion adherents.
The faith represents around 25% of Asia's population and is the largest religion in Asia.
However, it is mostly concentrated in South Asia.
Over 80% of the populations of both India and Nepal adhere to Hinduism, alongside significant communities in Bangladesh, Pakistan, Bhutan, Sri Lanka and Bali, Indonesia.
Many overseas Indians in countries such as Burma, Singapore and Malaysia also adhere to Hinduism.
Buddhism has a great following in mainland Southeast Asia and East Asia.
Buddhism is the religion of the majority of the populations of Cambodia (96%), Thailand (95%), Burma (80–89%), Japan (36–96%), Bhutan (75–84%), Sri Lanka (70%), Laos (60–67%) and Mongolia (53–93%).
Large Buddhist populations also exist in Singapore (33–51%), Taiwan (35–93%), South Korea (23–50%), Malaysia (19–21%), Nepal (9–11%), Vietnam (10–75%), China (20–50%), North Korea (2–14%), and small communities in India and Bangladesh.
In many Chinese communities, Mahayana Buddhism is easily syncretized with Taoism, thus exact religious statistics is difficult to obtain and may be understated or overstated.
The Communist-governed countries of China, Vietnam and North Korea are officially atheist, thus the number of Buddhists and other religious adherents may be under-reported.
Jainism is found mainly in India and in oversea Indian communities such as the United States and Malaysia.
Sikhism is found in Northern India and amongst overseas Indian communities in other parts of Asia, especially Southeast Asia.
Confucianism is found predominantly in Mainland China, South Korea, Taiwan and in overseas Chinese populations.
Taoism is found mainly in Mainland China, Taiwan, Malaysia and Singapore.
Taoism is easily syncretized with Mahayana Buddhism for many Chinese, thus exact religious statistics is difficult to obtain and may be understated or overstated.
Some of the events pivotal in the Asia territory related to the relationship with the outside world in the post-Second World War were:

The polymath Rabindranath Tagore, a Bengali poet, dramatist, and writer from Santiniketan, now in West Bengal, India, became in 1913 the first Asian Nobel laureate.
He won his Nobel Prize in Literature for notable impact his prose works and poetic thought had on English, French, and other national literatures of Europe and the Americas.
He is also the writer of the national anthems of Bangladesh and India.
Other Asian writers who won Nobel Prize for literature include Yasunari Kawabata (Japan, 1968), Kenzaburō Ōe (Japan, 1994), Gao Xingjian (China, 2000), Orhan Pamuk (Turkey, 2006), and Mo Yan (China, 2012).
Some may consider the American writer, Pearl S. Buck, an honorary Asian Nobel laureate, having spent considerable time in China as the daughter of missionaries, and based many of her novels, namely "The Good Earth" (1931) and "The Mother" (1933), as well as the biographies of her parents of their time in China, "The Exile" and "Fighting Angel", all of which earned her the Literature prize in 1938.
Also, Mother Teresa of India and Shirin Ebadi of Iran were awarded the Nobel Peace Prize for their significant and pioneering efforts for democracy and human rights, especially for the rights of women and children.
Ebadi is the first Iranian and the first Muslim woman to receive the prize.
Another Nobel Peace Prize winner is Aung San Suu Kyi from Burma for her peaceful and non-violent struggle under a military dictatorship in Burma.
She is a nonviolent pro-democracy activist and leader of the National League for Democracy in Burma (Myanmar) and a noted prisoner of conscience.
She is a Buddhist and was awarded the Nobel Peace Prize in 1991.
Chinese dissident Liu Xiaobo was awarded the Nobel Peace Prize for "his long and non-violent struggle for fundamental human rights in China" on 8 October 2010.
He is the first Chinese citizen to be awarded a Nobel Prize of any kind while residing in China.
In 2014, Kailash Satyarthi from India and Malala Yousafzai from Pakistan were awarded the Nobel Peace Prize "for their struggle against the suppression of children and young people and for the right of all children to education".
Sir C.V.
Raman is the first Asian to get a Nobel prize in Sciences.
He won the Nobel Prize in Physics "for his work on the scattering of light and for the discovery of the effect named after him".
Japan has won the most Nobel Prizes of any Asian nation with 24 followed by India which has won 13.
Amartya Sen, (born 3 November 1933) is an Indian economist who was awarded the 1998 Nobel Memorial Prize in Economic Sciences for his contributions to welfare economics and social choice theory, and for his interest in the problems of society's poorest members.
Other Asian Nobel Prize winners include Subrahmanyan Chandrasekhar, Abdus Salam, Malala Yousafzai, Robert Aumann, Menachem Begin, Aaron Ciechanover, Avram Hershko, Daniel Kahneman, Shimon Peres, Yitzhak Rabin, Ada Yonath, Yasser Arafat, José Ramos-Horta and Bishop Carlos Filipe Ximenes Belo of Timor Leste, Kim Dae-jung, and 13 Japanese scientists.
Most of the said awardees are from Japan and Israel except for Chandrasekhar and Raman (India), Abdus Salam and Malala yousafzai, (Pakistan), Arafat (Palestinian Territories), Kim (South Korea), and Horta and Belo (Timor Leste).
In 2006, Dr. Muhammad Yunus of Bangladesh was awarded the Nobel Peace Prize for the establishment of Grameen Bank, a community development bank that lends money to poor people, especially women in Bangladesh.
Dr. Yunus received his PhD in economics from Vanderbilt University, United States.
He is internationally known for the concept of micro credit which allows poor and destitute people with little or no collateral to borrow money.
The borrowers typically pay back money within the specified period and the incidence of default is very low.
The Dalai Lama has received approximately eighty-four awards over his spiritual and political career.
On 22 June 2006, he became one of only four people ever to be recognized with Honorary Citizenship by the Governor General of Canada.
On 28 May 2005, he received the Christmas Humphreys Award from the Buddhist Society in the United Kingdom.
Most notable was the Nobel Peace Prize, presented in Oslo, Norway on 10 December 1989.
Within the above-mentioned states are several partially recognized countries with limited to no international recognition.
None of them are members of the UN:
References to articles:

Special topics:

Lists:





</doc>
<doc id="690" url="https://en.wikipedia.org/wiki?curid=690" title="Aruba">
Aruba

Aruba ( ; , Papiamento: ) is an island and a constituent country of the Kingdom of the Netherlands in the southern Caribbean Sea, located about west of the main part of the Lesser Antilles and north of the coast of Venezuela.
It measures long from its northwestern to its southeastern end and across at its widest point.
Together with Bonaire and Curaçao, Aruba forms a group referred to as the ABC islands.
Collectively, Aruba and the other Dutch islands in the Caribbean are often called the Dutch Caribbean.
Aruba is one of the four countries that form the Kingdom of the Netherlands, along with the Netherlands, Curaçao, and Sint Maarten; the citizens of these countries are all Dutch nationals.
Aruba has no administrative subdivisions, but, for census purposes, is divided into eight regions.
Its capital is Oranjestad.
Unlike much of the Caribbean region, Aruba has a dry climate and an arid, cactus-strewn landscape.
This climate has helped tourism as visitors to the island can reliably expect warm, sunny weather.
It has a land area of and is densely populated, with a total of 102,484 inhabitants at the 2010 Census.
It lies outside Hurricane Alley.
The name Aruba may have different origins:

Aruba's first inhabitants are thought to have been Caquetío Amerindians from the Arawak tribe, who migrated there from Venezuela to escape attacks by the Caribs.
Fragments of the earliest known Indian settlements date back to 1000 AD.
As sea currents made canoe travel to other Caribbean islands difficult, Caquetio culture remained more closely associated with that of mainland South America.
Europeans first learned of Aruba following the explorations for Spain by Amerigo Vespucci and Alonso de Ojeda in the summer of 1499.
Both described Aruba as an "island of giants", remarking on the comparatively large stature of the native Caquetíos compared to Europeans.
Gold was not discovered on Aruba for another 300 years.
Vespucci returned to Spain with stocks of cotton and brazilwood from the island and described houses built into the ocean.
Vespucci and Ojeda's tales spurred interest in Aruba, and Spaniards soon colonized the island.
Because it had low rainfall, Aruba was not considered profitable for the plantation system and the economics of the slave trade.
Aruba was colonized by Spain for over a century.
"Simas", the "Cacique", or chief, in Aruba, welcomed the first Catholic priests in Aruba, who gave him a wooden cross as a gift.
In 1508, the Spanish Crown appointed Alonso de Ojeda as its first Governor of Aruba, as part of "Nueva Andalucía".
Arawaks spoke the "broken Spanish" which their ancestors had learned on Hispaniola.
Another governor appointed by Spain was Juan Martínez de Ampiés.
A "cédula real" decreed in November 1525 gave Ampiés, factor of Española, the right to repopulate Aruba.
In 1528, Ampiés was replaced by a representative of the House of Welser of Augsburg.
The Netherlands seized Aruba from Spain in 1636 in the course of the Thirty Years' War.
Since 1636, Aruba has been under Dutch administration, initially governed by Peter Stuyvesant, later appointed to New Amsterdam (New York City).
Stuyvesant was on a special mission in Aruba in November and December 1642.
The island was included under the Dutch West India Company (W.I.C.)
administration, as "New Netherland and Curaçao", from 1648 to 1664.
In 1667 the Dutch administration appointed an Irishman as "Commandeur" in Aruba.
The Dutch took control 135 years after the Spanish, leaving the Arawaks to farm and graze livestock, and used the island as a source of meat for other Dutch possessions in the Caribbean.
Aruba's proximity to South America resulted in interaction with cultures of the coastal areas more than a century after independence of Netherlands from Spain; architectural similarities can be seen between the 19th-century parts of Oranjestad and the nearby Venezuelan city of Coro in Falcón State.
Historically, Dutch was not widely spoken on the island outside of colonial administration; its use increased in the late 19th and early 20th centuries.
Students on Curaçao, Aruba, and Bonaire were taught predominantly in Spanish until the late 19th century, when the British took Curaçao, Aruba, and Bonaire.
Teaching of Spanish was restored when Dutch rule resumed in 1815.
Also, efforts were made to introduce bilingual popular education in Dutch and Papiamentu in the late 19th century.
During the Napoleonic wars, the British Empire took control over the island, between 1799 and 1802, and between 1804 and 1816, before handing it back to the Dutch.
During World War II with the occupation of the Netherlands in 1940 the oil facilities in Aruba came under the administration of the Dutch government-in-exile in London, and Aruba continued to supply oil to the British and their allies.
In August 1947, Aruba presented its first "Staatsreglement" (constitution), for Aruba's "status aparte" as an autonomous state within the Kingdom of the Netherlands.
By 1954, the Charter of the Kingdom of the Netherlands was established, providing a framework for relations between Aruba and the rest of the Kingdom.
In 1972, at a conference in Suriname, Betico Croes (MEP), a politician from Aruba, proposed a "sui-generis" Dutch Commonwealth of four states: Aruba, the Netherlands, Suriname and the Netherlands Antilles, each to have its own nationality.
C. Yarzagaray, a parliamentary member representing the AVP political party, proposed a referendum so that the people of Aruba could choose whether they wanted total independence or "Status Aparte" as a full autonomous state under the Crown.
Croes worked in Aruba to inform and prepare the people of Aruba for independence.
In 1976, he appointed a committee that chose the national flag and anthem, introducing them as symbols of Aruba's sovereignty and independence.
He set 1981 as a target date for independence.
In March 1977, the first Referendum for Self Determination was held with the support of the United Nations; 82% of the participants voted for independence.
The Island Government of Aruba assigned the Institute of Social Studies in The Hague to prepare a study for independence; it was titled "Aruba en Onafhankelijkheid, achtergronden, modaliteiten en mogelijkheden; een rapport in eerste aanleg" (Aruba and independence, backgrounds, modalities and opportunities; a preliminary report) (1978).
At the conference in The Hague in 1981, Aruba's independence was set for the year 1991.
In March 1983, Aruba reached an official agreement within the Kingdom for its independence, to be developed in a series of steps as the Crown granted increasing autonomy.
In August 1985 Aruba drafted a constitution that was unanimously approved.
On 1 January 1986, after elections were held for its first parliament, Aruba seceded from the Netherlands Antilles; it officially became a country of the Kingdom of the Netherlands.
Full independence was projected in 1996.
After his death in 1986, Croes was proclaimed "Libertador di Aruba".
At a convention in The Hague in 1990, at the request of Aruba's Prime Minister, the governments of Aruba, the Netherlands, and the Netherlands Antilles postponed indefinitely its transition to full independence.
The article scheduling Aruba's complete independence was rescinded in 1995, although the process could be revived after another referendum.
Aruba is a generally flat, riverless island in the Leeward Antilles island arc of the Lesser Antilles in the southern part of the Caribbean.
It has white sandy beaches on the western and southern coasts of the island, relatively sheltered from fierce ocean currents.
This is where most tourist development has occurred.
The northern and eastern coasts, lacking this protection, are considerably more battered by the sea and have been left largely untouched by humans.
The hinterland of the island features some rolling hills, the best known of which are called Hooiberg at and Mount Jamanota, the highest on the island at above sea level.
Oranjestad, the capital, is located at .
To the east of Aruba are Bonaire and Curaçao, two island territories which once formed the southwest part of the Netherlands Antilles.
This group of islands is sometimes called the ABC islands.
They are located on the South American continental shelf and therefore geographically listed as part of South America.
The Natural Bridge was a large, naturally formed limestone bridge on the island's north shore.
It was a popular tourist destination until its collapse in 2005.
The island, with a population of just over 100,000 inhabitants, does not have major cities.
The island is divided into six districts.
Most of the island's population resides in or around the two major city-like districts of Oranjestad (Capital) and San Nicolaas.
Oranjestad and San Nicolaas are both divided into two districts for census purposes only.
The districts are as follows:


The island of Aruba, being isolated from the main land of South America, has fostered the evolution of multiple endemic animals.
The island provides a habitat for the endemic Aruban Whiptail and Aruba Rattlesnake, as well as an endemic subspecies of Burrowing Owl and Brown-throated Parakeet.
The rattlesnake and the owl are printed on the Aruban currency.
The flora of Aruba differs from the typical tropical island vegetation.
Xeric scrublands are common, with various forms of cacti, thorny shrubs, and evergreens.
The most known plant is the Aloe vera, which has a place on the Coat of Arms of Aruba.
By the Köppen climate classification, Aruba has a hot semi-arid climate (Köppen "BSh").
Mean monthly temperature in Oranjestad varies little from to , moderated by constant trade winds from the Atlantic Ocean, which come from the north-east.
Yearly rainfall barely exceeds in Oranjestad, although it is extremely variable and can range from as little as during strong El Niño years (e.g.
1911/1912, 1930/1931, 1982/1983, 1997/1998) to over in La Niña years like 1933/1934, 1970/1971 or 1988/1989.
In terms of nationality, the population is estimated to be 82.1% Dutch, 6.6% Colombian, 2.2% Venezuelan, 2.2% Dominican, 1.2% Haitian, 0.1% unspecified.
In terms of ethnic composition, the population is estimated to be 75% mixed European/Amerindian/African, 15% Black and 10% other ethnicities.
The Arawak heritage is stronger on Aruba than on most Caribbean islands, and a quite big portion of Arubans who claim their ethnicity as Dutch possess Arawak blood.
Although no full-blooded Aboriginals remain, the features of the islanders clearly indicate their genetic Arawak heritage.
Most of the population is descended from Caquetio Indians and Dutch and to a lesser extent of Africans, Spanish, Portuguese, English, French, and Sephardic Jewish ancestors.
Aruba is a home of Chinese, and Indo Caribbeans and Javanese, who descend largely from workers contracted from India and the island of Java in the former Dutch East Indies (modern Indonesia).
Recently, there has been substantial immigration to the island from neighboring American and Caribbean nations, possibly attracted by the higher paid jobs.
In 2007, new immigration laws were introduced to help control the growth of the population by restricting foreign workers to a maximum of three years residency on the island.
Demographically, Aruba has felt the impact of its proximity to Venezuela.
Many of Aruba's families are descended from Venezuelan immigrants.
There is a seasonal increase of Venezuelans living in second homes.
As Aruba has a little proximity to Colombia, Colombian residents and their children are found here.
The official languages are Dutch and Papiamento.
However, Dutch is the sole language for all administration and legal matters, Papiamento is the predominant language on Aruba.
It is a creole language, spoken on Aruba, Bonaire, and Curaçao, that incorporates words from Portuguese, West African languages, Dutch, and Spanish.
English is known by many; its usage has grown due to tourism.
Other common languages spoken, based on the size of their community, are Portuguese, Chinese, German, Spanish, and French.
In recent years, the government of Aruba has shown an increased interest in acknowledging the cultural and historical importance of its native language.
Although spoken Papiamento is fairly similar among the several Papiamento-speaking islands, there is a big difference in written Papiamento.
The orthography differs per island and even per group of people.
Some are more oriented towards Portuguese and use the equivalent spelling (e.g.
"y" instead of "j"), where others are more oriented towards Dutch.
The book "The Buccaneers of America", first published in 1678, states through eyewitness account that the natives on Aruba spoke Spanish already.
Spanish became an important language in the 18th century due to the close economic ties with Spanish colonies in what are now Venezuela and Colombia, and several Venezuelan TV networks are received, and the fact that Aruba has a presence of Venezuelan and Colombian residents.
The oldest government official statement written in Papiamento dates from 1803.
Around 12.6% of the population today speaks Spanish.
Use of English dates to the early 19th century, when the British took Curaçao, Aruba and Bonaire.
When Dutch rule resumed in 1815, officials already noted wide use of the language.
Aruba has four newspapers published in Papiamento: "Diario", "Bon Dia", "Solo di Pueblo" and "Awe Mainta"; and three in English: "Aruba Daily", "Aruba Today" and "The News".
"Amigoe" is a newspaper published in Dutch.
Aruba also has 18 radio stations (two AM and 16 FM) and two local television stations (Telearuba, and Channel 22).
Aruba is a polyglot society.
Most of Aruba's population is able to converse in at least two of the languages of Papiamentu, Dutch, English, and Spanish.
Three-quarters of the population is Roman Catholic.
For census purposes, Aruba is divided into eight regions, which have no administrative functions:

As a constituent country of the Kingdom of the Netherlands, Aruba's politics take place within a framework of a 21-member Parliament and an eight-member Cabinet.
The governor of Aruba is appointed for a six-year term by the monarch, and the prime minister and deputy prime minister are elected by the Staten (or "Parlamento") for four-year terms.
The Staten is made up of 21 members elected by direct, popular vote to serve a four-year term.
Together with the Netherlands, the countries of Aruba, Curaçao and Sint Maarten form the Kingdom of the Netherlands.
As they share the same Dutch citizenship, these four countries still also share the Dutch passport as the Kingdom of the Netherlands passport.
As Aruba, Curaçao and Sint Maarten have small populations, the three countries had to limit immigration.
To protect their population, they have the right to control the admission and expulsion of people from the Netherlands.
Aruba is designated as a member of the Overseas Countries and Territories (OCT) and is thus officially not a part of the European Union, though Aruba can and does receive support from the European Development Fund.
The Aruban legal system is based on the Dutch model.
In Aruba, legal jurisdiction lies with the "Gerecht in Eerste Aanleg" (Court of First Instance) on Aruba, the "Gemeenschappelijk Hof van Justitie van Aruba, Curaçao, Sint Maarten en van Bonaire, Sint Eustatius en Saba" (Joint Court of Justice of Aruba, Curaçao, Sint Maarten, and of Bonaire, Sint Eustatius and Saba) and the "Hoge Raad der Nederlanden" (Supreme Court of Justice of the Netherlands).
The "Korps Politie Aruba" (Aruba Police Force) is the island's law enforcement agency and operates district precincts in Oranjestad, Noord, San Nicolaas, and Santa Cruz, where it is headquartered.
Deficit spending has been a staple in Aruba's history, and modestly high inflation has been present as well.
By 2006, the government's debt had grown to 1.883 billion Aruban florins.
Aruba received some development aid from the Dutch government each year through 2009, as part of a deal (signed as "Aruba's Financial Independence") in which the Netherlands gradually reduced its financial help to the island each successive year.
In 2006, the Aruban government changed several tax laws to reduce the deficit.
Direct taxes have been converted to indirect taxes as proposed by the IMF.
A 3% tax has been introduced on sales and services, while income taxes have been lowered and revenue taxes for business reduced by 20%.
The government compensated workers with 3.1% for the effect that the B.B.O.
would have on the inflation for 2007.
Aruba's educational system is patterned after the Dutch system of education.
The Government of Aruba finances the public national education system.
There are mostly public schools, and there are private schools, including the International School of Aruba and Schakel College.
There are two medical schools, Aureus University School of Medicine and Xavier University School of Medicine, as well as its own national university, the University of Aruba.
Aruba has one of the highest standards of living in the Caribbean region.
There is a low unemployment rate.
The GDP per capita for Aruba was estimated to be $28,924 in 2014; among the highest in the Caribbean and the Americas.
Its main trading partners are Colombia, the United States, Venezuela, and the Netherlands.
The island's economy has been dominated by three main industries: tourism, aloe export, and petroleum refining (The Lago Oil and Transport Company and the Arend Petroleum Maatschappij Shell Co.).
Before the "Status Aparte" (a separate completely autonomous country/state within the Kingdom), oil processing was the dominant industry in Aruba despite expansion of the tourism sector.
Today, the influence of the oil processing business is minimal.
The size of the agriculture and manufacturing sectors also remains minimal.
Aloe was introduce to Aruba in 1840 but did not become a big export till 1890.
Cornelius Eman founded Aruba Aloe Balm and the industry had become very important to the economy.
At one point two thirds of the island was covered in Aloe Vera fields and the first plantation covered 150 acres and it is still used today at 127 years old.
Aruba had become the largest exporter of aloe in the world, also because of Aruba's climate and dry soil the aloe plants flourished and the aloin content was twenty two percent while the aloin content in the rest of the world was only as high as fifteen percent (arubablog.net) From this Aruba now has its own line of aloe and that contains skin care products, deodorant, sun care, shower and hair products (Aruba aloe).
The official exchange rate of the Aruban florin is pegged to the US dollar at 1.79 florins to 1 USD.
Because of this fact, and due to a large number of American tourists, many businesses operate using US dollars instead of florins, especially in the hotel and resort districts.
About three quarters of the Aruban gross national product is earned through tourism or related activities.
Most tourists are from the United States (predominantly from the north-east US), Canada, the Netherlands and South America, mainly Venezuela and Colombia.
As part of the Kingdom of the Netherlands, citizens of the Netherlands can travel with relative ease to Aruba and other islands of the Dutch Antilles.
No visas are needed for Dutch citizens, only a passport, and although the currency used in Aruba is different (the Netherlands uses the Euro), money can be easily exchanged at a local bank for Aruban Florins.
For the facilitation of the passengers whose destination is the United States, the United States Department of Homeland Security (DHS), U.S.
Customs and Border Protection (CBP) full pre-clearance facility in Aruba has been in effect since 1 February 2001 with the expansion in the Queen Beatrix Airport.
United States and Aruba have had the agreement since 1986.
It began as a USDA and Customs post.
Since 2008, Aruba has been the only island to have this service for private flights.
Defense on "Aruba" is the responsibility of the Kingdom of the Netherlands.
The Netherlands Military forces that protect "Aruba" include the Royal Netherlands Navy, the Netherlands Marine Corps and the Netherlands Coastguard.
There is also a small indigenous "Arubaanse Militie" (ARUMIL) of about platoon strength.
All forces are stationed at Marines Barracks Savaneta.
Furthermore, in 1999 the U.S.
Department of Defense established a Forward Operating Location (FOL) at the airport.
On 18 March, Aruba celebrates its National Day.
In 1976, Aruba presented its National Anthem (Aruba Dushi Tera) and Flag.
Aruba has a varied culture.
According to the "Bureau Burgelijke Stand en Bevolkingsregister" (BBSB), in 2005 there were ninety-two different nationalities living on the island.
Dutch influence can still be seen, as in the celebration of "Sinterklaas" on 5 and 6 December and other national holidays like 27 April, when in Aruba and the rest of the Kingdom of the Netherlands the King's birthday or "Dia di Rey" (Koningsdag) is celebrated.
Christmas and New Year's Eve are celebrated with the typical music and songs for gaitas for Christmas and the Dande for New Year, and "ayaca", "ponche crema", ham, and other typical foods and drinks.
Millions of florins worth of fireworks are burnt at midnight on New Year's Eve.
On 25 January, Betico Croes' birthday is celebrated.
Dia di San Juan is celebrated on 24 June.
Besides Christmas, the religious holy days of the Feast of the Ascension and Good Friday are holidays on the island.
The holiday of Carnaval is also an important one in Aruba, as it is in many Caribbean and Latin American countries, and, like Mardi Gras, that goes on for weeks.
Its celebration in Aruba started, around the 1950s, influenced by the inhabitants from Venezuela and the nearby islands (Curaçao, St.
Vincent, Trinidad, Barbados, St.
Maarten and Anguilla) who came to work for the Oil refinery.
Over the years the Carnival Celebration has changed and now starts from the beginning of January till the Tuesday before Ash Wednesday with a large parade on the last Sunday of the festivities (Sunday before Ash Wednesday).
Tourism from the United States has recently increased the visibility of American culture on the island, with such celebrations as Halloween and Thanksgiving Day in November.
Aruba's Queen Beatrix International Airport is located near Oranjestad.
According to the Aruba Airport Authority, almost 1.7 million travelers used the airport in 2005, 61% of whom were Americans.
Aruba has two ports, Barcadera and Playa, which are located in Oranjestad and Barcadera.
The Port of Playa services all the cruise-ship lines, including Royal Caribbean, Carnival Cruise Lines, NCL, Holland America Line, Disney Cruise Line and others.
Nearly one million tourists enter this port per year.
Aruba Ports Authority, owned and operated by the Aruban government, runs these seaports.
Arubus is a government-owned bus company.
Its buses operate from 3:30 a.m.
until 12:30 a.m., 365 days a year.
Small private vans also provide transportation services in certain areas such Hotel Area, San Nicolaas, Santa Cruz and Noord.
A street car service runs on rails on the Mainstreet.
Water-en Energiebedrijf Aruba, N.V.
(W.E.B.)
produces potable industrial water at the world's third largest desalination plant.
Average daily consumption in Aruba is about .
There are two telecommunications providers: Setar, a government-based company and Digicel, both of which are privately owned.
Setar is the provider of services such as internet, video conferencing, GSM wireless technology and land lines.
Digicel is Setar's competitor in wireless technology using the GSM platform.
</doc>
<doc id="691" url="https://en.wikipedia.org/wiki?curid=691" title="Articles of Confederation">
Articles of Confederation

The Articles of Confederation, formally the Articles of Confederation and Perpetual Union, was an agreement among the 13 original states of the United States of America that served as its first constitution.
It was approved, after much debate (between July 1776 and November 1777), by the Second Continental Congress on November 15, 1777, and sent to the states for ratification.
The Articles of Confederation came into force on March 1, 1781, after being ratified by all 13 states.
A guiding principle of the Articles was to preserve the independence and sovereignty of the states.
The central government established by the Articles received only those powers which the former colonies had recognized as belonging to king and parliament.
The Articles formed a war-time confederation of states, with an extremely limited central government.
While unratified, the document was used by the Congress to conduct business, direct the American Revolutionary War, conduct diplomacy with foreign nations, and deal with territorial issues and Native American relations.
The adoption of the Articles made few perceptible changes in the federal government, because it did little more than legalize what the Continental Congress had been doing.
That body was renamed the Congress of the Confederation; but Americans continued to call it the "Continental Congress", since its organization remained the same.
As the Confederation Congress attempted to govern the continually growing American states, delegates discovered that the limitations placed upon the central government rendered it ineffective at doing so.
As the government's weaknesses became apparent, especially after Shays' Rebellion, individuals began asking for changes to the Articles.
Their hope was to create a stronger national government.
Initially, some states met to deal with their trade and economic problems.
However, as more states became interested in meeting to change the Articles, a meeting was set in Philadelphia on May 25, 1787.
This became the Constitutional Convention.
It was quickly realized that changes would not work, and instead the entire Articles needed to be replaced.
On March 4, 1789, the government under the Articles was replaced with the federal government under the Constitution.
The new Constitution provided for a much stronger federal government by establishing a chief executive (the President), courts, and taxing powers.
The political push to increase cooperation among the then-loyal colonies began with the Albany Congress in 1754 and Benjamin Franklin's proposed Albany Plan, an inter-colonial collaboration to help solve mutual local problems.
The Articles of Confederation would bear some resemblance to it.
Over the next two decades, some of the basic concepts it addressed would strengthen and others would weaken, particularly the degree of deserved loyalty to the crown.
With civil disobedience resulting in coercive, and what the colonials perceived as intolerable acts of Parliament, and armed conflict resulting in dissidents being proclaimed rebels and outside the King's protection, any loyalty remaining shifted toward independence and how to achieve it.
In 1775, with events outpacing communications, the Second Continental Congress began acting as the provisional government that would run the American Revolutionary War and gain the colonies their collective independence.
It was an era of constitution writing—most states were busy at the task—and leaders felt the new nation must have a written constitution, even though other nations did not.
During the war, Congress exercised an unprecedented level of political, diplomatic, military and economic authority.
It adopted trade restrictions, established and maintained an army, issued fiat money, created a military code and negotiated with foreign governments.
To transform themselves from outlaws into a legitimate nation, the colonists needed international recognition for their cause and foreign allies to support it.
In early 1776, Thomas Paine argued in the closing pages of the first edition of "Common Sense" that the "custom of nations" demanded a formal declaration of American independence if any European power were to mediate a peace between the Americans and Great Britain.
The monarchies of France and Spain in particular could not be expected to aid those they considered rebels against another legitimate monarch.
Foreign courts needed to have American grievances laid before them persuasively in a "manifesto" which could also reassure them that the Americans would be reliable trading partners.
Without such a declaration, Paine concluded, "[t]he custom of all courts is against us, and will be so, until, by an independence, we take rank with other nations."
Beyond improving their existing association, the records of the Second Continental Congress show that the need for a declaration of independence was intimately linked with the demands of international relations.
On June 7, 1776, Richard Henry Lee introduced a resolution before the Continental Congress declaring the colonies independent; at the same time he also urged Congress to resolve "to take the most effectual measures for forming foreign Alliances" and to prepare a plan of confederation for the newly independent states.
Congress then created three overlapping committees to draft the Declaration, a Model Treaty, and the Articles of Confederation.
The Declaration announced the states' entry into the international system; the model treaty was designed to establish amity and commerce with other states; and the Articles of Confederation, which established "a firm league" among the thirteen free and independent states, constituted an international agreement to set up central institutions for the conduct of vital domestic and foreign affairs.
On June 12, 1776, a day after appointing a committee to prepare a draft of the Declaration of Independence, the Second Continental Congress resolved to appoint a committee of 13 to prepare a draft of a constitution for a union of the states.
The committee met repeatedly, and chairman John Dickinson presented their results to the Congress on July 12, 1776.
There were long debates on such issues as sovereignty, the exact powers to be given the confederate government, whether to have a judiciary, and voting procedures.
The final draft of the Articles was prepared in the summer of 1777 and the Second Continental Congress approved them for ratification by the individual states on November 15, 1777, after a year of debate.
Consensus was achieved by dividing sovereignty between the states and the central government, with a unicameral legislature that protected the liberty of the individual states.
The Articles of Confederation was submitted to the states for ratification in November 1777.
The first state to ratify was Virginia on December 16, 1777; 12 states had ratified the Articles by February 1779, 14 months into the process.
The lone holdout, Maryland, refused to go along until the landed states, especially Virginia, had indicated they were prepared to cede their claims west of the Ohio River to the Union.
It would be two years before the Maryland General Assembly became satisfied that the various states would follow through, and voted to ratify.
During this time, Congress observed the Articles as its de facto frame of government.
Maryland finally ratified the Articles on February 2, 1781.
Congress was informed of Maryland's assent on March 1, and officially proclaimed the Articles of Confederation to be the law of the land.
The several states ratified the Articles of Confederation on the following dates:

The Articles of Confederation contain a preamble, thirteen articles, a conclusion, and a signatory section.
The individual articles set the rules for current and future operations of the confederation's central government.
Under the Articles, the states retained sovereignty over all governmental functions not specifically relinquished to the national Congress, which was empowered to make war and peace, negotiate diplomatic and commercial agreements with foreign countries, and to resolve disputes between the states.
The document also stipulates that its provisions "shall be inviolably observed by every state" and that "the Union shall be perpetual".
Summary of the purpose and content of each of the 13 articles:

Under the Articles, Congress had the authority to regulate and fund the Continental Army, but it lacked the power to compel the States to comply with requests for either troops or funding.
This left the military vulnerable to inadequate funding, supplies, and even food.
Further, although the Articles enabled the states to present a unified front when dealing with the European powers, as a tool to build a centralized war-making government, they were largely a failure; Historian Bruce Chadwick wrote:

The Continental Congress, before the Articles were approved, had promised soldiers a pension of half pay for life.
However Congress had no power to compel the states to fund this obligation, and as the war wound down after the victory at Yorktown the sense of urgency to support the military was no longer a factor.
No progress was made in Congress during the winter of 1783–84.
General Henry Knox, who would later become the first Secretary of War under the Constitution, blamed the weaknesses of the Articles for the inability of the government to fund the army.
The army had long been supportive of a strong union.
Knox wrote:

As Congress failed to act on the petitions, Knox wrote to Gouverneur Morris, four years before the Philadelphia Convention was convened, "As the present Constitution is so defective, why do not you great men call the people together and tell them so; that is, to have a convention of the States to form a better Constitution."
Once the war had been won, the Continental Army was largely disbanded.
A very small national force was maintained to man the frontier forts and to protect against Native American attacks.
Meanwhile, each of the states had an army (or militia), and 11 of them had navies.
The wartime promises of bounties and land grants to be paid for service were not being met.
In 1783, George Washington defused the Newburgh conspiracy, but riots by unpaid Pennsylvania veterans forced Congress to leave Philadelphia temporarily.
The Congress from time to time during the Revolutionary War requisitioned troops from the states.
Any contributions were voluntary, and in the debates of 1788 the Federalists (who supported the proposed new Constitution) claimed that state politicians acted unilaterally, and contributed when the Continental army protected their state's interests.
The Anti-Federalists claimed that state politicians understood their duty to the Union and contributed to advance its needs.
Dougherty (2009) concludes that generally the States' behavior validated the Federalist analysis.
This helps explain why the Articles of Confederation needed reforms.
The 1783 Treaty of Paris, which ended hostilities with Great Britain, languished in Congress for several months because too few delegates were present at any one time to constitute a quorum so that it could be ratified.
Afterward, the problem only got worse as Congress had no power to enforce attendance.
Rarely did more than half of the roughly sixty delegates attend a session of Congress at the time, causing difficulties in raising a quorum.
The resulting paralysis embarrassed and frustrated many American nationalists, including George Washington.
Many of the most prominent national leaders, such as Washington, John Adams, John Hancock, and Benjamin Franklin, retired from public life, served as foreign delegates, or held office in state governments; and for the general public, local government and self-rule seemed quite satisfactory.
This served to exacerbate Congress's impotence.
Inherent weaknesses in the confederation's frame of government also frustrated the ability of the government to conduct foreign policy.
In 1786, Thomas Jefferson, concerned over the failure of Congress to fund an American naval force to confront the Barbary pirates, wrote in a diplomatic correspondence to James Monroe that, "It will be said there is no money in the treasury.
There never will be money in the treasury till the Confederacy shows its teeth."
Furthermore, the 1786 Jay–Gardoqui Treaty with Spain also showed weakness in foreign policy.
In this treaty, which was never ratified, the United States was to give up rights to use the Mississippi River for 25 years, which would have economically strangled the settlers west of the Appalachian Mountains.
Finally, due to the Confederation's military weakness, it could not compel the British army to leave frontier forts which were on American soil — forts which, in 1783, the British promised to leave, but which they delayed leaving pending U.S.
implementation of other provisions such as ending action against Loyalists and allowing them to seek compensation.
This incomplete British implementation of the Treaty of Paris would later be resolved by the implementation of Jay's Treaty in 1795, after the federal Constitution came into force.
Under the Articles of Confederation, the central government's power was kept quite limited.
The Confederation Congress could make decisions, but lacked enforcement powers.
Implementation of most decisions, including modifications to the Articles, required unanimous approval of all thirteen state legislatures.
Congress was denied any powers of taxation: it could only request money from the states.
The states often failed to meet these requests in full, leaving both Congress and the Continental Army chronically short of money.
As more money was printed by Congress, the continental dollars depreciated.
In 1779, George Washington wrote to John Jay, who was serving as the president of the Continental Congress, "that a wagon load of money will scarcely purchase a wagon load of provisions."
Mr. Jay and the Congress responded in May by requesting $45 million from the States.
In an appeal to the States to comply, Jay wrote that the taxes were "the price of liberty, the peace, and the safety of yourselves and posterity."
He argued that Americans should avoid having it said "that America had no sooner become independent than she became insolvent" or that "her infant glories and growing fame were obscured and tarnished by broken contracts and violated faith."
The States did not respond with any of the money requested from them.
Congress had also been denied the power to regulate either foreign trade or interstate commerce and, as a result, all of the States maintained control over their own trade policies.
The states and the Confederation Congress both incurred large debts during the Revolutionary War, and how to repay those debts became a major issue of debate following the War.
Some States paid off their war debts and others did not.
Federal assumption of the states' war debts became a major issue in the deliberations of the Constitutional Convention.
Nevertheless, the Confederation Congress did take two actions with long-lasting impact.
The Land Ordinance of 1785 and Northwest Ordinance created territorial government, set up protocols for the admission of new states and the division of land into useful units, and set aside land in each township for public use.
This system represented a sharp break from imperial colonization, as in Europe, and it established the precedent by which the national (later, federal) government would be sovereign and expand westward—as opposed to the existing states doing so under their sovereignty.
The Land Ordinance of 1785 established both the general practices of land surveying in the west and northwest and the land ownership provisions used throughout the later westward expansion beyond the Mississippi River.
Frontier lands were surveyed into the now-familiar squares of land called the township (36 square miles), the section (one square mile), and the quarter section (160 acres).
This system was carried forward to most of the States west of the Mississippi (excluding areas of Texas and California that had already been surveyed and divided up by the Spanish Empire).
Then, when the Homestead Act was enacted in 1867, the quarter section became the basic unit of land that was granted to new settler-farmers.
The Northwest Ordinance of 1787 noted the agreement of the original states to give up northwestern land claims, organized the Northwest Territory and laid the groundwork for the eventual creation of new states.
While it didn't happen under the articles, the land north of the Ohio River and west of the (present) western border of Pennsylvania ceded by Massachusetts, Connecticut, New York, Pennsylvania, and Virginia, eventually became the states of: Ohio, Indiana, Illinois, Michigan, and Wisconsin, and the part of Minnesota east of the Mississippi River.
The Northwest Ordinance of 1787 also made great advances in the abolition of slavery.
New states admitted to the union in this territory would never be slave states.
No new states were admitted to the Union under the Articles of Confederation.
The Articles provided for a blanket acceptance of the Province of Quebec (referred to as "Canada" in the Articles) into the United States if it chose to do so.
It did not, and the subsequent Constitution carried no such special provision of admission.
Additionally, ordinances to admit Frankland (later modified to Franklin), Kentucky, and Vermont to the Union were considered, but none were approved.
Under the Articles of Confederation, the presiding officer of Congress—referred to in many official records as "President of the United States in Congress Assembled"—chaired the Committee of the States when Congress was in recess, and performed other administrative functions.
He was not, however, an executive in the way the later President of the United States is a chief executive, since all of the functions he executed were under the direct control of Congress.
There were 10 presidents of Congress under the Articles.
The first, Samuel Huntington, had been serving as president of the Continental Congress since September 28, 1779.
The peace treaty left the United States independent and at peace but with an unsettled governmental structure.
The Articles envisioned a permanent confederation, but granted to the Congress—the only federal institution—little power to finance itself or to ensure that its resolutions were enforced.
There was no president, no executive agencies, no judiciary and no tax base.
The absence of a tax base meant that there was no way to pay off state and national debts from the war years except by requesting money from the states, which seldom arrived.
Although historians generally agree that the Articles were too weak to hold the fast-growing nation together, they do give credit to the settlement of the western issue, as the states voluntarily turned over their lands to national control.
By 1783, with the end of the British blockade, the new nation was regaining its prosperity.
However, trade opportunities were restricted by the mercantilism of the British and French empires.
The ports of the British West Indies were closed to all staple products which were not carried in British ships.
France and Spain established similar policies.
Simultaneously, new manufacturers faced sharp competition from British products which were suddenly available again.
Political unrest in several states and efforts by debtors to use popular government to erase their debts increased the anxiety of the political and economic elites which had led the Revolution.
The apparent inability of the Congress to redeem the public obligations (debts) incurred during the war, or to become a forum for productive cooperation among the states to encourage commerce and economic development, only aggravated a gloomy situation.
In 1786–87, Shays' Rebellion, an uprising of dissidents in western Massachusetts against the state court system, threatened the stability of state government.
The Continental Congress printed paper money which was so depreciated that it ceased to pass as currency, spawning the expression "not worth a continental".
Congress could not levy taxes and could only make requisitions upon the States.
Less than a million and a half dollars came into the treasury between 1781 and 1784, although the governors had been asked for two million in 1783 alone.
When John Adams went to London in 1785 as the first representative of the United States, he found it impossible to secure a treaty for unrestricted commerce.
Demands were made for favors and there was no assurance that individual states would agree to a treaty.
Adams stated it was necessary for the States to confer the power of passing navigation laws to Congress, or that the States themselves pass retaliatory acts against Great Britain.
Congress had already requested and failed to get power over navigation laws.
Meanwhile, each State acted individually against Great Britain to little effect.
When other New England states closed their ports to British shipping, Connecticut hastened to profit by opening its ports.
By 1787 Congress was unable to protect manufacturing and shipping.
State legislatures were unable or unwilling to resist attacks upon private contracts and public credit.
Land speculators expected no rise in values when the government could not defend its borders nor protect its frontier population.
The idea of a convention to revise the Articles of Confederation grew in favor.
Alexander Hamilton realized while serving as Washington's top aide that a strong central government was necessary to avoid foreign intervention and allay the frustrations due to an ineffectual Congress.
Hamilton led a group of like-minded nationalists, won Washington's endorsement, and convened the Annapolis Convention in 1786 to petition Congress to call a constitutional convention to meet in Philadelphia to remedy the long-term crisis.
The Second Continental Congress approved the Articles for distribution to the states on November 15, 1777.
A copy was made for each state and one was kept by the Congress.
On November 28, the copies sent to the states for ratification were unsigned, and the cover letter, dated November 17, had only the signatures of Henry Laurens and Charles Thomson, who were the President and Secretary to the Congress.
The Articles, however, were unsigned, and the date was blank.
Congress began the signing process by examining their copy of the Articles on June 27, 1778.
They ordered a final copy prepared (the one in the National Archives), and that delegates should inform the secretary of their authority for ratification.
On July 9, 1778, the prepared copy was ready.
They dated it, and began to sign.
They also requested each of the remaining states to notify its delegation when ratification was completed.
On that date, delegates present from New Hampshire, Massachusetts, Rhode Island, Connecticut, New York, Pennsylvania, Virginia and South Carolina signed the Articles to indicate that their states had ratified.
New Jersey, Delaware and Maryland could not, since their states had not ratified.
North Carolina and Georgia also were unable to sign that day, since their delegations were absent.
After the first signing, some delegates signed at the next meeting they attended.
For example, John Wentworth of New Hampshire added his name on August 8.
John Penn was the first of North Carolina's delegates to arrive (on July 10), and the delegation signed the Articles on July 21, 1778.
The other states had to wait until they ratified the Articles and notified their Congressional delegation.
Georgia signed on July 24, New Jersey on November 26, and Delaware on February 12, 1779.
Maryland refused to ratify the Articles until every state had ceded its western land claims.
Chevalier de La Luzerne, French Minister to the United States, felt that the Articles would help strengthen the American government.
In 1780 when Maryland requested France provide naval forces in the Chesapeake Bay for protection from the British (who were conducting raids in the lower part of the bay), he indicated that French Admiral Destouches would do what he could but La Luzerne also “sharply pressed” Maryland to ratify the Articles, thus suggesting the two issues were related.
On February 2, 1781, the much-awaited decision was taken by the Maryland General Assembly in Annapolis.
As the last piece of business during the afternoon Session, "among engrossed Bills" was "signed and sealed by Governor Thomas Sim Lee in the Senate Chamber, in the presence of the members of both Houses... an Act to empower the delegates of this state in Congress to subscribe and ratify the articles of confederation" and perpetual union among the states.
The Senate then adjourned "to the first Monday in August next."
The decision of Maryland to ratify the Articles was reported to the Continental Congress on February 12.
The confirmation signing of the Articles by the two Maryland delegates took place in Philadelphia at noon time on March 1, 1781, and was celebrated in the afternoon.
With these events, the Articles were entered into force and the United States of America came into being as a sovereign federal state.
Congress had debated the Articles for over a year and a half, and the ratification process had taken nearly three and a half years.
Many participants in the original debates were no longer delegates, and some of the signers had only recently arrived.
The Articles of Confederation and Perpetual Union were signed by a group of men who were never present in the Congress at the same time.
The signers and the states they represented were:
Connecticut

Delaware

Georgia

Maryland

Massachusetts Bay

New Hampshire
New Jersey

New York

North Carolina

Pennsylvania

Rhode Island and Providence Plantations

South Carolina

Virginia
Roger Sherman (Connecticut) was the only person to sign all four great state papers of the United States: the Continental Association, the United States Declaration of Independence, the Articles of Confederation and the United States Constitution.
Robert Morris (Pennsylvania) signed three of the great state papers of the United States: the United States Declaration of Independence, the Articles of Confederation and the United States Constitution.
John Dickinson (Delaware), Daniel Carroll (Maryland) and Gouverneur Morris (New York), along with Sherman and Robert Morris, were the only five people to sign both the Articles of Confederation and the United States Constitution (Gouverneur Morris represented Pennsylvania when signing the Constitution).
Original parchment pages of the Articles of Confederation, National Archives and Records Administration.
On January 21, 1786, the Virginia Legislature, following James Madison's recommendation, invited all the states to send delegates to Annapolis, Maryland to discuss ways to reduce interstate conflict.
At what came to be known as the Annapolis Convention, the few state delegates in attendance endorsed a motion that called for all states to meet in Philadelphia in May 1787 to discuss ways to improve the Articles of Confederation in a "Grand Convention."
Although the states' representatives to the Constitutional Convention in Philadelphia were only authorized to amend the Articles, the representatives held secret, closed-door sessions and wrote a new constitution.
The new Constitution gave much more power to the central government, but characterization of the result is disputed.
The general goal of the authors was to get close to a republic as defined by the philosophers of the Age of Enlightenment, while trying to address the many difficulties of the interstate relationships.
Historian Forrest McDonald, using the ideas of James Madison from "Federalist 39", describes the change this way:

In May 1786, Charles Pinckney of South Carolina proposed that Congress revise the Articles of Confederation.
Recommended changes included granting Congress power over foreign and domestic commerce, and providing means for Congress to collect money from state treasuries.
Unanimous approval was necessary to make the alterations, however, and Congress failed to reach a consensus.
The weakness of the Articles in establishing an effective unifying government was underscored by the threat of internal conflict both within and between the states, especially after Shays' Rebellion threatened to topple the state government of Massachusetts.
Historian Ralph Ketcham comments on the opinions of Patrick Henry, George Mason, and other Anti-Federalists who were not so eager to give up the local autonomy won by the revolution:

Historians have given many reasons for the perceived need to replace the articles in 1787.
Jillson and Wilson (1994) point to the financial weakness as well as the norms, rules and institutional structures of the Congress, and the propensity to divide along sectional lines.
Rakove (1988) identifies several factors that explain the collapse of the Confederation.
The lack of compulsory direct taxation power was objectionable to those wanting a strong centralized state or expecting to benefit from such power.
It could not collect customs after the war because tariffs were vetoed by Rhode Island.
Rakove concludes that their failure to implement national measures "stemmed not from a heady sense of independence but rather from the enormous difficulties that all the states encountered in collecting taxes, mustering men, and gathering supplies from a war-weary populace."
The second group of factors Rakove identified derived from the substantive nature of the problems the Continental Congress confronted after 1783, especially the inability to create a strong foreign policy.
Finally, the Confederation's lack of coercive power reduced the likelihood for profit to be made by political means, thus potential rulers were uninspired to seek power.
When the war ended in 1783, certain special interests had incentives to create a new "merchant state," much like the British state people had rebelled against.
In particular, holders of war scrip and land speculators wanted a central government to pay off scrip at face value and to legalize western land holdings with disputed claims.
Also, manufacturers wanted a high tariff as a barrier to foreign goods, but competition among states made this impossible without a central government.
Political scientist David C. Hendrickson writes that two prominent political leaders in the Confederation, John Jay of New York and Thomas Burke of North Carolina believed that "the authority of the congress rested on the prior acts of the several states, to which the states gave their voluntary consent, and until those obligations were fulfilled, neither nullification of the authority of congress, exercising its due powers, nor secession from the compact itself was consistent with the terms of their original pledges."
According to Article XIII of the Confederation, any alteration had to be approved unanimously: 
[T]he Articles of this Confederation shall be inviolably observed by every State, and the Union shall be perpetual; nor shall any alteration at any time hereafter be made in any of them; unless such alteration be agreed to in a Congress of the United States, and be afterwards confirmed by the legislatures of every State.
On the other hand, Article VII of the proposed Constitution stated that it would become effective after ratification by a mere nine states, without unanimity:
The Ratification of the Conventions of nine States, shall be sufficient for the Establishment of this Constitution between the States so ratifying the Same.
The apparent tension between these two provisions was addressed at the time, and remains a topic of scholarly discussion.
In 1788, James Madison remarked (in "Federalist No.
40") that the issue had become moot: "As this objection...has been in a manner waived by those who have criticised the powers of the convention, I dismiss it without further observation."
Nevertheless, it is an interesting historical and legal question whether opponents of the Constitution could have plausibly attacked the Constitution on that ground.
At the time, there were state legislators who argued that the Constitution was not an alteration of the Articles of Confederation, but rather would be a complete replacement so the unanimity rule did not apply.
Moreover, the Confederation had proven woefully inadequate and therefore was supposedly no longer binding.
Modern scholars such as Francisco Forrest Martin agree that the Articles of Confederation had lost its binding force because many states had violated it, and thus "other states-parties did not have to comply with the Articles' unanimous consent rule".
In contrast, law professor Akhil Amar suggests that there may not have really been any conflict between the Articles of Confederation and the Constitution on this point; Article VI of the Confederation specifically allowed side deals among states, and the Constitution could be viewed as a side deal until all states ratified it.
On July 3, 1788, the Congress received New Hampshire's all-important ninth ratification of the proposed Constitution, thus, according to its terms, establishing it as the new framework of governance for the ratifying states.
The following day delegates considered a bill to admit Kentucky into the Union as a sovereign state.
The discussion ended with Congress making the determination that, in light of this development, it would be "unadvisable" to admit Kentucky into the Union, as it could do so "under the Articles of Confederation" only, but not "under the Constitution".
By the end of July 1788, 11 of the 13 states had ratified the new Constitution.
Congress continued to convene under the Articles with a quorum until October.
On Saturday, September 13, 1788, the Confederation Congress voted the resolve to implement the new Constitution, and on Monday, September 15 published an announcement that the new Constitution had been ratified by the necessary nine states, set the first Wednesday in February 1789 for the presidential electors to meet and select a new president, and set the first Wednesday of March 1789 as the day the new government would take over and the government under the Articles of Confederation would come to an end.
On that same September 13, it determined that New York would remain the national capital.
</doc>
<doc id="694" url="https://en.wikipedia.org/wiki?curid=694" title="Asia Minor (disambiguation)">
Asia Minor (disambiguation)

Asia Minor is an alternative name for Anatolia, the westernmost protrusion of Asia, comprising the majority of the Republic of Turkey.
It may also refer to:



</doc>
<doc id="698" url="https://en.wikipedia.org/wiki?curid=698" title="Atlantic Ocean">
Atlantic Ocean

The Atlantic Ocean is the second largest of the world's oceans, with an area of about .
It covers approximately 20 percent of the Earth's surface and about 29 percent of its water surface area.
It separates the "Old World" from the "New World".
The Atlantic Ocean occupies an elongated, S-shaped basin extending longitudinally between Europe and Africa to the east, and the Americas to the west.
As one component of the interconnected global ocean, it is connected in the north to the Arctic Ocean, to the Pacific Ocean in the southwest, the Indian Ocean in the southeast, and the Southern Ocean in the south (other definitions describe the Atlantic as extending southward to Antarctica).
The Equatorial Counter Current subdivides it into the North Atlantic Ocean and the South Atlantic Ocean at about 8°N.
Scientific explorations of the Atlantic include the Challenger expedition, the German Meteor expedition, Columbia University's Lamont-Doherty Earth Observatory and the United States Navy Hydrographic Office.
The oldest known mentions of an "Atlantic" sea come from Stesichorus around mid-sixth century BC (Sch.
A. R.
1.
211): "Atlantikoi pelágei" (Greek: Ἀτλαντικῷ πελάγει; English: 'the Atlantic sea'; etym.
'Sea of Atlantis') and in "The Histories" of Herodotus around 450 BC (Hdt.
1.202.4): "Atlantis thalassa" (Greek: Ἀτλαντὶς θάλασσα; English: 'Sea of Atlantis' or 'the Atlantis sea') where the name refers to "the sea beyond the pillars of Heracles" which is said to be part of the sea that surrounds all land.
Thus, on one hand, the name refers to Atlas, the Titan in Greek mythology, who supported the heavens and who later appeared as a frontispiece in Medieval maps and also lent his name to modern atlases.
On the other hand, to early Greek sailors and in Ancient Greek mythological literature such as the "Iliad" and the "Odyssey", this all-encompassing ocean was instead known as Oceanus, the gigantic river that encircled the world; in contrast to the enclosed seas well known to the Greeks: the Mediterranean and the Black Sea.
In contrast, the term "Atlantic" originally referred specifically to the Atlas Mountains in Morocco and the sea off the Strait of Gibraltar and the North African coast.
The Greek word "thalassa" has been reused by scientists for the huge Panthalassa ocean that surrounded the supercontinent Pangaea hundreds of millions of years ago.
The term "Aethiopian Ocean", derived from Ancient Ethiopia, was applied to the Southern Atlantic as late as the mid-19th century.
During the Age of Discovery, the Atlantic was also known to English cartographers as the Great Western Ocean.
The International Hydrographic Organization (IHO) defined the limits of the oceans and seas in 1953, but some of these definitions have been revised since then and some are not used by various authorities, institutions, and countries, see for example the CIA World Factbook.
Correspondingly, the extent and number of oceans and seas varies.
The Atlantic Ocean is bounded on the west by North and South America.
It connects to the Arctic Ocean through the Denmark Strait, Greenland Sea, Norwegian Sea and Barents Sea.
To the east, the boundaries of the ocean proper are Europe: the Strait of Gibraltar (where it connects with the Mediterranean Sea–one of its marginal seas–and, in turn, the Black Sea, both of which also touch upon Asia) and Africa.
In the southeast, the Atlantic merges into the Indian Ocean.
The 20° East meridian, running south from Cape Agulhas to Antarctica defines its border.
In the 1953 definition it extends south to Antarctica, while in later maps it is bounded at the 60° parallel by the Southern Ocean.
The Atlantic has irregular coasts indented by numerous bays, gulfs and seas.
These include the Baltic Sea, Black Sea, Caribbean Sea, Davis Strait, Denmark Strait, part of the Drake Passage, Gulf of Mexico, Labrador Sea, Mediterranean Sea, North Sea, Norwegian Sea, almost all of the Scotia Sea, and other tributary water bodies.
Including these marginal seas the coast line of the Atlantic measures compared to for the Pacific.
Including its marginal seas, the Atlantic covers an area of or 23.5% of the global ocean and has a volume of or 23.3% of the total volume of the earth's oceans.
Excluding its marginal seas, the Atlantic covers and has a volume of .
The North Atlantic covers (11.5%) and the South Atlantic (11.1%).
The average depth is and the maximum depth, the Milwaukee Deep in the Puerto Rico Trench, is .
The bathymetry of the Atlantic is dominated by a submarine mountain range called the Mid-Atlantic Ridge (MAR).
It runs from 87°N or south of the North Pole to the subantarctic Bouvet Island at 42°S.
The MAR divides the Atlantic longitudinally into two halves, in each of which a series of basins are delimited by secondary, transverse ridges.
The MAR reaches above along most of its length, but is interrupted by larger transform faults at two places: the Romanche Trench near the Equator and the Gibbs Fracture Zone at 53°N.
The MAR is a barrier for bottom water, but at these two transform faults deep water currents can pass from one side to the other.
The MAR rises above the surrounding ocean floor and its rift valley is the divergent boundary between the North American and Eurasian plates in the North Atlantic and the South American and African plates in the South Atlantic.
The MAR produces basaltic volcanoes in Eyjafjallajökull, Iceland, and pillow lava on the ocean floor.
The depth of water at the apex of the ridge is less than in most places, while the bottom of the ridge is three times as deep.
The MAR is intersected by two perpendicular ridges: the Azores–Gibraltar Transform Fault, the boundary between the Nubian and Eurasian plates, intersects the MAR at the Azores Triple Junction, on either side of the Azores microplate, near the 40°N.
A much vaguer, nameless boundary, between the North American and South American plates, intersects the MAR near or just north of the Fifteen-Twenty Fracture Zone, approximately at 16°N.
In the 1870s, the Challenger expedition discovered parts of what is now known as the Mid-Atlantic Ridge, or:

Most of the MAR runs under water but where it reaches the surfaces it has produced volcanic islands.
While nine of these have collectively been nominated a World Heritage Site for their geological value, four of them are considered of "Outstanding Universal Value" based on their cultural and natural criteria: Þingvellir, Iceland; Landscape of the Pico Island Vineyard Culture, Portugal; Gough and Inaccessible Islands, United Kingdom; and Brazilian Atlantic Islands: Fernando de Noronha and Atol das Rocas Reserves, Brazil.
Continental shelves in the Atlantic are wide off Newfoundland, southern-most South America, and north-eastern Europe.
In the western Atlantic carbonate platforms dominate large areas, for example the Blake Plateau and Bermuda Rise.
The Atlantic is surrounded by passive margins except at a few locations where active margins form deep trenches: the Puerto Rico Trench ( maximum depth) in the western Pacific and South Sandwich Trench () in the South Atlantic.
There are numerous submarine canyons off north-eastern North America, western Europe, and north-western Africa.
Some of these canyons extend along the continental rises and farther into the abyssal plains as deep-sea channels.
In 1922 a historic moment in cartography and oceanography occurred.
The USS Stewart used a Navy Sonic Depth Finder to draw a continuous map across the bed of the Atlantic.
This involved little guesswork because the idea of sonar is straight forward with pulses being sent from the vessel, which bounce off the ocean floor, then return to the vessel.
The deep ocean floor is thought to be fairly flat with occasional deeps, abyssal plains, trenches, seamounts, basins, plateaus, canyons, and some guyots.
Various shelves along the margins of the continents constitute about 11% of the bottom topography with few deep channels cut across the continental rise.
The mean depth between 60°N and 60°S is , or close to the average for the global ocean, with a modal depth between .
In the South Atlantic the Walvis Ridge and Rio Grande Rise form barriers to ocean currents.
The Laurentian Abyss is found off the eastern coast of Canada.
Surface water temperatures, which vary with latitude, current systems, and season and reflect the latitudinal distribution of solar energy, range from below to over .
Maximum temperatures occur north of the equator, and minimum values are found in the polar regions.
In the middle latitudes, the area of maximum temperature variations, values may vary by .
From October to June the surface is usually covered with sea ice in the Labrador Sea, Denmark Strait, and Baltic Sea.
The Coriolis effect circulates North Atlantic water in a clockwise direction, whereas South Atlantic water circulates counter-clockwise.
The south tides in the Atlantic Ocean are semi-diurnal; that is, two high tides occur during each 24 lunar hours.
In latitudes above 40° North some east-west oscillation, known as the North Atlantic oscillation, occurs.
On average, the Atlantic is the saltiest major ocean; surface water salinity in the open ocean ranges from 33 to 37 parts per thousand (3.3 – 3.7%) by mass and varies with latitude and season.
Evaporation, precipitation, river inflow and sea ice melting influence surface salinity values.
Although the lowest salinity values are just north of the equator (because of heavy tropical rainfall), in general the lowest values are in the high latitudes and along coasts where large rivers enter.
Maximum salinity values occur at about 25° north and south, in subtropical regions with low rainfall and high evaporation.
The high surface salinity in the Atlantic, on which the Atlantic thermohaline circulation is dependent, is maintained by two processes: the Agulhas Leakage/Rings, which brings salty Indian Ocean waters into the South Atlantic, and the "Atmospheric Bridge", which evaporates subtropical Atlantic waters and exports it to the Pacific.
The Atlantic Ocean consists of four major, upper water masses with distinct temperature and salinity.
The Atlantic Subarctic Upper Water in the northern-most North Atlantic is the source for Subarctic Intermediate Water and North Atlantic Intermediate Water.
North Atlantic Central Water can be divided into the Eastern and Western North Atlantic central Water since the western part is strongly affected by the Gulf Stream and therefore the upper layer is closer to underlying fresher subpolar intermediate water.
The eastern water is saltier because of its proximity to Mediterranean Water.
North Atlantic Central Water flows into South Atlantic Central Water at 15°N.
There are five intermediate waters: four low-salinity waters formed at subpolar latitudes and one high-salinity formed through evaporation.
Arctic Intermediate Water, flows from north to become the source for North Atlantic Deep Water south of the Greenland-Scotland sill.
These two intermediate waters have different salinity in the western and eastern basins.
The wide range of salinities in the North Atlantic is caused by the asymmetry of the northern subtropical gyre and the large number of contributions from a wide range of sources: Labrador Sea, Norwegian-Greenland Sea, Mediterranean, and South Atlantic Intermediate Water.
The North Atlantic Deep Water (NADW) is a complex of four water masses, two that form by deep convection in the open ocean — Classical and Upper Labrador Sea Water — and two that form from the inflow of dense water across the Greenland-Iceland-Scotland sill — Denmark Strait and Iceland-Scotland Overflow Water.
Along its path across Earth the composition of the NADW is affected by other water masses, especially Antarctic Bottom Water and Mediterranean Overflow Water.
The NADW is fed by a flow of warm shallow water into the northern North Atlantic which is responsible for the anomalous warm climate in Europe.
Changes in the formation of NADW have been linked to global climate changes in the past.
Since man-made substances were introduced into the environment, the path of the NADW can be traced throughout its course by measuring tritium and radiocarbon from nuclear weapon tests in the 1960s and CFCs.
The clockwise warm-water North Atlantic Gyre occupies the northern Atlantic, and the counter-clockwise warm-water South Atlantic Gyre appears in the southern Atlantic.
In the North Atlantic, surface circulation is dominated by three inter-connected currents: the Gulf Stream which flows north-east from the North American coast at Cape Hatteras; the North Atlantic Current, a branch of the Gulf Stream which flows northward from the Grand Banks; and the Subpolar Front, an extension of the North Atlantic Current, a wide, vaguely defined region separating the subtropical gyre from the subpolar gyre.
This system of currents transport warm water into the North Atlantic, without which temperatures in the North Atlantic and Europe would plunge dramatically.
North of the North Atlantic Gyre, the cyclonic North Atlantic Subpolar Gyre plays a key role in climate variability.
It is governed by ocean currents from marginal seas and regional topography, rather than being steered by wind, both in the deep ocean and at sea level.
The subpolar gyre forms an important part of the global thermohaline circulation.
Its eastern portion includes eddying branches of the North Atlantic Current which transport warm, saline waters from the subtropics to the north-eastern Atlantic.
There this water is cooled during winter and forms return currents that merge along the eastern continental slope of Greenland where they form an intense (40–50 Sv) current which flows around the continental margins of the Labrador Sea.
A third of this water become parts of the deep portion of the North Atlantic Deep Water (NADW).
The NADW, in its turn, feed the meridional overturning circulation (MOC), the northward heat transport of which is threatened by anthropogenic climate change.
Large variations in the subpolar gyre on a decade-century scale, associated with the North Atlantic oscillation, are especially pronounced in Labrador Sea Water, the upper layers of the MOC.
The South Atlantic is dominated by the anti-cyclonic southern subtropical gyre.
The South Atlantic Central Water originates in this gyre, while Antarctic Intermediate Water originates in the upper layers of the circumpolar region, near the Drake Passage and Falkland Islands.
Both these currents receive some contribution from the Indian Ocean.
On the African east coast the small cyclonic Angola Gyre lies embedded in the large subtropical gyre.
The southern subtropical gyre is partly masked by a wind-induced Ekman layer.
The residence time of the gyre is 4.4–8.5 years.
North Atlantic Deep Water flows southerward below the thermocline of the subtropical gyre.
The Sargasso Sea in the western North Atlantic can be defined as the area where two species of "Sargassum" ("S. fluitans" and "natans") float, an area wide and encircled by the Gulf Stream, North Atlantic Drift, and North Equatorial Current.
This population of seaweed probably originated from Tertiary ancestors on the European shores of the former Tethys Ocean and has, if so, maintained itself by vegetative growth, floating in the ocean for millions of years.
Other species endemic to the Sargasso Sea include the sargassum fish, a predator with algae-like appendages who hovers motionless among the "Sargassum".
Fossils of similar fishes have been found in fossil bays of the former Tethys Ocean, in what is now the Carpathian region, that were similar to the Sargasso Sea.
It is possible that the population in the Sargasso Sea migrated to the Atlantic as the Tethys closed at the end of the Miocene around 17 Ma.
The origin of the Sargasso fauna and flora remained enigmatic for centuries.
The fossils found in the Carpathians in the mid-20th century, often called the "quasi-Sargasso assemblage", finally showed that this assemblage originated in the Carpathian Basin from where it migrated over Sicily to the Central Atlantic where it evolved into modern species of the Sargasso Sea.
The location of the spawning ground for European eels remained unknown for decades.
In the early 19th century it was discovered that the southern Sargasso Sea is the spawning ground for both the European and American eel and that the former migrate more than and the latter .
Ocean currents such as the Gulf Stream transport eel larvae from the Sargasso Sea to foraging areas in North America, Europe, and Northern Africa.
Recent but disputed research suggests that eels possibly use Earth's magnetic field to navigate through the ocean both as larvae and as adults.
Climate is influenced by the temperatures of the surface waters and water currents as well as winds.
Because of the ocean's great capacity to store and release heat, maritime climates are more moderate and have less extreme seasonal variations than inland climates.
Precipitation can be approximated from coastal weather data and air temperature from water temperatures.
The oceans are the major source of the atmospheric moisture that is obtained through evaporation.
Climatic zones vary with latitude; the warmest zones stretch across the Atlantic north of the equator.
The coldest zones are in high latitudes, with the coldest regions corresponding to the areas covered by sea ice.
Ocean currents influence climate by transporting warm and cold waters to other regions.
The winds that are cooled or warmed when blowing over these currents influence adjacent land areas.
The Gulf Stream and its northern extension towards Europe, the North Atlantic Drift is thought to have at least some influence on climate.
For example, the Gulf Stream helps moderate winter temperatures along the coastline of southeastern North America, keeping it warmer in winter along the coast than inland areas.
The Gulf Stream also keeps extreme temperatures from occurring on the Florida Peninsula.
In the higher latitudes, the North Atlantic Drift, warms the atmosphere over the oceans, keeping the British Isles and north-western Europe mild and cloudy, and not severely cold in winter like other locations at the same high latitude.
The cold water currents contribute to heavy fog off the coast of eastern Canada (the Grand Banks of Newfoundland area) and Africa's north-western coast.
In general, winds transport moisture and air over land areas.
Icebergs are common from early February to the end of July across the shipping lanes near the Grand Banks of Newfoundland.
The ice season is longer in the polar regions, but there is little shipping in those areas.
Hurricanes are hazard in the western parts of the North Atlantic during the summer and autumn.
Due to a consistently strong wind shear and a weak Intertropical Convergence Zone, they are practically unknown in the South Atlantic.
The break-up of Pangaea began in the Central Atlantic, between North America and Northwest Africa, where rift basins opened during the Late Triassic and Early Jurassic.
This period also saw the first stages of the uplift of the Atlas Mountains.
The exact timing is controversial with estimates ranging from 200 to 170 Ma.
The opening of the Atlantic Ocean coincided with the initial break-up of the supercontinent Pangaea, both of which were initiated by the eruption of the Central Atlantic Magmatic Province (CAMP), one of the most extensive and voluminous large igneous provinces in Earth's history associated with the Triassic–Jurassic extinction event, one of Earth's major extinction events.
Theoliitic dikes, sills, and lava flows from the CAMP eruption at 200 Ma have been found in West Africa, eastern North America, and northern South America.
The extent of the volcanism has been estimated to of which covered what is now northern and central Brazil.
The formation of the Central American Isthmus closed the Central American Seaway at the end of the Pliocene 2.8 Ma ago.
The formation of the isthmus resulted in the migration and extinction of many land-living animals, known as the Great American Interchange, but the closure of the seaway resulted in a "Great American Schism" as it affected ocean currents, salinity, and temperatures in both the Atlantic and Pacific.
Marine organisms on both sides of the isthmus became isolated and either diverged or went extinct.
Geologically the Northern Atlantic is the area delimited to the south by two conjugate margins, Newfoundland and Iberia, and to the north by the Arctic Eurasian Basin.
The opening of the Northern Atlantic closely followed the margins of its predecessor, the Iapetus Ocean, and spread from the Central Atlantic in six stages: Iberia–Newfoundland, Porcupine–North America, Eurasia–Greenland, Eurasia–North America.
Active and inactive spreading systems in this area are marked by the interaction with the Iceland hotspot.
West Gondwana (South America and Africa) broke up in the Early Cretaceous to form the South Atlantic.
The apparent fit between the coastlines of the two continents was noted on the first maps that included the South Atlantic and it was also the subject of the first computer-assisted plate tectonic reconstructions in 1965.
This magnificent fit, however, has since then proven problematic and later reconstructions have introduced various deformation zones along the shorelines to accommodate the northward-propagating break-up.
Intra-continental rifts and deformations have also been introduced to subdivide both continental plates into sub-plates.
Geologically the South Atlantic can be divided into four segments: Equatorial segment, from 10°N to the Romanche Fracture Zone (RFZ);; Central segment, from RFZ to Florianopolis Fracture Zone (FFZ, north of Walvis Ridge and Rio Grande Rise); Southern segment, from FFZ to the Agulhas-Falkland Fracture Zone (AFFZ); and Falkland segment, south of AFFZ.
In the southern segment the Early Cretaceous (133–130 Ma) intensive magmatism of the Paraná–Etendeka Large Igneous Province produced by the Tristan hotspot resulted in an estimated volume of .
It covered an area of in Brazil, Paraguay, and Uruguay and in Africa.
Dyke swarms in Brazil, Angola, eastern Paraguay, and Namibia, however, suggest the LIP originally covered a much larger area and also indicate failed rifts in all these areas.
Associated offshore basaltic flows reach as far south as the Falkland Islands and South Africa.
Traces of magmatism in both offshore and onshore basins in the central and southern segments have been dated to 147–49 Ma with two peaks between 143–121 Ma and 90–60 Ma.
In the Falkland segment rifting began with dextral movements between the Patagonia and Colorado sub-plates between the Early Jurassic (190 Ma) and the Early Cretaceous (126.7 Ma).
Around 150 Ma sea-floor spreading propagated northward into the southern segment.
No later than 130 Ma rifting had reached the Walvis Ridge–Rio Grande Rise.
In the central segment rifting started to break Africa in two by opening the Benue Trough around 118 Ma.
Rifting in the central segment, however, coincided with the Cretaceous Normal Superchron (also known as the Cretaceous quiet period), a 40 Ma period without magnetic reversals, which makes it difficult to date sea-floor spreading in this segment.
The equatorial segment is the last phase of the break-up, but, because it is located on the Equator, magnetic anomalies cannot be used for dating.
Various estimates date the propagation of sea-floor spreading in this segment to the period 120–96 Ma.
This final stage, nevertheless, coincided with or resulted in the end of continental extension in Africa.
About 50 Ma the opening of the Drake Passage resulted from a change in the motions and separation rate of the South American and Antarctic plates.
First small ocean basins opened and a shallow gateway appeared during the Middle Eocene.
34–30 Ma a deeper seaway developed, followed by an Eocene–Oligocene climatic deterioration and the growth of the Antarctic ice sheet.
An embryonic subduction margin is potentially developing west of Gibraltar.
The Gibraltar Arc in the western Mediterranean is migrating westward into the Central Atlantic where it joins the converging African and Eurasian plates.
Together these three tectonic forces are slowly developing into a new subduction system in the eastern Atlantic Basin.
Meanwhile, the Scotia Arc and Caribbean Plate in the western Atlantic Basin are eastward-propagating subduction systems that might, together with the Gibraltar system, represent the beginning of the closure of the Atlantic Ocean and the final stage of the Atlantic Wilson cycle.
Humans evolved in Africa; first by diverging from other apes around 7 Ma; then developing stone tools around 2.6 Ma; to finally evolve as modern humans around 100 kya.
The earliest evidences for the complex behavior associated with this behavioral modernity has been found in the Greater Cape Floristic Region (GCFR) along the coast of South Africa.
During the latest glacial stages the now-submerged plains of the Agulhas Bank were exposed above sea level, extending the South African coastline farther south by hundreds of kilometers.
A small population of modern humans — probably fewer than a thousand reproducing individuals — survived glacial maxima by exploring the high diversity offered by these Palaeo-Agulhas plains.
The GCFR is delimited to the north by the Cape Fold Belt and the limited space south of it resulted in the development of social networks out of which complex Stone Age technologies emerged.
Human history thus begins on the coasts of South Africa where the Atlantic Benguela Upwelling and Indian Ocean Agulhas Current meet to produce an intertidal zone on which shellfish, fur seal, fish and sea birds provided the necessary protein sources.
The African origin of this modern behaviour is evidenced by 70,000 years-old engravings from Blombos Cave, South Africa.
Mitochondrial DNA (mtDNA) studies indicate that 80–60,000 years ago a major demographic expansion within Africa, derived from a single, small population, coincided with the emergence of behavioral complexity and the rapid MIS 5–4 environmental changes.
This group of people not only expanded over the whole of Africa, but also started to disperse out of Africa into Asia, Europe, and Australasia around 65,000 years ago and quickly replaced the archaic humans in these regions.
During the Last Glacial Maximum (LGM) 20,000 years ago humans had to abandon their initial settlements along the European North Atlantic coast and retreat to the Mediterranean.
Following rapid climate changes at the end of the LGM this region was repopulated by Magdalenian culture.
Other hunter-gatherers followed in waves interrupted by large-scale hazards such as the Laacher See volcanic eruption, the inundation of Doggerland (now the North Sea), and the formation of the Baltic Sea.
The European coasts of the North Atlantic were permanently populated about 9–8.5 thousand years ago.
This human dispersal left abundant traces along the coasts of the Atlantic Ocean.
50 ka-old, deeply stratified shell middens found in Ysterfontein on the western coast of South Africa are associated with the Middle Stone Age (MSA).
The MSA population was small and dispersed and the rate of their reproduction and exploitation was less intense than those of later generations.
While their middens resemble 12–11 ka-old Late Stone Age (LSA) middens found on every inhabited continent, the 50–45 ka-old Enkapune Ya Muto in Kenya probably represents the oldest traces of the first modern humans to disperse out of Africa.
The same development can be seen in Europe.
In La Riera Cave (23–13 ka) in Asturias, Spain, only some 26,600 molluscs were deposited over 10 ka.
In contrast, 8–7 ka-old shell middens in Portugal, Denmark, and Brazil generated thousands of tons of debris and artefacts.
The Ertebølle middens in Denmark, for example, accumulated of shell deposits representing some 50 million molluscs over only a thousand years.
This intensification in the exploitation of marine resources has been described as accompanied by new technologies — such as boats, harpoons, and fish-hooks — because many caves found in the Mediterranean and on the European Atlantic coast have increased quantities of marine shells in their upper levels and reduced quantities in their lower.
The earliest exploitation, however, took place on the now submerged shelves, and most settlements now excavated were then located several kilometers from these shelves.
The reduced quantities of shells in the lower levels can represent the few shells that were exported inland.
During the LGM the Laurentide Ice Sheet covered most of northern North America while Beringia connected Siberia to Alaska.
In 1973 late American geoscientist Paul S. Martin proposed a "blitzkrieg" colonization of the Americas by which Clovis hunters migrated into North America around 13,000 years ago in a single wave through an ice-free corridor in the ice sheet and "spread southward explosively, briefly attaining a density sufficiently large to overkill much of their prey."
Others later proposed a "three-wave" migration over the Bering Land Bridge.
These hypotheses remained the long-held view regarding the settlement of the Americas, a view challenged by more recent archaeological discoveries: the oldest archaeological sites in the Americas have been found in South America; sites in north-east Siberia report virtually no human presence there during the LGM; and most Clovis artefacts have been found in eastern North America along the Atlantic coast.
Furthermore, colonisation models based on mtDNA, yDNA, and atDNA data respectively support neither the "blitzkrieg" nor the "three-wave" hypotheses but they also deliver mutually ambiguous results.
Contradictory data from archaeology and genetics will most likely deliver future hypotheses that will, eventually, confirm each other.
A proposed route across the Pacific to South America could explain early South American finds and another hypothesis proposes a northern path, through the Canadian Arctic and down the North American Atlantic coast.
Early settlements across the Atlantic have been suggested by alternative theories, ranging from purely hypothetical to mostly disputed, including the Solutrean hypothesis and some of the Pre-Columbian trans-oceanic contact theories.
The Norse settlement of the Faroe Islands and Iceland began during the 9th and 10th centuries.
A settlement on Greenland was established before 1000 CE, but contact with it was lost in 1409 and it was finally abandoned during the early Little Ice Age.
This setback was caused by a range of factors: an unsustainable economy resulted in erosion and denudation, while conflicts with the local Inuit resulted in the failure to adapt their Arctic technologies; a colder climate resulted in starvation; and the colony got economically marginalized as the Great Plague and Barbary pirates harvested its victims on Iceland in the 15th century.
Iceland was initially settled 865–930 CE following a warm period when winter temperatures hovered around which made farming favorable at high latitudes.
This did not last, however, and temperatures quickly dropped; at 1080 CE summer temperatures had reached a maximum of .
The "Landnámabók" ("Book of Settlement") records disastrous famines during the first century of settlement — "men ate foxes and ravens" and "the old and helpless were killed and thrown over cliffs" — and by the early 1200s hay had to be abandoned for short-season crops such as barley.
Christopher Columbus discovered the Americas in 1492 under Spanish flag.
Six years later Vasco da Gama reached India under Portuguese flag, by navigating south around the Cape of Good Hope, thus proving that the Atlantic and Indian Oceans are connected.
In 1500, in his voyage to India following Vasco da Gama, Pedro Alvares Cabral reached Brazil, taken by the currents of the South Atlantic Gyre.
Following these explorations, Spain and Portugal quickly conquered and colonized large territories in the New World and forced the Amerindian population into slavery in order to explore the vast quantities of silver and gold they found.
Spain and Portugal monopolized this trade in order to keep other European nations out, but conflicting interests nevertheless led to a series of Spanish-Portuguese wars.
A peace treaty mediated by the Pope divided the conquered territories into Spanish and Portuguese sectors while keeping other colonial powers away.
England, France, and the Dutch Republic enviously watched the Spanish and Portuguese wealth grow and allied themselves with pirates such as Henry Mainwaring and Alexandre Exquemelin.
They could explore the convoys leaving the Americas because prevailing winds and currents made the transport of heavy metals slow and predictable.
In the colonies of the Americas, depredation, disease, and slavery quickly reduced the indigenous population of the Americas to the extent that the Atlantic slave trade had to be introduced to replace them — a trade that became norm and an integral part of the colonization.
Between the 15th century and 1888, when Brazil became the last part of the Americas to end slave trade, an estimated ten million Africans were exported as slaves, most of them destined for agricultural labour.
The slave trade was officially abolished in the British Empire and the United States in 1808, and slavery itself was abolished in the British Empire in 1838 and in the United States in 1865 after the Civil War.
From Columbus to the Industrial Revolution Trans-Atlantic trade, including colonialism and slavery, became crucial for Western Europe.
For European countries with a direct access to the Atlantic (including Britain, France, the Netherlands, Portugal, and Spain) 1500–1800 was a period of sustained growth during which these countries grew richer than those in Eastern Europe and Asia.
Colonialism evolved as part of the Trans-Atlantic trade, but this trade also strengthened the position of merchant groups at the expense of monarchs.
Growth was more rapid in non-absolutist countries, such as Britain and the Netherlands, and more limited in absolutist monarchies, such as Portugal, Spain, and France, where profit mostly or exclusively benefited the monarchy and its allies.
Trans-Atlantic trade also resulted in an increasing urbanization: in European countries facing the Atlantic urbanization grew from 8% in 1300, 10.1% in 1500, to 24.5% in 1850; in other European countries from 10% in 1300, 11.4% in 1500, to 17% in 1850.
Likewise, GDP doubled in Atlantic countries but rose by only 30% in the rest of Europe.
By end of the 17th century the volume of the Trans-Atlantic trade had surpassed that of the Mediterranean trade.
The Atlantic has contributed significantly to the development and economy of surrounding countries.
Besides major transatlantic transportation and communication routes, the Atlantic offers abundant petroleum deposits in the sedimentary rocks of the continental shelves.
The Atlantic harbors petroleum and gas fields, fish, marine mammals (seals and whales), sand and gravel aggregates, placer deposits, polymetallic nodules, and precious stones.
Gold deposits are a mile or two under water on the ocean floor, however the deposits are also encased in rock that must be mined through.
Currently, there is no cost-effective way to mine or extract gold from the ocean to make a profit.
Various international treaties attempt to reduce pollution caused by environmental threats such as oil spills, marine debris, and the incineration of toxic wastes at sea.
The shelves of the Atlantic hosts one of the world's richest fishing resources.
The most productive areas include the Grand Banks of Newfoundland, the Scotian Shelf, Georges Bank off Cape Cod, the Bahama Banks, the waters around Iceland, the Irish Sea, the Bay of Fundy, the Dogger Bank of the North Sea, and the Falkland Banks.
Fisheries have, however, undergone significant changes since the 1950s and global catches can now be divided into three groups of which only two are observed in the Atlantic: fisheries in the Eastern Central and South-West Atlantic oscillate around a globally stable value, the rest of the Atlantic is in overall decline following historical peaks.
The third group, "continuously increasing trend since 1950", is only found in the Indian Ocean and Western Pacific.
In the North-East Atlantic total catches decreased between the mid-1970s and the 1990s and reached 8.7 million tons in 2013.
Blue whiting reached a 2.4 million tons peak in 2004 but was down to 628,000 tons in 2013.
Recovery plans for cod, sole, and plaice have reduced mortality in these species.
Arctic cod reached its lowest levels in the 1960s–1980s but is now recovered.
Arctic saithe and haddock are considered fully fished; Sand eel is overfished as was capelin which has now recovered to fully fished.
Limited data makes the state of redfishes and deep-water species difficult to assess but most likely they remain vulnerable to overfishing.
Stocks of northern shrimp and Norwegian lobster are in good condition.
In the North-East Atlantic 21% of stocks are considered overfished.
In the North-West Atlantic landings have decreased from 4.2 million tons in the early 1970s to 1.9 million tons in 2013.
During the 21st century some species have shown weak signs of recovery, including Greenland halibut, yellowtail flounder, Atlantic halibut, haddock, spiny dogfish, while other stocks shown no such signs, including cod, witch flounder, and redfish.
Stocks of invertebrates, in contrast, remain at record levels of abundance.
31% of stocks are overfished in the North-west Atlantic.
In 1497 John Cabot became the first to explore mainland North America and one of his major discoveries was the abundant resources of Atlantic cod off Newfoundland.
Referred to as "Newfoundland Currency" this discovery yielded some 200 million tons of fish over five centuries.
In the late 19th and early 20th centuries new fisheries started to exploit haddock, mackerel, and lobster.
From the 1950s to the 1970s the introduction of European and Asian distant-water fleets in the area dramatically increased the fishing capacity and number of exploited species.
It also expanded the exploited areas from near-shore to the open sea and to great depths to include deep-water species such as redfish, Greenland halibut, witch flounder, and grenadiers.
Overfishing in the area was recognised as early as the 1960s but, because this was occurring on international waters, it took until the late 1970s before any attempts to regulate was made.
In the early 1990s this finally resulted in the collapse of the Atlantic northwest cod fishery.
The population of a number of deep-sea fishes also collapsed in the process, including American plaice, redfish, and Greenland halibut, together with flounder and grenadier.
In the Eastern Central Atlantic small pelagic fishes constitute about 50% of landings with sardine reaching 0.6–1.0 million tons per year.
Pelagic fish stocks are considered fully fishes or overfished, with sardines south of Cape Bojador the notable exception.
Almost half of stocks are fished at biologically unsustainable levels.
Total catches have been fluctuating since the 1970s; reaching 3.9 million tons in 2013 or slightly less than the peak production in 2010.
In the Western Central Atlantic catches have been decreasing since 2000 and reached 1.3 million tons in 2013.
The most important species in the area, Gulf menhaden, reached a million tons in the mid-1980s but only half a million tons in 2013 and is now considered fully fished.
Round sardinella was an important species in the 1990s but is now considered overfished.
Groupers and snappers are overfished and northern brown shrimp and American cupped oyster are considered fully fished approaching overfished.
44% of stocks are being fished at unsustainable levels.
In the South-East Atlantic catches have decreased from 3.3 million tons in the early 1970s to 1.3 million tons in 2013.
Horse mackerel and hake are the most important species, together representing almost half of the landings.
Off South Africa and Namibia deep-water hake and shallow-water Cape hake have recovered to sustainable levels since regulations were introduced in 2006 and the states of Southern African pilchard and anchovy have improved to fully fished in 2013.
In the South-West Atlantic a peak was reached in the mid-1980s and catches now fluctuate between 1.7 and 2.6 million tons.
The most important species, the Argentine shortfin squid, which reached half a million tons in 2013 or half the peak value, is considered fully fished to overfished.
Another important species was the Brazilian sardinella, with a production of 100,000 tons in 2013 it is now considered overfished.
Half the stocks in this area are being fished at unsustainable levels: Whitehead’s round herring has not yet reached fully fished but Cunene horse mackerel is overfished.
The sea snail perlemoen abalone is targeted by illegal fishing and remain overfished.
Endangered marine species include the manatee, seals, sea lions, turtles, and whales.
Drift net fishing can kill dolphins, albatrosses and other seabirds (petrels, auks), hastening the fish stock decline and contributing to international disputes.
Municipal pollution comes from the eastern United States, southern Brazil, and eastern Argentina; oil pollution in the Caribbean Sea, Gulf of Mexico, Lake Maracaibo, Mediterranean Sea, and North Sea; and industrial waste and municipal sewage pollution in the Baltic Sea, North Sea, and Mediterranean Sea.
North Atlantic hurricane activity has increased over past decades because of increased sea surface temperature (SST) at tropical latitudes, changes that can be attributed to either the natural Atlantic Multidecadal Oscillation (AMO) or to anthropogenic climate change.
A 2005 report indicated that the Atlantic meridional overturning circulation (AMOC) slowed down by 30% between 1957 and 2004.
If the AMO were responsible for SST variability, the AMOC would have increased in strength, which is apparently not the case.
Furthermore, it is clear from statistical analyses of annual tropical cyclones that these changes do not display multidecadal cyclicity.
Therefore, these changes in SST must be caused by human activities.
The ocean mixed layer plays an important role heat storage over seasonal and decadal time-scales, whereas deeper layers are affected over millennia and has a heat capacity about 50 times that of the mixed layer.
This heat uptake provides a time-lag for climate change but it also results in a thermal expansion of the oceans which contribute to sea-level rise.
21st century global warming will probably result in an equilibrium sea-level rise five times greater than today, whilst melting of glaciers, including that of the Greenland ice-sheet, expected to have virtually no effect during the 21st century, will probably result in a sea-level rise of 3–6 m over a millennium.
On 7 June 2006, Florida's wildlife commission voted to take the manatee off the state's endangered species list.
Some environmentalists worry that this could erode safeguards for the popular sea creature.
Marine pollution is a generic term for the entry into the ocean of potentially hazardous chemicals or particles.
The biggest culprits are rivers and with them many agriculture fertilizer chemicals as well as livestock and human waste.
The excess of oxygen-depleting chemicals leads to hypoxia and the creation of a dead zone.
Marine debris, which is also known as marine litter, describes human-created waste floating in a body of water.
Oceanic debris tends to accumulate at the center of gyres and coastlines, frequently washing aground where it is known as beach litter.
</doc>
<doc id="700" url="https://en.wikipedia.org/wiki?curid=700" title="Arthur Schopenhauer">
Arthur Schopenhauer

Arthur Schopenhauer ( ; ; 22 February 1788 – 21 September 1860) was a German philosopher.
He is best known for his 1818 work "The World as Will and Representation" (expanded in 1844), wherein he characterizes the phenomenal world as the product of a blind and insatiable metaphysical will.
Proceeding from the transcendental idealism of Immanuel Kant, Schopenhauer developed an atheistic metaphysical and ethical system that has been described as an exemplary manifestation of philosophical pessimism, rejecting the contemporaneous post-Kantian philosophies of German idealism.
Schopenhauer was among the first thinkers in Western philosophy to share and affirm significant tenets of Eastern philosophy (e.g., asceticism, the world-as-appearance), having initially arrived at similar conclusions as the result of his own philosophical work.
Though his work failed to garner substantial attention during his life, Schopenhauer has had a posthumous impact across various disciplines, including philosophy, literature, and science.
His writing on aesthetics, morality, and psychology influenced thinkers and artists throughout the 19th and 20th centuries.
Those who cited his influence include Friedrich Nietzsche, Richard Wagner, Leo Tolstoy, Ludwig Wittgenstein, Erwin Schrödinger, Otto Rank, Gustav Mahler, Joseph Campbell, Albert Einstein, Carl Jung, Thomas Mann, Émile Zola, George Bernard Shaw, Jorge Luis Borges and Samuel Beckett.
Schopenhauer was born on 22 February 1788, in the city of Danzig (then part of the Polish–Lithuanian Commonwealth; present day Gdańsk, Poland) on Heiligegeistgasse (known in the present day as Św.
Ducha 47), the son of Johanna Schopenhauer (née Trosiener) and Heinrich Floris Schopenhauer, both descendants of wealthy German-Dutch patrician families.
Both of them weren't very religious, supported the French Revolution, were republican, cosmopolitan and Anglophile.
When Danzig became part of Prussia in 1793, Heinrich moved to Hamburg - a free city with republican constitution, protected by Britain and Holland against Prussian aggression - although his firm continued trading in Danzig where most of their extended families remained.
Adele, Arthur's only sibling was born on 12 July 1797.
In 1797 Arthur was sent to Le Havre to live for two years with the family of his father's business associate, Grégoire de Blésimaire.
He seemed to enjoy his stay there, learned to speak French fluently and started a friendship with Jean Anthime Grégoire de Blésimaire, his peer, which lasted for a large part of their lives.
As early as 1799, Arthur started playing the flute.
In 1803 he joined his parents on their long tour of Holland, Britain, France, Switzerland, Austria and Prussia; it was mostly a pleasure tour although Heinrich also visited some of his business associates.
Heinrich gave his son a choice - he could stay at home and start preparations for university education, or he could travel with them and then continue his merchant education.
Arthur would later deeply regret his choice because he found his merchant training tedious.
He spent twelve weeks of the tour attending a school in Wimbledon where he was very unhappy and appalled by very strict but intellectually shallow Anglican religiosity which he would continue to sharply criticize later in life despite his general Anglophilia.
He was also under great pressure from his father who became very critical of his educational results.
In fact Heinrich Floris became so fussy that even his wife started to doubt his mental health.
In 1805, Heinrich Floris died by drowning in a canal by their home in Hamburg.
Although it was possible that his death was accident, his wife and son believed that it was suicide because he was very prone to unsociable behavior, anxiety and depression which became especially pronounced in his last months of life.
Arthur showed similar moodiness since his youth and often acknowledged that he inherited it from his father; there were also several other instances of serious mental health issues on his father's side of family.
His mother Johanna was generally described as vivacious and sociable.
Despite the hardships, Schopenhauer seemed to like his father and later mentioned him always in a positive light.
Heinrich Schopenhauer left the family with a decent inheritance that was split in three among Johanna and the children.
Arthur Schopenhauer would be entitled to control of his part when he reached the age of majority.
He invested it conservatively in government bonds and earned annual interest that was more than double the salary of a university professor.
Arthur endured two long years of drudgery as a merchant in honor of his dead father, and because of his own doubts about being too old to start a life of a scholar.
Most of his prior education was practical merchant training and he had some trouble with learning Latin which was a prerequisite for any academic career.
His mother soon moved with his sister Adele to Weimar—then the centre of German literature—to enjoy social life among celebrated writers and artists.
Arthur lived in Hamburg with his friend Jean Anthime who was also studying to become a merchant.
After quitting his merchant apprenticeship, with some encouragement from his mother, he dedicated himself to studies at the Gotha gymnasium () in Saxe-Gotha-Altenburg, but he also enjoyed social life among local nobility spending large amounts of money which caused concern to his frugal mother.
He left Gymnasium after writing a satirical poem about one of the lecturers.
Although Arthur claimed that he left voluntarily, his mother's letter indicates that he was expelled.
He moved to Weimar but didn't live with his mother who even tried to discourage him from coming by explaining that they wouldn't get along very well.
Their relationship deteriorated even further due to their temperamental differences.
He accused his mother of being financially irresponsible, flirtatious and seeking to remarry, which he considered an insult to his father's memory.
His mother, while professing her love to him, criticized him sharply for being moody, tactless, and argumentative—and urged him to improve his behavior so he would not alienate people.
Arthur concentrated on his studies which were now going very well and he also enjoyed the usual social life such as balls, parties and theater.
By that time Johanna's famous salon was well established among local intellectuals and dignitaries, most celebrated of them being Goethe.
Arthur attended her parties, usually when he knew that Goethe would be there—though the famous writer and statesman didn't even seem to notice the young and unknown student.
It is possible that Goethe kept distance because Johanna warned him about her son's depressive and combative nature, or because Goethe was then on bad terms with Arthur's language instructor and roommate, Franz Passow.
Schopenhauer was also captivated by the beautiful Karoline Jagemann, mistress of Karl August, Grand Duke of Saxe-Weimar-Eisenach, and he wrote to her his only known love poem.
Despite his later celebration of asceticism and negative views of sexuality, Schopenhauer occasionally had sexual affairs, usually with women of lower social status, such as servants, actresses, and sometimes even paid prostitutes.
In a letter to his friend Anthime he claims that such affairs continued even in his mature age and admits that he had two out-of-wedlock daughters (born in 1819 and 1836), both of whom died in infancy.
In their youthful correspondence Arthur and Anthime were somewhat boastful and competitive about their sexual exploits—but Schopenhauer seemed aware that women usually didn’t find him very charming or physically attractive, and his desires often remained unfulfilled.
He left Weimar to become a student at the University of Göttingen in 1809.
There are no written reasons about why Schopenhauer chose that university instead of then more famous University of Jena but Göttingen was known as a more modern, scientifically oriented, with less attention given to theology.
Law or medicine were usual choices for young men of Schopenhauer's status who also needed career and income; he choose medicine due to his scientific interests.
Among his notable professors were Bernhard Friedrich Thibaut, Arnold Hermann Ludwig Heeren, Johann Friedrich Blumenbach, Friedrich Stromeyer, Heinrich Adolf Schrader, Johann Tobias Mayer and Konrad Johann Martin Langenbeck.
He studied metaphysics, psychology and logic under Gottlob Ernst Schulze, the author of "Aenesidemus", who made a strong impression and advised him to concentrate on Plato and Immanuel Kant.
He decided to switch from medicine to philosophy around 1810-11 and he left Göttingen which didn't have a strong philosophy program (besides Schulze the only other philosophy professor was Friedrich Bouterwek whom Schopenhauer disliked).
He didn't regret his medicinal and scientific studies.
He claimed that they were necessary for a philosopher, and even in Berlin he attended more lectures in sciences than in philosophy.
During his days at Göttingen, he spent a lot of time studying, but also continued his flute playing and social life.
His friends included Friedrich Gotthilf Osann, Karl Witte, Christian Charles Josias von Bunsen, and William Backhouse Astor Sr..

He arrived to the newly founded University of Berlin for the winter semester of 1811-12.
At the same time his mother just started her literary career; she published her first book in 1810, a biography of her friend Karl Ludwig Fernow, which was a critical success.
Arthur attended lectures by the prominent post-Kantian philosopher Johann Gottlieb Fichte but quickly found many points of disagreement with his "Wissenschaftslehre" and he also found his lectures tedious and hard to understand.
He later mentioned Fichte only in critical, negative terms—seeing his philosophy as a lower quality version of Kant's and considering it useful only because Fichte's poor arguments unintentionally highlighted some failings of Kantianism.
He also attended the lectures of the famous theologian Friedrich Schleiermacher whom he also quickly came to dislike.
His notes and comments on Schleiermacher's lectures show that Schopenhauer was becoming very critical of religion and moving towards atheism.
He learned a lot by self-directed reading; besides Plato, Kant and Fichte he also read the works of Schelling, Fries, Jacobi, Bacon, Locke, and a lot of current scientific literature.
He attended philological courses by August Böckh and Friedrich August Wolf and continued his naturalistic interests with courses by Martin Heinrich Klaproth, Paul Erman, Johann Elert Bode, Ernst Gottfried Fischer, Johann Horkel, Friedrich Christian Rosenthal and Hinrich Lichtenstein (Lichtenstein was also a friend whom he met at one of his mother's parties in Weimar).
Schopenhauer left Berlin in a rush in 1813 fearing that the city could be attacked and that he could be pressed into military service as Prussia just joined the war against France.
He returned to Weimar but left after less than a month disgusted by the fact that his mother was now living with her supposed lover, Georg Friedrich Conrad Ludwig Müller von Gerstenbergk, a civil servant fourteen years younger than her; he considered the relationship an act of infidelity to his father's memory.
He settled for a while in Rudolstadt hoping that no army would pass through the small town.
He spent his time in solitude, hiking in the mountains and the Thuringian forest and writing his dissertation, "On the Fourfold Root of the Principle of Sufficient Reason".
He completed his dissertation at about the same time as the French army was defeated at the Battle of Leipzig.
He became irritated by the arrival of soldiers to the town and accepted his mother's invitation to visit her in Weimar.
She tried to convince him that her relationship with Gerstenbergk was platonic and that she had no intentions of remarrying.
But Schopenhauer remained suspicious and often came in conflict with Gerstenbergk because he considered him untalented, pretentious, and nationalistic.
His mother just published her second book, "Reminiscences of a Journey in the Years 1803, 1804, and 1805", a description of their family tour of Europe, which quickly became a hit.
She found his dissertation incomprehensible and said it was unlikely that anyone would ever buy a copy.
In a fit of temper Arthur told her that people would read his work long after the "rubbish" she wrote was totally forgotten.
In fact, although they considered her novels of dubious quality, the Brockhaus publishing firm held her in high esteem because they consistently sold well.
Hans Brockhaus (1888-1965) later claimed that his predecessors "...saw nothing in this manuscript, but wanted to please one of our best-selling authors by publishing her son's work.
We published more and more of her son Arthur's work and today nobody remembers Johanna, but her son's works are in steady demand and contribute to Brockhaus'[s] reputation."
He kept large portraits of the pair in his office in Leipzig for the edification of his new editors.
Also contrary to his mother's prediction, Schopenhauer's dissertation made an impression on Goethe to whom he sent it as a gift.
Although it is doubtful that Goethe agreed with Schopenhauer's philosophical positions he was impressed by his intellect and extensive scientific education.
Their subsequent meetings and correspondence were a great honor to a young philosopher who was finally acknowledged by his intellectual hero.
They mostly discussed Goethe's newly published (and somewhat lukewarmly received) work on color theory.
Schopenhauer soon started writing his own treatise on the subject, "On Vision and Colors", which in many points differed from his teacher's.
Although they remained polite towards each other, their growing theoretical disagreements – and especially Schopenhauer's tactless criticisms and extreme self-confidence – soon made Goethe become distant again and after 1816 their correspondence became less frequent.
Schopenhauer later admitted that he was greatly hurt by this rejection, but he continued to praise Goethe, and considered his color theory a great introduction to his own.
Another important experience during his stay in Weimar was his acquaintance with Friedrich Majer – a historian of religion, orientalist and disciple of Herder – who introduced him to the Eastern philosophy.
Schopenhauer was immediately impressed by the "Upanishads" and the Buddha and put them at par with Plato and Kant.
He continued his studies by reading the "Bhagavad Gita", an amateurish German journal "Asiatisches Magazin" and "Asiatick Researches" by The Asiatic Society.
Although he loved Hindu texts he was more interested in Buddhism which he came to regard as the best religion.
However, his early studies were constrained by the lack of adequate literature, and were mostly restricted to Early Buddhism.
He also claimed that he formulated most of his ideas independently, and only later realized the similarities with Buddhism.
As the relationship with his mother fell to a new low he left Weimar and moved to Dresden in May 1814.
He continued his philosophical studies, enjoyed the cultural life, socialized with intellectuals and engaged in sexual affairs.
His friends in Dresden were Johann Gottlob von Quandt, Friedrich Laun, Karl Christian Friedrich Krause and Ludwig Sigismund Ruhl, a young painter who made a romanticized portrait of him in which he improved some of Schopenhauer's unattractive physical features.
His criticisms of local artists occasionally caused public quarrels when he ran into them in public.
However, his main occupation during his stay in Dresden was his seminal philosophical work, "The World as Will and Representation", which he started writing in 1814 and finished in 1818.
He was recommended to Friedrich Arnold Brockhaus by Baron Ferdinand von Biedenfeld, an acquaintance of his mother.
Although the publisher accepted his manuscript, Schopenhauer made a poor impression because of his quarrelsome and fussy attitude and very poor sales of the book after it was published in December 1818.
In September 1818, while waiting for his book to be published and conveniently escaping an affair with a maid that caused an unwanted pregnancy, Schopenhauer left Dresden for a yearlong vacation in Italy.
He visited Venice, Bologna, Florence, Naples and Milan, travelling alone or accompanied by mostly English tourists he met.
He spent winter months in Rome where he accidentally met his acquaintance Karl Witte and engaged in numerous quarrels with German tourists in Caffe Greco, among them Johann Friedrich Böhmer who also mentioned his insulting remarks and unpleasant character.
He enjoyed art, architecture, ancient ruins, attended plays and operas, continued his philosophical contemplation and love affairs.
One of his affairs supposedly became serious, and for a while he contemplated marriage to a rich Italian noblewoman—but despite his mentioning this several times, no details are known and it may have been Schopenhauer exaggerating.
He corresponded regularly with his sister Adele and became close to her as her relationship with Johanna and Gerstenbergk also deteriorated.
She informed him about their financial troubles as the banking house of A. L. Muhl in Danzig – in which her mother invested their whole savings and Arthur a third of his – was near bankruptcy.
Arthur offered to share his assets but his mother refused and became further enraged by his insulting comments.
The women managed to receive only thirty percent of their savings while Arthur, using his business knowledge, took a suspicious and aggressive stance towards the banker and eventually received his part in full.
The affair additionally worsened the relationships among all three members of Schopenhauer family.
He shortened his stay in Italy because of the trouble with Muhl and returned to Dresden.
Disturbed by the financial risk and the lack of responses to his book he decided to take an academic position since it provided him both with income and the opportunity to promote his views.
He contacted his friends at universities in Heidelberg, Göttingen and Berlin and found Berlin most attractive.
He scheduled his lectures to coincide with those of the famous philosopher G. W. F. Hegel, whom Schopenhauer described as a "clumsy charlatan".
He was especially appalled by Hegel's supposedly poor knowledge of natural sciences and tried to engage him in a quarrel about it already at his test lecture in March 1820.
Hegel was also facing political suspicions at the time when many progressive professors were fired, while Schopenhauer carefully mentioned in his application that he had no interest in politics.
Despite their differences and the arrogant request to schedule lectures at the same time as his own, Hegel still voted to accept Schopenhauer to the university.
However, only five students turned up to Schopenhauer's lectures, and he dropped out of academia.
A late essay, "On University Philosophy", expressed his resentment towards the work conducted in academies.
After his academic failure he continued to travel extensively, visiting Leipzig, Nuremberg, Stuttgart, Schaffhausen, Vevey, Milan and spending eight months in Florence.
However, before he left for his three-year travel, he had an incident with his Berlin neighbor, forty-seven-year-old seamstress Caroline Louise Marquet.
The details of the August 1821 incident are unknown.
He claimed that he just pushed her from his entrance after she rudely refused to leave, and she purposely fell on the ground so she could sue him.
She claimed that he attacked her so violently that she had become paralyzed on her right side and unable to work.
She immediately sued him, and the process lasted until May 1827, when a court found Schopenhauer guilty and forced him to pay her an annual pension until her death in 1842.
Schopenhauer enjoyed Italy, where he studied art and socialized with Italian and English nobles.
It was his last visit to the country.
He left for Munich and stayed there for a year, mostly recuperating from various health issues, some of them possibly caused by venereal diseases (the treatment his doctor used suggests syphilis).
He contacted publishers offering to translate Hume into German and Kant into English but his proposals were declined.
Returning to Berlin he began to study Spanish in order to read some of his favorite authors in their original language.
He liked Pedro Calderón de la Barca, Lope de Vega, Miguel de Cervantes, and especially Baltasar Gracián.
He also made failed attempts to publish his translations of their works.
Few attempts to revive his lectures – again scheduled at the same time as Hegel's – also failed, as did his inquiries about relocating to other universities.
During his Berlin years Schopenhauer occasionally mentioned his desire to marry and have a family.
For a while he was unsuccessfully courting 17-year-old Flora Weiss, who was 22 years younger than him.
His unpublished writings from that time show that he was already very critical of monogamy but still not advocating polygyny – instead musing about a polyamorous relationship he called "tetragamy".
He had an on and off relationship with a young dancer Caroline Richter (she also used surname Medon after one of her ex-lovers).
They met when he was 33 and she was 19 and working at the Berlin Opera.
She already had numerous lovers and an out-of-wedlock son, and later gave birth to another son, this time to an unnamed foreign diplomat.
(She soon had another pregnancy but it was stillborn).
As Schopenhauer was preparing to escape Berlin in 1831, due to cholera epidemic, he offered to take her with him on the condition that she leaves her young son.
She refused and he went alone; in his will he left her a significant sum of money but insisted that it should not be in any way spent on her second son.
Schopenhauer claimed that in his last year in Berlin he had a prophetic dream which urged him to escape the city.
As he arrived in his new home in Frankfurt he supposedly had another supernatural experience, an apparition of his dead father and his mother who was still alive.
This experience led him to spend some time investigating paranormal phenomena and magic.
He was quite critical of the available studies and claimed that they were mostly ignorant or fraudulent, but he did believe that there are authentic cases of such phenomena and tried to explain them through his metaphysics as manifestations of the will.
Upon his arrival in Frankfurt he experienced a period of depression and declining health.
He renewed his correspondence with his mother, and she seemed concerned that he might commit suicide like his father.
By now Johanna and Adele were living very modestly.
Johanna's writing didn’t bring her much income, and her popularity was waning.
Their correspondence remained reserved, and Arthur Schopenhauer seemed undisturbed by her death in 1838.
His relationship with his sister grew closer and he corresponded with her until she died in 1849.
In July 1832 Schopenhauer left Frankfurt for Mannheim but returned in July 1833 to remain there for the rest of his life, except for a few short journeys.
He lived alone except for a succession of pet poodles named Atman and Butz.
In 1836, he published "On the Will in Nature".
In 1836 he sent his essay "On the Freedom of the Will" to the contest of the "Royal Norwegian Society of Sciences" and won the prize next year.
He sent another essay, "On the Basis of Morality", to the "Royal Danish Society for Scientific Studies" but didn’t win the prize despite being the only contestant.
The Society was appalled that several distinguished contemporary philosophers were mentioned in a very offensive manner, claimed that the essay missed the point and that the arguments were not adequate.
Schopenhauer, who was very self-confident that he will win, was enraged by this rejection.
He published both essays as "The Two Basic Problems of Ethics" and in the preface to the second edition of this book, in 1860, he was still pouring insults on Royal Danish Society.
First edition, published in 1841, again failed to draw attention to his philosophy.
Two years later, after some negotiations, he managed to convince his publisher, Brockhaus, to print the second, updated edition of "The World as Will and Representation".
The book was again mostly ignored and few reviews were mixed or negative.
However, Schopenhauer did start to attract some followers, mostly outside academia, among practical professionals (several of them were lawyers) who pursued private philosophical studies.
He jokingly referred to them as "evangelists" and "apostles".
One of the most active early followers was Julius Frauenstädt who wrote numerous articles promoting Schopenhauer's philosophy.
He was also instrumental in finding another publisher after Brockhaus refused to publish "Parerga and Paralipomena" believing that it would be another failure.
Though Schopenhauer later stopped corresponding with him, claiming that he did not adhere closely enough to his ideas, Frauenstädt continued to promote Schopehnauer's work.
They renewed their communication in 1859 and Schopenhauer named him heir for his literary estate.
He also became the editor of the first collected works of Schopenhauer.
In 1848 Schopenhauer witnessed violent upheaval in Frankfurt after General Hans Adolf Erdmann von Auerswald and Prince Felix Lichnowsky were murdered.
He became worried for his own safety and property.
Even earlier in life he had such worries and kept a sword and loaded pistols near his bed to defend himself from thieves.
He gave a friendly welcome to Austrian soldiers who wanted to shoot revolutionaries from his window and as they were leaving he gave one of the officers his opera glasses to help him monitor rebels.
The rebellion passed without any loss to Schopenhauer and he later praised Alfred I, Prince of Windisch-Grätz for restoring order.
He even modified his will, leaving a large part of his property to a Prussian fund that helped soldiers who became invalids while fighting rebellion in 1848 or the families of soldiers who died in battle.
As Young Hegelians were advocating change and progress Schopenhauer claimed that misery is natural for humans—and that even if some utopian society were established, people would still fight each other out of boredom, or would starve due to overpopulation.
In 1851 Schopenhauer published "Parerga and Paralipomena", which, as the title says, contains essays that are supplementary to his main work, and are mostly comprehensible to readers unfamiliar with his earlier philosophy.
It was his first successful, widely read book, partly due to the work of his disciples who wrote praising reviews.
The essays that proved most popular were the ones that actually didn’t contain the basic philosophical ideas of his system.
Many academic philosophers considered him a great stylist and cultural critic but didn’t take his philosophy seriously.
His early critics liked to point out similarities of his ideas to those Fichte and Schelling, or claim that there are numerous contradictions in his philosophy.
Both criticisms enraged Schopenhauer.
However, he was becoming less interested in intellectual fights, but encouraged his disciples to do so.
His private notes and correspondence show that he acknowledged some of the criticisms regarding contradictions, inconsistencies, and vagueness in his philosophy, but claimed that he wasn’t concerned about harmony and agreement in his propositions and that some of his ideas shouldn’t be taken literally but instead as metaphors.
Academic philosophers were also starting to notice his work.
In 1856 University of Leipzig sponsored an essay contest about Schopenhauer's philosophy which was won by Rudolf Seydel’s very critical essay.
Schopenhauer's friend Jules Lunteschütz made a first of his four portraits of him – which Schopenhauer didn’t particularly like – that was soon sold to a wealthy landowner Carl Ferdinand Wiesike who built a house to display it.
Schopenhauer seemed flattered and amused by this, and would claim that it was his first chapel.
As his fame increased copies of his paintings and photographs were being sold and admirers were visiting the places where he lived and wrote his works.
People visited Frankfurt's "Englischer Hof" to observe him dining.
Admirers gave him gifts and asked for autographs.
He complained, however, that he still felt isolated due to his not very social nature and the fact that many of his good friends already died from old age.
He remained healthy in his old age which he attributed to regular walks no matter the weather, and always getting enough sleep.
He had a great appetite and could read without glasses but his hearing was declining since his youth and he developed problems with rheumatism.
He remained active and lucid, continued his reading, writing and correspondences until his death.
The numerous notes that he made during these years, amongst others on aging, were published posthumously under the title "Senilia".
In the spring of 1860 his health started to decline, he experienced shortness of breath and heart palpitations; in September he suffered inflammation of the lungs and although he was starting to recover he remained very weak.
His last friend to visit him was Wilhelm Gwinner and according to him Schopenhauer was concerned that he won’t be able to finish his planned additions to "Parerga and Paralipomena" but was at peace with dying.
He died of pulmonary-respiratory failure, on 21 September 1860 while sitting at home on his couch.
He was 72.
Schopenhauer saw his philosophy as a continuation of that of Kant, and used the results of his epistemological investigations, that is, transcendental idealism, as starting point for his own:

Kant had argued the empirical world is merely a complex of appearances whose existence and connection occur only in our representations.
Schopenhauer reiterates this in the first sentence of his main work: "The world is my representation."
We do not draw empirical laws from nature, but prescribe them to it.
Schopenhauer praises Kant for his distinction between appearance and the things-in-themselves that appear, whereas the general consensus in German Idealism was that this was the weakest spot of Kant's theory, since according to Kant causality can find application on objects of experience only, and consequently, things-in-themselves cannot be the cause of appearances, as Kant argued.
The inadmissibility of this reasoning was also acknowledged by Schopenhauer.
He insisted that this distinction was a true conclusion, drawn from false premises.
In November 1813 Goethe invited Schopenhauer for research on his Theory of Colours.
Although Schopenhauer considered colour theory a minor matter, he accepted the invitation out of admiration for Goethe.
Nevertheless, these investigations led him to his most important discovery in epistemology: finding a demonstration for the a priori nature of causality.
Kant openly admitted that it was Hume's skeptical assault on causality that motivated the critical investigations of "Critique of Pure Reason".
In it, he gives an elaborate proof to show that causality is given a priori.
After G.E.
Schulze had made it plausible that Kant had not disproven Hume's skepticism, it was up to those loyal to the project of Kant to prove this important matter.
The difference between the approach of Kant and Schopenhauer was this: Kant simply declared that the empirical content of perception is "given" to us from outside, an expression with which Schopenhauer often expressed his dissatisfaction.
He, on the other hand, was occupied with: how do we get this empirical content of perception; how is it possible to comprehend subjective sensations "limited to my skin" as the objective perception of things that lie "outside" of me?
Causality is therefore not an empirical concept drawn from objective perceptions, but objective perception presupposes knowledge of causality.
Hereby Hume's skepticism is disproven.
By this intellectual operation, comprehending every effect in our sensory organs as having an external cause, the external world arises.
With vision, finding the cause is essentially simplified due to light acting in straight lines.
We are seldom conscious of the process, that interprets the double sensation in both eyes as coming from one object; that turns the upside down impression; and that adds depth to make from the planimetrical data stereometrical perception with distance between objects.
Schopenhauer stresses the importance of the intellectual nature of perception, the senses furnish the raw material by which the intellect produces the world as representation.
He set out his theory of perception for the first time in "On Vision and Colors", and in the subsequent editions of Fourfold Root an extensive exposition is given in § 21.
Schopenhauer developed a system called metaphysical voluntarism.
For Schopenhauer, human desire was futile, illogical, directionless, and, by extension, so was all human action in the world.
Einstein paraphrased his views as follows: "Man can indeed do what he wants, but he cannot will what he wants."
In this sense, he adhered to the Fichtean principle of idealism: "The world is "for" a subject."
This idealism so presented, immediately commits it to an ethical attitude, unlike the purely epistemological concerns of Descartes and Berkeley.
To Schopenhauer, the Will is a blind force that controls not only the actions of individual, intelligent agents, but ultimately all observable phenomena—an evil to be terminated via mankind's duties: asceticism and chastity.
He is credited with one of the most famous opening lines of philosophy: "The world is my representation."
Friedrich Nietzsche was greatly influenced by this idea of Will, although he eventually rejected it.
For Schopenhauer, human desiring, "willing", and craving cause suffering or pain.
A temporary way to escape this pain is through aesthetic contemplation (a method comparable to Zapffe's ""Sublimation"").
Aesthetic contemplation allows one to escape this pain—albeit temporarily—because it stops one perceiving the world as mere presentation.
Instead, one no longer perceives the world as an object of perception (therefore as subject to the Principle of Sufficient Grounds; time, space and causality) from which one is separated; rather one becomes one with that perception: ""one can thus no longer separate the perceiver from the perception"" ("The World as Will and Representation", section 34).
From this immersion with the world one no longer views oneself as an individual who suffers in the world due to one's individual will but, rather, becomes a ""subject of cognition"" to a perception that is ""Pure, will-less, timeless"" (section 34) where the essence, "ideas", of the world are shown.
Art is the practical consequence of this brief aesthetic contemplation as it attempts to depict one's immersion with the world, thus tries to depict the essence/pure ideas of the world.
Music, for Schopenhauer, was the purest form of art because it was the one that depicted the will itself without it appearing as subject to the Principle of Sufficient Grounds, therefore as an individual object.
According to Daniel Albright, "Schopenhauer thought that music was the only art that did not merely copy ideas, but actually embodied the will itself".
He deemed music a timeless, universal language comprehended everywhere, that can imbue global enthusiasm, if in possession of a significant melody.
Schopenhauer's realist views on mathematics are evident in his criticism of the contemporaneous attempts to prove the parallel postulate in Euclidean geometry.
Writing shortly before the discovery of hyperbolic geometry demonstrated the logical independence of the axiom—and long before the general theory of relativity revealed that it does not necessarily express a property of physical space—Schopenhauer criticized mathematicians for trying to use indirect concepts to prove what he held was directly evident from intuitive perception.
Throughout his writings, Schopenhauer criticized the logical derivation of philosophies and mathematics from mere concepts, instead of from intuitive perceptions.
Although Schopenhauer could see no justification for trying to prove Euclid's parallel postulate, he did see a reason for examining another of Euclid's axioms.
This follows Kant's reasoning.
The task of ethics is not to prescribe moral actions that ought to be done, but to investigate moral actions.
Philosophy is always theoretical: its task to explain what is given.
According to Kant's teaching of transcendental idealism, space and time are forms of our sensibility due to which the phenomena appear in multiplicity.
Reality in itself is free from all multiplicity, not in the sense that an object is one, but that it is outside the "possibility" of multiplicity.
From this follows that two individuals, though they appear as distinct, are in-themselves not distinct.
The appearances are entirely subordinated to the principle of sufficient reason.
The egoistic individual who focuses his aims completely on his own interests has therefore to deal with empirical laws as good as he can.
What is relevant for ethics are individuals who can act against their own self-interest.
If we take for example a man who suffers when he sees his fellow men living in poverty, and consequently uses a significant part of his income to support "their" needs instead his "own" pleasures, then the simplest way to describe this is that he makes "less distinction between himself" and others than is usually made.
Regarding how the things "appear" to us, the egoist is right to assert the gap between two individuals, but the altruist experiences the sufferings of others as his own.
In the same way a compassionate man cannot hurt animals, though they appear as distinct from himself.
What motivates the altruist is compassion.
The sufferings of others is for him not a cold matter to which he is indifferent, but he feels connected to all beings.
Compassion is thus the basis of morality.
Schopenhauer calls the principle through which multiplicity appears the "principium individuationis".
When we behold nature we see that it is a cruel battle for existence.
Individual manifestations of the will can maintain themselves at only at the expense of others—the will, as the only thing that exists, has no other option but to devour itself to experience pleasure.
This is a fundamental characteristic of the will, and cannot be circumvented.
Tormenter and tormented are one.
Suffering is the moral retribution of our attachment to pleasure.
Schopenhauer deemed that this truth was expressed by Christian dogma of original sin and in Eastern religions with the dogma of rebirth.
He who sees through the "principium individuationis" and comprehends suffering "in general" as his own, will see suffering everywhere, and instead of using all his force to fight for the happiness of his individual manifestation, he will abhor life itself, of which he knows how inseparably it is connected with suffering.
A happy individual life midst of a world of suffering is for him like beggar who dreams one night that he is a king.
Those who have experienced this intuitive knowledge can no longer affirm life, but will exhibit asceticism and quietism, meaning that they are no longer sensitive to motives, are not concerned about their individual welfare, and accept the evil others inflict on them without resisting.
They welcome poverty, do not seek nor flee death.
Human life is a ceaseless struggle for satisfaction, and instead of renewing this contract, the ascetic breaks it.
It matters little whether these ascetics adhered the dogmata of Christianity or Dharmic religions, since their way of living is the result of intuitive knowledge.
Schopenhauer referred to asceticism as the denial of the will to live.
Philosophers have not traditionally been impressed by the tribulations of sex, but Schopenhauer addressed it and related concepts forthrightly:

He named a force within man that he felt took invariable precedence over reason: the Will to Live or Will to Life ("Wille zum Leben"), defined as an inherent drive within human beings, and indeed all creatures, to stay alive; a force that inveigles us into reproducing.
Schopenhauer refused to conceive of love as either trifling or accidental, but rather understood it as an immensely powerful force that lay unseen within man's psyche, guaranteeing the quality of the human race:

It has often been argued that Schopenhauer's thoughts on sexuality foreshadowed the theory of evolution, a claim which seems to have been met with satisfaction by Darwin as he included a quote of the German philosopher in his Descent of Man after having read such a claim.
This has also been noted about Freud's concepts of the libido and the unconscious mind, and evolutionary psychology in general.
Schopenhauer's politics were, for the most part, an echo of his system of ethics (the latter being expressed in "Die beiden Grundprobleme der Ethik", available in English as two separate books, "On the Basis of Morality" and "On the Freedom of the Will").
Ethics also occupies about one quarter of his central work, "The World as Will and Representation".
In occasional political comments in his "Parerga and Paralipomena" and "Manuscript Remains", Schopenhauer described himself as a proponent of limited government.
What was essential, he thought, was that the state should "leave each man free to work out his own salvation," and so long as government was thus limited, he would "prefer to be ruled by a lion than one of [his] fellow rats"—i.e., by a monarch, rather than a democrat.
Schopenhauer shared the view of Thomas Hobbes on the necessity of the state, and of state action, to check the destructive tendencies innate to our species.
He also defended the independence of the legislative, judicial and executive branches of power, and a monarch as an impartial element able to practise justice (in a practical and everyday sense, not a cosmological one).
He declared monarchy as "that which is natural to man" for "intelligence has always under a monarchical government a much better chance against its irreconcilable and ever-present foe, stupidity" and disparaged republicanism as "unnatural as it is unfavourable to the higher intellectual life and the arts and sciences".
Schopenhauer, by his own admission, did not give much thought to politics, and several times he writes proudly of how little attention he had paid "to political affairs of [his] day".
In a life that spanned several revolutions in French and German government, and a few continent-shaking wars, he did indeed maintain his aloof position of "minding not the times but the eternities".
He wrote many disparaging remarks about Germany and the Germans.
A typical example is, "For a German it is even good to have somewhat lengthy words in his mouth, for he thinks slowly, and they give him time to reflect."
Schopenhauer attributed civilizational primacy to the northern "white races" due to their sensitivity and creativity (except for the ancient Egyptians and Hindus, whom he saw as equal):

The highest civilization and culture, apart from the ancient Hindus and Egyptians, are found exclusively among the white races; and even with many dark peoples, the ruling caste or race is fairer in colour than the rest and has, therefore, evidently immigrated, for example, the Brahmans, the Incas, and the rulers of the South Sea Islands.
All this is due to the fact that necessity is the mother of invention because those tribes that emigrated early to the north, and there gradually became white, had to develop all their intellectual powers and invent and perfect all the arts in their struggle with need, want and misery, which in their many forms were brought about by the climate.
This they had to do in order to make up for the parsimony of nature and out of it all came their high civilization.
Despite this, he was adamantly against differing treatment of races, was fervently anti-slavery, and supported the abolitionist movement in the United States.
He describes the treatment of "[our] innocent black brothers whom force and injustice have delivered into [the slave-master's] devilish clutches" as "belonging to the blackest pages of mankind's criminal record".
Schopenhauer additionally maintained a marked metaphysical and political anti-Judaism.
Schopenhauer argued that Christianity constituted a revolt against what he styled the materialistic basis of Judaism, exhibiting an Indian-influenced ethics reflecting the Aryan-Vedic theme of spiritual self-conquest.
He saw this as opposed to what he held was the ignorant drive toward earthly utopianism and superficiality of a worldly "Jewish" spirit:

While all other religions endeavor to explain to the people by symbols the metaphysical significance of life, the religion of the Jews is entirely immanent and furnishes nothing but a mere war-cry in the struggle with other nations.
The State, Schopenhauer claimed, punishes criminals to prevent future crimes.
It does so by placing "beside every possible motive for committing a wrong a more powerful motive for leaving it undone, in the inescapable punishment.
Accordingly, the criminal code is as complete a register as possible of counter-motives to all criminal actions that can possibly be imagined ..." He claimed this doctrine was not original to him.
Previously, it appeared in the writings of Plato, Seneca, Hobbes, Pufendorf, and Anselm Feuerbach.
In Schopenhauer's 1851 essay "On Women", he expressed his opposition to what he called "Teutonico-Christian stupidity" of reflexive unexamined reverence ("abgeschmackten Weiberveneration") for the female.
Schopenhauer wrote that "Women are directly fitted for acting as the nurses and teachers of our early childhood by the fact that they are themselves childish, frivolous and short-sighted."
He opined that women are deficient in artistic faculties and sense of justice, and expressed opposition to monogamy.
Indeed, Rodgers and Thompson in "Philosophers Behaving Badly" call Schopenhauer "a misogynist without rival in ... Western philosophy".
He claimed that "woman is by nature meant to obey".
The essay does give some compliments, however: that "women are decidedly more sober in their judgment than [men] are", and are more sympathetic to the suffering of others.
Schopenhauer's writings have influenced many, from Friedrich Nietzsche to nineteenth-century feminists.
Schopenhauer's biological analysis of the difference between the sexes, and their separate roles in the struggle for survival and reproduction, anticipates some of the claims that were later ventured by sociobiologists and evolutionary psychologists.
When the elderly Schopenhauer sat for a sculpture portrait by the Prussian sculptor Elisabet Ney in 1859, he was much impressed by the young woman's wit and independence, as well as by her skill as a visual artist.
After his time with Ney, he told Richard Wagner's friend Malwida von Meysenbug, "I have not yet spoken my last word about women.
I believe that if a woman succeeds in withdrawing from the mass, or rather raising herself above the mass, she grows ceaselessly and more than a man."
Schopenhauer viewed personality and intellect as being inherited.
He quotes Horace's saying, "From the brave and good are the brave descended" ("Odes", iv, 4, 29) and Shakespeare's line from "Cymbeline", "Cowards father cowards, and base things sire base" (IV, 2) to reinforce his hereditarian argument.
Mechanistically, Schopenhauer believed that a person inherits his level of intellect through his mother, and personal character through one's father.
This belief in heritability of traits informed Schopenhauer's view of love – placing it at the highest level of importance.
For Schopenhauer the "final aim of all love intrigues, be they comic or tragic, is really of more importance than all other ends in human life.
What it all turns upon is nothing less than the composition of the next generation.
... It is not the weal or woe of any one individual, but that of the human race to come, which is here at stake."
This view of the importance for the species of whom we choose to love was reflected in his views on eugenics or good breeding.
Here Schopenhauer wrote:

With our knowledge of the complete unalterability both of character and of mental faculties, we are led to the view that a real and thorough improvement of the human race might be reached not so much from outside as from within, not so much by theory and instruction as rather by the path of generation.
Plato had something of the kind in mind when, in the fifth book of his "Republic", he explained his plan for increasing and improving his warrior caste.
If we could castrate all scoundrels and stick all stupid geese in a convent, and give men of noble character a whole harem, and procure men, and indeed thorough men, for all girls of intellect and understanding, then a generation would soon arise which would produce a better age than that of Pericles.
In another context, Schopenhauer reiterated his eugenic thesis: "If you want Utopian plans, I would say: the only solution to the problem is the despotism of the wise and noble members of a genuine aristocracy, a genuine nobility, achieved by mating the most magnanimous men with the cleverest and most gifted women.
This proposal constitutes my Utopia and my Platonic Republic."
Analysts (e.g., Keith Ansell-Pearson) have suggested that Schopenhauer's anti-egalitarianist sentiment and his support for eugenics influenced the neo-aristocratic philosophy of Friedrich Nietzsche, who initially considered Schopenhauer his mentor.
As a consequence of his monistic philosophy, Schopenhauer was very concerned about the welfare of animals.
For him, all individual animals, including humans, are essentially the same, being phenomenal manifestations of the one underlying Will.
The word "will" designated, for him, force, power, impulse, energy, and desire; it is the closest word we have that can signify both the real essence of all external things and also our own direct, inner experience.
Since every living thing possesses will, then humans and animals are fundamentally the same and can recognize themselves in each other.
For this reason, he claimed that a good person would have sympathy for animals, who are our fellow sufferers.
In 1841, he praised the establishment, in London, of the Society for the Prevention of Cruelty to Animals, and also the Animals' Friends Society in Philadelphia.
Schopenhauer even went so far as to protest against the use of the pronoun "it" in reference to animals because it led to the treatment of them as though they were inanimate things.
To reinforce his points, Schopenhauer referred to anecdotal reports of the look in the eyes of a monkey who had been shot and also the grief of a baby elephant whose mother had been killed by a hunter.
He was very attached to his succession of pet poodles.
Schopenhauer criticized Spinoza's belief that animals are a mere means for the satisfaction of humans.
In the third, expanded edition of "The World as Will and Representation" (1859), Schopenhauer added an appendix to his chapter on the "Metaphysics of Sexual Love".
He wrote that pederasty did have the benefit of preventing ill-begotten children.
Concerning this, he stated that "the vice we are considering appears to work directly against the aims and ends of nature, and that in a matter that is all important and of the greatest concern to her it must in fact serve these very aims, although only indirectly, as a means for preventing greater evils".
Schopenhauer ends the appendix with the statement that "by expounding these paradoxical ideas, I wanted to grant to the professors of philosophy a small favour.
I have done so by giving them the opportunity of slandering me by saying that I defend and commend pederasty."
Schopenhauer read the Latin translation of the ancient Hindu texts, the Upanishads, which French writer Anquetil du Perron had translated from the Persian translation of Prince Dara Shukoh entitled "Sirre-Akbar" ("The Great Secret").
He was so impressed by their philosophy that he called them "the production of the highest human wisdom", and believed they contained superhuman concepts.
The Upanishads was a great source of inspiration to Schopenhauer.
Writing about them, he said:

It is the most satisfying and elevating reading (with the exception of the original text) which is possible in the world; it has been the solace of my life and will be the solace of my death.
The book "Oupnekhat" (Upanishad) always lay open on his table, and he invariably studied it before sleeping at night.
He called the opening up of Sanskrit literature "the greatest gift of our century" and predicted that the philosophy and knowledge of the Upanishads would become the cherished faith of the West.
Schopenhauer was first introduced to the 1802 Latin Upanishad translation through Friedrich Majer.
They met during the winter of 1813–1814 in Weimar at the home of Schopenhauer's mother according to the biographer Safranski.
Majer was a follower of Herder, and an early Indologist.
Schopenhauer did not begin a serious study of the Indic texts, however, until the summer of 1814.
Sansfranski maintains that between 1815 and 1817, Schopenhauer had another important cross-pollination with Indian thought in Dresden.
This was through his neighbor of two years, Karl Christian Friedrich Krause.
Krause was then a minor and rather unorthodox philosopher who attempted to mix his own ideas with that of ancient Indian wisdom.
Krause had also mastered Sanskrit, unlike Schopenhauer, and the two developed a professional relationship.
It was from Krause that Schopenhauer learned meditation and received the closest thing to expert advice concerning Indian thought.
Most noticeable, in the case of Schopenhauer's work, was the significance of the Chandogya Upanishad, whose Mahāvākya, Tat Tvam Asi, is mentioned throughout "The World as Will and Representation".
Schopenhauer noted a correspondence between his doctrines and the Four Noble Truths of Buddhism.
Similarities centered on the principles that life involves suffering, that suffering is caused by desire (taṇhā), and that the extinction of desire leads to liberation.
Thus three of the four "truths of the Buddha" correspond to Schopenhauer's doctrine of the will.
In Buddhism, however, while greed and lust are always unskillful, desire is ethically variable – it can be skillful, unskillful, or neutral.
For Schopenhauer, will had ontological primacy over the intellect.
In other words, desire is prior to thought.
Schopenhauer felt this was similar to notions of puruṣārtha or goals of life in Vedānta Hinduism.
In Schopenhauer's philosophy, denial of the will is attained by either:

However, Buddhist nirvāṇa is not equivalent to the condition that Schopenhauer described as denial of the will.
Nirvāṇa is not the extinguishing of the "person" as some Western scholars have thought, but only the "extinguishing" (the literal meaning of nirvana) of the flames of greed, hatred, and delusion that assail a person's character.
Occult historian Joscelyn Godwin (born 1945) stated, "It was Buddhism that inspired the philosophy of Arthur Schopenhauer, and, through him, attracted Richard Wagner."
This Orientalism reflected the struggle of the German Romantics, in the words of Leon Poliakov, to "free themselves from Judeo-Christian fetters".
In contradistinction to Godwin's claim that Buddhism inspired Schopenhauer, the philosopher himself made the following statement in his discussion of religions:
If I wished to take the results of my philosophy as the standard of truth, I should have to concede to Buddhism pre-eminence over the others.
In any case, it must be a pleasure to me to see my doctrine in such close agreement with a religion that the majority of men on earth hold as their own, for this numbers far more followers than any other.
And this agreement must be yet the more pleasing to me, inasmuch as "in my philosophizing I have certainly not been under its influence" [emphasis added].
For up till 1818, when my work appeared, there was to be found in Europe only a very few accounts of Buddhism.
Buddhist philosopher Nishitani Keiji, however, sought to distance Buddhism from Schopenhauer.
While Schopenhauer's philosophy may sound rather mystical in such a summary, his methodology was resolutely empirical, rather than speculative or transcendental:
Philosophy ... is a science, and as such has no articles of faith; accordingly, in it nothing can be assumed as existing except what is either positively given empirically, or demonstrated through indubitable conclusions.
Also note:
This actual world of what is knowable, in which we are and which is in us, remains both the material and the limit of our consideration.
The argument that Buddhism affected Schopenhauer's philosophy more than any other Dharmic faith loses more credence when viewed in light of the fact that Schopenhauer did not begin a serious study of Buddhism until after the publication of "The World as Will and Representation" in 1818.
Scholars have started to revise earlier views about Schopenhauer's discovery of Buddhism.
Proof of early interest and influence, however, appears in Schopenhauer's 1815/16 notes (transcribed and translated by Urs App) about Buddhism.
They are included in a recent case study that traces Schopenhauer's interest in Buddhism and documents its influence.
Other scholarly work questions how similar Schopenhauer's philosophy actually is to Buddhism.
Some traditions in Western esotericism and parapsychology interested Schopenhauer and influenced his philosophical theories.
He praised animal magnetism as evidence for the reality of magic in his "On the Will in Nature", and went so far as to accept the division of magic into left-hand and right-hand magic, although he doubted the existence of demons.
Schopenhauer grounded magic in the Will and claimed all forms of magical transformation depended on the human Will, not on ritual.
This theory notably parallels Aleister Crowley's system of magick and its emphasis on human will.
Given the importance of the Will to Schopenhauer's overarching system, this amounts to "suggesting his whole philosophical system had magical powers."
Schopenhauer rejected the theory of disenchantment and claimed philosophy should synthesize itself with magic, which he believed amount to "practical metaphysics."
Neoplatonism, including the traditions of Plotinus and to a lesser extent Marsilio Ficino, has also been cited as an influence on Schopenhauer.
Schopenhauer had a wide range of interests, from science and opera to occultism and literature.
In his student years Schopenhauer went more often to lectures in the sciences than philosophy.
He kept a strong interest as his personal library contained near to 200 books of scientific literature at his death, and his works refer to scientific titles not found in the library.
Many evenings were spent in the theatre, opera and ballet; the operas of Mozart, Rossini and Bellini were especially esteemed.
Schopenhauer considered music the highest art, and played the flute during his whole life.
As a polyglot, the philosopher knew German, Italian, Spanish, French, English, Latin and ancient Greek, and he was an avid reader of poetry and literature.
He particularly revered Goethe, Petrarch, Calderón and Shakespeare.
If Goethe had not been sent into the world simultaneously with Kant in order to counterbalance him, so to speak, in the spirit of the age, the latter would have been haunted like a nightmare many an aspiring mind and would have oppressed it with great affliction.
But now the two have an infinitely wholesome effect from opposite directions and will probably raise the German spirit to a height surpassing even that of antiquity.
In philosophy, his most important influences were, according to himself, Kant, Plato and the Upanishads.
Concerning the Upanishads and Vedas, he writes in "The World as Will and Representation":
If the reader has also received the benefit of the Vedas, the access to which by means of the Upanishads is in my eyes the greatest privilege which this still young century (1818) may claim before all previous centuries, if then the reader, I say, has received his initiation in primeval Indian wisdom, and received it with an open heart, he will be prepared in the very best way for hearing what I have to tell him.
It will not sound to him strange, as to many others, much less disagreeable; for I might, if it did not sound conceited, contend that every one of the detached statements which constitute the Upanishads, may be deduced as a necessary result from the fundamental thoughts which I have to enunciate, though those deductions themselves are by no means to be found there.
Schopenhauer saw Bruno and Spinoza as unique philosophers who were not bound to their age or nation.
"Both were fulfilled by the thought, that as manifold the appearances of the world may be, it is still "one" being, that appears in all of them.
... Consequently, there is no place for God as creator of the world in their philosophy, but God is the world itself."
Schopenhauer expressed his regret that Spinoza stuck for the presentation of his philosophy with the concepts of scholasticism and Cartesian philosophy, and tried to use geometrical proofs that do not hold because of the vagueness and wideness of the definitions.
It is the common preference of philosophers of abstraction over perception.
Bruno on the other hand, who knew much about nature and ancient literature, presents his ideas with Italian vividness, and is amongst philosophers the only one who comes near Plato's poetic and dramatic power of exposition.
Schopenhauer noted that their philosophies do not provide any ethics, and it is therefore very remarkable that Spinoza called his main work "Ethics".
In fact, it could be considered complete from the standpoint of life-affirmation, if one completely ignores morality and self-denial.
It is yet even more remarkable that Schopenhauer mentions Spinoza as an example of the denial of the will, if one uses the French biography by Jean Maximilien Lucas as the key to "Tractatus de Intellectus Emendatione".
The importance of Kant for Schopenhauer, in philosophy as well as on a personal level, can hardly be overstated.
The philosophy of Kant was the foundation of his own.
Schopenhauer maintained that Kant stands in the same relation to philosophers such as Berkeley and Plato, as Copernicus to Hicetas, Philolaus, and Aristarchus: Kant succeeded in demonstrating what previous philosophers merely asserted.
In his study room one bust was of Buddha, the other was of Kant.
The bond which Schopenhauer felt with the philosopher of Königsberg may be esteemed in a poem he dedicated to Kant:

Schopenhauer dedicated one fifth of his main work, "The World as Will and Representation", to a criticism of the Kantian philosophy.
The leading figures of post-Kantian philosophy, Fichte, Schelling and Hegel, were not respected by Schopenhauer.
He argued that they were no philosophers at all, who merely sought to impress the public.
Schelling was deemed the most talented of the three, and Schopenhauer wrote that he would recommend his "elucidatory paraphrase of the highly important doctrine of Kant" concerning the intelligible character, if he had been honest enough to admit he was showing off with the thoughts of Kant, instead of hiding this relation in a cunning manner.
Schopenhauer's favourite subject of attacks was Hegel, whom he considered unworthy even of Fichte and Schelling.
Whereas Fichte was merely a windbag, Hegel was a "stupid and clumsy charlatan".
Karl Popper agreed with this distinction.
Schopenhauer had a large posthumous effect and remained the most influential German philosopher until the First World War.
His philosophy was a starting point for a new generation of philosophers, which consisted of Julius Bahnsen, Paul Deussen, Lazar von Hellenbach, Karl Robert Eduard von Hartmann, Ernst Otto Lindner, Philipp Mainländer, Friedrich Nietzsche, Olga Plümacher and Agnes Talbert.
His legacy shaped the intellectual debate, and forced movements that were utterly opposed to him, neo-Kantianism and positivism, to address issues they would otherwise have completely ignored, and in doing so he changed them markedly.
The French writer Maupassant commented that "to-day even those who execrate him seem to carry in their own souls particles of his thought."
Other philosophers of the 19th century who cited his influence include Hans Vaihinger, Volkelt, Solovyov and Weininger.
Schopenhauer was well read amongst physicists, most notably Einstein, Schrödinger, Wolfgang Pauli, and Majorana.
Einstein described Schopenhauer's thoughts as a "continual consolation" and called him a genius.
In his Berlin study three figures hung on the wall: Faraday, Maxwell, Schopenhauer.
Konrad Wachsmann recalled: "He often sat with one of the well-worn Schopenhauer volumes, and as he sat there, he seemed so pleased, as if he were engaged with a serene and cheerful work."
When Erwin Schrödinger discovered Schopenhauer ("the greatest savant of the West") he considered switching his study of physics to philosophy.
He maintained the idealistic views during the rest of his life.
Wolfgang Pauli accepted the main tenet of Schopenhauer's metaphysics, that the thing-in-itself is will.
But most of all Schopenhauer is famous for his influence on artists.
Richard Wagner became one of the earliest and most famous adherents of the Schopenhauerian philosophy.
The admiration was not mutual, and Schopenhauer proclaimed: "I remain faithful to Rossini and Mozart!"
See also Influence of Schopenhauer on Tristan und Isolde.
Under the influence of Schopenhauer Leo Tolstoy became convinced that the truth of all religions lies in self-renunciation.
When he read his philosophy he exclaimed "at present I am convinced that Schopenhauer is the greatest genius among men.
... It is the whole world in an incomparably beautiful and clear reflection."
He said that what he has written in "War and Peace" is also said by Schopenhauer in "The World as Will and Representation".
Jorge Luis Borges remarked that the reason he had never attempted to write a systematic account of his world view, despite his penchant for philosophy and metaphysics in particular, was because Schopenhauer had already written it for him.
Other figures in literature who were strongly influenced by Schopenhauer were Thomas Mann, Afanasy Fet, J.-K.
Huysmans and George Santayana.
Sergei Prokofiev, although initially reluctant to engage with works noted for their pessimism, became fascinated with Schopenhauer after reading "Aphorisms on the Wisdom of LifeI" in "Parerga and ParalipomenaI."
"With his truths Schopenhauer gave me a spiritual world and an awareness of happiness."
Friedrich Nietzsche owed the awakening of his philosophical interest to reading "The World as Will and Representation" and admitted that he was one of the few philosophers that he respected, dedicating to him his essay "Schopenhauer als Erzieher" one of his "Untimely Meditations".
As a teenager, Ludwig Wittgenstein adopted Schopenhauer's epistemological idealism.
However, after his study of the philosophy of mathematics, he rejected epistemological transcendental idealism for Gottlob Frege's conceptual realism.
In later years, Wittgenstein was highly dismissive of Schopenhauer, describing him as an ultimately shallow thinker: "Schopenhauer has quite a crude mind ... where real depth starts, his comes to an end."
His friend Bertrand Russell had a low opinion on the philosopher, and attacked him in his famous "History of Western Philosophy" for hypocritically praising asceticism yet not acting upon it.
On the opposite isle of Russell on the foundations of mathematics, the Dutch mathematician L. E. J. Brouwer incorporated the ideas of Kant and Schopenhauer in intuitionism, where mathematics is considered a purely mental activity, instead of an analytic activity wherein objective properties of reality are revealed.
Brouwer was also influenced by Schopenhauer's metaphysics, and wrote an essay on mysticism.
</doc>
<doc id="701" url="https://en.wikipedia.org/wiki?curid=701" title="Angola">
Angola

Angola (; ), officially the Republic of Angola (; Kikongo, Kimbundu and ), is a west-coast country of south-central Africa.
It is the seventh-largest country in Africa, bordered by Namibia to the south, the Democratic Republic of the Congo to the north, Zambia to the east, and the Atlantic Ocean to the west.
Angola has an exclave province, the province of Cabinda that borders the Republic of the Congo and the Democratic Republic of the Congo.
The capital and largest city of Angola is Luanda.
Although inhabited since the Paleolithic Era, what is now Angola was molded by Portuguese colonisation.
It began with, and was for centuries limited to, coastal settlements and trading posts established starting in the 16th century.
In the 19th century, European settlers slowly and hesitantly began to establish themselves in the interior.
The Portuguese colony that became Angola did not have its present borders until the early 20th century because of resistance by groups such as the Cuamato, the Kwanyama and the Mbunda.
After a protracted anti-colonial struggle, independence was achieved in 1975 as the Marxist–Leninist People's Republic of Angola, a one-party state supported by the Soviet Union and Cuba.
The civil war between the ruling People's Movement for the Liberation of Angola (MPLA) and the insurgent anti-communist National Union for the Total Independence of Angola (UNITA), supported by the United States and apartheid South Africa, lasted until 2002.
The sovereign state has since become a relatively stable unitary, presidential constitutional republic.
Angola has vast mineral and petroleum reserves, and its economy is among the fastest-growing in the world, especially since the end of the civil war; however, the standard of living remains low for most of the population, and life expectancy in Angola is among the lowest in the world, while infant mortality is among the highest.
Angola's economic growth is highly uneven, with most of the nation's wealth concentrated in a disproportionately small sector of the population.
Angola is a member state of the United Nations, OPEC, African Union, the Community of Portuguese Language Countries, and the Southern African Development Community.
A highly multiethnic country, Angola's 25.8 million people span tribal groups, customs, and traditions.
Angolan culture reflects centuries of Portuguese rule, in the predominance of the Portuguese language and of the Catholic Church.
The name "Angola" comes from the Portuguese colonial name "Reino de Angola (Kingdom of Angola)", which appeared as early as Dias de Novais's 1571 charter.
The toponym was derived by the Portuguese from the title "ngola" held by the kings of Ndongo.
Ndongo in the highlands, between the Kwanza and Lukala Rivers, was nominally a possession of the Kingdom of Kongo, but was seeking greater independence in the 16th century.
Modern Angola was populated predominantly by nomadic Khoi and San prior to the first Bantu migrations.
The Khoi and San peoples were neither pastoralists nor cultivators, 
but hunter-gatherers.
They were displaced by Bantu peoples arriving from the north, most of whom likely originated in what is today northwestern Nigeria and southern Niger.
Bantu speakers introduced the cultivation of bananas and taro, as well as large cattle herds, to Angola's central highlands and the Luanda plain.
To its south lay the Kingdom of Ndongo, from which the area of the later Portuguese colony was sometimes known as "Dongo".
Portuguese explorer Diogo Cão reached the area in 1484.
The previous year, the Portuguese had established relations with the Kongo, which stretched at the time from modern Gabon in the north to the Kwanza River in the south.
The Portuguese established their primary early trading post at Soyo, which is now the northernmost city in Angola apart from the Cabinda exclave.
Paulo Dias de Novais founded São Paulo de Loanda (Luanda) in 1575 with a hundred families of settlers and four hundred soldiers.
Benguela was fortified in 1587 and became a township in 1617.
The Portuguese established several other settlements, forts and trading posts along the Angolan coast, principally trading in Angolan slaves for Brazilian plantations.
Local slave dealers provided a large number of slaves for the Portuguese Empire, usually in exchange for manufactured goods from Europe.
This part of the Atlantic slave trade continued until after Brazil's independence in the 1820s.
Despite Portugal's territorial claims in Angola, its control over much of the country's vast interior was minimal.
In the 16th century Portugal gained control of the coast through a series of treaties and wars.
Life for European colonists was difficult and progress slow.
John Iliffe notes that "Portuguese records of Angola from the 16th century show that a great famine occurred on average every seventy years; accompanied by epidemic disease, it might kill one-third or one-half of the population, destroying the demographic growth of a generation and forcing colonists back into the river valleys".
During the Portuguese Restoration War, the Dutch West India Company occupied the principal settlement of Luanda in 1641, using alliances with local peoples to carry out attacks against Portuguese holdings elsewhere.
A fleet under Salvador de Sá retook Luanda in 1648; reconquest of the rest of the territory was completed by 1650.
New treaties with the Kongo were signed in 1649; others with Njinga's Kingdom of Matamba and Ndongo followed in 1656.
The conquest of Pungo Andongo in 1671 was the last major Portuguese expansion from Luanda, as attempts to invade Kongo in 1670 and Matamba in 1681 failed.
Colonial outposts also expanded inward from Benguela, but until the late 19th century the inroads from Luanda and Benguela were very limited.
Hamstrung by a series of political upheavals in the early 1800s, Portugal was slow to mount a large scale annexation of Angolan territory.
The slave trade was abolished in Angola in 1836, and in 1854 the colonial government freed all its existing slaves.
Four years later, a more progressive administration appointed by Lisbon abolished slavery altogether.
However, these decrees remained largely unenforceable, and the Portuguese depended on assistance from the British Royal Navy to enforce their ban on the slave trade.
This coincided with a series of renewed military expeditions into the hinterland.
By the mid-nineteenth century Portugal had established its dominion as far east as the Congo River and as far south as Mossâmedes.
Until the late 1880s, Lisbon entertained proposals to link Angola with its colony in Mozambique but was blocked by British and Belgian opposition.
In this period, the Portuguese came up against different forms of armed resistance from various peoples in Angola.
The Berlin Conference in 1884–1885 set the colony's borders, delineating the boundaries of Portuguese claims in Angola, although many details were unresolved until the 1920s.
Trade between Portugal and her African territories also rapidly increased as a result of protective tariffs, leading to increased development, and a wave of new Portuguese immigrants.
Under colonial law, black Angolans were forbidden from forming political parties or labour unions.
The first nationalist movements did not take root until after World War II, spearheaded by a largely Westernised, Portuguese-speaking urban class which included many mestiços.
During the early 1960s they were joined by other associations stemming from "ad hoc" labour activism in the rural workforce.
Portugal's refusal to address increasing Angolan demands for self-determination provoked an armed conflict which erupted in 1961 with the Baixa de Cassanje revolt and gradually evolved into a protracted war of independence that persisted for the next twelve years.
Throughout the conflict, three militant nationalist movements with their own partisan guerrilla wings emerged from the fighting between the Portuguese government and local forces, supported to varying degrees by the Portuguese Communist Party.
The "National Front for the Liberation of Angola" (FNLA) recruited from Bakongo refugees in Zaire.
Benefiting from particularly favourable political circumstances in Léopoldville, and especially from a common border with Zaire, Angolan political exiles were able to build up a power base among a large expatriate community from related families, clans, and traditions.
People on both sides of the border spoke mutually intelligible dialects and enjoyed shared ties to the historical Kingdom of Kongo.
Though as foreigners skilled Angolans could not take advantage of Mobutu Sese Seko's state employment programme, some found work as middlemen for the absentee owners of various lucrative private ventures.
The migrants eventually formed the FNLA with the intention of making a bid for political power upon their envisaged return to Angola.
A largely Ovimbundu guerrilla initiative against the Portuguese in central Angola from 1966 was spearheaded by Jonas Savimbi and the "National Union for the Total Independence of Angola" (UNITA).
It remained handicapped by its geographic remoteness from friendly borders, the ethnic fragmentation of the Ovimbundu, and the isolation of peasants on European plantations where they had little opportunity to mobilise.
During the late 1950s, the rise of the Marxist–Leninist "Popular Movement for the Liberation of Angola" (MPLA) in the east and Dembos hills north of Luanda came to hold special significance.
Formed as a coalition resistance movement by the Angolan Communist Party, the organisation's leadership remained predominantly Ambundu and courted public sector workers in Luanda.
Although both the MPLA and its rivals accepted material assistance from the Soviet Union or the People's Republic of China, the former harboured strong anti-imperialist views and was openly critical of the United States and its support for Portugal.
This allowed it to win important ground on the diplomatic front, soliciting support from nonaligned governments in Morocco, Ghana, Guinea, Mali, and the United Arab Republic.
The MPLA attempted to move its headquarters from Conakry to Léopoldville in October 1961, renewing efforts to create a common front with the FNLA, then known as the "Union of Angolan Peoples" (UPA) and its leader Holden Roberto.
Roberto turned down the offer.
When the MPLA first attempted to insert its own insurgents into Angola, the cadres were ambushed and annihilated by UPA partisans on Roberto's orders—setting a precedent for the bitter factional strife which would later ignite the Angolan Civil War.
Throughout the war of independence, the three rival nationalist movements were severely hampered by political and military factionalism, as well as their inability to unite guerrilla efforts against the Portuguese.
Between 1961 and 1975 the MPLA, UNITA, and the FNLA competed for influence in the Angolan population and the international community.
The Soviet Union and Cuba became especially sympathetic towards the MPLA and supplied that party with arms, ammunition, funding, and training.
They also backed UNITA militants until it became clear that the latter was at irreconcilable odds with the MPLA.
The collapse of Portugal's Estado Novo government following the 1974 Carnation Revolution suspended all Portuguese military activity in Africa and the brokering of a ceasefire pending negotiations for Angolan independence.
Encouraged by the Organisation of African Unity, Holden Roberto, Jonas Savimbi, and MPLA chairman Agostinho Neto met in Mombasa in early January 1975 and agreed to form a coalition government.
This was ratified by the Alvor Agreement later that month, which called for general elections and set the country's independence date for 11 November 1975.
All three factions, however, followed up on the ceasefire by taking advantage of the gradual Portuguese withdrawal to seize various strategic positions, acquire more arms, and enlarge their militant forces.
The rapid influx of weapons from numerous external sources, especially the Soviet Union and the United States, as well as the escalation of tensions between the nationalist parties, fueled a new outbreak of hostilities.
With tacit American and Zairean support the FNLA began massing large numbers of troops in northern Angola in an attempt to gain military superiority.
Meanwhile, the MPLA began securing control of Luanda, a traditional Ambundu stronghold.
Sporadic violence broke out in Luanda over the next few months after the FNLA attacked MPLA forces in March 1975.
The fighting intensified with street clashes in April and May, and UNITA became involved after over two hundred of its members were massacred by an MPLA contingent that June.
An upswing in Soviet arms shipments to the MPLA influenced a decision by the Central Intelligence Agency to likewise provide substantial covert aid to the FNLA and UNITA.
In August 1975, the MPLA requested direct assistance from the Soviet Union in the form of ground troops.
The Soviets declined, offering to send advisers but no troops; however, Cuba was more forthcoming and in late September dispatched nearly five hundred combat personnel to Angola, along with sophisticated weaponry and supplies.
By independence there were over a thousand Cuban soldiers in the country.
They were kept supplied by a massive airbridge carried out with Soviet aircraft.
The persistent buildup of Cuban and Soviet military aid allowed the MPLA to drive its opponents from Luanda and blunt an abortive intervention by Zairean and South African troops, which had deployed in a belated attempt to assist the FNLA and UNITA.
The FNLA was largely annihilated, although UNITA managed to withdraw its civil officials and militia from Luanda and seek sanctuary in the southern provinces.
From there, Savimbi continued to mount a determined insurgent campaign against the MPLA.
Between 1975 and 1991, the MPLA implemented an economic and political system based on the principles of scientific socialism, incorporating central planning and a Marxist–Leninist one-party state.
It embarked on an ambitious programme of nationalisation, and the domestic private sector was essentially abolished.
Privately owned enterprises were nationalised and incorporated into a single umbrella of state-owned enterprises known as "Unidades Economicas Estatais" (UEE).
Under the MPLA, Angola experienced a significant degree of modern industrialisation.
However, corruption and graft also increased and public resources were either allocated inefficiently or simply embezzled by officials for personal enrichment.
The ruling party survived an attempted coup d'état by the Maoist-oriented Communist Organisation of Angola (OCA) in 1977, which was suppressed after a series of bloody political purges left thousands of OCA supporters dead.
The MPLA abandoned its former Marxist ideology at its third party congress in 1990, and declared social democracy to be its new platform.
Angola subsequently became a member of the International Monetary Fund; restrictions on the market economy were also reduced in an attempt to draw foreign investment.
By May 1991 it reached a peace agreement with UNITA, the Bicesse Accords, which scheduled new general elections for September 1992.
When the MPLA secured a major electoral victory, UNITA objected to the results of both the presidential and legislative vote count and returned to war.
Following the election, the Halloween massacre occurred from October 30 to November 1, where MPLA forces killed thousands of UNITA supporters.
On 22 March 2002, Jonas Savimbi was killed in action against government troops.
UNITA and the MPLA reached a cease-fire shortly afterwards.
UNITA gave up its armed wing and assumed the role of a major opposition party.
Although the political situation of the country began to stabilise, regular democratic processes did not prevail until the elections in Angola in 2008 and 2012 and the adoption of a new constitution in 2010, all of which strengthened the prevailing dominant-party system.
Angola has a serious humanitarian crisis; the result of the prolonged war, of the abundance of minefields, of the continued political (and to a much lesser degree) military activities in favour of the independence of the exclave of Cabinda (carried out in the context of the protracted Cabinda conflict by the FLEC), but most of all, by the depredation of the country's rich mineral resources by the régime.
While most of the internally displaced have now settled around the capital, in the so-called "musseques", the general situation for Angolans remains desperate.
Drought in 2016 caused the worst food crisis in Southern Africa in 25 years.
Drought affected 1.4 million people across seven of Angola's 18 provinces.
Food prices rose and acute malnutrition rates doubled, with more than 95,000 children affected.
Food insecurity was expected to worsen from July to December 2016.
At , Angola is the world's twenty-third largest country - comparable in size to Mali, or twice the size of France or of Texas.
It lies mostly between latitudes 4° and 18°S, and longitudes 12° and 24°E.
Angola borders Namibia to the south, Zambia to the east, the Democratic Republic of the Congo to the north-east and the South Atlantic Ocean to the west.
The coastal exclave of Cabinda in the north has borders with the Republic of the Congo to the north and with the Democratic Republic of the Congo to the south.
Angola's capital, Luanda, lies on the Atlantic coast in the northwest of the country.
Angola, although located in a tropical zone, has a climate that is not characterized for this region, due to the confluence of three factors:


As a result, Angola's climate is characterized by two seasons: rainfall from October to April and drought, known as "Cacimbo", from May to August, drier, as the name implies, and with lower temperatures.
On the other hand, while the coastline has high rainfall rates, decreasing from North to South and from to , with average annual temperatures above , the interior zone can be divided into three areas:


 

The Angolan government is composed of three branches of government: executive, legislative and judicial.
The executive branch of the government is composed of the President, the Vice-Presidents and the Council of Ministers.
The legislative branch comprises a 220-seat unicameral legislature elected from both provincial and nationwide constituencies.
For decades, political power has been concentrated in the presidency.
The Constitution of 2010 establishes the broad outlines of government structure and delineates the rights and duties of citizens.
The legal system is based on Portuguese law and customary law but is weak and fragmented, and courts operate in only 12 of more than 140 municipalities.
A Supreme Court serves as the appellate tribunal; a Constitutional Court does not hold the powers of judicial review.
Governors of the 18 provinces are appointed by the president.
After the end of the civil war the regime came under pressure from within as well as from the international community to become more democratic and less authoritarian.
Its reaction was to implement a number of changes without substantially changing its character.
Angola is classified as 'not free' by Freedom House in the Freedom in the World 2014 report.
The report noted that the August 2012 parliamentary elections, in which the ruling Popular Movement for the Liberation of Angola won more than 70% of the vote, suffered from serious flaws, including outdated and inaccurate voter rolls.
Voter turnout dropped from 80% in 2008 to 60%.
Angola scored poorly on the 2013 Ibrahim Index of African Governance.
It was ranked 39 out of 52 sub-Saharan African countries, scoring particularly badly in the areas of participation and human rights, sustainable economic opportunity and human development.
The Ibrahim Index uses a number of variables to compile its list which reflects the state of governance in Africa.
The new constitution, adopted in 2010, did away with presidential elections, introducing a system in which the president and the vice-president of the political party that wins the parliamentary elections automatically become president and vice-president.
Directly or indirectly, the president controls all other organs of the state, so there is "de facto" no separation of powers.
In the classifications used in constitutional law, this government falls under the category of "authoritarian regime."
On 16 October 2014, Angola was elected for the second time as a non-permanent member of the UN Security Council, with 190 favourable votes out of 193.
The mandate began on 1 January 2015 and lasts for two years.
Also that month, the country took on the leadership of the African ministers and governors at the International Monetary Fund and the World Bank, following debates at the annual meetings of both entities.
Since January 2014 the Republic of Angola has held the rotating presidency of the International Conference on the Great Lakes Region (ICGLR).
In 2015, the executive secretary of ICGLR, Ntumba Luaba, called Angola an example to be followed because of the significant progress it made over the 12 years of peace, particularly in terms of socioeconomic and political-military stability.
After 38 years of rule, in 2017 President dos Santos stepped down from MPLA leadership.
The leader of the winning party at the parliamentary elections in August 2017 become the next president of Angola.
The MPLA selected Defense Minister General João Lourenço and won the election.
In what has been described as a political purge to cement his power and reduce the influence of the Dos Santos family, Lourenço subsequently sacked the chief of the national police, Ambrósio de Lemos, and the head of the intelligence service, Apolinário José Pereira.
Both are considered allies of former president Dos Santos.
He also removed Isabel Dos Santos, daughter of the former president, as head of the country's state oil company Sonangol.
The Angolan Armed Forces (AAF) is headed by a Chief of Staff who reports to the Minister of Defence.
There are three divisions—the Army (Exército), Navy (Marinha de Guerra, MGA) and National Air Force (Força Aérea Nacional, FAN).
Total manpower is about 110,000.
Its equipment includes Russian-manufactured fighters, bombers and transport planes.
There are also Brazilian-made EMB-312 Tucanos for training, Czech-made L-39s for training and bombing, and a variety of western-made aircraft such as the C-212\Aviocar, Sud Aviation Alouette III, etc.
A small number of AAF personnel are stationed in the Democratic Republic of the Congo (Kinshasa) and the Republic of the Congo (Brazzaville).
The National Police departments are Public Order, Criminal Investigation, Traffic and Transport, Investigation and Inspection of Economic Activities, Taxation and Frontier Supervision, Riot Police and the Rapid Intervention Police.
The National Police are in the process of standing up an air wing, to provide helicopter support for operations.
The National Police are developing their criminal investigation and forensic capabilities.
The force has an estimated 6,000 patrol officers, 2,500 taxation and frontier supervision officers, 182 criminal investigators and 100 financial crimes detectives and around 90 economic activity inspectors.
The National Police have implemented a modernisation and development plan to increase the capabilities and efficiency of the total force.
In addition to administrative reorganisation, modernisation projects include procurement of new vehicles, aircraft and equipment, construction of new police stations and forensic laboratories, restructured training programmes and the replacement of AKM rifles with 9 mm Uzis for officers in urban areas.
A Supreme Court serves as a court of appeal.
The Constitutional Court is the supreme body of the constitutional jurisdiction, its Organic Law was approved by Law no.
2/08, of June 17, and the legal system is based on Portuguese and customary laws, but it is weak and fragmented.
There are only 12 courts in more than 140 counties in the country.
With the approval of Law no.
2/08, of June 17 – Organic Law of the Constitutional Court and Law n.
3/08, of June 17 – Organic Law of the Constitutional Process, the Legal Creation of the Constitutional Court.
Its first task was the validation of the candidacies of the political parties to the legislative elections of 5 September 2008.Thus, on June 25, 2008, the Constitutional Court was institutionalized and its Judicial Counselors assumed the position before the President of the Republic.
Currently, seven advisory judges are present, four men and three women.
In 2014, a new penal code took effect in Angola.
The classification of money-laundering as a crime is one of the novelties in the new legislation.
On 16 October 2014, Angola was elected for the second time a non-permanent member of the United Nations Security Council, with 190 favorable votes out of a total of 193.
The term of office begins on 1 January 2015 and lasts for two years.
Since January 2014, the Republic of Angola has been chairing the International Conference for the Great Lakes Region (CIRGL).
[80] In 2015, CIRGL Executive Secretary Ntumba Luaba said that Angola is the example to be followed by the members of the organization, due to the significant progress made during the 12 years of peace, namely in terms of socio-economic stability and political- military.
Homosexual acts are currently illegal in Angola.
However, in February 2017, the Angolan Parliament approved a new penal code which does not outlaw homosexual acts.
The law will take effect in late 2017.
In 2010, the Angolan Government refused to receive openly gay Isi Yanouka as the new Israeli ambassador, allegedly due to his sexual orientation.
, Angola is divided into eighteen provinces ("províncias") and 162 municipalities.
The municipalities are further divided into 559 communes (townships).
The provinces are:

With an area of approximately , the Northern Angolan province of Cabinda is unusual in being separated from the rest of the country by a strip, some wide, of the Democratic Republic of Congo along the lower Congo River.
Cabinda borders the Congo Republic to the north and north-northeast and the DRC to the east and south.
The town of Cabinda is the chief population centre.
According to a 1995 census, Cabinda had an estimated population of 600,000, approximately 400,000 of whom live in neighbouring countries.
Population estimates are, however, highly unreliable.
Consisting largely of tropical forest, Cabinda produces hardwoods, coffee, cocoa, crude rubber and palm oil.
The product for which it is best known, however, is its oil, which has given it the nickname, "the Kuwait of Africa".
Cabinda's petroleum production from its considerable offshore reserves now accounts for more than half of Angola's output.
Most of the oil along its coast was discovered under Portuguese rule by the Cabinda Gulf Oil Company (CABGOC) from 1968 onwards.
Ever since Portugal handed over sovereignty of its former overseas province of Angola to the local independence groups (MPLA, UNITA and FNLA), the territory of Cabinda has been a focus of separatist guerrilla actions opposing the Government of Angola (which has employed its armed forces, the FAA—Forças Armadas Angolanas) and Cabindan separatists.
The Front for the Liberation of the Enclave of Cabinda-Armed Forces of Cabinda (FLEC-FAC) announced a virtual Federal Republic of Cabinda under the Presidency of N'Zita Henriques Tiago.
One of the characteristics of the Cabindan independence movement is its constant fragmentation, into smaller and smaller factions.
Angola has diamonds, oil, gold, copper and a rich wildlife (dramatically impoverished during the civil war), forest and fossil fuels.
Since independence, oil and diamonds have been the most important economic resource.
Smallholder and plantation agriculture dramatically dropped in the Angolan Civil War, but began to recover after 2002.
The transformation industry of the late colonial period collapsed at independence, because of the exodus of most of the ethnic Portuguese population, but it has begun to re-emerge with updated technologies, partly because of an influx of new Portuguese entrepreneurs.
Similar developments have taken place in the service sector.
Angola's economy has in recent years moved on from the disarray caused by a quarter-century of Angolan civil war to become the fastest-growing economy in Africa and one of the fastest-growing in the world, with an average GDP growth of 20% between 2005 and 2007.
In the period 2001–10, Angola had the world's highest annual average GDP growth, at 11.1%.
In 2004, the Exim Bank of China approved a $2 billion line of credit to Angola, to be used for rebuilding Angola's infrastructure, and to limit the influence of the International Monetary Fund there.
China is Angola's biggest trade partner and export destination as well as the fourth-largest source of imports.
Bilateral trade reached $27.67 billion in 2011, up 11.5% year-on-year.
China's imports, mainly crude oil and diamonds, increased 9.1% to $24.89 billion while China's exports to Angola, including mechanical and electrical products, machinery parts and construction materials, surged 38.8%.
The oil glut led to a local price for unleaded gasoline of £0.37 a gallon.
"The Economist" reported in 2008 that diamonds and oil make up 60% of Angola's economy, almost all of the country's revenue and all of its dominant exports.
Growth is almost entirely driven by rising oil production which surpassed in late 2005 and was expected to grow to by 2007.
Control of the oil industry is consolidated in Sonangol Group, a conglomerate owned by the Angolan government.
In December 2006, Angola was admitted as a member of OPEC.
Operations in its diamond mines include partnerships between state-run Endiama and mining companies such as ALROSA which operate in Angola.
The Angolan economy grew 18% in 2005, 26% in 2006 and 17.6% in 2007.
Due to the global recession the economy contracted an estimated −0.3% in 2009.
The security brought about by the 2002 peace settlement has allowed the resettlement of 4 million displaced persons and a resulting large-scale increases in agriculture production.
Although the country's economy has grown significantly since Angola achieved political stability in 2002, mainly due to fast-rising earnings in the oil sector, Angola faces huge social and economic problems.
These are in part a result of almost continual armed conflict from 1961 on, although the highest level of destruction and socio-economic damage took place after the 1975 independence, during the long years of civil war.
However, high poverty rates and blatant social inequality chiefly stem from persistent authoritarianism, "neo-patrimonial" practices at all levels of the political, administrative, military and economic structures, and of a pervasive corruption.
The main beneficiaries are political, administrative, economic and military power holders, who have accumulated (and continue to accumulate) enormous wealth.
"Secondary beneficiaries" are the middle strata which are about to become social classes.
However, almost half the population has to be considered poor, with dramatic differences between the countryside and the cities (where by now slightly more than 50% of the people live).
A study carried out in 2008 by the Angolan Instituto Nacional de Estatística found that in rural areas roughly 58% must be classified as "poor" according to UN norms, but in the urban areas only 19%, and an overall rate of 37%.
In cities, a majority of families, well beyond those officially classified as poor, must adopt a variety of survival strategies.
In urban areas social inequality is most evident and it's extreme in Luanda.
In the Human Development Index Angola constantly ranks in the bottom group.
According to the Heritage Foundation, a conservative American think tank, oil production from Angola has increased so significantly that Angola now is China's biggest supplier of oil.
“China has extended three multibillion dollar lines of credit to the Angolan government; two loans of $2 billion from China Exim Bank, one in 2004, the second in 2007, as well as one loan in 2005 of $2.9 billion from China International Fund Ltd.”

Growing oil revenues also created opportunities for corruption: according to a recent Human Rights Watch report, 32 billion US dollars disappeared from government accounts in 2007–2010.
Furthermore, Sonangol, the state-run oil company, controls 51% of Cabinda's oil.
Due to this market control the company ends up determining the profit received by the government and the taxes it pays.
The council of foreign affairs states that the World Bank mentioned that Sonangol " is a taxpayer, it carries out quasi-fiscal activities, it invests public funds, and, as concessionaire, it is a sector regulator.
This multifarious work programme creates conflicts of interest and characterises a complex relationship between Sonangol and the government that weakens the formal budgetary process and creates uncertainty as regards the actual fiscal stance of the state."
Before independence in 1975, Angola was a breadbasket of southern Africa and a major exporter of bananas, coffee and sisal, but three decades of civil war (1975–2002) destroyed fertile countryside, left it littered with landmines and drove millions into the cities.
The country now depends on expensive food imports, mainly from South Africa and Portugal, while more than 90% of farming is done at the family and subsistence level.
Thousands of Angolan small-scale farmers are trapped in poverty.
The enormous differences between the regions pose a serious structural problem for the Angolan economy, illustrated by the fact that about one third of economic activities are concentrated in Luanda and neighbouring Bengo province, while several areas of the interior suffer economic stagnation and even regression.
One of the economic consequences of the social and regional disparities is a sharp increase in Angolan private investments abroad.
The small fringe of Angolan society where most of the asset accumulation takes place seeks to spread its assets, for reasons of security and profit.
For the time being, the biggest share of these investments is concentrated in Portugal where the Angolan presence (including the family of the state president) in banks as well as in the domains of energy, telecommunications, and mass media has become notable, as has the acquisition of vineyards and orchards as well as of touristic enterprises.
Sub-Saharan Africa nations are globally achieving impressive improvements in well-being, according to a report by Tony Blair's Africa Governance Initiative and the Boston Consulting Group.
Angola has upgraded critical infrastructure, an investment made possible by funds from the nation's development of oil resources.
According to this report, just slightly more than ten years after the end of the civil war Angola's standard of living has overall greatly improved.
Life expectancy, which was just 46 years in 2002, reached 51 in 2011.
Mortality rates for children fell from 25 percent in 2001 to 19 percent in 2010 and the number of students enrolled in primary school has tripled since 2001.
However, at the same time the social and economic inequality that has characterised the country since long has not diminished, but on the contrary deepened in all respects.
With a stock of assets corresponding to 70 billion Kz (6.8 billion USD), Angola is now the third largest financial market in sub-Saharan Africa, surpassed only by Nigeria and South Africa.
According to the Angolan Minister of Economy, Abraão Gourgel, the financial market of the country grew modestly from 2002 and now lies in third place at the level of sub-Saharan Africa.
Angola's economy is expected to grow by 3.9 percent in 2014 said the International Monetary Fund (IMF), robust growth in the non-oil economy, mainly driven by a very good performance in the agricultural sector, is expected to offset a temporary drop in oil production.
Angola's financial system is maintained by the National Bank of Angola and managed by governor .
According to a study on the banking sector, carried out by Deloitte, the monetary policy led by Banco Nacional de Angola (BNA), the Angolan national bank, allowed a decrease in the inflation rate put at 7.96% in December 2013, which contributed to the sector's growth trend.
Estimates released by Angola's central bank, said country's economy should grow at an annual average rate of 5 percent over the next four years, boosted by the increasing participation of the private sector.
On 19 December 2014, the Capital Market in Angola started.
BODIVA (Angola Securities and Debt Stock Exchange, in English) received the secondary public debt market, and it is expected to start the corporate debt market by 2015, but the stock market should be a reality only in 2016.
Agriculture and forestry is an area of potential opportunity for the country.
The African Economic Outlook organization states that “Angola requires 4.5 million tonnes a year of grain but grows only about 55% of the corn it needs, 20% of the rice and just 5% of its required wheat”.
In addition, the World Bank estimates that “less than 3 percent of Angola's abundant fertile land is cultivated and the economic potential of the forestry sector remains largely unexploited" .
Transport in Angola consists of:

Travel on highways outside of towns and cities in Angola (and in some cases within) is often not best advised for those without four-by-four vehicles.
While a reasonable road infrastructure has existed within Angola, time and the war have taken their toll on the road surfaces, leaving many severely potholed, littered with broken asphalt.
In many areas drivers have established alternate tracks to avoid the worst parts of the surface, although careful attention must be paid to the presence or absence of landmine warning markers by the side of the road.
The Angolan government has contracted the restoration of many of the country's roads.
The road between Lubango and Namibe, for example, was completed recently with funding from the European Union, and is comparable to many European main routes.
Completing the road infrastructure is likely to take some decades, but substantial efforts are already being made.
Transport is an important aspect in Angola because it is strategically located and it could become a regional logistics hub.
In addition Angola has some of the most important and biggest ports and so it is vital to connect them to the interior of the country as well as to neighbouring countries.
Tourism is restarting on the heels of the long ended stop in the civil war, and very few tourists venture anywhere in Angola yet due to lack of infrastructure.
The telecommunications industry is considered one of the main strategic sectors in Angola.
In October 2014, the building of an optic fiber underwater cable was announced.
This project aims to turn Angola into a continental hub, thus improving Internet connections both nationally and internationally.
On 11 March 2015, the First Angolan Forum of Telecommunications and Information Technology was held in Luanda under the motto "The challenges of telecommunications in the current context of Angola", to promote debate on topical issues on telecommunications in Angola and worldwide.
A study of this sector, presented at the forum, said Angola had the first telecommunications operator in Africa to test LTE – with speeds up to 400Mbit/s – and mobile penetration of about 75%; there are about 3.5 million smartphone in the Angolan market; There are about of optical fibre installed in the country.
The first Angolan satellite, AngoSat-1, was launched into orbit on 26 December 2017.
It was launched from the Baikonur space center in Kazakhstan on board a Zenit 3F rocket.
The satellite was built by Russia's RSC Energia, a subsidiary of the state-run space industry player Roscosmos.
The satellite payload was supplied by Airbus Defence & Space.
Due to an on-board power failure during solar panel deployment, on Dec.
27, RSC Energia revealed that they lost communications contact with the satellite.
Although, subsequent attempts to restore communications with the satellite were successful, the satellite eventually stopped sending data and RSC Energia confirmed that AngoSat-1 was inoperable.
The launch of AngoSat-1 was aimed at ensuring telecommunications throughout the country.
According to Aristides Safeca, Secretary of State for Telecommunications, the satellite was aimed at providing telecommunications services, TV, internet and e-government and was expected to remain in orbit "at best" for 18 years.
A replacement satellite named AngoSat-2 is in the works and is expected to be in service by 2020.
The management of the top-level domain '.ao' will pass from Portugal to Angola in 2015, following new legislation.
A joint decree of minister of Telecommunications and Information Technologies José Carvalho da Rocha and the minister of Science and Technology, Maria Cândida Pereira Teixeira, states that "under the massification" of that Angolan domain, "conditions are created for the transfer of the domain root '.ao' of Portugal to Angola".
Angola has a population of 24,383,301 inhabitants according to the preliminary results of its 2014 census, the first one conducted or carried out since 15 December 1970.
It is composed of Ovimbundu (language Umbundu) 37%, Ambundu (language Kimbundu) 23%, Bakongo 13%, and 32% other ethnic groups (including the Chokwe, the Ovambo, the Ganguela and the Xindonga) as well as about 2% "mestiços" (mixed European and African), 1.6% Chinese and 1% European.
The Ambundu and Ovimbundu ethnic groups combined form a majority of the population, at 62%.
The population is forecast to grow to over 60 million people to 2050, 2.7 times the 2014 population.
However, on 23 March 2016, official data revealed by Angola's National Statistic Institute – Instituto Nacional de Estatística (INE), states that Angola has a population of 25.789.024 inhabitants.
It is estimated that Angola was host to 12,100 refugees and 2,900 asylum seekers by the end of 2007.
11,400 of those refugees were originally from the Democratic Republic of Congo, who arrived in the 1970s.
there were an estimated 400,000 Democratic Republic of the Congo migrant workers, at least 220,000 Portuguese, and about 259,000 Chinese living in Angola.
Since 2003, more than 400,000 Congolese migrants have been expelled from Angola.
Prior to independence in 1975, Angola had a community of approximately 350,000 Portuguese, but the vast majority left after independence and the ensuing civil war.
However, Angola has recovered its Portuguese minority in recent years; currently, there are about 200,000 registered with the consulates, and increasing due to the debt crisis in Portugal and the relative prosperity in Angola.
The Chinese population stands at 258,920, mostly composed of temporary migrants.
Also, there is a small Brazilian community of about 5,000 people.
The total fertility rate of Angola is 5.54 children born per woman (2012 estimates), the 11th highest in the world.
The languages in Angola are those originally spoken by the different ethnic groups and Portuguese, introduced during the Portuguese colonial era.
The most widely spoken indigenous languages are Umbundu, Kimbundu and Kikongo, in that order.
Portuguese is the official language of the country.
Although the exact numbers of those fluent in Portuguese or who speak Portuguese as a first language are unknown, a 2012 study mentions that Portuguese is the first language of 39% of the population.
In 2014, a census carried out by the Instituto Nacional de Estatística in Angola mentions that 71.15% of the nearly 25.8 million inhabitants of Angola (meaning around 18.3 million people) use Portuguese as a first or second language.
There are about 1,000 religious communities, mostly Christian, in Angola.
While reliable statistics are nonexistent, estimates have it that more than half of the population are Catholics, while about a quarter adhere to the Protestant churches introduced during the colonial period: the Congregationalists mainly among the Ovimbundu of the Central Highlands and the coastal region to its west, the Methodists concentrating on the Kimbundu speaking strip from Luanda to Malanje, the Baptists almost exclusively among the Bakongo of the north-west (now present in Luanda as well) and dispersed Adventists, Reformed and Lutherans.
In Luanda and region there subsists a nucleus of the "syncretic" Tocoists and in the north-west a sprinkling of Kimbanguism can be found, spreading from the Congo/Zaïre.
Since independence, hundreds of Pentecostal and similar communities have sprung up in the cities, where by now about 50% of the population is living; several of these communities/churches are of Brazilian origin.
In a study assessing nations' levels of religious regulation and persecution with scores ranging from 0 to 10 where 0 represented low levels of regulation or persecution, Angola was scored 0.8 on Government Regulation of Religion, 4.0 on Social Regulation of Religion, 0 on Government Favoritism of Religion and 0 on Religious Persecution.
Foreign missionaries were very active prior to independence in 1975, although since the beginning of the anti-colonial fight in 1961 the Portuguese colonial authorities expelled a series of Protestant missionaries and closed mission stations based on the belief that the missionaries were inciting pro-independence sentiments.
Missionaries have been able to return to the country since the early 1990s, although security conditions due to the civil war have prevented them until 2002 from restoring many of their former inland mission stations.
The Catholic Church and some major Protestant denominations mostly keep to themselves in contrast to the "New Churches" which actively proselytize.
Catholics, as well as some major Protestant denominations, provide help for the poor in the form of crop seeds, farm animals, medical care and education.
Epidemics of cholera, malaria, rabies and African hemorrhagic fevers like Marburg hemorrhagic fever, are common diseases in several parts of the country.
Many regions in this country have high incidence rates of tuberculosis and high HIV prevalence rates.
Dengue, filariasis, leishmaniasis and onchocerciasis (river blindness) are other diseases carried by insects that also occur in the region.
Angola has one of the highest infant mortality rates in the world and one of the world's lowest life expectancies.
A 2007 survey concluded that low and deficient niacin status was common in Angola.
Demographic and Health Surveys is currently conducting several surveys in Angola on malaria, domestic violence and more.
In September 2014, the Angolan Institute for Cancer Control (IACC) was created by presidential decree, and it will integrate the National Health Service in Angola.
The purpose of this new centre is to ensure health and medical care in oncology, policy implementation, programmes and plans for prevention and specialised treatment.
This cancer institute will be assumed as a reference institution in the central and southern regions of Africa.
In 2014, Angola launched a national campaign of vaccination against measles, extended to every child under ten years old and aiming to go to all 18 provinces in the country.
The measure is part of the Strategic Plan for the Elimination of Measles 2014–2020 created by the Angolan Ministry of Health which includes strengthening routine immunisation, a proper dealing with measles cases, national campaigns, introducing a second dose of vaccination in the national routine vaccination calendar and active epidemiological surveillance for measles.
This campaign took place together with the vaccination against polio and vitamin A supplementation.
A yellow fever outbreak, the worst in the country in three decades began in December 2015.
By August 2016, when the outbreak began to subside, nearly 4,000 people were suspected of being infected.
As many as 369 may have died.
The outbreak began in the capital, Luanda, and spread to at least 16 of the 18 provinces.
Although by law education in Angola is compulsory and free for eight years, the government reports that a percentage of pupils are not attending due to a lack of school buildings and teachers.
Pupils are often responsible for paying additional school-related expenses, including fees for books and supplies.
In 1999, the gross primary enrollment rate was 74 percent and in 1998, the most recent year for which data are available, the net primary enrollment rate was 61 percent.
Gross and net enrollment ratios are based on the number of pupils formally registered in primary school and therefore do not necessarily reflect actual school attendance.
There continue to be significant disparities in enrollment between rural and urban areas.
In 1995, 71.2 percent of children ages 7 to 14 years were attending school.
It is reported that higher percentages of boys attend school than girls.
During the Angolan Civil War (1975–2002), nearly half of all schools were reportedly looted and destroyed, leading to current problems with overcrowding.
The Ministry of Education recruited 20,000 new teachers in 2005 and continued to implement teacher trainings.
Teachers tend to be underpaid, inadequately trained and overworked (sometimes teaching two or three shifts a day).
Some teachers may reportedly demand payment or bribes directly from their pupils.
Other factors, such as the presence of landmines, lack of resources and identity papers, and poor health prevent children from regularly attending school.
Although budgetary allocations for education were increased in 2004, the education system in Angola continues to be extremely under-funded.
According to estimates by the UNESCO Institute for Statistics, the adult literacy rate in 2011 was 70.4%.
By 2015, this had increased to 71.1%.
82.9% of males and 54.2% of women are literate as of 2001.
Since independence from Portugal in 1975, a number of Angolan students continued to be admitted every year at high schools, polytechnical institutes and universities in Portugal, Brazil and Cuba through bilateral agreements; in general, these students belong to the elites.
In September 2014, the Angolan Ministry of Education announced an investment of 16 million Euros in the computerisation of over 300 classrooms across the country.
The project also includes training teachers at a national level, "as a way to introduce and use new information technologies in primary schools, thus reflecting an improvement in the quality of teaching."
In 2010, the Angolan government started building the Angolan Media Libraries Network, distributed throughout several provinces in the country to facilitate the people's access to information and knowledge.
Each site has a bibliographic archive, multimedia resources and computers with Internet access, as well as areas for reading, researching and socialising.
The plan envisages the establishment of one media library in each Angolan province by 2017.
The project also includes the implementation of several media libraries, in order to provide the several contents available in the fixed media libraries to the most isolated populations in the country.
At this time, the mobile media libraries are already operating in the provinces of Luanda, Malanje, Uíge, Cabinda and Lunda South.
As for REMA, the provinces of Luanda, Benguela, Lubango and Soyo have currently working media libraries.
The "substrate" of Angolan culture is African, predominantly Bantu, while Portuguese culture has had a significant impact, specifically in terms of language and religion.
The diverse ethnic communities – the Ovimbundu, Ambundu, Bakongo, Chokwe, Mbunda and other peoples – to varying degrees maintain their own cultural traits, traditions and languages, but in the cities, where slightly more than half of the population now lives, a mixed culture has been emerging since colonial times; in Luanda, since its foundation in the 16th century.
In this urban culture, the Portuguese heritage has become more and more dominant.
African roots are evident in music and dance, and is moulding the way in which Portuguese is spoken.
This process is well reflected in contemporary Angolan literature, especially in the works of Angolan authors.
In 2014, Angola resumed the National Festival of Angolan Culture after a 25-year break.
The festival took place in all the provincial capitals and lasted for 20 days, with the theme "Culture as a Factor of Peace and Development".
In 1972, one of Angola's first feature films, Sarah Maldoror's internationally co-produced "Sambizanga", was released at the Carthage Film Festival to critical acclaim, winning the "Tanit d'Or", the festival's highest prize.
Basketball is the most popular sport in Angola.
Its national team has won the AfroBasket 11 times and holds the record of most titles.
As a top team in Africa, it's a regular competitor at the Summer Olympic Games and the FIBA World Cup.
In football, Angola hosted the 2010 Africa Cup of Nations.
The Angola national football team qualified for the 2006 FIFA World Cup, as this was their first appearance on the World Cup finals stage.
They were eliminated after one defeat and two draws in the group stage.
They won 3 COSAFA Cups and finished runner up in 2011 African Nations Championship.
Angola has participated in the World Women's Handball Championship for several years.
The country has also appeared in the Summer Olympics for seven years and both regularly competes in and once has hosted the FIRS Roller Hockey World Cup, where the best finish is sixth.
Angola is also often believed to have historic roots in the martial art "Capoeira Angola" and "Batuque" which were practiced by enslaved African Angolans transported as part of the Atlantic slave trade.
</doc>
<doc id="704" url="https://en.wikipedia.org/wiki?curid=704" title="Demographics of Angola">
Demographics of Angola

This article is about the demographic features of the population of Angola, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.
According to 2014 census data, Angola had a population of 25,789,024 inhabitants in 2014.
Ethnically, there are three main groups, each speaking a Bantu language: the Ovimbundu who represent 37% of the population, the Ambundu with 25%, and the Bakongo 13%.
Other numerically important groups include the closely interrelated Chokwe and Lunda, the Ganguela and Nyaneka-Khumbi (in both cases classification terms that stand for a variety of small groups), the Ovambo, the Herero, the Xindonga and scattered residual groups of San.
In addition, mixed race (European and African) people amount to about 2%, with a small (1%) population of whites, mainly ethnically Portuguese.
As a former overseas territory of Portugal until 1975, Angola possesses a Portuguese population of over 200,000, a number that has been growing from 2000 onwards, because of Angola's growing demand for qualified human resources.
Besides the Portuguese, significant numbers of people from other European and from diverse Latin American countries (especially Brazil) can be found.
From the 2000s, many Chinese have settled and started up small businesses, while at least as many have come as workers for large enterprises (construction or other).
Observers claim that the Chinese community in Angola might include as many as 300,000 persons at the end of 2010, but reliable statistics are not at this stage available.
In 1974/75, over 25,000 Cuban soldiers arrived in Angola to help the MPLA forces at the beginning of the Angolan Civil War.
Once this was over, a massive development cooperation in the field of health and education brought in numerous civil personnel from Cuba.
However, only a very small percentage of all these people has remained in Angola, either for personal reasons (intermarriage) or as professionals (e.g., medical doctors).
The largest religious denomination is Catholicism, to which adheres about half the population.
Roughly 26% are followers of traditional forms of Protestantism (Congregationals, Methodists, Baptista, Lutherans, Reformed), but over the last decades there has in addition been a growth of Pentecostal communities and African Initiated Churches.
In 2006, one out of 221 people were Jehovah's Witnesses.
Blacks from Mali, Nigeria and Senegal are mostly Sunnite Muslims, but do not make up more than 1 - 2% of the population.
By now few Angolans retain African traditional religions following different ethnic faiths.
According to the total population was in , compared to only 4 148 000 in 1950.
The proportion of children below the age of 15 in 2010 was 46.6%, 50.9% was between 15 and 65 years of age, while 2.5% was 65 years or older

Structure of the population (DHS 2011) (Males 19 707, Females 20 356 = 40 063) :

Registration of vital events is in Angola not complete.
The Population Department of the United Nations prepared the following estimates.
Total Fertility Rate (TFR) (Wanted TFR) and Crude Birth Rate (CBR):

The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.
The population is growing by 3.52% annually.
There are 44.2 births and 9.2 deaths per 1,000 citizens.
The net migration rate is 0.2 migrants per 1,000 citizens.
The fertility rate of Angola is 6.16 children born per woman as of 2017.
The infant mortality rate is 67.6 deaths for every 1,000 live births with 73.3 deaths for males and 61.8 deaths for females for every 1,000 live births.
Life expectancy at birth is 60.2 years; 58.2 years for males and 62.3 years for females.
According to the CIA World Factbook, 2% of adults (aged 15–49) are living with HIV/AIDS (as of 2009).
The risk of contracting disease is very high.
There are food and waterborne diseases, bacterial and protozoal diarrhea, hepatitis A, and typhoid fever; vectorborne diseases, malaria, African trypanosomiasis (sleeping sickness); respiratory disease: meningococcal meningitis, and schistosomiasis, a water contact disease, as of 2005.
Roughly 37% of Angolans are Ovimbundu, 25% are Ambundu, 13% are Bakongo, 2% are mestiço, 1-2% are white Africans, and people from other African ethnicities make up 22% of Angola's population.
Angola is a majority Christian country.
Official statistics don't exist, but it is estimated that over 80% belong to a Christian church or community.
More than half are Catholic, the remaining ones comprising members of traditional Protestant churches as well as of Pentecostal communities.
Only 1 - 2% are Muslims - generally immigrants from other African countries.
Traditional indigenous religions are practized by a very small minority, generally in peripheral rural societies.
Literacy is quite low, with 71.1% of the population over the age of 15 able to read and write in Portuguese.
82% of males and 60.7% of women are literate as of 2015.
Portuguese is the official language of Angola, but Bantu and other African languages are also widely spoken.
In fact, Kikongo, Kimbundu, Umbundu, Tuchokwe, Nganguela, and Ukanyama have the official status of "national languages".
The mastery of Portuguese is widespread; in the cities the overwhelming majority are either fluent in Portuguese or have at least a reasonable working knowledge of this language; an increasing minority are native Portuguese speakers and have a poor, if any, knowledge of an African language.
</doc>
<doc id="705" url="https://en.wikipedia.org/wiki?curid=705" title="Politics of Angola">
Politics of Angola

Since the adoption of a new constitution in 2010, the politics of Angola takes place in a framework of a presidential republic, whereby the President of Angola is both head of state and head of government, and of a multi-party system.
Executive power is exercised by the government.
Legislative power is vested in the President, the government and parliament.
Angola changed from a one-party Marxist-Leninist system ruled by the Popular Movement for the Liberation of Angola (MPLA), in place since independence in 1975, to a multiparty democracy based on a new constitution adopted in 1992.
That same year the first parliamentary and presidential elections were held.
The MPLA won an absolute majority in the parliamentary elections.
In the presidential elections, President José Eduardo dos Santos won the first round election with more than 49% of the vote to Jonas Savimbi's 40%.
A runoff election would have been necessary, but never took place.
The renewal of civil war immediately after the elections, which were considered as fraudulent by UNITA, and the collapse of the Lusaka Protocol, created a split situation.
To a certain degree the new democratic institutions worked, notably the National Assembly, with the active participation of UNITA's and the FNLA's elected MPs - while José Eduardo dos Santos continued to exercise his functions without democratic legitimation.
However the armed forces of the MPLA (now the official armed forces of the Angolan state) and of UNITA fought each other until the leader of UNITA, Jonas Savimbi, was killed in action in 2002.
From 2002 to 2010, the system as defined by the constitution of 1992 functioned in a relatively normal way.
The executive branch of the government was composed of the President, the Prime Minister and Council of Ministers.
The Council of Ministers, composed of all ministers and vice ministers, met regularly to discuss policy issues.
Governors of the 18 provinces were appointed by and served at the pleasure of the president.
The Constitutional Law of 1992 established the broad outlines of government structure and the rights and duties of citizens.
The legal system was based on Portuguese and customary law but was weak and fragmented.
Courts operated in only 12 of more than 140 municipalities.
A Supreme Court served as the appellate tribunal; a Constitutional Court with powers of judicial review was never constituted despite statutory authorization.
In practice, power was more and more concentrated in the hands of the President who, supported by an ever-increasing staff, largely controlled parliament, government, and the judiciary.
The 26-year-long civil war has ravaged the country's political and social institutions.
The UN estimates of 1.8 million internally displaced persons (IDPs), while generally the accepted figure for war-affected people is 4 million.
Daily conditions of life throughout the country and specifically Luanda (population approximately 6 million) mirror the collapse of administrative infrastructure as well as many social institutions.
The ongoing grave economic situation largely prevents any government support for social institutions.
Hospitals are without medicines or basic equipment, schools are without books, and public employees often lack the basic supplies for their day-to-day work.
The 2010 constitution grants the President almost absolute power.
Elections for the National assembly are to take place every five years, and the President is automatically the leader of the winning party or coalition.
It is for the President to appoint (and dismiss) all of the following:
The President is also provided a variety of powers, like defining the policy of the country.
Even though it's not up to him/her to make laws (only to promulgate them and make edicts), the President is the leader of the winning party.
The only "relevant" post that is not directly appointed by the President is the Vice-President, which is the second in the winning party.
The National Assembly ("Assembleia Nacional") has 223 members, elected for a four-year term, 130 members by proportional representation, 90 members in provincial districts, and 3 members to represent Angolans abroad.
The next general elections, due for 1997, have been rescheduled for 5 September 2008.
The ruling party MPLA won 82% (191 seats in the National Assembly) and the main opposition party won only 10% (16 seats).
The elections however have been described as only partly free but certainly not fair.
A White Book on the elections in 2008 lists up all irregularities surrounding the Parliamentary elections of 2008.
Supreme Court (or "Tribunal da Relacao") judges of the Supreme Court are appointed by the president.
The Constitutional Court, with the power of judicial review, contains 11 justices.
Four are appointed by the President, four by the National Assembly, two by the Superior Council of the Judiciary, and one elected by the public.
Angola has eighteen provinces (provincias, singular - provincia); Bengo, Benguela, Bie, Cabinda, Cuando Cubango, Cuanza Norte, Cuanza Sul, Cunene, Huambo, Huila, Luanda, Lunda Norte, Lunda Sul, Malanje, Moxico, Namibe, Uige, Zaire

Front for the Liberation of the Enclave of Cabinda or FLEC (Henrique N'zita Tiago; António Bento Bembe)

African, Caribbean and Pacific Group of States, AfDB, CEEAC, United Nations Economic Commission for Africa, FAO, Group of 77, IAEA, IBRD, ICAO, International Criminal Court (signatory), ICFTU, International Red Cross and Red Crescent Movement, International Development Association, IFAD, IFC, IFRCS, International Labour Organization, International Monetary Fund, International Maritime Organization, Interpol, IOC, International Organization for Migration, ISO (correspondent), ITU, Non-Aligned Council (temporary), UNCTAD, UNESCO, UNIDO, UPU, World Customs Organization, World Federation of Trade Unions, WHO, WIPO, WMO, WToO, WTrO




</doc>
<doc id="706" url="https://en.wikipedia.org/wiki?curid=706" title="Economy of Angola">
Economy of Angola

The Economy of Angola is one of the fastest-growing in the world, with reported annual average GDP growth of 11.1 percent from 2001 to 2010.
It is still recovering from 27 years of the civil war that plagued the country from its independence in 1975 to 2002.
Despite extensive oil and gas resources, diamonds, hydroelectric potential, and rich agricultural land, Angola remains poor, and a third of the population relies on subsistence agriculture.
Since 2002, when the 27-year civil war ended, the nation has worked to repair and improve ravaged infrastructure and weakened political and social institutions.
High international oil prices and rising oil production have contributed to the very strong economic growth since 1998, but corruption and public-sector mismanagement remain, particularly in the oil sector, which accounts for over 50 percent of GDP, over 90 percent of export revenue, and over 80 percent of government revenue.
The Portuguese explorer Diogo Cão reached the Angolan coast in 1484, after which Portugal began to found trading posts and forts along the shore.
Paulo Dias de Novais founded Sāo Paulo de Loanda (Luanda) in 1575.
São Felipe de Benguella (Benguela) followed in 1587.
The principal early trade was in slaves.
Portuguese merchants purchased the slaves from the local Imbangala and Mbundu peoples, notable slave hunters, and sold them to the sugarcane plantations in Brazil.
Brazilian ships were frequent visitors to Luanda and Benguela and Angola functioned as a kind of colony of Brazil, with Brazilian Jesuits active in its religious and educational centers.
The Portuguese Empire was neglected during the period of the Iberian Union, which lasted from 1580 to 1640.
The Dutch, bitter enemies of their former masters in Spain, invaded many Portuguese overseas possessions.
During Portugal's separatist war against Spain, the Dutch occupied Luanda from 1640 to 1648, calling it "Fort Aardenburgh".
The Dutch used the territory to supply their own slaves to the sugarcane plantations of Northeastern Brazil (Pernambuco, Olinda, Recife), which they had also seized from Portugal.
John Maurice, Prince of Nassau-Siegen, conquered the Portuguese possessions of Saint George del Mina, Saint Thomas, and Luanda, Angola, on the west coast of Africa.
Portugal recovered the territory between 1648 and 1650.
In the high plains, the Planalto, the most important native states were Bié and Bailundo, the latter being noted for its production of foodstuffs and rubber.
Portugal expanded into their territory, but did not control much of the interior prior to the late 19th century.
The Portuguese started to develop townships, trading posts, logging camps and small processing factories.
From 1764 onwards, there was a gradual change from a slave-based society to one based on production for domestic consumption and export.
Following the independence of Brazil in 1822, the slave trade was formally abolished in 1836.
However it did continue locally into the 20th century.
In 1844, Angola's ports were opened to foreign shipping.
By 1850, Luanda was one of the greatest and most developed Portuguese cities in the vast Portuguese Empire outside of Mainland Portugal, full of trading companies, exporting peanut oil, copal, timber, and cocoa.
The principal exports of the post-slave economy in the 19th century were rubber, beeswax, and ivory.
Maize, tobacco, dried meat and cassava flour also began to be locally produced.
Prior to the First World War, exportation of coffee, palm kernels and oil, cattle, leather and hides, and salt fish joined the principal exports, with small quantities of gold and cotton also being produced.
Grains, sugar, and rum were also produced for local consumption.
The principal imports were foodstuffs, cotton goods, hardware, and British coal.
Legislation against foreign traders was implemented in the 1890s.
The territory's prosperity, however, continued to depend on plantations worked by labor "indentured" from the interior.
From the 1920s to the 1960s, strong economic growth, abundant natural resources and development of infrastructure, led to the arrival of even more Portuguese settlers.
Petroleum was known to exist as early as the mid-19th century, but modern exploitation didn't begin until in 1955.
Production began in the Cuanza basin in the 1950s, in the Congo basin in the 1960s, and in the exclave of Cabinda in 1968.
The Portuguese government granted operating rights for Block Zero to the Cabinda Gulf Oil Company, a subsidiary of ChevronTexaco, in 1955.
Oil production surpassed the exportation of coffee as Angola's largest export in 1973.
A military-led coup d'état, started on April 25, 1974, in Lisbon, overthrew the Marcelo Caetano government in Portugal, and promised to hand over power to an independent Angolan government.
Mobutu Sese Seko, the President of Zaire, met with António de Spínola, the transitional President of Portugal, on September 15, 1974, on Sal island in Cape Verde, crafting a plan to empower Holden Roberto of the National Liberation Front of Angola, Jonas Savimbi of UNITA, and Daniel Chipenda of the MPLA's eastern faction at the expense of MPLA leader Agostinho Neto while retaining the façade of national unity.
Mobutu and Spínola wanted to present Chipenda as the MPLA head, Mobutu particularly preferring Chipenda over Neto because Chipenda supported autonomy for Cabinda.
The Angolan exclave has immense petroleum reserves estimated at around 300 million tons (~300 kg) which Zaire, and thus the Mobutu government, depended on for economic survival.
After independence thousands of white Portuguese left, most of them to Portugal and many travelling overland to South Africa.
There was an immediate crisis because the indigenous African population lacked the skills and knowledge needed to run the country and maintain its well-developed infrastructure.
The Angolan government created Sonangol, a state-run oil company, in 1976.
Two years later Sonangol received the rights to oil exploration and production in all of Angola.
After independence from Portugal in 1975, Angola was ravaged by a horrific civil war between 1975 and 2002.
United Nations Angola Verification Mission III and MONUA spent USD1.5 billion overseeing implementation of the Lusaka Protocol, a 1994 peace accord that ultimately failed to end the civil war.
The protocol prohibited UNITA from buying foreign arms, a provision the United Nations largely did not enforce, so both sides continued to build up their stockpile.
UNITA purchased weapons in 1996 and 1997 from private sources in Albania and Bulgaria, and from Zaire, South Africa, Republic of the Congo, Zambia, Togo, and Burkina Faso.
In October 1997 the UN imposed travel sanctions on UNITA leaders, but the UN waited until July 1998 to limit UNITA's exportation of diamonds and freeze UNITA bank accounts.
While the U.S.
government gave US$250 million to UNITA between 1986 and 1991, UNITA made US$1.72 billion between 1994 and 1999 exporting diamonds, primarily through Zaire to Europe.
At the same time the Angolan government received large amounts of weapons from the governments of Belarus, Brazil, Bulgaria, China, and South Africa.
While no arms shipment to the government violated the protocol, no country informed the U.N.
Register on Conventional Weapons as required.
Despite the increase in civil warfare in late 1998, the economy grew by an estimated 4% in 1999.
The government introduced new currency denominations in 1999, including a 1 and 5 kwanza note.
An economic reform effort was launched in 1998.
Angola ranked 160 of 174 nations in the United Nations Human Development Index in 2000.
In April 2000 Angola started an International Monetary Fund (IMF) Staff-Monitored Program (SMP).
The program formally lapsed in June 2001, but the IMF remains engaged.
In this context the Government of Angola has succeeded in unifying exchange rates and has raised fuel, electricity, and water rates.
The Commercial Code, telecommunications law, and Foreign Investment Code are being modernized.
A privatization effort, prepared with World Bank assistance, has begun with the BCI bank.
Nevertheless, a legacy of fiscal mismanagement and corruption persists.
The civil war internally displaced 3.8 million people, 32% of the population, by 2001.
The security brought about by the 2002 peace settlement has led to the resettlement of 4 million displaced persons, thus resulting in large-scale increases in agriculture production.
Angola produced over of diamonds in 2003, and production was expected to grow to per year by 2007.
In 2004 China's Eximbank approved a $2 billion line of credit to Angola to rebuild infrastructure.
The economy grew 18% in 2005 and growth was expected to reach 26% in 2006 and stay above 10% for the rest of the decade.
The construction industry is taking advantage of the growing economy, with various housing projects stimulated by the government initiatives for example the "Angola Investe" program and the "Casa Feliz" or "Meña" projects.
Not all public construction projects are functional.
A case in point: Kilamba Kiaxi, where a whole new satellite town of Luanda, consisting of housing facilities for several hundreds of thousands of people, was completely uninhabited for over four years because of skyrocketing prices, but completely sold out after the government decreased the original price and created mortgage plans at around the election time thus made it affordable for middle-class people.
ChevronTexaco started pumping from Block 14 in January 2000, but production decreased to in 2007 due to poor-quality oil.
Angola joined the Organization of the Petroleum Exporting Countries on January 1, 2007.
Cabinda Gulf Oil Company found Malange-1, an oil reservoir in Block 14, on August 9, 2007.
Despite its abundant natural resources, output per capita is among the world's lowest.
Subsistence agriculture provides the main livelihood for 85% of the population.
Oil production and the supporting activities are vital to the economy, contributing about 45% to GDP and 90% of exports.
Growth is almost entirely driven by rising oil production which surpassed in late-2005 and which is expected to grow to by 2007.
Control of the oil industry is consolidated in Sonangol Group, a conglomerate owned by the Angolan government.
With revenues booming from oil exports, the government has started to implement ambitious development programs to build roads and other basic infrastructure for the nation.
In the last decade of the colonial period, Angola was a major African food exporter but now imports almost all its food.
Severe wartime conditions, including extensive planting of landmines throughout the countryside, have brought agricultural activities to a near-standstill.
Some efforts to recover have gone forward, however, notably in fisheries.
Coffee production, though a fraction of its pre-1975 level, is sufficient for domestic needs and some exports.
Expanding oil production is now almost half of GDP and 90% of exports, at .
Diamonds provided much of the revenue for Jonas Savimbi's UNITA rebellion through illicit trade.
Other rich resources await development: gold, forest products, fisheries, iron ore, coffee, and fruits.
This is a chart of trend of nominal gross domestic product of Angola at market prices using International Monetary Fund data; figures are in millions of units.
The following table shows the main economic indicators in 1980–2017.
Inflation below 5 % is in green.
Exports in 2004 reached US$10,530,764,911.
The vast majority of Angola's exports, 92% in 2004, are petroleum products.
US$785 million worth of diamonds, 7.5% of exports, were sold abroad that year.
Nearly all of Angola's oil goes to the United States, in 2006, making it the eighth largest supplier of oil to the United States, and to China, in 2006.
In the first quarter of 2008, Angola became the main exporter of oil to China.
The rest of its petroleum exports go to Europe and Latin America.
U.S.
companies account for more than half the investment in Angola, with Chevron-Texaco leading the way.
The U.S.
exports industrial goods and services, primarily oilfield equipment, mining equipment, chemicals, aircraft, and food, to Angola, while principally importing petroleum.
Trade between Angola and South Africa exceeded US$300 million in 2007.
From the 2000s many Chinese have settled and started up businesses.
Angola produces and exports more petroleum than any other nation in sub-Saharan Africa, surpassing Nigeria in the 2000s.
In January 2007 Angola became a member of OPEC.
By 2010 production is expected to double the 2006 output level with development of deep-water offshore oil fields.
Oil sales generated US$1.71 billion in tax revenue in 2004 and now makes up 80% of the government's budget, a 5% increase from 2003, and 45% of GDP.
Chevron Corporation produces and receives , 27% of Angolan oil.
Total S.A., ExxonMobil, Eni, Petrobras and BP also operate in the country.
Block Zero provides the majority of Angola's crude oil production with produced annually.
The largest fields in Block Zero are Takula (Area A), Numbi (Area A), and Kokongo (Area B).
Chevron operates in Block Zero with a 39.2% share.
SONANGOL, the state oil company, Total, and Eni own the rest of the block.
Chevron also operates Angola's first producing deepwater section, Block 14, with .
The United Nations has criticized the Angolan government for using torture, rape, summary executions, arbitrary detention, and disappearances, actions which Angolan government has justified on the need to maintain oil output.
Angola is the third-largest trading partner of the United States in Sub-Saharan Africa, largely because of its petroleum exports.
The U.S.
imports 7% of its oil from Angola, about three times as much as it imported from Kuwait just prior to the Gulf War in 1991.
The U.S.
Government has invested US$4 billion in Angola's petroleum sector.
Oil makes up over 90% of Angola's exports.
Angola is the third largest producer of diamonds in Africa and has only explored 40% of the diamond-rich territory within the country, but has had difficulty in attracting foreign investment because of corruption, human rights violations, and diamond smuggling.
Production rose by 30% in 2006 and Endiama, the national diamond company of Angola, expects production to increase by 8% in 2007 to 10 million carats annually.
The government is trying to attract foreign companies to the provinces of Bié, Malanje and Uíge.
The Angolan government loses $375 million annually from diamond smuggling.
In 2003 the government began Operation Brilliant, an anti-smuggling investigation that arrested and deported 250,000 smugglers between 2003 and 2006.
Rafael Marques, a journalist and human rights activist, described the diamond industry in his 2006 "Angola's Deadly Diamonds" report as plagued by "murders, beatings, arbitrary detentions and other human rights violations."
Marques called on foreign countries to boycott Angola's "conflict diamonds".
In December 2014, the Bureau of International Labor Affairs issued a "List of Goods Produced by Child Labor or Forced Labor" that classified Angola as one of the major diamond-producing African countries relying on both child labor and forced labor.
The U.S.
Department of Labor reported that "there is little publicly available information on [Angola's] efforts to enforce child labor law".
Diamonds accounted for 1.48% of Angolan exports in 2014.
Under Portuguese rule, Angola began mining iron in 1957, producing 1.2 million tons in 1967 and 6.2 million tons by 1971.
In the early 1970s, 70% of Portuguese Angola's iron exports went to Western Europe and Japan.
After independence in 1975, the Angolan Civil War (1975–2002) destroyed most of the territory's mining infrastructure.
The redevelopment of the Angolan mining industry started in the late 2000s.
</doc>
<doc id="708" url="https://en.wikipedia.org/wiki?curid=708" title="Transport in Angola">
Transport in Angola

Transport in Angola comprises:

There are three separate railway lines in Angola:

Reconstruction of these three lines began in 2005 and is expected to be completed by the end of the year 2012.
The Benguela Railway already connects to the Democratic Republic of the Congo.
In April 2012, the Zambian Development Agency (ZDA) and an Angolan company signed a memorandum of understanding (MoU) to build a multi-product pipeline from Lobito to Lusaka, Zambia, to deliver various refined products to Zambia.
Angola plans to build an oil refinery in Lobito in the coming years.
The government plans to build a deep-water port at Barra do Dande, north of Luanda, in Bengo province near Caxito.
Angola had an estimated total of 43 airports as of 2004, of which 31 had paved runways as of 2005.
There is an international airport at Luanda.
International and domestic services are maintained by TAAG Angola Airlines, Aeroflot, British Airways, Brussels Airlines, Lufthansa, Air France, Air Namibia, Cubana, Ethiopian Airlines, Emirates, Delta Air Lines, Royal Air Maroc, Iberia, Hainan Airlines, Kenya Airways, South African Airways, TAP Air Portugal and several regional carriers.
In 2003, domestic and international carriers carried 198,000 passengers.
There are airstrips for domestic transport at Benguela, Cabinda, Huambo, Moçâmedes, and Catumbela.
"This article comes from the CIA World Factbook 2003."
</doc>
<doc id="709" url="https://en.wikipedia.org/wiki?curid=709" title="Angolan Armed Forces">
Angolan Armed Forces

The Angolan Armed Forces (Portuguese: "Forças Armadas Angolanas") or FAA are the military of Angola.
The FAA include the General Staff of the Armed Forces and three components: the Army ("Exército"), the Navy ("Marinha de Guerra") and the National Air Force ("National Air Force").
Reported total manpower in 2013 was about 107,000.
The FAA is headed by Chief of the General Staff Geraldo Sachipengo Nunda since 2010, who reports to the Minister of National Defense, currently Salviano de Jesus Sequeira.
The FAA succeeded to the previous People's Armed Forces for the Liberation of Angola (FAPLA) following the abortive Bicesse Accord with the Armed Forces of the Liberation of Angola (FALA), armed wing of the National Union for the Total Independence of Angola (UNITA).
As part of the peace agreement, troops from both armies were to be demilitarized and then integrated.
Integration was never completed as UNITA and FALA went back to war in 1992.
Later, consequences for FALA personnel in Luanda were harsh with FAPLA veterans persecuting their erstwhile opponents in certain areas and reports of vigilantism.
The Army ("Exército") is the land component of the FAA.
It is organized in six military regions (Cabinda, Luanda, North, Center, East and South), with an infantry division being based in each one.
Distributed by the six military regions / infantry divisions, there are 25 motorized infantry brigades, one tank brigade and one engineering brigade.
The Army also includes an artillery regiment, the Military Artillery School, the Army Military Academy, an anti-aircraft defense group, a composite land artillery group, a military police regiment, a logistical transportation regiment and a field artillery brigade.
The Army further includes the Special Forces Brigade (including Commandos and Special Operations units), but this unit is under the direct command of the General Staff of the FAA.
On August 1, 1974 a few months after a military coup d'état had overthrown the Lisbon regime and proclaimed its intention of granting independence to Angola, the MPLA announced the formation of FAPLA, which replaced the EPLA.
By 1976 FAPLA had been transformed from lightly armed guerrilla units into a national army capable of sustained field operations.
In 1990-91, the Army had ten military regions and an estimated 73+ 'brigades', each with a mean strength of 1,000 and comprising inf, tank, APC, artillery, and AA units as required.
The Library of Congress said in 1990 that '[t]he regular army's 91,500 troops were organized into more than seventy brigades ranging from 750 to 1,200 men each and deployed throughout the ten military regions.
Most regions were commanded by lieutenant colonels, with majors as deputy commanders, but some regions were commanded by majors.
Each region consisted of one to four provinces, with one or more infantry brigades assigned to it.
The brigades were generally dispersed in battalion or smaller unit formations to protect strategic terrain, urban centers, settlements, and critical infrastructure such as bridges and factories.
Counterintelligence agents were assigned to all field units to thwart UNITA infiltration.
The army's diverse combat capabilities were indicated by its many regular and motorised infantry brigades with organic or attached armor, artillery, and air defense units; two militia infantry brigades; four antiaircraft artillery brigades; ten tank battalions; and six artillery battalions.
These forces were concentrated most heavily in places of strategic importance and recurring conflict: the oil-producing Cabinda Province, the area around the capital, and the southern provinces where UNITA and South African forces operated.'
It was reported in 2011 that the army was by far the largest of the services with about 120,000 men and women.
The Angolan Army has around 29,000 "ghost workers" who remain enrolled in the ranks of the FAA and therefore receive a salary.
In 2013, the International Institute for Strategic Studies reported that the FAA had six divisions, the 1st, 5th, and 6th with two or three infantry brigades, and the 2nd, 3rd, and 4th with five to six infantry brigades.
The 4th Division included a tank regiment.
A separate tank brigade and special forces brigade were also reported.
As of 2011, the IISS reported the ground forces had 42 armoured/infantry regiments ('detachments/groups - strength varies') and 16 infantry 'brigades'.
These probably comprised infantry, tanks, APC, artillery, and AA units as required.
Major equipment included over 140 main battle tanks, 600 reconnaissance vehicles, over 920 AFVs, infantry fighting vehicles, 298 howitzers.
It was reported on May 3, 2007, that the Special Forces Brigade of the Angolan Armed Forces (FAA) located at Cabo Ledo region, northern Bengo Province, would host a 29th anniversary celebration for the entire armed forces.
The brigade was reportedly formed on 5 May 1978 and under the command at the time of Colonel Paulo Falcao.
The Army operates a large amount of Russian, Soviet and ex-Warsaw pact hardware.
A large amount of its equipment was acquired in the 1980s and 1990s most likely because of hostilities with neighbouring countries and its civil war which lasted from November 1975 until 2002.
There is an interest from the Angolan Army for the Brazilian ASTROS II multiple rocket launcher.
Many of Angola's weapons are of Portuguese colonial and Warsaw Pact origin.
Jane's Information Group lists the following as in service:






The National Air Force of Angola (FANA, "Força Aérea Nacional de Angola") is the air component of the FAA.
It is organized in six aviation regiments, each including several squadrons.
To each of the regiments correspond an air base.
Besides the aviation regiments, there is also a Pilot Training School.
The Air Force's personnel total about 8,000; its equipment includes transport aircraft and six Russian-manufactured Sukhoi Su-27 fighter aircraft.
In 2002 one was lost during the civil war with UNITA forces.
In 1991, the Air Force/Air Defense Forces had 8,000 personnel and 90 combat-capable aircraft, including 22 fighters, 59 fighter ground attack aircraft and 16 attack helicopters.
The Angola Navy (MGA, "Marinha de Guerra de Angola") is the naval component of the FAA.
It is organized in two naval zones (North and South), with naval bases in Luanda, Lobito and Moçâmedes.
It includes a Marines Brigade and a Marines School, based in Ambriz.
The Navy numbers about 1,000 personnel and operates only a handful of small patrol craft and barges.
The Navy has been neglected and ignored as a military arm mainly due to the guerrilla struggle against the Portuguese and the nature of the civil war.
From the early 1990s to the present the Angolan Navy has shrunk from around 4,200 personnel to around 1,000, resulting in the loss of skills and expertise needed to maintain equipment.
In order to protect Angola's 1 600 km long coastline, the Angolan Navy is undergoing modernisation but is still lacking in many ways.
Portugal has been providing training through its Technical Military Cooperation (CTM) programme.
The Navy is requesting procurement of a frigate, three corvettes, three offshore patrol vessel and additional fast patrol boats.
Most of the vessels in the navy's inventory dates back from the 1980s or earlier, and many of its ships are inoperable due to age and lack of maintenance.
However the navy acquired new boats from Spain and France in the 1990s.
Germany has delivered several Fast Attack Craft for border protection in 2011.
In September 2014 it was reported that the Angolan Navy would acquire seven Macaé-class patrol vessels from Brazil as part of a Technical Memorandum of Understanding (MoU) covering the production of the vessels as part of Angola's Naval Power Development Programme (Pronaval).
The military of Angola aims to modernize its naval capability, presumably due to a rise in maritime piracy within the Gulf of Guinea which may have an adverse effect on the country's economy.
The navy's current known inventory includes the following:


The navy also has several aircraft for maritime patrol:

The FAA include several types of special forces, namely the Commandos, the Special Operations and the Marines.
The Angolan special forces follow the general model of the analogous Portuguese special forces, receiving a similar training.
The Commandos and the Special forces are part of the Special Forces Brigade (BRIFE, "Brigada de Forças Especiais"), based at Cabo Ledo, in the Bengo Province.
The BRIFE includes two battalions of commandos, a battalion of special operations and sub-units of combat support and service support.
The BRIFE also included the Special Actions Group (GAE, "Grupo de Ações Especiais"), which is presently inactive and that was dedicated to long range reconnaissance, covert and sabotage operations.
In the Cabo Ledo base is also installed the Special Forces Training School (EFFE, "Escola de Formação de Forças Especiais").
Both the BRIFE and the EFFE are directly under the Directorate of Special Forces of the General Staff of the Armed Forces.
The marines ("fuzileiros navais") constitute the Marines Brigade of the Angolan Navy.
The Marines Brigade is not permanently dependent of the Directorate of Special Forces, but can detach their units and elements to be put under the command of that body for the conduction of exercises or real operations.
Since the disbandment of the Angolan Parachute Battalion in 2004, the FAA do not have a specialized paratrooper unit.
However, elements of the commandos, special operations and marines are parachute qualified.
The FAPLA's main counterinsurgency effort was directed against UNITA in the southeast, and its conventional capabilities were demonstrated principally in the undeclared South African Border War.
The FAPLA first performed its external assistance mission with the dispatch of 1,000 to 1,500 troops to São Tomé and Príncipe in 1977 to bolster the socialist regime of President Manuel Pinto da Costa.
During the next several years, Angolan forces conducted joint exercises with their counterparts and exchanged technical operational visits.
The Angolan expeditionary force was reduced to about 500 in early 1985.
The Angolan Armed Forces were controversially involved in training the armed forces of fellow Lusophone states Cape Verde and Guinea-Bissau.
In the case of the latter, the 2012 Guinea-Bissau coup d'état was cited by the coup leaders as due to Angola's involvement in trying to "reform" the military in connivance with the civilian leadership.
A small number of FAA personnel are stationed in the Democratic Republic of the Congo (Kinshasa) and the Republic of the Congo (Brazzaville).
A presence during the unrest in Ivory Coast, 2010–2011, were not officially confirmed.
However, the "Frankfurter Allgemeine Zeitung", citing "Jeune Afrique", said that among President Gbagbo's guards were 92 personnel of President Dos Santos's Presidential Guard Unit.
Angola is basically interested in the participation of the FAA operations of the African Union and has formed special units for this purpose.
David Birmingham, African Affairs, Vol.
77, No.
309 (Oct., 1978), pp.
554–564
Published by: Oxford University Press on behalf of The Royal African Society



</doc>
<doc id="710" url="https://en.wikipedia.org/wiki?curid=710" title="Foreign relations of Angola">
Foreign relations of Angola

The foreign relations of Angola are based on Angola's strong support of U.S.
foreign policy as the Angolan economy is dependent on U.S.
foreign aid.
From 1975 to 1989, Angola was aligned with the Eastern bloc, in particular the Soviet Union, Libya, and Cuba.
Since then, it has focused on improving relationships with Western countries, cultivating links with other Portuguese-speaking countries, and asserting its own national interests in Central Africa through military and diplomatic intervention.
In 1993, it established formal diplomatic relations with the United States.
It has entered the Southern African Development Community as a vehicle for improving ties with its largely Anglophone neighbors to the south.
Zimbabwe and Namibia joined Angola in its military intervention in the Democratic Republic of the Congo, where Angolan troops remain in support of the Joseph Kabila government.
It also has intervened in the Republic of the Congo (Brazzaville) to support the existing government in that country.
Since 1998, Angola has successfully worked with the United Nations Security Council to impose and carry out sanctions on UNITA.
More recently, it has extended those efforts to controls on conflict diamonds, the primary source of revenue for UNITA during the Civil War that ended in 2002.
At the same time, Angola has promoted the revival of the Community of Portuguese-Speaking Countries (CPLP) as a forum for cultural exchange and expanding ties with Portugal (its former ruler) and Brazil (which shares many cultural affinities with Angola) in particular.
Angola is a member of the Port Management Association of Eastern and Southern Africa (PMAESA).
</doc>
<doc id="711" url="https://en.wikipedia.org/wiki?curid=711" title="Albert Sidney Johnston">
Albert Sidney Johnston

Albert Sidney Johnston (February 2, 1803 – April 6, 1862) served as a general in three different armies: the Texian ("i.e." Republic of Texas) Army, the United States Army, and the Confederate States Army.
He saw extensive combat during his 34-year military career, fighting actions in the Black Hawk War, Texas War of Independence, the Mexican–American War, the Utah War, and the American Civil War.
Considered by Confederate States President Jefferson Davis to be the finest general officer in the Confederacy before the later emergence of Robert E. Lee, he was killed early in the Civil War at the Battle of Shiloh on April 6, 1862.
Johnston was the highest-ranking officer, Union or Confederate, killed during the entire war.
Davis believed the loss of General Johnston "was the turning point of our fate."
Johnston was unrelated to Confederate general Joseph E. Johnston.
Johnston was born in Washington, Kentucky, the youngest son of Dr. John and Abigail (Harris) Johnston.
His father was a native of Salisbury, Connecticut.
Although Albert Johnston was born in Kentucky, he lived much of his life in Texas, which he considered his home.
He was first educated at Transylvania University in Lexington, Kentucky, where he met fellow student Jefferson Davis.
Both were appointed to the United States Military Academy at West Point, New York, Davis two years behind Johnston.
In 1826, Johnston graduated eighth of 41 cadets in his class from West Point with a commission as a brevet second lieutenant in the 2nd U.S.
Infantry.
Johnston was assigned to posts in New York and Missouri and served in the brief Black Hawk War in 1832 as chief of staff to Bvt.
Brig.
Gen.
Henry Atkinson.
In 1829 he married Henrietta Preston, sister of Kentucky politician and future Civil War general William Preston.
They had one son, William Preston Johnston, who became a colonel in the Confederate States Army.
The senior Johnston resigned his commission in 1834 in order to care for his dying wife in Kentucky, who succumbed two years later to tuberculosis.
After serving as Secretary of War for the Republic of Texas from 1838 to 1840, Johnston resigned and returned to Kentucky.
In 1843, he married Eliza Griffin, his late wife's first cousin.
The couple moved to Texas, where they settled on a large plantation in Brazoria County.
Johnston named the property "China Grove".
Here they raised Johnston's two children from his first marriage and the first three children born to Eliza and him.
(A sixth child was born later when they lived in Los Angeles).
In 1836 Johnston moved to Texas.
He enlisted as a private in the Texian Army during the Texas War of Independence against the Republic of Mexico.
He was named Adjutant General as a colonel in the Republic of Texas Army on August 5, 1836.
On January 31, 1837, he became senior brigadier general in command of the Texas Army.
On February 5, 1837, he fought in a duel with Texas Brig.
Gen.
Felix Huston, as they challenged each other for the command of the Texas Army; Johnston refused to fire on Huston and lost the position after he was wounded in the pelvis.
On December 22, 1838, Mirabeau B. Lamar, the second president of the Republic of Texas, appointed Johnston as Secretary of War.
He provided for the defense of the Texas border against Mexican invasion, and in 1839 conducted a campaign against Indians in northern Texas.
In February 1840, he resigned and returned to Kentucky.
Johnston returned to Texas during the Mexican–American War (1846-1848), under General Zachary Taylor as a colonel of the 1st Texas Rifle Volunteers.
The enlistments of his volunteers ran out just before the Battle of Monterrey.
Johnston convinced a few volunteers to stay and fight as he served as the inspector general of volunteers and fought at the battles of Monterrey and Buena Vista.
He remained on his plantation after the war until he was appointed by later 12th President Zachary Taylor to the U.S.
Army as a major and was made a paymaster in December 1849.
He served in that role for more than five years, making six tours, and traveling more than annually on the Indian frontier of Texas.
He served on the Texas frontier at Fort Mason and elsewhere in the West.
In 1855, 14th President Franklin Pierce appointed him colonel of the new 2nd U.S.
Cavalry (the unit that preceded the modern 5th U.S.
), a new regiment, which he organized.
On August 19, 1856, Gen.
Persifor Smith, at the request of Kansas Territorial Governor Wilson Shannon, sent Col.
Johnston with 1300 men composed of the 2d Cavalry Dragoons from Fort Riley, a battalion of the 6th Infantry and Capt.
Howe's artillery company from Jefferson Barracks, near St.
Louis to protect the territorial capital at Lecompton from an imminent attack by James Henry Lane and his abolitionist "Army of the North."
As a key figure in the Utah War, Johnston led U.S.
troops who established a non-Mormon government in the Mormon territory.
He received a brevet promotion to brigadier general in 1857 for his service in Utah.
He spent 1860 in Kentucky until December 21, when he sailed for California to take command of the Department of the Pacific.
At the outbreak of the American Civil War, Johnston was the commander of the U.S.
Army Department of the Pacific in California.
Like many regular army officers from the South, he was opposed to secession.
But he resigned his commission soon after he heard of the secession of his adopted state of Texas.
It was accepted by the War Department on May 6, 1861, effective May 3.
On April 28 he moved to Los Angeles, the home of his wife's brother John Griffin.
Considering staying in California with his wife and five children, Johnston remained there until May.
Soon, under suspicion by local Union officials, he evaded arrest and joined the Los Angeles Mounted Rifles as a private, leaving Warner's Ranch May 27.
He participated in their trek across the southwestern deserts to Texas, crossing the Colorado River into the Confederate Territory of Arizona on July 4, 1861.
Early in the Civil War, Confederate President Jefferson Davis decided that the Confederacy would attempt to hold as much of its territory as possible, and therefore distributed military forces around its borders and coasts.
In the summer of 1861, Davis appointed several generals to defend Confederate lines from the Mississippi River east to the Allegheny Mountains.
The most sensitive, and in many ways the most crucial areas, along the Mississippi River and in western Tennessee along the Tennessee and the Cumberland rivers were placed under the command of Maj.
Gen.
Leonidas Polk and Brig.
Gen.
Gideon J. Pillow.
The latter had initially been in command in Tennessee as that State's top general.
Their impolitic occupation of Columbus, Kentucky, on September 3, 1861, two days before Johnston arrived in the Confederacy's capital of Richmond, Virginia, after his cross-country journey, drove Kentucky from its stated neutrality.
The majority of Kentuckians allied with the Union camp.
Polk and Pillow's action gave Union Brig.
Gen.
Ulysses S. Grant an excuse to take control of the strategically located town of Paducah, Kentucky, without raising the ire of most Kentuckians and the pro-Union majority in the State legislature.
On September 10, 1861, Johnston was assigned to command the huge area of the Confederacy west of the Allegheny Mountains, except for coastal areas.
He became commander of the Confederacy's western armies in the area often called the Western Department or Western Military Department.
Johnston's appointment as a full general by his friend and admirer Jefferson Davis already had been confirmed by the Confederate Senate on August 31, 1861.
The appointment had been backdated to rank from May 30, 1861, making him the second highest ranking general in the Confederate States Army.
Only Adjutant General and Inspector General Samuel Cooper ranked ahead of him.
After his appointment, Johnston immediately headed for his new territory.
He was permitted to call on governors of Arkansas, Tennessee and Mississippi for new troops, although this authority was largely stifled by politics, especially with respect to Mississippi.
On September 13, 1861, Johnston ordered Brig.
Gen.
Felix Zollicoffer with 4,000 men to occupy Cumberland Gap in Kentucky in order to block Union troops from coming into eastern Tennessee.
The Kentucky legislature had voted to side with the Union after the occupation of Columbus by Polk.
By September 18, Johnston had Brig.
Gen.
Simon Bolivar Buckner with another 4,000 men blocking the railroad route to Tennessee at Bowling Green, Kentucky.
Johnston had fewer than 40,000 men spread throughout Kentucky, Tennessee, Arkansas and Missouri.
Of these, 10,000 were in Missouri under Missouri State Guard Maj.
Gen.
Sterling Price.
Johnston did not quickly gain many recruits when he first requested them from the governors, but his more serious problem was lacking sufficient arms and ammunition for the troops he already had.
As the Confederate government concentrated efforts on the units in the East, they gave Johnston small numbers of reinforcements and minimal amounts of arms and material.
Johnston maintained his defense by conducting raids and other measures to make it appear he had larger forces than he did, a strategy that worked for several months.
Johnston's tactics had so annoyed and confused Union Brig.
Gen.
William Tecumseh Sherman in Kentucky that he became paranoid and mentally unstable.
Sherman overestimated Johnston's forces, and had to be relieved by Brig.
Gen.
Don Carlos Buell on November 9, 1861.
East Tennessee (a heavily pro-Union region of the South during the Civil War) was held for the Confederacy by two unimpressive brigadier generals appointed by Jefferson Davis: Felix Zollicoffer, a brave but untrained and inexperienced officer, and soon-to-be Maj.
Gen.
George B. Crittenden, a former U.S.
Army officer with apparent alcohol problems.
While Crittenden was away in Richmond, Zollicoffer moved his forces to the north bank of the upper Cumberland River near Mill Springs (now Nancy, Kentucky), putting the river to his back and his forces into a trap.
Zollicoffer decided it was impossible to obey orders to return to the other side of the river because of scarcity of transport and proximity of Union troops.
When Union Brig.
Gen.
George H. Thomas moved against the Confederates, Crittenden decided to attack one of the two parts of Thomas's command at Logan's Cross Roads near Mill Springs before the Union forces could unite.
At the Battle of Mill Springs on January 19, 1862, the ill-prepared Confederates, after a night march in the rain, attacked the Union force with some initial success.
As the battle progressed, Zollicoffer was killed, Crittenden was unable to lead the Confederate force (he may have been intoxicated), and the Confederates were turned back and routed by a Union bayonet charge, suffering 533 casualties from their force of 4,000.
The Confederate troops who escaped were assigned to other units as General Crittenden faced an investigation of his conduct.
After the Confederate defeat at the Mill Springs, Davis sent Johnston a brigade and a few other scattered reinforcements.
He also assigned him Gen.
P. G. T. Beauregard, who was supposed to attract recruits because of his victories early in the war, and act as a competent subordinate for Johnston.
The brigade was led by Brig.
Gen.
John B. Floyd, considered incompetent.
He took command at Fort Donelson as the senior general present just before Union Brig.
Gen.
Ulysses S. Grant attacked the fort.
Historians believe the assignment of Beauregard to the west stimulated Union commanders to attack the forts before Beauregard could make a difference in the theater.
Union officers heard that he was bringing 15 regiments with him, but this was an exaggeration of his forces.
Based on the assumption that Kentucky neutrality would act as a shield against a direct invasion from the north, Tennessee initially had sent men to Virginia and concentrated defenses in the Mississippi Valley, circumstances that no longer applied in September 1861.
Even before Johnston arrived in Tennessee, construction of two forts had been started to defend the Tennessee and the Cumberland rivers, which provided avenues into the State from the north.
Both forts were located in Tennessee in order to respect Kentucky neutrality, but these were not in ideal locations.
Fort Henry on the Tennessee River was in an unfavorable low-lying location, commanded by hills on the Kentucky side of the river.
Fort Donelson on the Cumberland River, although in a better location, had a vulnerable land side and did not have enough heavy artillery to defend against gunboats.
Maj.
Gen.
Polk ignored the problems of the forts when he took command.
After Johnston took command, Polk at first refused to comply with Johnston's order to send an engineer, Lt.
Joseph K. Dixon, to inspect the forts.
After Johnston asserted his authority, Polk had to allow Dixon to proceed.
Dixon recommended that the forts be maintained and strengthened, although they were not in ideal locations, because much work had been done on them and the Confederates might not have time to build new ones.
Johnston accepted his recommendations.
Johnston wanted Major Alexander P. Stewart to command the forts but President Davis appointed Brig.
Gen.
Lloyd Tilghman as commander.
To prevent Polk from dissipating his forces by allowing some men to join a partisan group, Johnston ordered him to send Brig.
Gen.
Gideon Pillow and 5,000 men to Fort Donelson.
Pillow took up a position at nearby Clarksville, Tennessee and did not move into the fort until February 7, 1862.
Alerted by a Union reconnaissance on January 14, 1862, Johnston ordered Tilghman to fortify the high ground opposite Fort Henry, which Polk had failed to do despite Johnston's orders.
Tilghman failed to act decisively on these orders, which in any event were too late to be adequately carried out.
Gen.
Beauregard arrived at Johnston's headquarters at Bowling Green on February 4, 1862, and was given overall command of Polk's force at the western end of Johnston's line at Columbus, Kentucky.
On February 6, 1862, Union Navy gunboats quickly reduced the defenses of ill-sited Fort Henry, inflicting 21 casualties on the small remaining Confederate force.
Brig.
Gen.
Lloyd Tilghman surrendered the 94 remaining officers and men of his approximately 3,000-man force which had not been sent to Fort Donelson before U.S.
Grant's force could even take up their positions.
Johnston knew he could be trapped at Bowling Green if Fort Donelson fell, so he moved his force to Nashville, the capital of Tennessee and an increasingly important Confederate industrial center, beginning on February 11, 1862.
Johnston also reinforced Fort Donelson with 12,000 more men, including those under Floyd and Pillow, a curious decision in view of his thought that the Union gunboats alone might be able to take the fort.
He did order the commanders of the fort to evacuate the troops if the fort could not be held.
The senior generals sent to the fort to command the enlarged garrison, Gideon J. Pillow and John B. Floyd, squandered their chance to avoid having to surrender most of the garrison and on February 16, 1862, Brig.
Gen.
Simon Buckner, having been abandoned by Floyd and Pillow, surrendered Fort Donelson.
Colonel Nathan Bedford Forrest escaped with his cavalry force of about 700 men before the surrender.
The Confederates suffered about 1,500 casualties with an estimated 12,000 to 14,000 taken prisoner.
Union casualties were 500 killed, 2,108 wounded, 224 missing.
Johnston, who had little choice in allowing Floyd and Pillow to take charge at Fort Donelson on the basis of seniority after he ordered them to add their forces to the garrison, took the blame and suffered calls for his removal because a full explanation to the press and public would have exposed the weakness of the Confederate position.
His passive defensive performance while positioning himself in a forward position at Bowling Green, spreading his forces too thinly, not concentrating his forces in the face of Union advances, and appointing or relying upon inadequate or incompetent subordinates subjected him to criticism at the time and by later historians.
The fall of the forts exposed Nashville to imminent attack, and it fell without resistance to Union forces under Brig.
Gen.
Buell on February 25, 1862, two days after Johnston had to pull his forces out in order to avoid having them captured as well.
Johnston had various remaining military units scattered throughout his territory and retreating to the south to avoid being cut off.
Johnston himself retreated with the force under his personal command, the Army of Central Kentucky, from the vicinity of Nashville.
With Beauregard's help, Johnston decided to concentrate forces with those formerly under Polk and now already under Beauregard's command at the strategically located railroad crossroads of Corinth, Mississippi, which he reached by a circuitous route.
Johnston kept the Union forces, now under the overall command of the ponderous Maj.
Gen.
Henry Halleck, confused and hesitant to move, allowing Johnston to reach his objective undetected.
This delay allowed Jefferson Davis finally to send reinforcements from the garrisons of coastal cities and another highly rated but prickly general, Braxton Bragg, to help organize the western forces.
Bragg at least calmed the nerves of Beauregard and Polk who had become agitated by their apparent dire situation in the face of numerically superior forces before the arrival of Johnston on March 24, 1862.
Johnston's army of 17,000 men gave the Confederates a combined force of about 40,000 to 44,669 men at Corinth.
On March 29, 1862, Johnston officially took command of this combined force, which continued to use the Army of the Mississippi name under which it had been organized by Beauregard on March 5.
Johnston now planned to defeat the Union forces piecemeal before the various Union units in Kentucky and Tennessee under Grant with 40,000 men at nearby Pittsburg Landing, Tennessee, and the now Maj.
Gen.
Don Carlos Buell on his way from Nashville with 35,000 men, could unite against him.
Johnston started his army in motion on April 3, 1862, intent on surprising Grant's force as soon as the next day, but they moved slowly due to their inexperience, bad roads and lack of adequate staff planning.
Due to the delays, as well as several contacts with the enemy, Johnston's second in command, P. G. T. Beauregard, felt the element of surprise had been lost and recommended calling off the attack.
Johnston decided to proceed as planned, stating "I would fight them if they were a million."
His army was finally in position within a mile or two of Grant's force, and undetected, by the evening of April 5, 1862.
Johnston launched a massive surprise attack with his concentrated forces against Grant at the Battle of Shiloh on April 6, 1862.
As the Confederate forces overran the Union camps, Johnston seemed to be everywhere, personally leading and rallying troops up and down the line on his horse.
At about 2:30 pm, while leading one of those charges against a Union camp near the "Peach Orchard," he was wounded, taking a bullet behind his right knee.
He apparently did not think the wound was serious at the time, or even possibly did not feel it.
It is possible that Johnston's duel in 1837 had caused nerve damage or numbness to his right leg and that he did not feel the wound to his leg as a result.
The bullet had in fact clipped a part of his popliteal artery and his boot was filling up with blood.
There were no medical personnel on scene at the time, since Johnston had sent his personal surgeon to care for the wounded Confederate troops and Yankee prisoners earlier in the battle.
Within a few minutes, Johnston was observed by his staff to be nearly fainting.
Among his staff was Isham G. Harris, the Governor of Tennessee, who had ceased to make any real effort to function as governor after learning that Abraham Lincoln had appointed Andrew Johnson as military governor of Tennessee.
Seeing Johnston slumping in his saddle and his face turning deathly pale, Harris asked: "General, are you wounded?"
Johnston glanced down at his leg wound, then faced Harris and replied in a weak voice his last words: "Yes... and I fear seriously."
Harris and other staff officers removed Johnston from his horse and carried him to a small ravine near the "Hornets Nest" and desperately tried to aid the general who had lost consciousness by this point.
Harris then sent an aide to fetch Johnston's surgeon but did not apply a tourniquet to Johnson's wounded leg.
Before a doctor could be found, Johnston died from blood loss a few minutes later.
It is believed that Johnston may have lived for as long as one hour after receiving his fatal wound.
Ironically, it was later discovered that Johnston had a tourniquet in his pocket when he died.
Harris and the other officers wrapped General Johnston's body in a blanket so as not to damage the troops' morale with the sight of the dead general.
Johnston and his wounded horse, Fire Eater, were taken to his field headquarters on the Corinth road, where his body remained in his tent until the Confederate Army withdrew to Corinth the next day, April 7, 1862, after failing to gain a decisive victory over the Union armies.
From there, his body was taken to the home of Colonel William Inge, which had been his headquarters in Corinth.
It was covered in the Confederate flag and lay in state for several hours.
It is probable that a Confederate soldier fired the fatal round.
No Union soldiers had ever been observed to have gotten behind Johnston during the fatal charge, but it is known that many Confederates were firing at the Union lines while Johnston charged well in advance of his soldiers.
Furthermore, the surgeon who later dug the bullet out of Johnston's leg identified the round as one fired from a Pattern 1853 Enfield.
No Union troops in the area in which Johnston was hit had been issued Enfield rifles, but the Enfield rifle was standard issue for the Confederate forces Johnston was leading.
Johnston was the highest-ranking fatality of the war on either side, and his death was a strong blow to the morale of the Confederacy.
At the time, Davis considered him the best general in the country.
Johnston was survived by his wife Eliza and six children.
His wife and five younger children, including one born after he went to war, chose to live out their days at home in Los Angeles with Eliza's brother, Dr. John Strother Griffin.
Johnston's eldest son, Albert Sidney Jr.
(born in Texas), had already followed him into the Confederate States Army.
In 1863, after taking home leave in Los Angeles, Albert Jr.
was on his way out of San Pedro harbor on a ferry.
While a steamer was taking on passengers from the ferry, a wave swamped the smaller boat, causing its boilers to explode.
Albert Jr.
was killed in the accident.
Killed in action, General Johnston received the highest praise ever given by the Confederate government; accounts were published, on December 20, 1862, and thereafter, in the Los Angeles "Star" of his family's hometown.
Johnston Street, Hancock Street, and Griffin Avenue, each in northeast Los Angeles, are named after the general and his family, who lived in the neighborhood.
Johnston was initially buried in New Orleans.
In 1866, a joint resolution of the Texas Legislature was passed to have his body moved and reinterred at the Texas State Cemetery in Austin.
The re-interment occurred in 1867.
Forty years later, the state appointed Elisabet Ney to design a monument and sculpture of him to be erected at the grave site, installed in 1905.
The Texas Historical Commission has erected a historical marker near the entrance of what was once Johnston's plantation.
An adjacent marker was erected by the San Jacinto Chapter of the Daughters of The Republic of Texas and the Lee, Roberts, and Davis Chapter of the United Daughters of the Confederate States of America.
In 1916, the University of Texas at Austin recognized several confederate veterans (including Johnston) with statues on its South Mall.
On August 21, 2017, as part of the wave of confederate monument removals in America, Johnston's statue was taken down.
Plans were announced to add it to the Briscoe Center for American History on the east side of the university campus.
</doc>
<doc id="713" url="https://en.wikipedia.org/wiki?curid=713" title="Android (robot)">
Android (robot)

An android is a robot or other artificial being designed to resemble a human, and often made from a flesh-like material.
Historically, androids were completely within the domain of science fiction and frequently seen in film and television, but recent advances in robot technology now allow the design of functional and realistic humanoid robots.
The word was coined from the Greek root ἀνδρ- "andr"-, "man" (male, as opposed to ἀνθρωπ- "anthrōp"-, human being) and the suffix "", "having the form or likeness of".
In Greek, however, ανδροειδής is an adjective.
While the term "android" is used in reference to human-looking robots in general, a robot with a female appearance can also be referred to as a "gynoid".
The "Oxford English Dictionary" traces the earliest use (as "Androides") to Ephraim Chambers' "Cyclopaedia," in reference to an automaton that St.
Albertus Magnus allegedly created.
The term "android" appears in US patents as early as 1863 in reference to miniature human-like toy automatons.
The term "android" was used in a more modern sense by the French author Auguste Villiers de l'Isle-Adam in his work "Tomorrow's Eve" (1886).
This story features an artificial humanlike robot named Hadaly.
As said by the officer in the story, "In this age of Realien advancement, who knows what goes on in the mind of those responsible for these mechanical dolls."
The term made an impact into English pulp science fiction starting from Jack Williamson's "The Cometeers" (1936) and the distinction between mechanical robots and fleshy androids was popularized by Edmond Hamilton's Captain Future (1940–1944).
Although Karel Čapek's robots in "R.U.R.
(Rossum's Universal Robots)" (1921)—the play that introduced the word "robot" to the world—were organic artificial humans, the word "robot" has come to primarily refer to mechanical humans, animals, and other beings.
The term "android" can mean either one of these, while a cyborg ("cybernetic organism" or "bionic man") would be a creature that is a combination of organic and mechanical parts.
The term "droid", popularized by George Lucas in the original "Star Wars" film and now used widely within science fiction, originated as an abridgment of "android", but has been used by Lucas and others to mean any robot, including distinctly non-human form machines like R2-D2.
The word "android" was used in "" episode "What Are Little Girls Made Of?"
The abbreviation "andy", coined as a pejorative by writer Philip K. Dick in his novel "Do Androids Dream of Electric Sheep?
", has seen some further usage, such as within the TV series "Total Recall 2070".
Authors have used the term "android" in more diverse ways than "robot" or "cyborg".
In some fictional works, the difference between a robot and android is only their appearance, with androids being made to look like humans on the outside but with robot-like internal mechanics.
In other stories, authors have used the word "android" to mean a wholly organic, yet artificial, creation.
Other fictional depictions of androids fall somewhere in between.
Eric G. Wilson, who defines androids as a "synthetic human being", distinguishes between three types of androids, based on their body's composition:

Although human morphology is not necessarily the ideal form for working robots, the fascination in developing robots that can mimic it can be found historically in the assimilation of two concepts: "simulacra" (devices that exhibit likeness) and "automata" (devices that have independence).
Several projects aiming to create androids that look, and, to a certain degree, speak or act like a human being have been launched or are underway.
Japanese robotics have been leading the field since the 1970s.
Waseda University initiated the WABOT project in 1967, and in 1972 completed the WABOT-1, the first android, a full-scale humanoid intelligent robot.
Its limb control system allowed it to walk with the lower limbs, and to grip and transport objects with hands, using tactile sensors.
Its vision system allowed it to measure distances and directions to objects using external receptors, artificial eyes and ears.
And its conversation system allowed it to communicate with a person in Japanese, with an artificial mouth.
In 1984, WABOT-2 was revealed, and made a number of improvements.
It was capable of playing the organ.
Wabot-2 had 10 fingers and two feet, and was able to read a score of music.
It was also able to accompany a person.
In 1986, Honda began its humanoid research and development program, to create humanoid robots capable of interacting successfully with humans.
The Intelligent Robotics Lab, directed by Hiroshi Ishiguro at Osaka University, and Kokoro Co., Ltd.
have demonstrated the Actroid at Expo 2005 in Aichi Prefecture, Japan and released the Telenoid R1 in 2010.
In 2006, Kokoro Co.
developed a new "DER 2" android.
The height of the human body part of DER2 is 165 cm.
There are 47 mobile points.
DER2 can not only change its expression but also move its hands and feet and twist its body.
The "air servosystem" which Kokoro Co.
developed originally is used for the actuator.
As a result of having an actuator controlled precisely with air pressure via a servosystem, the movement is very fluid and there is very little noise.
DER2 realized a slimmer body than that of the former version by using a smaller cylinder.
Outwardly DER2 has a more beautiful proportion.
Compared to the previous model, DER2 has thinner arms and a wider repertoire of expressions.
Once programmed, it is able to choreograph its motions and gestures with its voice.
The Intelligent Mechatronics Lab, directed by Hiroshi Kobayashi at the Tokyo University of Science, has developed an android head called "Saya", which was exhibited at Robodex 2002 in Yokohama, Japan.
There are several other initiatives around the world involving humanoid research and development at this time, which will hopefully introduce a broader spectrum of realized technology in the near future.
Now Saya is "working" at the Science University of Tokyo as a guide.
The Waseda University (Japan) and NTT Docomo's manufacturers have succeeded in creating a shape-shifting robot "WD-2".
It is capable of changing its face.
At first, the creators decided the positions of the necessary points to express the outline, eyes, nose, and so on of a certain person.
The robot expresses its face by moving all points to the decided positions, they say.
The first version of the robot was first developed back in 2003.
After that, a year later, they made a couple of major improvements to the design.
The robot features an elastic mask made from the average head dummy.
It uses a driving system with a 3DOF unit.
The WD-2 robot can change its facial features by activating specific facial points on a mask, with each point possessing three degrees of freedom.
This one has 17 facial points, for a total of 56 degrees of freedom.
As for the materials they used, the WD-2's mask is fabricated with a highly elastic material called Septom, with bits of steel wool mixed in for added strength.
Other technical features reveal a shaft driven behind the mask at the desired facial point, driven by a DC motor with a simple pulley and a slide screw.
Apparently, the researchers can also modify the shape of the mask based on actual human faces.
To "copy" a face, they need only a 3D scanner to determine the locations of an individual's 17 facial points.
After that, they are then driven into position using a laptop and 56 motor control boards.
In addition, the researchers also mention that the shifting robot can even display an individual's hair style and skin color if a photo of their face is projected onto the 3D Mask.
Prof Nadia Thalmann, a Nanyang Technological University scientist, directed efforts of the Institute for Media Innovation along with the School of Computer Engineering in the development of a social robot, Nadine.
Nadine is powered by software similar to Apple's Siri or Microsoft's Cortana.
Nadine may become a personal assistant in offices and homes in future, or she may become a companion for the young and the elderly.
Assoc Prof Gerald Seet from the School of Mechanical & Aerospace Engineering and the BeingThere Centre led a three-year R&D development in tele-presence robotics, creating EDGAR.
A remote user can control EDGAR with the user's face and expressions displayed on the robot's face in real time.
The robot also mimics their upper body movements.
KITECH researched and developed EveR-1, an android interpersonal communications model capable of emulating human emotional expression via facial "musculature" and capable of rudimentary conversation, having a vocabulary of around 400 words.
She is tall and weighs , matching the average figure of a Korean woman in her twenties.
EveR-1's name derives from the Biblical Eve, plus the letter "r" for "robot".
EveR-1's advanced computing processing power enables speech recognition and vocal synthesis, at the same time processing lip synchronization and visual recognition by 90-degree micro-CCD cameras with face recognition technology.
An independent microchip inside her artificial brain handles gesture expression, body coordination, and emotion expression.
Her whole body is made of highly advanced synthetic jelly silicon and with 60 artificial joints in her face, neck, and lower body; she is able to demonstrate realistic facial expressions and sing while simultaneously dancing.
In South Korea, the Ministry of Information and Communication has an ambitious plan to put a robot in every household by 2020.
Several robot cities have been planned for the country: the first will be built in 2016 at a cost of 500 billion won (440 million USD), of which 50 billion is direct government investment.
The new robot city will feature research and development centers for manufacturers and part suppliers, as well as exhibition halls and a stadium for robot competitions.
The country's new Robotics Ethics Charter will establish ground rules and laws for human interaction with robots in the future, setting standards for robotics users and manufacturers, as well as guidelines on ethical standards to be programmed into robots to prevent human abuse of robots and vice versa.
Walt Disney and a staff of Imagineers created Great Moments with Mr. Lincoln that debuted at the 1964 New York World's Fair.
Hanson Robotics, Inc., of Texas and KAIST produced an android portrait of Albert Einstein, using Hanson's facial android technology mounted on KAIST's life-size walking bipedal robot body.
This Einstein android, also called "Albert Hubo", thus represents the first full-body walking android in history (see video at).
Hanson Robotics, the FedEx Institute of Technology, and the University of Texas at Arlington also developed the android portrait of sci-fi author Philip K. Dick (creator of "Do Androids Dream of Electric Sheep?
", the basis for the film "Blade Runner"), with full conversational capabilities that incorporated thousands of pages of the author's works.
In 2005, the PKD android won a first place artificial intelligence award from AAAI.
Androids are a staple of science fiction.
Isaac Asimov pioneered the fictionalization of the science of robotics and artificial intelligence, notably in his 1950s series "I, Robot".
One thing common to most fictional androids is that the real-life technological challenges associated with creating thoroughly human-like robots—such as the creation of strong artificial intelligence—are assumed to have been solved.
Fictional androids are often depicted as mentally and physically equal or superior to humans—moving, thinking and speaking as fluidly as them.
The tension between the nonhuman substance and the human appearance—or even human ambitions—of androids is the dramatic impetus behind most of their fictional depictions.
Some android heroes seek, like Pinocchio, to become human, as in the film "Bicentennial Man", or Data in "".
Others, as in the film "Westworld", rebel against abuse by careless humans.
Android hunter Deckard in "Do Androids Dream of Electric Sheep?"
and its film adaptation "Blade Runner" discovers that his targets appear to be, in some ways, more "human" than he is.
Android stories, therefore, are not essentially stories "about" androids; they are stories about the human condition and what it means to be human.
One aspect of writing about the meaning of humanity is to use discrimination against androids as a mechanism for exploring racism in society, as in "Blade Runner".
Perhaps the clearest example of this is John Brunner's 1968 novel "Into the Slave Nebula", where the blue-skinned android slaves are explicitly shown to be fully human.
More recently, the androids Bishop and Annalee Call in the films "Aliens" and "Alien Resurrection" are used as vehicles for exploring how humans deal with the presence of an "Other".
Female androids, or "gynoids", are often seen in science fiction, and can be viewed as a continuation of the long tradition of men attempting to create the stereotypical "perfect woman".
Examples include the Greek myth of "Pygmalion" and the female robot Maria in Fritz Lang's "Metropolis".
Some gynoids, like Pris in "Blade Runner", are designed as sex-objects, with the intent of "pleasing men's violent sexual desires", or as submissive, servile companions, such as in "The Stepford Wives".
Fiction about gynoids has therefore been described as reinforcing "essentialist ideas of femininity", although others have suggested that the treatment of androids is a way of exploring racism and misogyny in society.
The 2015 Japanese film "Sayonara", starring Geminoid F, was promoted as "the first movie to feature an android performing opposite a human actor".
</doc>
<doc id="717" url="https://en.wikipedia.org/wiki?curid=717" title="Alberta">
Alberta

Alberta () is a western province of Canada.
With an estimated population of 4,067,175 as of 2016 census, it is Canada's fourth most populous province and the most populous of Canada's three prairie provinces.
Its area is about .
Alberta and its neighbour Saskatchewan were districts of the Northwest Territories until they were established as provinces on September 1, 1905.
The premier has been Rachel Notley since May 2015.
Alberta is bounded by the provinces of British Columbia to the west and Saskatchewan to the east, the Northwest Territories to the north, and the U.S.
state of Montana to the south.
Alberta is one of three Canadian provinces and territories to border only a single U.S.
state and one of only two landlocked provinces.
It has a predominantly humid continental climate, with stark contrasts over a year; but seasonal temperature average swings are smaller than in areas further east, due to winters being warmed by occasional chinook winds bringing sudden warming.
Alberta's capital, Edmonton, is near the geographic centre of the province and is the primary supply and service hub for Canada's crude oil, the Athabasca oil sands and other northern resource industries.
About south of the capital is Calgary, the largest city in Alberta.
Calgary and Edmonton centre Alberta's two census metropolitan areas, both of which have populations exceeding one million, while the province has 16 census agglomerations.
Tourist destinations in the province include Banff, Canmore, Drumheller, Jasper, Sylvan Lake and Lake Louise.
Alberta is named after Princess Louise Caroline Alberta (1848–1939), the fourth daughter of Queen Victoria.
Princess Louise was the wife of John Campbell, Marquess of Lorne, Governor General of Canada (1878–83).
Lake Louise and Mount Alberta were also named in her honour.
Alberta, with an area of , is the fourth-largest province after Quebec, Ontario and British Columbia.
To the south, the province borders on the 49th parallel north, separating it from the U.S.
state of Montana, while to the north the 60th parallel north divides it from the Northwest Territories.
To the east, the 110th meridian west separates it from the province of Saskatchewan, while on the west its boundary with British Columbia follows the 120th meridian west south from the Northwest Territories at 60°N until it reaches the Continental Divide at the Rocky Mountains, and from that point follows the line of peaks marking the Continental Divide in a generally southeasterly direction until it reaches the Montana border at 49°N.
The province extends north to south and east to west at its maximum width.
Its highest point is at the summit of Mount Columbia in the Rocky Mountains along the southwest border while its lowest point is on the Slave River in Wood Buffalo National Park in the northeast.
With the exception of the semi-arid steppe of the south-eastern section, the province has adequate water resources.
There are numerous rivers and lakes used for swimming, fishing and a range of water sports.
There are three large lakes, Lake Claire () in Wood Buffalo National Park, Lesser Slave Lake (), and Lake Athabasca () which lies in both Alberta and Saskatchewan.
The longest river in the province is the Athabasca River which travels from the Columbia Icefield in the Rocky Mountains to Lake Athabasca.
The largest river is the Peace River with an average flow of 2161 m/s.
The Peace River originates in the Rocky Mountains of northern British Columbia and flows through northern Alberta and into the Slave River, a tributary of the Mackenzie River.
Alberta's capital city, Edmonton, is located at about the geographic centre of the province.
It is the most northerly major city in Canada, and serves as a gateway and hub for resource development in northern Canada.
The region, with its proximity to Canada's largest oil fields, has most of western Canada's oil refinery capacity.
Calgary is about south of Edmonton and north of Montana, surrounded by extensive ranching country.
Almost 75% of the province's population lives in the Calgary–Edmonton Corridor.
The land grant policy to the railroads served as a means to populate the province in its early years.
Most of the northern half of the province is boreal forest, while the Rocky Mountains along the southwestern boundary are largely forested (see Alberta Mountain forests and Alberta-British Columbia foothills forests).
The southern quarter of the province is prairie, ranging from shortgrass prairie in the southeastern corner to mixed grass prairie in an arc to the west and north of it.
The central aspen parkland region extending in a broad arc between the prairies and the forests, from Calgary, north to Edmonton, and then east to Lloydminster, contains the most fertile soil in the province and most of the population.
Much of the unforested part of Alberta is given over either to grain or to dairy farming, with mixed farming more common in the north and centre, while ranching and irrigated agriculture predominate in the south.
The Alberta badlands are located in southeastern Alberta, where the Red Deer River crosses the flat prairie and farmland, and features deep canyons and striking landforms.
Dinosaur Provincial Park, near Brooks, Alberta, showcases the badlands terrain, desert flora, and remnants from Alberta's past when dinosaurs roamed the then lush landscape.
Alberta has a humid continental climate with warm summers and cold winters.
The province is open to cold arctic weather systems from the north, which often produce extremely cold conditions in winter.
As the fronts between the air masses shift north and south across Alberta, the temperature can change rapidly.
Arctic air masses in the winter produce extreme minimum temperatures varying from in northern Alberta to in southern Alberta, although temperatures at these extremes are rare.
In the summer, continental air masses have produced record maximum temperatures from in the mountains to over in southeastern Alberta.
Alberta extends for over from north to south; its climate, therefore, varies considerably.
Average high temperatures in January range from in the southwest to in the far north.
The climate is also influenced by the presence of the Rocky Mountains to the southwest, which disrupt the flow of the prevailing westerly winds and cause them to drop most of their moisture on the western slopes of the mountain ranges before reaching the province, casting a rain shadow over much of Alberta.
The northerly location and isolation from the weather systems of the Pacific Ocean cause Alberta to have a dry climate with little moderation from the ocean.
Annual precipitation ranges from in the southeast to in the north, except in the foothills of the Rocky Mountains where total precipitation including snowfall can reach annually.
The province is the namesake of the Alberta clipper, a type of intense, fast-moving winter storm that generally forms over or near the province and pushed with great speed by the continental polar jetstream descends over the rest of Southern Canada and the northern tier of the United States.
In the summer, the average daytime temperatures range from around in the Rocky Mountain valleys and far north, up to around in the dry prairie of the southeast.
The northern and western parts of the province experience higher rainfall and lower evaporation rates caused by cooler summer temperatures.
The south and east-central portions are prone to drought-like conditions sometimes persisting for several years, although even these areas can receive heavy precipitation, sometimes resulting in flooding.
Alberta is a sunny province.
Annual bright sunshine totals range between 1,900 up to just under 2,600 hours per year.
Northern Alberta gets about 18 hours of daylight in the summer.
In southwestern Alberta, the cold winters are frequently interrupted by warm, dry chinook winds blowing from the mountains, which can propel temperatures upward from frigid conditions to well above the freezing point in a very short period.
During one chinook recorded at Pincher Creek, temperatures soared from in just one hour.
The region around Lethbridge has the most chinooks, averaging 30 to 35 chinook days per year.
Calgary has a 56% chance of a white Christmas, while Edmonton has an 86% chance.
Northern Alberta is mostly covered by boreal forest and has a subarctic climate.
The agricultural area of southern Alberta has a semi-arid steppe climate because the annual precipitation is less than the water that evaporates or is used by plants.
The southeastern corner of Alberta, part of the Palliser Triangle, experiences greater summer heat and lower rainfall than the rest of the province, and as a result suffers frequent crop yield problems and occasional severe droughts.
Western Alberta is protected by the mountains and enjoys the mild temperatures brought by winter chinook winds.
Central and parts of northwestern Alberta in the Peace River region are largely aspen parkland, a biome transitional between prairie to the south and boreal forest to the north.
After Saskatchewan, Alberta experiences the most tornadoes in Canada with an average of 15 verified per year.
Thunderstorms, some of them severe, are frequent in the summer, especially in central and southern Alberta.
The region surrounding the Calgary–Edmonton Corridor is notable for having the highest frequency of hail in Canada, which is caused by orographic lifting from the nearby Rocky Mountains, enhancing the updraft/downdraft cycle necessary for the formation of hail.
In central and northern Alberta the arrival of spring is marked by the early flowering of the prairie crocus anemone; this member of the buttercup family has been recorded flowering as early as March, though April is the usual month for the general population.
Other prairie flora known to flower early are the golden bean and wild rose.
Members of the sunflower family blossom on the prairie in the summer months between July and September.
The southern and east central parts of Alberta are covered by short prairie grass, which dries up as summer lengthens, to be replaced by hardy perennials such as the prairie coneflower, fleabane, and sage.
Both yellow and white sweet clover can be found throughout the southern and central areas of the province.
The trees in the parkland region of the province grow in clumps and belts on the hillsides.
These are largely deciduous, typically aspen, poplar, and willow.
Many species of willow and other shrubs grow in virtually any terrain.
On the north side of the North Saskatchewan River evergreen forests prevail for thousands of square kilometres.
Aspen poplar, balsam poplar (or in some parts cottonwood), and paper birch are the primary large deciduous species.
Conifers include jack pine, Rocky Mountain pine, lodgepole pine, both white and black spruce, and the deciduous conifer tamarack.
The four climatic regions (alpine, boreal forest, parkland, and prairie) of Alberta are home to many different species of animals.
The south and central prairie was the land of the bison, commonly known as buffalo, its grasses providing pasture and breeding ground for millions of buffalo.
The buffalo population was decimated during early settlement, but since then buffalo have made a comeback, living on farms and in parks all over Alberta.
Alberta is home to many large carnivores.
Among them are the grizzly and black bears, which are found in the mountains and wooded regions.
Smaller carnivores of the canine and feline families include coyotes, wolves, fox, lynx, bobcat and mountain lion (cougar).
Herbivorous animals are found throughout the province.
Moose, mule deer, elk, and white-tailed deer are found in the wooded regions, and pronghorn can be found in the prairies of southern Alberta.
Bighorn sheep and mountain goats live in the Rocky Mountains.
Rabbits, porcupines, skunks, squirrels and many species of rodents and reptiles live in every corner of the province.
Alberta is home to only one variety of venomous snake, the prairie rattlesnake.
Central and northern Alberta and the region farther north is the nesting ground of many migratory birds.
Vast numbers of ducks, geese, swans and pelicans arrive in Alberta every spring and nest on or near one of the hundreds of small lakes that dot northern Alberta.
Eagles, hawks, owls and crows are plentiful, and a huge variety of smaller seed and insect-eating birds can be found.
Alberta, like other temperate regions, is home to mosquitoes, flies, wasps, and bees.
Rivers and lakes are populated with pike, walleye, whitefish, rainbow, speckled, brown trout, and sturgeon.
Bull trout, native to the province, is Alberta's provincial fish.
Turtles are found in some water bodies in the southern part of the province.
Frogs and salamanders are a few of the amphibians that make their homes in Alberta.
Alberta is the only province in Canada—as well as one of the few places in the world—that is free of Norwegian rats.
Since the early 1950s, the Government of Alberta has operated a rat-control program, which has been so successful that only isolated instances of wild rat sightings are reported, usually of rats arriving in the province aboard trucks or by rail.
In 2006, Alberta Agriculture reported zero findings of wild rats; the only rat interceptions have been domesticated rats that have been seized from their owners.
It is illegal for individual Albertans to own or keep Norwegian rats of any description; the animals can only be kept in the province by zoos, universities and colleges, and recognized research institutions.
In 2009, several rats were
found and captured, in small pockets in southern Alberta, putting Alberta's rat-free status in jeopardy.
A colony of rats were subsequently found in a landfill near Medicine Hat in 2012, and again in 2014.
Alberta has one of the greatest diversities and abundances of Late Cretaceous dinosaur fossils in the world.
Taxa are represented by complete fossil skeletons, isolated material, microvertebrate remains, and even mass graves.
At least 38 dinosaur type specimens were collected in the province.
The Foremost Formation, Oldman Formation and Dinosaur Park Formations collectively comprise the Judith River Group and are the most thoroughly studied dinosaur-bearing strata in Alberta.
Dinosaur-bearing strata are distributed widely throughout Alberta.
The Dinosaur Provincial Park area contains outcrops of the Dinosaur Park Formation and Oldman Formation.
In the central and southern regions of Alberta are intermittent Scollard Formation outcrops.
In the Drumheller Valley and Edmonton regions there are exposed Horseshoe Canyon facies.
Other formations have been recorded as well, like the Milk River and Foremost Formations.
However, these latter two have a lower diversity of documented dinosaurs, primarily due to their lower total fossil quantity and neglect from collectors who are hindered by the isolation and scarcity of exposed outcrops.
Their dinosaur fossils are primarily teeth recovered from microvertebrate fossil sites.
Additional geologic formations that have produced only few fossils are the Belly River Group and St.
Mary River Formations of the southwest and the northwestern Wapiti Formation.
The Wapiti Formation contains two "Pachyrhinosaurus" bone beds that break its general trend of low productivity, however.
The Bearpaw Formation represents strata deposited during a marine transgression.
Dinosaurs are known from this formation, but represent specimens washed out to sea or reworked from older sediments.
Paleo-Indians arrived in Alberta at least 10,000 years ago, toward the end of the last ice age.
They are thought to have migrated from Siberia to Alaska on a land bridge across the Bering Strait and then possibly moved down the east side of the Rocky Mountains through Alberta to settle the Americas.
Others may have migrated down the coast of British Columbia and then moved inland.
Over time they differentiated into various First Nations peoples, including the Plains Indian tribes of southern Alberta such as those of the Blackfoot Confederacy and the Plains Cree, who generally lived by hunting buffalo, and the more northerly tribes such as the Woodland Cree and Chipewyan who hunted, trapped, and fished for a living.
After the British arrival in Canada, approximately half of the province of Alberta, south of the Athabasca River drainage, became part of Rupert's Land which consisted of all land drained by rivers flowing into Hudson Bay.
This area was granted by Charles II of England to the Hudson's Bay Company (HBC) in 1670, and rival fur trading companies were not allowed to trade in it.
After the arrival of French Canadians in the west around 1731, they settled near fur trading posts, establishing communities such as Lac La Biche and Bonnyville.
Fort La Jonquière was established near what is now Calgary in 1752.
The Athabasca River and the rivers north of it were not in HBC territory because they drained into the Arctic Ocean instead of Hudson Bay, and they were prime habitat for fur-bearing animals.
The first explorer of the Athabasca region was Peter Pond, who learned of the Methye Portage, which allowed travel from southern rivers into the rivers north of Rupert's Land.
Fur traders formed the North West Company (NWC) of Montreal to compete with the HBC in 1779.
The NWC occupied the northern part of Alberta territory.
Peter Pond built Fort Athabasca on Lac la Biche in 1778.
Roderick Mackenzie built Fort Chipewyan on Lake Athabasca ten years later in 1788.
His cousin, Sir Alexander Mackenzie, followed the North Saskatchewan River to its northernmost point near Edmonton, then setting northward on foot, trekked to the Athabasca River, which he followed to Lake Athabasca.
It was there he discovered the mighty outflow river which bears his name—the Mackenzie River—which he followed to its outlet in the Arctic Ocean.
Returning to Lake Athabasca, he followed the Peace River upstream, eventually reaching the Pacific Ocean, and so he became the first European to cross the North American continent north of Mexico.
The extreme southernmost portion of Alberta was part of the French (and Spanish) territory of Louisiana, sold to the United States in 1803; in 1818, the portion of Louisiana north of the Forty-Ninth Parallel was ceded to Great Britain.
Fur trade expanded in the north, but bloody battles occurred between the rival HBC and NWC, and in 1821 the British government forced them to merge to stop the hostilities.
The amalgamated Hudson's Bay Company dominated trade in Alberta until 1870, when the newly formed Canadian Government purchased Rupert's Land.
Northern Alberta was included in the North-Western Territory until 1870, when it and Rupert's land became Canada's Northwest Territories.
The District of Alberta was created as part of the North-West Territories in 1882.
As settlement increased, local representatives to the North-West Legislative Assembly were added.
After a long campaign for autonomy, in 1905 the District of Alberta was enlarged and given provincial status, with the election of Alexander Cameron Rutherford as the first premier.
Less than a decade later, the First World War presented special challenges to the new province as an extraordinary number of volunteers left relatively few workers to maintain services and production.
Over 50% of Alberta's doctors volunteered for service overseas.
On June 21, 2013, during the 2013 Alberta floods Alberta experienced heavy rainfall that triggered catastrophic flooding throughout much of the southern half of the province along the Bow, Elbow, Highwood and Oldman rivers and tributaries.
A dozen municipalities in Southern Alberta declared local states of emergency on June 21 as water levels rose and numerous communities were placed under evacuation orders.
In 2016, a wildfire resulted in the largest evacuation of residents in Alberta's history, as more than 80,000 people were ordered to evacuate.
The 2016 census reported Alberta had a population of 4,067,175 living in 1,527,678 of its 1,654,129 total dwellings, an 11.6% change from its 2011 population of 3,645,257.
With a land area of , it had a population density of in 2016.
Statistics Canada estimated the province to have a population of 4,280,127 in Q1 of 2017.
Since 2000, Alberta's population has experienced a relatively high rate of growth, mainly because of its burgeoning economy.
Between 2003 and 2004, the province had high birthrates (on par with some larger provinces such as British Columbia), relatively high immigration, and a high rate of interprovincial migration compared to other provinces.
In 2016, Alberta continued to have the youngest population among the provinces with a median age of 36.7 years, compared with the national median of 41.2 years.
Also in 2016, Alberta had the smallest proportion of seniors (12.3%) among the provinces and one of the highest population shares of children (19.2%), further contributing to Alberta's young and growing population.
About 81% of the population lives in urban areas and only about 19% in rural areas.
The Calgary–Edmonton Corridor is the most urbanized area in the province and is one of the most densely populated areas of Canada.
Many of Alberta's cities and towns have experienced very high rates of growth in recent history.
Alberta's population rose from 73,022 in 1901 to 3,290,350 according to the 2006 census.
The 2006 census found that English, with 2,576,670 native speakers, was the most common mother tongue of Albertans, representing 79.99% of the population.
The next most common mother tongues were Chinese with 97,275 native speakers (3.02%), followed by German with 84,505 native speakers (2.62%) and French with 61,225 (1.90%).
Other mother tongues include: Punjabi, with 36,320 native speakers (1.13%); Tagalog, with 29,740 (0.92%); Ukrainian, with 29,455 (0.91%); Spanish, with 29,125 (0.90%); Polish, with 21,990 (0.68%); Arabic, with 20,495 (0.64%); Dutch, with 19,980 (0.62%); and Vietnamese, with 19,350 (0.60%).
The most common aboriginal language is Cree 17,215 (0.53%).
Other common mother tongues include Italian with 13,095 speakers (0.41%); Urdu with 11,275 (0.35%); and Korean with 10,845 (0.33%); then Hindi 8,985 (0.28%); Persian 7,700 (0.24%); Portuguese 7,205 (0.22%); and Hungarian 6,770 (0.21%).
Alberta has considerable ethnic diversity.
In line with the rest of Canada, many immigrants originated from England, Scotland, Ireland, Wales and France, but large numbers also came from other parts of Europe, notably Germany, Ukraine and Scandinavia.
According to Statistics Canada, Alberta is home to the second-highest proportion (two percent) of Francophones in western Canada (after Manitoba).
Despite this, relatively few Albertans claim French as their mother tongue.
Many of Alberta's French-speaking residents live in the central and northwestern regions of the province.
As reported in the 2001 census, the Chinese represented nearly four percent of Alberta's population, and East Indians represented more than two percent.
Both Edmonton and Calgary have historic Chinatowns, and Calgary has Canada's third-largest Chinese community.
The Chinese presence began with workers employed in the building of the Canadian Pacific Railway in the 1880s.
Aboriginal Albertans make up approximately three percent of the population.
In the 2006 Canadian census, the most commonly reported ethnic origins among Albertans were: 885,825 English (27.2%); 679,705 German (20.9%); 667,405 Canadian (20.5%); 661,265 Scottish (20.3%); 539,160 Irish (16.6%); 388,210 French (11.9%); 332,180 Ukrainian (10.2%); 172,910 Dutch (5.3%); 170,935 Polish (5.2%); 169,355 North American Indian (5.2%); 144,585 Norwegian (4.4%); and 137,600 Chinese (4.2%).
(Each person could choose as many ethnicities as were applicable.)
Amongst those of British origins, the Scots have had a particularly strong influence on place-names, with the names of many cities and towns including Calgary, Airdrie, Canmore, and Banff having Scottish origins.
Alberta is the third most diverse province in terms of visible minorities after British Columbia and Ontario with 13.9% of the population consisting of visible minorities.
Over one third of the populations of Calgary and Edmonton belong to a visible minority group.
Aboriginal Identity Peoples make up 5.8% of the population, about half of whom consist of North American Indians and the other half are Metis.
There are also small number of Inuit people in Alberta.
The number of Aboriginal Identity Peoples have been increasing at a rate greater than the population of Alberta.
As of the 2011 National Household Survey, the largest religious group was Roman Catholic, representing 24.3% of the population.
Alberta had the second-highest percentage of non-religious residents among the provinces (after British Columbia) at 31.6% of the population.
Of the remainder, 7.5% of the population identified themselves as belonging to the United Church of Canada, while 3.9% were Anglican.
Lutherans made up 3.3% of the population while Baptists comprised 1.9%.
The remainder belonged to a wide variety of different religious affiliations, none of which constituted more than 2% of the population.
Members of LDS Church are mostly concentrated in the extreme south of the province.
Alberta has a population of Hutterites, a communal Anabaptist sect similar to the Mennonites, and has a significant population of Seventh-day Adventists.
Alberta is home to several Byzantine Rite Churches as part of the legacy of Eastern European immigration, including the Ukrainian Catholic Eparchy of Edmonton, and the Ukrainian Orthodox Church of Canada's Western Diocese which is based in Edmonton.
Muslims, Sikhs, Buddhists, and Hindus live in Alberta.
Muslims made up 3.2% of the population, Sikhs 1.5%, Buddhists 1.2%, and Hindus 1.0%.
Many of these are recent immigrants, but others have roots that go back to the first settlers of the prairies.
Canada's oldest mosque, the Al-Rashid Mosque, is located in Edmonton, whereas Calgary is home to Canada's largest mosque, the Baitun Nur Mosque.
Alberta is also home to a growing Jewish population of about 15,400 people who constituted 0.3% of Alberta's population.
Most of Alberta's Jews live in the metropolitan areas of Calgary (8,200) and Edmonton (5,500).
Alberta's economy was one of the strongest in the world, supported by the burgeoning petroleum industry and to a lesser extent, agriculture and technology.
In 2013 Alberta's per capita GDP exceeded that of the United States, Norway, or Switzerland, and was the highest of any province in Canada at C$84,390.
This was 56% higher than the national average of C$53,870 and more than twice that of some of the Atlantic provinces.
In 2006 the deviation from the national average was the largest for any province in Canadian history.
According to the 2006 census, the median annual family income after taxes was $70,986 in Alberta (compared to $60,270 in Canada as a whole).
In 2014, Alberta had the second-largest economy in Canada after Ontario, with a GDP exceeding C$376 billion.
Alberta's debt-to-GDP ratio is projected to reach 11.4% in fiscal year 2019–2020, compared to a surplus-to-GDP ratio of 13.4% in 2009–2010.
The Calgary-Edmonton Corridor is the most urbanized region in the province and one of the densest in Canada.
The region covers a distance of roughly 400 kilometres north to south.
In 2001, the population of the Calgary-Edmonton Corridor was 2.15 million (72% of Alberta's population).
It is also one of the fastest-growing regions in the country.
A 2003 study by TD Bank Financial Group found the corridor to be the only Canadian urban centre to amass a U.S.
level of wealth while maintaining a Canadian style quality of life, offering universal health care benefits.
The study found that GDP per capita in the corridor was 10% above average U.S.
metropolitan areas and 40% above other Canadian cities at that time.
The Fraser Institute states that Alberta also has very high levels of economic freedom and rates Alberta as the freest economy in Canada, and second-freest economy amongst U.S.
states and Canadian provinces.
The government of Alberta has invested its earnings wisely; as of September 30, 2013, official statistics reported nearly 500 holdings.
In 2014, Merchandise exports totalled US$121.4 billion.
Energy revenues totalled $111.7 billion and Energy resource exports totalled $90.8 billion.
Farm Cash receipts from agricultural products totalled $12.9 billion.
Shipments of forest products totalled $5.4 billion while exports were $2.7 billion.
Manufacturing sales totaled $79.4 billion, and Alberta's ICT industries generated over $13 billion in revenue.
In total, Alberta's 2014 GDP amassed $364.5 billion in 2007 dollars, or $414.3 billion in 2015 dollars.
In 2015, Alberta's GDP grew despite low oil prices; however, it was unstable with growth rates as high 4.4% and as low as 0.2%.
Should the GDP remain at an average of 2.2% for the last two quarters of 2015, Alberta's GDP should exceed $430 billion by the end of 2015.
However, RBC Economics research predicts Alberta's real GDP growth to only average 0.6% for the last two quarters of 2015.
This estimate predicts a real GDP growth of only 1.4% for 2015.
A positive is the predicted 10.8% growth in Nominal GDP, and possibly above 11% in 2016.
Alberta is the largest producer of conventional crude oil, synthetic crude, natural gas and gas products in Canada.
Alberta is the world's second-largest exporter of natural gas and the fourth-largest producer.
Two of the largest producers of petrochemicals in North America are located in central and north-central Alberta.
In both Red Deer and Edmonton, polyethylene and vinyl manufacturers produce products that are shipped all over the world.
Edmonton's oil refineries provide the raw materials for a large petrochemical industry to the east of Edmonton.
The Athabasca oil sands surrounding Fort McMurray have estimated unconventional oil reserves approximately equal to the conventional oil reserves of the rest of the world, estimated to be 1.6 trillion barrels (254 km).
Many companies employ both conventional strip mining and non-conventional in situ methods to extract the bitumen from the oil sands.
As of late 2006 there were over $100 billion in oil sands projects under construction or in the planning stages in northeastern Alberta.
Another factor determining the viability of oil extraction from the oil sands is the price of oil.
The oil price increases since 2003 have made it profitable to extract this oil, which in the past would give little profit or even a loss.
By mid-2014, however, rising costs and stabilizing oil prices were threatening the economic viability of some projects.
An example of this was the shelving of the Joslyn north project in the Athabasca region in May 2014.
With concerted effort and support from the provincial government, several high-tech industries have found their birth in Alberta, notably patents related to interactive liquid-crystal display systems.
With a growing economy, Alberta has several financial institutions dealing with civil and private funds.
Agriculture has a significant position in the province's economy.
The province has over three million head of cattle, and Alberta beef has a healthy worldwide market.
Nearly one half of all Canadian beef is produced in Alberta.
Alberta is one of the top producers of plains buffalo (bison) for the consumer market.
Sheep for wool and mutton are also raised.
Wheat and canola are primary farm crops, with Alberta leading the provinces in spring wheat production; other grains are also prominent.
Much of the farming is dryland farming, often with fallow seasons interspersed with cultivation.
Continuous cropping (in which there is no fallow season) is gradually becoming a more common mode of production because of increased profits and a reduction of soil erosion.
Across the province, the once common grain elevator is slowly being lost as rail lines are decreasing; farmers typically truck the grain to central points.
Alberta is the leading beekeeping province of Canada, with some beekeepers wintering hives indoors in specially designed barns in southern Alberta, then migrating north during the summer into the Peace River valley where the season is short but the working days are long for honeybees to produce honey from clover and fireweed.
Hybrid canola also requires bee pollination, and some beekeepers service this need.
Forestry plays a vital role in Alberta's economy, providing over 15,000 jobs and contributing billions of dollars annually.
Uses for harvested timber include pulpwood, hardwood, engineered wood and bioproducts such as chemicals and biofuels.
Recently, the United States has been Canada and Alberta's largest importer of hardwood and pulpwood, although continued trades issues with the U.S.
have likely been a contributing factor towards Alberta's increased focus on Asian markets.
Alberta has been a tourist destination from the early days of the twentieth century, with attractions including outdoor locales for skiing, hiking and camping, shopping locales such as West Edmonton Mall, Calgary Stampede, outdoor festivals, professional athletic events, international sporting competitions such as the Commonwealth Games and Olympic Games, as well as more eclectic attractions.
There are also natural attractions like Elk Island National Park, Wood Buffalo National Park, and the Columbia Icefield.
According to Alberta Economic Development, Calgary and Edmonton both host over four million visitors annually.
Banff, Jasper and the Rocky Mountains are visited by about three million people per year.
Alberta tourism relies heavily on Southern Ontario tourists, as well as tourists from other parts of Canada, the United States, and many other countries.
Alberta's Rockies include well-known tourist destinations Banff National Park and Jasper National Park.
The two mountain parks are connected by the scenic Icefields Parkway.
Banff is located west of Calgary on Highway 1, and Jasper is located west of Edmonton on Yellowhead Highway.
Five of Canada's fourteen UNESCO World heritage sites are located within the province: Canadian Rocky Mountain Parks, Waterton-Glacier International Peace Park, Wood Buffalo National Park, Dinosaur Provincial Park and Head-Smashed-In Buffalo Jump.
About 1.2 million people visit the Calgary Stampede, a celebration of Canada's own Wild West and the cattle ranching industry.
About 700,000 people enjoy Edmonton's K-Days (formerly Klondike Days and Capital EX).
Edmonton was the gateway to the only all-Canadian route to the Yukon gold fields, and the only route which did not require gold-seekers to travel the exhausting and dangerous Chilkoot Pass.
Another tourist destination that draws more than 650,000 visitors each year is the Drumheller Valley, located northeast of Calgary.
Drumheller, "Dinosaur Capital of The World", offers the Royal Tyrrell Museum of Palaeontology.
Drumheller also had a rich mining history being one of Western Canada's largest coal producers during the war years.
Located in east-central Alberta is Alberta Prairie Railway Excursions, a popular tourist attraction operated out of Stettler, that offers train excursions into the prairie and caters to tens of thousands of visitors every year.
Alberta has numerous ski resorts most notably Sunshine Village, Lake Louise, Marmot Basin, Norquay and Nakiska.
The Government of Alberta is organized as a parliamentary democracy with a unicameral legislature.
Its unicameral legislature—the Legislative Assembly—consists of eighty-seven members elected first past the post (FPTP) from single-member constituencies.
Locally municipal governments and school boards are elected and operate separately.
Their boundaries do not necessarily coincide.
As Canada's head of state, Queen Elizabeth II is the head of state for the Government of Alberta.
Her duties in Alberta are carried out by Lieutenant Governor Lois Mitchell.
The Queen and lieutenant governor are figureheads whose actions are highly restricted by custom and constitutional convention.
The lieutenant governor handles numerous honorific duties in the name of the Queen.
The government is headed by the premier.
The premier is normally a member of the Legislative Assembly, and draws all the members of the Cabinet from among the members of the Legislative Assembly.
The City of Edmonton is the seat of the provincial government—the capital of Alberta.
The current premier is Rachel Notley, sworn in on May 24, 2015.
The previous premier was Jim Prentice, who became the leader of the then governing Progressive Conservatives on September 6, 2014, following the resignation of Alison Redford and the interim leadership of Dave Hancock.
Prentice was sworn in as the 16th Premier of Alberta on September 15, 2014.
He called an early election on May 5, 2015 in which the opposition New Democratic Party (NDP) won a majority of the seats.
Prentice immediately resigned his seat and leadership of the PC party, but remained premier until Notley was sworn in on May 24, 2015.
Alberta's elections have tended to yield much more conservative outcomes than those of other Canadian provinces.
Since the 1960s, Alberta has had three main political parties, the Progressive Conservatives ("Conservatives" or "Tories"), the Liberals, and the social democratic New Democrats.
The Wildrose Party, a more conservative party formed in early 2008, gained much support in 2012 election and became the official opposition, a role it held until 2017 when it was dissolved and succeeded by the new United Conservative Party created by the merger of Wildrose and Progressive Conservatives.
The strongly conservative Social Credit Party was a power in Alberta for many decades, but fell from the political map after the Progressive Conservatives came to power in 1971.
For 44 years the Progressive Conservatives governed Alberta.
They lost the 2015 election to the NDP (which formed their own government for the first time in provincial history), signalling a possible shift to the left in the province, also indicated by the election of progressive mayors in both of Alberta's major cities.
Since becoming a province in 1905, Alberta has seen only four changes of government—only five parties have governed Alberta: the Liberals, from 1905 to 1921; the United Farmers of Alberta, from 1921 to 1935; the Social Credit Party, from 1935 to 1971, the Progressive Conservative Party, from 1971 to 2015, and, since 2015, the currently governing Alberta New Democratic Party.
Government revenue comes mainly from royalties on non-renewable natural resources (30.4%), personal income taxes (22.3%), corporate and other taxes (19.6%), and grants from the federal government primarily for infrastructure projects (9.8%).
Albertans are the lowest-taxed people in Canada, and Alberta is the only province in Canada without a provincial sales tax (but residents are still subject to the federal sales tax, the Goods and Services Tax of 5%).
It is also the only Canadian province to have a flat tax for personal income taxes, which is 10% of taxable income.
The Alberta personal income tax system maintains a progressive character by granting residents personal tax exemptions of $17,787, in addition to a variety of tax deductions for persons with disabilities, students, and the aged.
Alberta's municipalities and school jurisdictions have their own governments who usually work in co-operation with the provincial government.
Alberta also privatized alcohol distribution.
The privatization increased outlets from 304 stores to 1,726; 1,300 jobs to 4,000 jobs; and 3,325 products to 16,495 products.
Tax revenue also increased from $400 million to $700 million.
Albertan municipalities raise a significant portion of their income through levying property taxes.
The value of assessed property in Alberta was approximately $727 billion in 2011.
Most real property is assessed according to its market value.
The exceptions to market value assessment are farmland, railways, machinery & equipment and linear property, all of which is assessed by regulated rates.
Depending on the property type, property owners may appeal a property assessment to their municipal 'Local Assessment Review Board', 'Composite Assessment Review Board,' or the Alberta Municipal Government Board.
Policing in the province of Alberta upon its creation was the responsibility of the Royal Northwest Mounted Police.
In 1917, due to pressures of World War I, the Alberta Provincial Police was created.
This organization policed the province until it was disbanded as a Great Depression era cost cutting measure in 1932.
It was at that time the now renamed Royal Canadian Mounted Police resumed policing of the province, specifically RCMP "K" Division.
With the advent of the Alberta Sheriffs Branch, the duties of law enforcement in Alberta has been evolving as certain aspects, such as traffic enforcement, mobile surveillance and the close protection of the Premier of Alberta have been transferred to the Sheriffs.
In 2006, Alberta formed the Alberta Law Enforcement Response Teams (ALERT) to combat organized crime and the serious offences that accompany it.
ALERT is made up of members of the RCMP, Sheriffs Branch and various major municipal police forces in Alberta.
Military bases in Alberta include Canadian Forces Base (CFB) Cold Lake, CFB Edmonton, CFB Suffield and CFB Wainwright.
Air force units stationed at CFB Cold Lake have access to the Cold Lake Air Weapons Range.
CFB Edmonton is the headquarters for the 3rd Canadian Division.
CFB Suffield hosts British troops and is the largest training facility in Canada.
Alberta has over of highways and roads, of which nearly are paved.
The main north-south corridor is Highway 2, which begins south of Cardston at the Carway border crossing and is part of the CANAMEX Corridor.
Highway 4, which effectively extends Interstate 15 into Alberta and is the busiest U.S.
gateway to the province, begins at the Coutts border crossing and ends at Lethbridge.
Highway 3 joins Lethbridge to Fort Macleod and links Highway 2 to Highway 4.
Highway 2 travels north through Fort Macleod, Calgary, Red Deer, and Edmonton.
North of Edmonton, the highway continues to Athabasca, then northwesterly along the south shore of Lesser Slave Lake into High Prairie, north to Peace River, west to Fairview and finally south to Grande Prairie, where it ends at an interchange with Highway 43.
The section of Highway 2 between Calgary and Edmonton has been named the Queen Elizabeth II Highway to commemorate the visit of the monarch in 2005.
Highway 2 is supplemented by two more highways that run parallel to it: Highway 22, west of Highway 2, known as "Cowboy Trail", and Highway 21, east of Highway 2.
Highway 43 travels northwest into Grande Prairie and the Peace River Country; Highway 63 travels northeast to Fort McMurray, the location of the Athabasca oil sands.
Alberta has two main east-west corridors.
The southern corridor, part of the Trans-Canada Highway system, enters the province near Medicine Hat, runs westward through Calgary, and leaves Alberta through Banff National Park.
The northern corridor, also part of the Trans-Canada network and known as the Yellowhead Highway (Highway 16), runs west from Lloydminster in eastern Alberta, through Edmonton and Jasper National Park into British Columbia.
One of the most scenic drives is along the Icefields Parkway, which runs for between Jasper and Lake Louise, with mountain ranges and glaciers on either side of its entire length.
A third corridor stretches across southern Alberta; Highway 3 runs between Crowsnest Pass and Medicine Hat through Lethbridge and forms the eastern portion of the Crowsnest Highway.
Another major corridor through central Alberta is Highway 11 (also known as the David Thompson Highway), which runs east from the Saskatchewan River Crossing in Banff National Park through Rocky Mountain House and Red Deer, connecting with Highway 12 west of Stettler.
The highway connects many of the smaller towns in central Alberta with Calgary and Edmonton, as it crosses Highway 2 just west of Red Deer.
Urban stretches of Alberta's major highways and freeways are often called "trails".
For example, Highway 2, the main north-south highway in the province, is called Deerfoot Trail as it passes through Calgary but becomes Calgary Trail (for southbound traffic) and Gateway Boulevard (for northbound traffic) as it enters Edmonton and then turns into St.
Albert Trail as it leaves Edmonton for the City of St.
Albert.
Calgary, in particular, has a tradition of calling its largest urban expressways "trails" and naming many of them after prominent First Nations individuals and tribes, such as Crowchild Trail, Deerfoot Trail, and Stoney Trail.
Calgary, Edmonton, Red Deer, Medicine Hat, and Lethbridge have substantial public transit systems.
In addition to buses, Calgary and Edmonton operate light rail transit (LRT) systems.
Edmonton LRT, which is underground in the downtown core and on the surface outside the CBD, was the first of the modern generation of light rail systems to be built in North America, while the Calgary C-Train has one of the highest number of daily riders of any LRT system in North America.
Alberta is well-connected by air, with international airports in both Calgary and Edmonton.
Calgary International Airport and Edmonton International Airport are the fourth- and fifth-busiest in Canada, respectively.
Calgary's airport is a hub for WestJet Airlines and a regional hub for Air Canada.
Calgary's airport primarily serves the Canadian prairie provinces (Alberta, Saskatchewan and Manitoba) for connecting flights to British Columbia, eastern Canada, 15 major U.S.
centres, nine European airports, one Asian airport and four destinations in Mexico and the Caribbean.
Edmonton's airport acts as a hub for the Canadian north and has connections to all major Canadian airports as well as 10 major U.S.
airports, 3 European airports and 6 Mexican and Caribbean airports.
There are more than of operating mainline railway in Alberta.
The vast majority of this trackage is owned by the Canadian Pacific Railway and Canadian National Railway companies, which operate railway freight across the province.
Additional railfreight service in the province is provided by two shortline railways: the Battle River Railway and Forty Mile Rail.
Passenger trains include Via Rail's Canadian (Toronto–Vancouver) or Jasper–Prince Rupert trains, which use the CN mainline and pass through Jasper National Park and parallel the Yellowhead Highway during at least part of their routes.
The Rocky Mountaineer operates two sections: one from Vancouver to Banff and Calgary over CP tracks, and a section that travels over CN tracks to Jasper.
Alberta provides a publicly funded health care system, Alberta Health Services, for all its citizens and residents as set out by the provisions of the Canada Health Act of 1984.
Alberta became Canada's second province (after Saskatchewan) to adopt a Tommy Douglas-style program in 1950, a precursor to the modern medicare system.
Alberta's health care budget is currently $22.5 billion during the 2018–2019 fiscal year (approximately 45% of all government spending), making it the best funded health care system per-capita in Canada.
Every hour the province spends more than $2.5 million, (or $60 million per day), to maintain and improve health care in the province.
Notable health, education, research, and resources facilities in Alberta, all of which are located within Calgary or Edmonton:

The Edmonton Clinic complex, completed in 2012, provides a similar research, education, and care environment as the Mayo Clinic in the United States.
All public health care services funded by the Government of Alberta are delivered operationally by Alberta Health Services.
AHS is the province's single health authority established on July 1, 2008, which replaced nine local health authorities.
AHS also funds all ground ambulance services in the province, as well as the province-wide STARS (Shock Trauma Air Rescue Society) air ambulance service.
As with any Canadian province, the Alberta Legislature has (almost) exclusive authority to make laws respecting education.
Since 1905 the Legislature has used this capacity to continue the model of locally elected public and separate school boards which originated prior to 1905, as well as to create and regulate universities, colleges, technical institutions and other educational forms and institutions (public charter schools, private schools, home schooling).
There are forty-two public school jurisdictions in Alberta, and seventeen operating separate school jurisdictions.
Sixteen of the operating separate school jurisdictions have a Catholic electorate, and one (St.
Albert) has a Protestant electorate.
In addition, one Protestant separate school district, Glen Avon, survives as a ward of the St.
Paul Education Region.
The City of Lloydminster straddles the Alberta/Saskatchewan border, and both the public and separate school systems in that city are counted in the above numbers: both of them operate according to Saskatchewan law.
For many years the provincial government has funded the greater part of the cost of providing K–12 education.
Prior to 1994 public and separate school boards in Alberta had the legislative authority to levy a local tax on property as a supplementary support for local education.
In 1994 the government of the province eliminated this right for public school boards, but not for separate school boards.
Since 1994 there has continued to be a tax on property in support of K–12 education; the difference is that the mill rate is now set by the provincial government, the money is collected by the local municipal authority and remitted to the provincial government.
The relevant legislation requires that all the money raised by this property tax must go to the support of K–12 education provided by school boards.
The provincial government pools the property tax funds from across the province and distributes them, according to a formula, to public and separate school jurisdictions and Francophone authorities.
Public and separate school boards, charter schools, and private schools all follow the Program of Studies and the curriculum approved by the provincial department of education (Alberta Education).
Homeschool tutors may choose to follow the Program of Studies or develop their own Program of Studies.
Public and separate schools, charter schools, and approved private schools all employ teachers who are certificated by Alberta Education, they administer Provincial Achievement Tests and Diploma Examinations set by Alberta Education, and they may grant high school graduation certificates endorsed by Alberta Education.
The University of Alberta, located in Edmonton and established in 1908, is Alberta's oldest and largest university.
The University of Calgary, once affiliated with the University of Alberta, gained its autonomy in 1966 and is now the second-largest university in Alberta.
Athabasca University, which focuses on distance learning, and the University of Lethbridge are located in Athabasca and Lethbridge respectively.
In early September 2009, Mount Royal University became Calgary's second public university, and in late September 2009, a similar move made MacEwan University Edmonton's second public university.
There are 15 colleges that receive direct public funding, along with two technical institutes, Northern Alberta Institute of Technology and Southern Alberta Institute of Technology.
There is also many private post-secondary institutions, mostly Christian Universities, bringing the total number of universities to 12.
Students may also receive government loans and grants while attending selected private institutions.
There has been some controversy in recent years over the rising cost of post-secondary education for students (as opposed to taxpayers).
In 2005, Premier Ralph Klein made a promise that he would freeze tuition and look into ways of reducing schooling costs.
Summer brings many festivals to the province of Alberta, especially in Edmonton.
The Edmonton Fringe Festival is the world's second-largest after the Edinburgh Festival.
Both Calgary and Edmonton host a number of annual festivals and events, including folk music festivals.
The city's "heritage days" festival sees the participation of over 70 ethnic groups.
Edmonton's Churchill Square is home to a large number of the festivals, including the large Taste of Edmonton & The Works Art & Design Festival throughout the summer months.
The City of Calgary is also famous for its Stampede, dubbed "The Greatest Outdoor Show on Earth".
The Stampede is Canada's biggest rodeo festival and features various races and competitions, such as calf roping and bull riding.
In line with the western tradition of rodeo are the cultural artisans that reside and create unique Alberta western heritage crafts.
The Banff Centre hosts a range of festivals and other events including the international Mountain Film Festival.
These cultural events in Alberta highlight the province's cultural diversity.
Most of the major cities have several performing theatre companies who entertain in venues as diverse as Edmonton's Arts Barns and the Francis Winspear Centre for Music.
Both Calgary and Edmonton are home to Canadian Football League and National Hockey League teams (the Stampeders/Flames and Eskimos/Oilers respectively).
Soccer, rugby union and lacrosse are also played professionally in Alberta.
Alberta Separatism has a long history of support.
In 1982, Alberta elected separatist Gordon Kesler to the Legislative Assembly of Alberta.
Kesler represented the Western Canada Concept, a party founded in response to the National Energy Program created by Pierre Trudeau.
In a September 2018 Ipsos poll, 25% of Albertans believed they would be better off separating from Canada.
Geopolitics expert Peter Zeihan devoted a chapter in his book "Accidental Superpower" to argue why Alberta would be better off as a U.S.
state.
Alberta has relationships with several provinces, states, and other entities worldwide.
</doc>
<doc id="728" url="https://en.wikipedia.org/wiki?curid=728" title="List of anthropologists">
List of anthropologists













</doc>
<doc id="734" url="https://en.wikipedia.org/wiki?curid=734" title="Actinopterygii">
Actinopterygii

Actinopterygii (), or the ray-finned fishes, constitute a class or subclass of the bony fishes.
The ray-finned fishes are so called because their fins are webs of skin supported by bony or horny spines ("rays"), as opposed to the fleshy, lobed fins that characterize the class Sarcopterygii (lobe-finned fish).
These actinopterygian fin rays attach directly to the proximal or basal skeletal elements, the radials, which represent the link or connection between these fins and the internal skeleton (e.g., pelvic and pectoral girdles).
Numerically, actinopterygians are the dominant class of vertebrates, comprising nearly 99% of the over 30,000 species of fish.
They are ubiquitous throughout freshwater and marine environments from the deep sea to the highest mountain streams.
Extant species can range in size from "Paedocypris", at , to the massive ocean sunfish, at , and the long-bodied oarfish, at .
Ray-finned fishes occur in many variant forms.
The main features of a typical ray-finned fish are shown in the adjacent diagram.
In nearly all ray-finned fish, the sexes are separate, and in most species the females spawn eggs that are fertilized externally, typically with the male inseminating the eggs after they are laid.
Development then proceeds with a free-swimming larval stage.
However other patterns of ontogeny exist, with one of the commonest being sequential hermaphroditism.
In most cases this involves protogyny, fish starting life as females and converting to males at some stage, triggered by some internal or external factor.
Protandry, where a fish converts from male to female, is much less common than protogyny.
Most families use external rather than internal fertilization.
Of the oviparous teleosts, most (79%) do not provide parental care.
Viviparity, ovoviviparity, or some form of parental care for eggs, whether by the male, the female, or both parents is seen in a significant fraction (21%) of the 422 teleost families; no care is likely the ancestral condition.
Viviparity is relatively rare and is found in about 6% of teleost species; male care is far more common than female care.
Male territoriality "preadapts" a species for evolving male parental care.
There are a few examples of fish that self-fertilise.
The mangrove rivulus is an amphibious, simultaneous hermaphrodite, producing both eggs and spawn and having internal fertilisation.
This mode of reproduction may be related to the fish's habit of spending long periods out of water in the mangrove forests it inhabits.
Males are occasionally produced at temperatures below and can fertilise eggs that are then spawned by the female.
This maintains genetic variability in a species that is otherwise highly inbred.
The earliest known fossil actinopterygiian is "Andreolepis hedei", dating back 420 million years (Late Silurian).
Remains have been found in Russia, Sweden, and Estonia.
Actinopterygians are divided into the subclasses Chondrostei and Neopterygii.
The Neopterygii, in turn, are divided into the infraclasses Holostei and Teleostei.
During the Mesozoic and Cenozoic the teleosts in particular diversified widely, and as a result, 96% of all known fish species are teleosts.
The cladogram shows the major groups of actinopterygians and their relationship to the terrestrial vertebrates (tetrapods) that evolved from a related group of fish.
Approximate dates are from Near et al., 2012.
The polypterids (bichirs and ropefish) are the sister lineage of all other actinopterygians, The Acipenseriformes (sturgeons and paddlefishes) are the sister lineage of Neopterygii, and Holostei (bowfin and gars) are the sister lineage of teleosts.
The Elopomorpha (eels and tarpons) appears to be the most basic teleosts.
The listing below follows Phylogenetic Classification of Bony Fishes with notes when this differs from Nelson, ITIS and FishBase and extinct groups from Van der Laan 2016.
</doc>
<doc id="736" url="https://en.wikipedia.org/wiki?curid=736" title="Albert Einstein">
Albert Einstein

Albert Einstein (; ; 14 March 1879 – 18 April 1955) was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics (alongside quantum mechanics).
His work is also known for its influence on the philosophy of science.
He is best known to the general public for his mass–energy equivalence formula , which has been dubbed "the world's most famous equation".
He received the 1921 Nobel Prize in Physics "for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect", a pivotal step in the development of quantum theory.
Near the beginning of his career, Einstein thought that Newtonian mechanics was no longer enough to reconcile the laws of classical mechanics with the laws of the electromagnetic field.
This led him to develop his special theory of relativity during his time at the Swiss Patent Office in Bern (1902–1909), Switzerland.
However, he realized that the principle of relativity could also be extended to gravitational fields, and he published a paper on general relativity in 1916 with his theory of gravitation.
He continued to deal with problems of statistical mechanics and quantum theory, which led to his explanations of particle theory and the motion of molecules.
He also investigated the thermal properties of light which laid the foundation of the photon theory of light.
In 1917, he applied the general theory of relativity to model the structure of the universe.
Einstein lived in Switzerland between 1895 and 1914, except for one year in Prague, during which time he renounced in German citizenship in 1896 then received his academic diploma from the Swiss federal polytechnic school (later the Eidgenössische Technische Hochschule, ETH) in Zürich in 1900.
After being stateless for more than five years, he acquired Swiss citizenship in 1901, which he kept for the rest of his life.
In 1905, he was awarded a PhD by the University of Zurich.
The same year, he published four groundbreaking papers during his renowned "annus mirabilis" (miracle year) which brought him to the notice of the academic world at the age of 26.
Einstein taught theoretical physics at Zurich between 1912 and 1914 before he left for Berlin, where he was elected to the Prussian Academy of Sciences.
In 1933, while Einstein was visiting the United States, Adolf Hitler came to power.
Because of his Jewish background, Einstein did not return to Germany.
He settled in the United States and became an American citizen in 1940.
On the eve of World War II, he endorsed a letter to President Franklin D. Roosevelt alerting him to the potential development of "extremely powerful bombs of a new type" and recommending that the US begin similar research.
This eventually led to the Manhattan Project.
Einstein supported the Allies, but he generally denounced the idea of using nuclear fission as a weapon.
He signed the Russell–Einstein Manifesto with British philosopher Bertrand Russell, which highlighted the danger of nuclear weapons.
He was affiliated with the Institute for Advanced Study in Princeton, New Jersey, until his death in 1955.
Einstein published more than 300 scientific papers and more than 150 non-scientific works.
His intellectual achievements and originality have made the word "Einstein" synonymous with "genius".
Eugene Wigner wrote of Einstein in comparison to his contemporaries that "Einstein's understanding was deeper even than Jancsi von Neumann's.
His mind was both more penetrating and more original than von Neumann's.
And that is a very remarkable statement."
Albert Einstein was born in Ulm, in the Kingdom of Württemberg in the German Empire, on 14 March 1879.
His parents were Hermann Einstein, a salesman and engineer, and Pauline Koch.
In 1880, the family moved to Munich, where Einstein's father and his uncle Jakob founded "Elektrotechnische Fabrik J. Einstein & Cie", a company that manufactured electrical equipment based on direct current.
The Einsteins were non-observant Ashkenazi Jews, and Albert attended a Catholic elementary school in Munich, from the age of 5, for three years.
At the age of 8, he was transferred to the Luitpold Gymnasium (now known as the Albert Einstein Gymnasium), where he received advanced primary and secondary school education until he left the German Empire seven years later.
In 1894, Hermann and Jakob's company lost a bid to supply the city of Munich with electrical lighting because they lacked the capital to convert their equipment from the direct current (DC) standard to the more efficient alternating current (AC) standard.
The loss forced the sale of the Munich factory.
In search of business, the Einstein family moved to Italy, first to Milan and a few months later to Pavia.
When the family moved to Pavia, Einstein, then 15, stayed in Munich to finish his studies at the Luitpold Gymnasium.
His father intended for him to pursue electrical engineering, but Einstein clashed with authorities and resented the school's regimen and teaching method.
He later wrote that the spirit of learning and creative thought was lost in strict rote learning.
At the end of December 1894, he travelled to Italy to join his family in Pavia, convincing the school to let him go by using a doctor's note.
During his time in Italy he wrote a short essay with the title "On the Investigation of the State of the Ether in a Magnetic Field".
Einstein always excelled at math and physics from a young age, reaching a mathematical level years ahead of his peers.
The twelve year old Einstein taught himself algebra and Euclidean geometry over a single summer.
Einstein also independently discovered his own original proof of the Pythagorean theorem at age 12.
A family tutor Max Talmud says that after he had given the 12 year old Einstein a geometry textbook, after a short time "[Einstein] had worked through the whole book.
He thereupon devoted himself to higher mathematics... Soon the flight of his mathematical genius was so high I could not follow."
His passion for geometry and algebra led the twelve year old to become convinced that nature could be understood as a "mathematical structure".
Einstein started teaching himself calculus at 12, and as a 14 year old he says he had "mastered integral and differential calculus".
At age 13, Einstein was introduced to Kant's "Critique of Pure Reason", and Kant became his favorite philosopher, his tutor stating: "At the time he was still a child, only thirteen years old, yet Kant's works, incomprehensible to ordinary mortals, seemed to be clear to him."
In 1895, at the age of 16, Einstein took the entrance examinations for the Swiss Federal Polytechnic in Zürich (later the Eidgenössische Technische Hochschule, ETH).
He failed to reach the required standard in the general part of the examination, but obtained exceptional grades in physics and mathematics.
On the advice of the principal of the Polytechnic, he attended the Argovian cantonal school (gymnasium) in Aarau, Switzerland, in 1895 and 1896 to complete his secondary schooling.
While lodging with the family of professor Jost Winteler, he fell in love with Winteler's daughter, Marie.
Albert's sister Maja later married Winteler's son Paul.
In January 1896, with his father's approval, Einstein renounced his citizenship in the German Kingdom of Württemberg to avoid military service.
In September 1896, he passed the Swiss Matura with mostly good grades, including a top grade of 6 in physics and mathematical subjects, on a scale of 1–6.
At 17, he enrolled in the four-year mathematics and physics teaching diploma program at the Zürich Polytechnic.
Marie Winteler, who was a year older, moved to Olsberg, Switzerland, for a teaching post.
Einstein's future wife, a 20-year old Serbian woman Mileva Marić, also enrolled at the Polytechnic that year.
She was the only woman among the six students in the mathematics and physics section of the teaching diploma course.
Over the next few years, Einstein and Marić's friendship developed into romance, and they read books together on extra-curricular physics in which Einstein was taking an increasing interest.
In 1900, Einstein passed the exams in Maths and Physics and was awarded the Federal Polytechnic teaching diploma.
There have been claims that Marić collaborated with Einstein on his 1905 papers, known as the "Annus Mirabilis" papers, but historians of physics who have studied the issue find no evidence that she made any substantive contributions.
An early correspondence between Einstein and Marić was discovered and published in 1987 which revealed that the couple had a daughter named "Lieserl", born in early 1902 in Novi Sad where Marić was staying with her parents.
Marić returned to Switzerland without the child, whose real name and fate are unknown.
The contents of Einstein's letter in September 1903 suggest that the girl was either given up for adoption or died of scarlet fever in infancy.
Einstein and Marić married in January 1903.
In May 1904, their son Hans Albert Einstein was born in Bern, Switzerland.
Their son Eduard was born in Zürich in July 1910.
The couple moved to Berlin in April 1914, but Marić returned to Zürich with their sons after learning that Einstein's chief romantic attraction was his first and second cousin Elsa.
They divorced on 14 February 1919, having lived apart for five years.
Eduard had a breakdown at about age 20 and was diagnosed with schizophrenia.
His mother cared for him and he was also committed to asylums for several periods, finally being committed permanently after her death.
In letters revealed in 2015, Einstein wrote to his early love Marie Winteler about his marriage and his strong feelings for her.
He wrote in 1910, while his wife was pregnant with their second child: "I think of you in heartfelt love every spare minute and am so unhappy as only a man can be".
He spoke about a "misguided love" and a "missed life" regarding his love for Marie.
Einstein married Elsa Löwenthal in 1919, after having a relationship with her since 1912.
She was a first cousin maternally and a second cousin paternally.
They emigrated to the United States in 1933.
Elsa was diagnosed with heart and kidney problems in 1935 and died in December 1936.
Among Einstein's well-known friends were Michele Besso, Paul Ehrenfest, Marcel Grossmann, János Plesch, Daniel Q. Posin, Maurice Solovine, and Stephen Wise.
After graduating in 1900, Einstein spent almost two frustrating years searching for a teaching post.
He acquired Swiss citizenship in February 1901, but for medical reasons was not conscripted.
With the help of Marcel Grossmann's father, he secured a job in Bern at the Federal Office for Intellectual Property, the patent office, as an assistant examiner – level III.
Einstein evaluated patent applications for a variety of devices including a gravel sorter and an electromechanical typewriter.
In 1903, his position at the Swiss Patent Office became permanent, although he was passed over for promotion until he "fully mastered machine technology".
Much of his work at the patent office related to questions about transmission of electric signals and electrical–mechanical synchronization of time, two technical problems that show up conspicuously in the thought experiments that eventually led Einstein to his radical conclusions about the nature of light and the fundamental connection between space and time.
With a few friends he had met in Bern, Einstein started a small discussion group in 1902, self-mockingly named "The Olympia Academy", which met regularly to discuss science and philosophy.
Their readings included the works of Henri Poincaré, Ernst Mach, and David Hume, which influenced his scientific and philosophical outlook.
In 1900, Einstein's paper "Folgerungen aus den Capillaritätserscheinungen" ("Conclusions from the Capillarity Phenomena") was published in the journal "Annalen der Physik".
On 30 April 1905, Einstein completed his thesis, with Alfred Kleiner, Professor of Experimental Physics, serving as "pro-forma" advisor.
As a result, Einstein was awarded a PhD by the University of Zürich, with his dissertation ""A New Determination of Molecular Dimensions"".
In that same year, which has been called Einstein's "annus mirabilis" (miracle year), he published four groundbreaking papers, on the photoelectric effect, Brownian motion, special relativity, and the equivalence of mass and energy, which were to bring him to the notice of the academic world, at the age of 26.
By 1908, he was recognized as a leading scientist and was appointed lecturer at the University of Bern.
The following year, after giving a lecture on electrodynamics and the relativity principle at the University of Zürich, Alfred Kleiner recommended him to the faculty for a newly created professorship in theoretical physics.
Einstein was appointed associate professor in 1909.
Einstein became a full professor at the German Charles-Ferdinand University in Prague in April 1911, accepting Austrian citizenship in the Austro-Hungarian Empire to do so.
During his Prague stay, he wrote 11 scientific works, five of them on radiation mathematics and on the quantum theory of solids.
In July 1912, he returned to his alma mater in Zürich.
From 1912 until 1914, he was professor of theoretical physics at the ETH Zurich, where he taught analytical mechanics and thermodynamics.
He also studied continuum mechanics, the molecular theory of heat, and the problem of gravitation, on which he worked with mathematician and friend Marcel Grossmann.
On 3 July 1913, he was voted for membership in the Prussian Academy of Sciences in Berlin.
Max Planck and Walther Nernst visited him the next week in Zurich to persuade him to join the academy, additionally offering him the post of director at the Kaiser Wilhelm Institute for Physics, which was soon to be established.
(Membership in the academy included paid salary and professorship without teaching duties at the Humboldt University of Berlin.)
He was officially elected to the academy on 24 July, and he accepted to move to the German Empire the next year.
His decision to move to Berlin was also influenced by the prospect of living near his cousin Elsa, with whom he had developed a romantic affair.
He joined the academy and thus the Berlin University on 1 April 1914.
As World War I broke out that year, the plan for Kaiser Wilhelm Institute for Physics was aborted.
The institute was established on 1 October 1917, with Einstein as its director.
In 1916, Einstein was elected president of the German Physical Society (1916–1918).
Based on calculations Einstein made in 1911, about his new theory of general relativity, light from another star should be bent by the Sun's gravity.
In 1919, that prediction was confirmed by Sir Arthur Eddington during the solar eclipse of 29 May 1919.
Those observations were published in the international media, making Einstein world-famous.
On 7 November 1919, the leading British newspaper "The Times" printed a banner headline that read: "Revolution in Science – New Theory of the Universe – Newtonian Ideas Overthrown".
In 1920, he became a Foreign Member of the Royal Netherlands Academy of Arts and Sciences.
In 1922, he was awarded the 1921 Nobel Prize in Physics "for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect".
While the general theory of relativity was still considered somewhat controversial, the citation also does not treat the cited work as an "explanation" but merely as a "discovery of the law", as the idea of photons was considered outlandish and did not receive universal acceptance until the 1924 derivation of the Planck spectrum by S. N. Bose.
Einstein was elected a Foreign Member of the Royal Society (ForMemRS) in 1921.
He also received the Copley Medal from the Royal Society in 1925.
Einstein visited New York City for the first time on 2 April 1921, where he received an official welcome by Mayor John Francis Hylan, followed by three weeks of lectures and receptions.
He went on to deliver several lectures at Columbia University and Princeton University, and in Washington he accompanied representatives of the National Academy of Science on a visit to the White House.
On his return to Europe he was the guest of the British statesman and philosopher Viscount Haldane in London, where he met several renowned scientific, intellectual and political figures, and delivered a lecture at King's College London.
He also published an essay, "My First Impression of the U.S.A.," in July 1921, in which he tried briefly to describe some characteristics of Americans, much as had Alexis de Tocqueville, who published his own impressions in "Democracy in America" (1835).
For some of his observations, Einstein was clearly surprised: "What strikes a visitor is the joyous, positive attitude to life ... The American is friendly, self-confident, optimistic, and without envy."
In 1922, his travels took him to Asia and later to Palestine, as part of a six-month excursion and speaking tour, as he visited Singapore, Ceylon and Japan, where he gave a series of lectures to thousands of Japanese.
After his first public lecture, he met the emperor and empress at the Imperial Palace, where thousands came to watch.
In a letter to his sons, he described his impression of the Japanese as being modest, intelligent, considerate, and having a true feel for art.
Because of Einstein's travels to the Far East, he was unable to personally accept the Nobel Prize for Physics at the Stockholm award ceremony in December 1922.
In his place, the banquet speech was held by a German diplomat, who praised Einstein not only as a scientist but also as an international peacemaker and activist.
On his return voyage, he visited Palestine for 12 days in what would become his only visit to that region.
He was greeted as if he were a head of state, rather than a physicist, which included a cannon salute upon arriving at the home of the British high commissioner, Sir Herbert Samuel.
During one reception, the building was stormed by people who wanted to see and hear him.
In Einstein's talk to the audience, he expressed happiness that the Jewish people were beginning to be recognized as a force in the world.
Einstein visited Spain for two weeks in 1923, where he briefly met Santiago Ramón y Cajal and also received a diploma from King Alfonso XIII naming him a member of the Spanish Academy of Sciences.
From 1922 to 1932, Einstein was a member of the International Committee on Intellectual Cooperation of the League of Nations in Geneva (with a few months of interruption in 1923–1924), a body created to promote international exchange between scientists, researchers, teachers, artists and intellectuals.
Originally slated to serve as the Swiss delegate, Secretary-General Eric Drummond was persuaded by Catholic activists Oskar Halecki and Giuseppe Motta to instead have him become the German delegate, thus allowing Gonzague de Reynold to take the Swiss spot, from which he promoted traditionalist Catholic values.
His former physics professor Hendrik Lorentz and the French chemist Marie Curie were also members of the committee.
In December 1930, Einstein visited America for the second time, originally intended as a two-month working visit as a research fellow at the California Institute of Technology.
After the national attention he received during his first trip to the US, he and his arrangers aimed to protect his privacy.
Although swamped with telegrams and invitations to receive awards or speak publicly, he declined them all.
After arriving in New York City, Einstein was taken to various places and events, including Chinatown, a lunch with the editors of "The New York Times", and a performance of "Carmen" at the Metropolitan Opera, where he was cheered by the audience on his arrival.
During the days following, he was given the keys to the city by Mayor Jimmy Walker and met the president of Columbia University, who described Einstein as "the ruling monarch of the mind".
Harry Emerson Fosdick, pastor at New York's Riverside Church, gave Einstein a tour of the church and showed him a full-size statue that the church made of Einstein, standing at the entrance.
Also during his stay in New York, he joined a crowd of 15,000 people at Madison Square Garden during a Hanukkah celebration.
Einstein next traveled to California, where he met Caltech president and Nobel laureate, Robert A. Millikan.
His friendship with Millikan was "awkward", as Millikan "had a penchant for patriotic militarism," where Einstein was a pronounced pacifist.
During an address to Caltech's students, Einstein noted that science was often inclined to do more harm than good.
This aversion to war also led Einstein to befriend author Upton Sinclair and film star Charlie Chaplin, both noted for their pacifism.
Carl Laemmle, head of Universal Studios, gave Einstein a tour of his studio and introduced him to Chaplin.
They had an instant rapport, with Chaplin inviting Einstein and his wife, Elsa, to his home for dinner.
Chaplin said Einstein's outward persona, calm and gentle, seemed to conceal a "highly emotional temperament," from which came his "extraordinary intellectual energy".
Chaplin's film, "City Lights", was to premiere a few days later in Hollywood, and Chaplin invited Einstein and Elsa to join him as his special guests.
Walter Isaacson, Einstein's biographer, described this as "one of the most memorable scenes in the new era of celebrity".
Chaplin visited Einstein at his home on a later trip to Berlin, and recalled his "modest little flat" and the piano at which he had begun writing his theory.
Chaplin speculated that it was "possibly used as kindling wood by the Nazis."
In February 1933 while on a visit to the United States, Einstein knew he could not return to Germany with the rise to power of the Nazis under Germany's new chancellor, Adolf Hitler.
While at American universities in early 1933, he undertook his third two-month visiting professorship at the California Institute of Technology in Pasadena.
He and his wife Elsa returned to Belgium by ship in March, and during the trip they learned that their cottage was raided by the Nazis and his personal sailboat confiscated.
Upon landing in Antwerp on 28 March, he immediately went to the German consulate and surrendered his passport, formally renouncing his German citizenship.
The Nazis later sold his boat and converted his cottage into a Hitler Youth camp.
In April 1933, Einstein discovered that the new German government had passed laws barring Jews from holding any official positions, including teaching at universities.
Historian Gerald Holton describes how, with "virtually no audible protest being raised by their colleagues", thousands of Jewish scientists were suddenly forced to give up their university positions and their names were removed from the rolls of institutions where they were employed.
A month later, Einstein's works were among those targeted by the German Student Union in the Nazi book burnings, with Nazi propaganda minister Joseph Goebbels proclaiming, "Jewish intellectualism is dead."
One German magazine included him in a list of enemies of the German regime with the phrase, "not yet hanged", offering a $5,000 bounty on his head.
In a subsequent letter to physicist and friend Max Born, who had already emigrated from Germany to England, Einstein wrote, "... I must confess that the degree of their brutality and cowardice came as something of a surprise."
After moving to the US, he described the book burnings as a "spontaneous emotional outburst" by those who "shun popular enlightenment," and "more than anything else in the world, fear the influence of men of intellectual independence."
Einstein was now without a permanent home, unsure where he would live and work, and equally worried about the fate of countless other scientists still in Germany.
He rented a house in De Haan, Belgium, where he lived for a few months.
In late July 1933, he went to England for about six weeks at the personal invitation of British naval officer Commander Oliver Locker-Lampson, who had become friends with Einstein in the preceding years.
To protect Einstein, Locker-Lampson had two assistants watch over him at his secluded cottage outside London, with photo of them carrying shotguns and guarding Einstein, published in the "Daily Herald" on 24 July 1933.
Locker-Lampson took Einstein to meet Winston Churchill at his home, and later, Austen Chamberlain and former Prime Minister Lloyd George.
Einstein asked them to help bring Jewish scientists out of Germany.
British historian Martin Gilbert notes that Churchill responded immediately, and sent his friend, physicist Frederick Lindemann, to Germany to seek out Jewish scientists and place them in British universities.
Churchill later observed that as a result of Germany having driven the Jews out, they had lowered their "technical standards" and put the Allies' technology ahead of theirs.
Einstein later contacted leaders of other nations, including Turkey's Prime Minister, İsmet İnönü, to whom he wrote in September 1933 requesting placement of unemployed German-Jewish scientists.
As a result of Einstein's letter, Jewish invitees to Turkey eventually totaled over "1,000 saved individuals".
Locker-Lampson also submitted a bill to parliament to extend British citizenship to Einstein, during which period Einstein made a number of public appearances describing the crisis brewing in Europe.
In one of his speeches he denounced Germany's treatment of Jews, while at the same time he introduced a bill promoting Jewish citizenship in Palestine, as they were being denied citizenship elsewhere.
In his speech he described Einstein as a "citizen of the world" who should be offered a temporary shelter in the UK.
Both bills failed, however, and Einstein then accepted an earlier offer from the Institute for Advanced Study, in Princeton, New Jersey, US, to become a resident scholar.
In October 1933 Einstein returned to the US and took up a position at the Institute for Advanced Study, noted for having become a refuge for scientists fleeing Nazi Germany.
At the time, most American universities, including Harvard, Princeton and Yale, had minimal or no Jewish faculty or students, as a result of their Jewish quotas, which lasted until the late 1940s.
Einstein was still undecided on his future.
He had offers from several European universities, including Christ Church, Oxford where he stayed for three short periods between May 1931 and June 1933 and was offered a 5-year studentship, but in 1935 he arrived at the decision to remain permanently in the United States and apply for citizenship.
Einstein's affiliation with the Institute for Advanced Study would last until his death in 1955.
He was one of the four first selected (two of the others being John von Neumann and Kurt Gödel) at the new Institute, where he soon developed a close friendship with Gödel.
The two would take long walks together discussing their work.
Bruria Kaufman, his assistant, later became a physicist.
During this period, Einstein tried to develop a unified field theory and to refute the accepted interpretation of quantum physics, both unsuccessfully.
In 1939, a group of Hungarian scientists that included émigré physicist Leó Szilárd attempted to alert Washington to ongoing Nazi atomic bomb research.
The group's warnings were discounted.
Einstein and Szilárd, along with other refugees such as Edward Teller and Eugene Wigner, "regarded it as their responsibility to alert Americans to the possibility that German scientists might win the race to build an atomic bomb, and to warn that Hitler would be more than willing to resort to such a weapon."
To make certain the US was aware of the danger, in July 1939, a few months before the beginning of World War II in Europe, Szilárd and Wigner visited Einstein to explain the possibility of atomic bombs, which Einstein, a pacifist, said he had never considered.
He was asked to lend his support by writing a letter, with Szilárd, to President Roosevelt, recommending the US pay attention and engage in its own nuclear weapons research.
The letter is believed to be "arguably the key stimulus for the U.S.
adoption of serious investigations into nuclear weapons on the eve of the U.S.
entry into World War II".
In addition to the letter, Einstein used his connections with the Belgian Royal Family and the Belgian queen mother to get access with a personal envoy to the White House's Oval Office.
Some say that as a result of Einstein's letter and his meetings with Roosevelt, the US entered the "race" to develop the bomb, drawing on its "immense material, financial, and scientific resources" to initiate the Manhattan Project.
For Einstein, "war was a disease ... [and] he called for resistance to war."
By signing the letter to Roosevelt, some argue he went against his pacifist principles.
In 1954, a year before his death, Einstein said to his old friend, Linus Pauling, "I made one great mistake in my life—when I signed the letter to President Roosevelt recommending that atom bombs be made; but there was some justification—the danger that the Germans would make them ..."

Einstein became an American citizen in 1940.
Not long after settling into his career at the Institute for Advanced Study (in Princeton, New Jersey), he expressed his appreciation of the meritocracy in American culture when compared to Europe.
He recognized the "right of individuals to say and think what they pleased", without social barriers, and as a result, individuals were encouraged, he said, to be more creative, a trait he valued from his own early education.
In his travel diaries from his 1922-23 visit to Asia, he expresses xenophobic and racist judgments on the Chinese, Japanese and Indian people he saw.
Einstein joined the National Association for the Advancement of Colored People (NAACP) in Princeton, where he campaigned for the civil rights of African Americans.
He considered racism America's "worst disease," seeing it as "handed down from one generation to the next".
As part of his involvement, he corresponded with civil rights activist W. E. B. Du Bois and was prepared to testify on his behalf during his trial in 1951.
When Einstein offered to be a character witness for Du Bois, the judge decided to drop the case.
In 1946 Einstein visited Lincoln University in Pennsylvania, a historically black college, where he was awarded an honorary degree.
(Lincoln was the first university in the United States to grant college degrees to African Americans; alumni include Langston Hughes and Thurgood Marshall.)
Einstein gave a speech about racism in America, adding, "I do not intend to be quiet about it."
A resident of Princeton recalls that Einstein had once paid the college tuition for a black student.
Einstein was a figurehead leader in helping establish the Hebrew University of Jerusalem, which opened in 1925, and was among its first Board of Governors.
Earlier, in 1921, he was asked by the biochemist and president of the World Zionist Organization, Chaim Weizmann, to help raise funds for the planned university.
He also submitted various suggestions as to its initial programs.
Among those, he advised first creating an Institute of Agriculture in order to settle the undeveloped land.
That should be followed, he suggested, by a Chemical Institute and an Institute of Microbiology, to fight the various ongoing epidemics such as malaria, which he called an "evil" that was undermining a third of the country's development.
Establishing an Oriental Studies Institute, to include language courses given in both Hebrew and Arabic, for scientific exploration of the country and its historical monuments, was also important.
Chaim Weizmann later became Israel's first president.
Upon his death while in office in November 1952 and at the urging of Ezriel Carlebach, Prime Minister David Ben-Gurion offered Einstein the position of President of Israel, a mostly ceremonial post.
The offer was presented by Israel's ambassador in Washington, Abba Eban, who explained that the offer "embodies the deepest respect which the Jewish people can repose in any of its sons".
Einstein declined, and wrote in his response that he was "deeply moved", and "at once saddened and ashamed" that he could not accept it.
Einstein developed an appreciation for music at an early age, and later wrote: "If I were not a physicist, I would probably be a musician.
I often think in music.
I live my daydreams in music.
I see my life in terms of music... I get most joy in life out of music."
His mother played the piano reasonably well and wanted her son to learn the violin, not only to instill in him a love of music but also to help him assimilate into German culture.
According to conductor Leon Botstein, Einstein began playing when he was 5, although he did not enjoy it at that age.
When he turned 13, he discovered the violin sonatas of Mozart, whereupon "Einstein fell in love" with Mozart's music and studied music more willingly.
He taught himself to play without "ever practicing systematically", he said, deciding that "love is a better teacher than a sense of duty."
At age 17, he was heard by a school examiner in Aarau as he played Beethoven's violin sonatas, the examiner stating afterward that his playing was "remarkable and revealing of 'great insight'."
What struck the examiner, writes Botstein, was that Einstein "displayed a deep love of the music, a quality that was and remains in short supply.
Music possessed an unusual meaning for this student."
Music took on a pivotal and permanent role in Einstein's life from that period on.
Although the idea of becoming a professional musician himself was not on his mind at any time, among those with whom Einstein played chamber music were a few professionals, and he performed for private audiences and friends.
Chamber music had also become a regular part of his social life while living in Bern, Zürich, and Berlin, where he played with Max Planck and his son, among others.
He is sometimes erroneously credited as the editor of the 1937 edition of the Köchel catalogue of Mozart's work; that edition was prepared by Alfred Einstein, who may have been a distant relation.
In 1931, while engaged in research at the California Institute of Technology, he visited the Zoellner family conservatory in Los Angeles, where he played some of Beethoven and Mozart's works with members of the Zoellner Quartet.
Near the end of his life, when the young Juilliard Quartet visited him in Princeton, he played his violin with them, and the quartet was "impressed by Einstein's level of coordination and intonation".
Einstein's political view was in favor of socialism and critical of capitalism, which he detailed in his essays such as "Why Socialism?".
Einstein offered and was called on to give judgments and opinions on matters often unrelated to theoretical physics or mathematics.
He strongly advocated the idea of a democratic global government that would check the power of nation-states in the framework of a world federation.
The FBI created a secret dossier on Einstein in 1932, and by the time of his death his FBI file was 1,427 pages long.
Einstein was deeply impressed by Mahatma Gandhi.
He exchanged written letters with Gandhi, and called him "a role model for the generations to come" in a letter writing about him.
Einstein spoke of his spiritual outlook in a wide array of original writings and interviews.
Einstein stated that he had sympathy for the impersonal pantheistic God of Baruch Spinoza's philosophy.
He did not believe in a personal God who concerns himself with fates and actions of human beings, a view which he described as naïve.
He clarified, however, that "I am not an atheist", preferring to call himself an agnostic, or a "deeply religious nonbeliever."
When asked if he believed in an afterlife, Einstein replied, "No.
And one life is enough for me."
Einstein was primarily affiliated with non-religious humanist and Ethical Culture groups in both the UK and US.
He served on the advisory board of the First Humanist Society of New York, and was an honorary associate of the Rationalist Association, which publishes "New Humanist" in Britain.
For the seventy-fifth anniversary of the New York Society for Ethical Culture, he stated that the idea of Ethical Culture embodied his personal conception of what is most valuable and enduring in religious idealism.
He observed, "Without 'ethical culture' there is no salvation for humanity."
On 17 April 1955, Einstein experienced internal bleeding caused by the rupture of an abdominal aortic aneurysm, which had previously been reinforced surgically by Rudolph Nissen in 1948.
He took the draft of a speech he was preparing for a television appearance commemorating the State of Israel's seventh anniversary with him to the hospital, but he did not live long enough to complete it.
Einstein refused surgery, saying, "I want to go when I want.
It is tasteless to prolong life artificially.
I have done my share; it is time to go.
I will do it elegantly."
He died in Princeton Hospital early the next morning at the age of 76, having continued to work until near the end.
During the autopsy, the pathologist of Princeton Hospital, Thomas Stoltz Harvey, removed Einstein's brain for preservation without the permission of his family, in the hope that the neuroscience of the future would be able to discover what made Einstein so intelligent.
Einstein's remains were cremated and his ashes were scattered at an undisclosed location.
In a memorial lecture delivered on 13 December 1965, at UNESCO headquarters, nuclear physicist Robert Oppenheimer summarized his impression of Einstein as a person: "He was almost wholly without sophistication and wholly without worldliness ... There was always with him a wonderful purity at once childlike and profoundly stubborn."
Throughout his life, Einstein published hundreds of books and articles.
He published more than 300 scientific papers and 150 non-scientific ones.
On 5 December 2014, universities and archives announced the release of Einstein's papers, comprising more than 30,000 unique documents.
Einstein's intellectual achievements and originality have made the word "Einstein" synonymous with "genius."
In addition to the work he did by himself he also collaborated with other scientists on additional projects including the Bose–Einstein statistics, the Einstein refrigerator and others.
The "Annus Mirabilis" papers are four articles pertaining to the photoelectric effect (which gave rise to quantum theory), Brownian motion, the special theory of relativity, and E = mc that Einstein published in the "Annalen der Physik" scientific journal in 1905.
These four works contributed substantially to the foundation of modern physics and changed views on space, time, and matter.
The four papers are:

Einstein's first paper submitted in 1900 to "Annalen der Physik" was on capillary attraction.
It was published in 1901 with the title "Folgerungen aus den Capillaritätserscheinungen", which translates as "Conclusions from the capillarity phenomena".
Two papers he published in 1902–1903 (thermodynamics) attempted to interpret atomic phenomena from a statistical point of view.
These papers were the foundation for the 1905 paper on Brownian motion, which showed that Brownian movement can be construed as firm evidence that molecules exist.
His research in 1903 and 1904 was mainly concerned with the effect of finite atomic size on diffusion phenomena.
Einstein returned to the problem of thermodynamic fluctuations, giving a treatment of the density variations in a fluid at its critical point.
Ordinarily the density fluctuations are controlled by the second derivative of the free energy with respect to the density.
At the critical point, this derivative is zero, leading to large fluctuations.
The effect of density fluctuations is that light of all wavelengths is scattered, making the fluid look milky white.
Einstein relates this to Rayleigh scattering, which is what happens when the fluctuation size is much smaller than the wavelength, and which explains why the sky is blue.
Einstein quantitatively derived critical opalescence from a treatment of density fluctuations, and demonstrated how both the effect and Rayleigh scattering originate from the atomistic constitution of matter.
Einstein's ""Zur Elektrodynamik bewegter Körper"" ("On the Electrodynamics of Moving Bodies") was received on 30 June 1905 and published 26 September of that same year.
It reconciled conflicts between Maxwell's equations (the laws of electricity and magnetism) and the laws of Newtonian mechanics by introducing changes to the laws of mechanics.
Observationally, the effects of these changes are most apparent at high speeds (where objects are moving at speeds close to the speed of light).
The theory developed in this paper later became known as Einstein's special theory of relativity.
This paper predicted that, when measured in the frame of a relatively moving observer, a clock carried by a moving body would appear to slow down, and the body itself would contract in its direction of motion.
This paper also argued that the idea of a luminiferous aether—one of the leading theoretical entities in physics at the time—was superfluous.
In his paper on mass–energy equivalence, Einstein produced "E" = "mc" as a consequence of his special relativity equations.
Einstein's 1905 work on relativity remained controversial for many years, but was accepted by leading physicists, starting with Max Planck.
Einstein originally framed special relativity in terms of kinematics (the study of moving bodies).
In 1908, Hermann Minkowski reinterpreted special relativity in geometric terms as a theory of spacetime.
Einstein adopted Minkowski's formalism in his 1915 general theory of relativity.
General relativity (GR) is a theory of gravitation that was developed by Einstein between 1907 and 1915.
According to general relativity, the observed gravitational attraction between masses results from the warping of space and time by those masses.
General relativity has developed into an essential tool in modern astrophysics.
It provides the foundation for the current understanding of black holes, regions of space where gravitational attraction is so strong that not even light can escape.
As Einstein later said, the reason for the development of general relativity was that the preference of inertial motions within special relativity was unsatisfactory, while a theory which from the outset prefers no state of motion (even accelerated ones) should appear more satisfactory.
Consequently, in 1907 he published an article on acceleration under special relativity.
In that article titled "On the Relativity Principle and the Conclusions Drawn from It", he argued that free fall is really inertial motion, and that for a free-falling observer the rules of special relativity must apply.
This argument is called the equivalence principle.
In the same article, Einstein also predicted the phenomena of gravitational time dilation, gravitational red shift and deflection of light.
In 1911, Einstein published another article "On the Influence of Gravitation on the Propagation of Light" expanding on the 1907 article, in which he estimated the amount of deflection of light by massive bodies.
Thus, the theoretical prediction of general relativity could for the first time be tested experimentally.
In 1916, Einstein predicted gravitational waves, ripples in the curvature of spacetime which propagate as waves, traveling outward from the source, transporting energy as gravitational radiation.
The existence of gravitational waves is possible under general relativity due to its Lorentz invariance which brings the concept of a finite speed of propagation of the physical interactions of gravity with it.
By contrast, gravitational waves cannot exist in the Newtonian theory of gravitation, which postulates that the physical interactions of gravity propagate at infinite speed.
The first, indirect, detection of gravitational waves came in the 1970s through observation of a pair of closely orbiting neutron stars, PSR B1913+16.
The explanation of the decay in their orbital period was that they were emitting gravitational waves.
Einstein's prediction was confirmed on 11 February 2016, when researchers at LIGO published the first observation of gravitational waves, detected on Earth on 14 September 2015, exactly one hundred years after the prediction.
While developing general relativity, Einstein became confused about the gauge invariance in the theory.
He formulated an argument that led him to conclude that a general relativistic field theory is impossible.
He gave up looking for fully generally covariant tensor equations, and searched for equations that would be invariant under general linear transformations only.
In June 1913, the Entwurf ("draft") theory was the result of these investigations.
As its name suggests, it was a sketch of a theory, less elegant and more difficult than general relativity, with the equations of motion supplemented by additional gauge fixing conditions.
After more than two years of intensive work, Einstein realized that the hole argument was mistaken and abandoned the theory in November 1915.
In 1917, Einstein applied the general theory of relativity to the structure of the universe as a whole.
He discovered that the general field equations predicted a universe that was dynamic, either contracting or expanding.
As observational evidence for a dynamic universe was not known at the time, Einstein introduced a new term, the cosmological constant, to the field equations, in order to allow the theory to predict a static universe.
The modified field equations predicted a static universe of closed curvature, in accordance with Einstein's understanding of Mach's principle in these years.
This model became known as the Einstein World or Einstein's static universe.
Following the discovery of the recession of the nebulae by Edwin Hubble in 1929, Einstein abandoned his static model of the universe, and proposed two dynamic models of the cosmos, The Friedmann-Einstein universe of 1931 and the Einstein–de Sitter universe of 1932.
In each of these models, Einstein discarded the cosmological constant, claiming that it was "in any case theoretically unsatisfactory".
In many Einstein biographies, it is claimed that Einstein referred to the cosmological constant in later years as his "biggest blunder".
The astrophysicist Mario Livio has recently cast doubt on this claim, suggesting that it may be exaggerated.
In late 2013, a team led by the Irish physicist Cormac O'Raifeartaigh discovered evidence that, shortly after learning of Hubble's observations of the recession of the nebulae, Einstein considered a steady-state model of the universe.
In a hitherto overlooked manuscript, apparently written in early 1931, Einstein explored a model of the expanding universe in which the density of matter remains constant due to a continuous creation of matter, a process he associated with the cosmological constant.
As he stated in the paper, "In what follows, I would like to draw attention to a solution to equation (1) that can account for Hubbel's ["sic"] facts, and in which the density is constant over time" ... "If one considers a physically bounded volume, particles of matter will be continually leaving it.
For the density to remain constant, new particles of matter must be continually formed in the volume from space."
It thus appears that Einstein considered a steady-state model of the expanding universe many years before Hoyle, Bondi and Gold.
However, Einstein's steady-state model contained a fundamental flaw and he quickly abandoned the idea.
General relativity includes a dynamical spacetime, so it is difficult to see how to identify the conserved energy and momentum.
Noether's theorem allows these quantities to be determined from a Lagrangian with translation invariance, but general covariance makes translation invariance into something of a gauge symmetry.
The energy and momentum derived within general relativity by Noether's prescriptions do not make a real tensor for this reason.
Einstein argued that this is true for fundamental reasons, because the gravitational field could be made to vanish by a choice of coordinates.
He maintained that the non-covariant energy momentum pseudotensor was in fact the best description of the energy momentum distribution in a gravitational field.
This approach has been echoed by Lev Landau and Evgeny Lifshitz, and others, and has become standard.
The use of non-covariant objects like pseudotensors was heavily criticized in 1917 by Erwin Schrödinger and others.
In 1935, Einstein collaborated with Nathan Rosen to produce a model of a wormhole, often called Einstein–Rosen bridges.
His motivation was to model elementary particles with charge as a solution of gravitational field equations, in line with the program outlined in the paper "Do Gravitational Fields play an Important Role in the Constitution of the Elementary Particles?".
These solutions cut and pasted Schwarzschild black holes to make a bridge between two patches.
If one end of a wormhole was positively charged, the other end would be negatively charged.
These properties led Einstein to believe that pairs of particles and antiparticles could be described in this way.
In order to incorporate spinning point particles into general relativity, the affine connection needed to be generalized to include an antisymmetric part, called the torsion.
This modification was made by Einstein and Cartan in the 1920s.
The theory of general relativity has a fundamental law—the Einstein equations which describe how space curves, the geodesic equation which describes how particles move may be derived from the Einstein equations.
Since the equations of general relativity are non-linear, a lump of energy made out of pure gravitational fields, like a black hole, would move on a trajectory which is determined by the Einstein equations themselves, not by a new law.
So Einstein proposed that the path of a singular solution, like a black hole, would be determined to be a geodesic from general relativity itself.
This was established by Einstein, Infeld, and Hoffmann for pointlike objects without angular momentum, and by Roy Kerr for spinning objects.
In a 1905 paper, Einstein postulated that light itself consists of localized particles ("quanta").
Einstein's light quanta were nearly universally rejected by all physicists, including Max Planck and Niels Bohr.
This idea only became universally accepted in 1919, with Robert Millikan's detailed experiments on the photoelectric effect, and with the measurement of Compton scattering.
Einstein concluded that each wave of frequency "f" is associated with a collection of photons with energy "hf" each, where "h" is Planck's constant.
He does not say much more, because he is not sure how the particles are related to the wave.
But he does suggest that this idea would explain certain experimental results, notably the photoelectric effect.
In 1907, Einstein proposed a model of matter where each atom in a lattice structure is an independent harmonic oscillator.
In the Einstein model, each atom oscillates independently—a series of equally spaced quantized states for each oscillator.
Einstein was aware that getting the frequency of the actual oscillations would be difficult, but he nevertheless proposed this theory because it was a particularly clear demonstration that quantum mechanics could solve the specific heat problem in classical mechanics.
Peter Debye refined this model.
Throughout the 1910s, quantum mechanics expanded in scope to cover many different systems.
After Ernest Rutherford discovered the nucleus and proposed that electrons orbit like planets, Niels Bohr was able to show that the same quantum mechanical postulates introduced by Planck and developed by Einstein would explain the discrete motion of electrons in atoms, and the periodic table of the elements.
Einstein contributed to these developments by linking them with the 1898 arguments Wilhelm Wien had made.
Wien had shown that the hypothesis of adiabatic invariance of a thermal equilibrium state allows all the blackbody curves at different temperature to be derived from one another by a simple shifting process.
Einstein noted in 1911 that the same adiabatic principle shows that the quantity which is quantized in any mechanical motion must be an adiabatic invariant.
Arnold Sommerfeld identified this adiabatic invariant as the action variable of classical mechanics.
In 1924, Einstein received a description of a statistical model from Indian physicist Satyendra Nath Bose, based on a counting method that assumed that light could be understood as a gas of indistinguishable particles.
Einstein noted that Bose's statistics applied to some atoms as well as to the proposed light particles, and submitted his translation of Bose's paper to the "Zeitschrift für Physik".
Einstein also published his own articles describing the model and its implications, among them the Bose–Einstein condensate phenomenon that some particulates should appear at very low temperatures.
It was not until 1995 that the first such condensate was produced experimentally by Eric Allin Cornell and Carl Wieman using ultra-cooling equipment built at the NIST–JILA laboratory at the University of Colorado at Boulder.
Bose–Einstein statistics are now used to describe the behaviors of any assembly of bosons.
Einstein's sketches for this project may be seen in the Einstein Archive in the library of the Leiden University.
Although the patent office promoted Einstein to Technical Examiner Second Class in 1906, he had not given up on academia.
In 1908, he became a "Privatdozent" at the University of Bern.
In ""Über die Entwicklung unserer Anschauungen über das Wesen und die Konstitution der Strahlung"" (""), on the quantization of light, and in an earlier 1909 paper, Einstein showed that Max Planck's energy quanta must have well-defined momenta and act in some respects as independent, point-like particles.
This paper introduced the "photon" concept (although the name "photon" was introduced later by Gilbert N. Lewis in 1926) and inspired the notion of wave–particle duality in quantum mechanics.
Einstein saw this wave–particle duality in radiation as concrete evidence for his conviction that physics needed a new, unified foundation.
In a series of works completed from 1911 to 1913, Planck reformulated his 1900 quantum theory and introduced the idea of zero-point energy in his "second quantum theory".
Soon, this idea attracted the attention of Einstein and his assistant Otto Stern.
Assuming the energy of rotating diatomic molecules contains zero-point energy, they then compared the theoretical specific heat of hydrogen gas with the experimental data.
The numbers matched nicely.
However, after publishing the findings, they promptly withdrew their support, because they no longer had confidence in the correctness of the idea of zero-point energy.
In 1917, at the height of his work on relativity, Einstein published an article in "Physikalische Zeitschrift" that proposed the possibility of stimulated emission, the physical process that makes possible the maser and the laser.
This article showed that the statistics of absorption and emission of light would only be consistent with Planck's distribution law if the emission of light into a mode with n photons would be enhanced statistically compared to the emission of light into an empty mode.
This paper was enormously influential in the later development of quantum mechanics, because it was the first paper to show that the statistics of atomic transitions had simple laws.
Einstein discovered Louis de Broglie's work and supported his ideas, which were received skeptically at first.
In another major paper from this era, Einstein gave a wave equation for de Broglie waves, which Einstein suggested was the Hamilton–Jacobi equation of mechanics.
This paper would inspire Schrödinger's work of 1926.
Einstein was displeased with modern quantum mechanics as it had evolved after 1925.
Contrary to popular belief, his doubts were not due to a conviction that God "is not playing at dice."
Indeed, it was Einstein himself, in his 1917 paper that proposed the possibility of stimulated emission, who first proposed the fundamental role of chance in explaining quantum processes.
Rather, he objected to what quantum mechanics implies about the nature of reality.
Einstein believed that a physical reality exists independent of our ability to observe it.
In contrast, Bohr and his followers maintained that all we can know are the results of measurements and observations, and that it makes no sense to speculate about an ultimate reality that exists beyond our perceptions.
The Bohr–Einstein debates were a series of public disputes about quantum mechanics between Einstein and Niels Bohr, who were two of its founders.
Their debates are remembered because of their importance to the philosophy of science.
Their debates would influence later interpretations of quantum mechanics.
In 1935, Einstein returned to the question of quantum mechanics in the "EPR paper".
In a thought experiment, he considered two particles which had interacted such that their properties were strongly correlated.
No matter how far the two particles were separated, a precise position measurement on one particle would result in equally precise knowledge of the position of the other particle; likewise a precise momentum measurement of one particle would result in equally precise knowledge of the momentum of the other particle, without needing to disturb the other particle in any way.
Given Einstein's concept of local realism, there were two possibilities: (1) either the other particle had these properties already determined, or (2) the process of measuring the first particle instantaneously affected the reality of the position and momentum of the second particle.
Einstein rejected this second possibility (popularly called "spooky action at a distance").
This principle distilled the essence of Einstein's objection to quantum mechanics.
As a physical principle, it was shown to be incorrect when the Aspect experiment of 1982 confirmed Bell's theorem, which J. S. Bell had delineated in 1964.
The results of these and subsequent experiments demonstrate that quantum physics cannot be represented by any version of the classical picture of physics.
Although Einstein was wrong, his clear prediction of the unusual properties of "entangled quantum states" has resulted in the EPR paper becoming among the top ten papers published in Physical Review.
It is considered a centerpiece of the development of quantum information theory.
Following his research on general relativity, Einstein entered into a series of attempts to generalize his geometric theory of gravitation to include electromagnetism as another aspect of a single entity.
In 1950, he described his "unified field theory" in a "Scientific American" article titled "On the Generalized Theory of Gravitation".
Although he continued to be lauded for his work, Einstein became increasingly isolated in his research, and his efforts were ultimately unsuccessful.
In his pursuit of a unification of the fundamental forces, Einstein ignored some mainstream developments in physics, most notably the strong and weak nuclear forces, which were not well understood until many years after his death.
Mainstream physics, in turn, largely ignored Einstein's approaches to unification.
Einstein's dream of unifying other laws of physics with gravity motivates modern quests for a theory of everything and in particular string theory, where geometrical fields emerge in a unified quantum-mechanical setting.
Einstein conducted other investigations that were unsuccessful and abandoned.
These pertain to force, superconductivity, and other research.
In addition to longtime collaborators Leopold Infeld, Nathan Rosen, Peter Bergmann and others, Einstein also had some one-shot collaborations with various scientists.
Einstein and De Haas demonstrated that magnetization is due to the motion of electrons, nowadays known to be the spin.
In order to show this, they reversed the magnetization in an iron bar suspended on a torsion pendulum.
They confirmed that this leads the bar to rotate, because the electron's angular momentum changes as the magnetization changes.
This experiment needed to be sensitive, because the angular momentum associated with electrons is small, but it definitively established that electron motion of some kind is responsible for magnetization.
Einstein suggested to Erwin Schrödinger that he might be able to reproduce the statistics of a Bose–Einstein gas by considering a box.
Then to each possible quantum motion of a particle in a box associate an independent harmonic oscillator.
Quantizing these oscillators, each level will have an integer occupation number, which will be the number of particles in it.
This formulation is a form of second quantization, but it predates modern quantum mechanics.
Erwin Schrödinger applied this to derive the thermodynamic properties of a semiclassical ideal gas.
Schrödinger urged Einstein to add his name as co-author, although Einstein declined the invitation.
In 1926, Einstein and his former student Leó Szilárd co-invented (and in 1930, patented) the Einstein refrigerator.
This absorption refrigerator was then revolutionary for having no moving parts and using only heat as an input.
On 11 November 1930, was awarded to Einstein and Leó Szilárd for the refrigerator.
Their invention was not immediately put into commercial production, and the most promising of their patents were acquired by the Swedish company Electrolux.
While traveling, Einstein wrote daily to his wife Elsa and adopted stepdaughters Margot and Ilse.
The letters were included in the papers bequeathed to The Hebrew University.
Margot Einstein permitted the personal letters to be made available to the public, but requested that it not be done until twenty years after her death (she died in 1986).
Einstein had expressed his interest in the plumbing profession and was made an honorary member of the Plumbers and Steamfitters Union.
Barbara Wolff, of The Hebrew University's Albert Einstein Archives, told the BBC that there are about 3,500 pages of private correspondence written between 1912 and 1955.
Corbis, successor to The Roger Richman Agency, licenses the use of his name and associated imagery, as agent for the university.
In the period before World War II, "The New Yorker" published a vignette in their "The Talk of the Town" feature saying that Einstein was so well known in America that he would be stopped on the street by people wanting him to explain "that theory".
He finally figured out a way to handle the incessant inquiries.
He told his inquirers "Pardon me, sorry!
Always I am mistaken for Professor Einstein."
Einstein has been the subject of or inspiration for many novels, films, plays, and works of music.
He is a favorite model for depictions of mad scientists and absent-minded professors; his expressive face and distinctive hairstyle have been widely copied and exaggerated.
"Time" magazine's Frederic Golden wrote that Einstein was "a cartoonist's dream come true".
Many popular quotations are often misattributed to him.
Einstein received numerous awards and honors and in 1922 he was awarded the 1921 Nobel Prize in Physics "for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect".
None of the nominations in 1921 met the criteria set by Alfred Nobel, so the 1921 prize was carried forward and awarded to Einstein in 1922.
</doc>
<doc id="737" url="https://en.wikipedia.org/wiki?curid=737" title="Afghanistan">
Afghanistan

Afghanistan (; Pashto/Dari: , Pashto: "Afġānistān" , Dari: "Afġānestān" ), officially the Islamic Republic of Afghanistan, is a landlocked country located within south-central Asia.
Afghanistan is bordered by Pakistan in the south and east; Iran in the west; Turkmenistan, Uzbekistan, and Tajikistan in the north; and in the far northeast, China.
Its territory covers and much of it is covered by the Hindu Kush mountain range, which experience very cold winters.
The north consists of fertile plains, whilst the south-west consists of deserts where temperatures can get very hot in summers.
Kabul serves as the capital and its largest city.
Human habitation in Afghanistan dates back to the Middle Paleolithic Era, and the country's strategic location along the Silk Road connected it to the cultures of the Middle East and other parts of Asia.
The land has historically been home to various peoples and has witnessed numerous military campaigns, including those by Alexander the Great, Mauryas, Muslim Arabs, Mongols, British, Soviet, and since 2001 by the United States with NATO-allied countries.
It has been called "unconquerable" and nicknamed the "graveyard of empires".
The land also served as the source from which the Kushans, Hephthalites, Samanids, Saffarids, Ghaznavids, Ghorids, Khaljis, Mughals, Hotaks, Durranis, and others have risen to form major empires.
The political history of the modern state of Afghanistan began with the Hotak and Durrani dynasties in the 18th century.
In the late 19th century, Afghanistan became a buffer state in the "Great Game" between British India and the Russian Empire.
Its border with British India, the Durand Line, was formed in 1893 but it is not recognized by the Afghan government and it has led to strained relations with Pakistan since the latter's independence in 1947.
Following the Third Anglo-Afghan War in 1919 the country was free of foreign influence, eventually becoming a monarchy under King Amanullah, until almost 50 years later when Zahir Shah was overthrown and a republic was established.
In 1978, after a second coup Afghanistan first became a socialist state and then a Soviet Union protectorate.
This evoked the Soviet–Afghan War in the 1980s against mujahideen rebels.
By 1996 most of Afghanistan was captured by the Islamic fundamentalist group the Taliban, who ruled most of the country as a totalitarian regime for over five years.
The Taliban were forcibly removed by the NATO-led coalition, and a new democratically-elected government political structure was formed.
Afghanistan is a unitary presidential Islamic republic with a population of 31 million, mostly composed of ethnic Pashtuns, Tajiks, Hazaras and Uzbeks.
It is a member of the United Nations, the Organisation of Islamic Cooperation, the Group of 77, the Economic Cooperation Organization, and the Non-Aligned Movement.
Afghanistan's economy is the world's 108th largest, with a GDP of $64.08 billion; the country fares much worse in terms of per-capita GDP (PPP), ranking 167th out of 186 countries in a 2016 report from the International Monetary Fund.
The name "Afghānistān" () is believed to be as old as the ethnonym "Afghan", which is documented in the 10th-century geography book "Hudud ul-'alam".
The root name "Afghan" was used historically in reference to a member of the ethnic Pashtuns, and the suffix "-stan" means "place of" in Persian.
Therefore, Afghanistan translates to "land of the Afghans" or, more specifically in a historical sense, to "land of the Pashtuns".
However, the modern Constitution of Afghanistan states that "[t]he word Afghan shall apply to every citizen of Afghanistan."
Excavations of prehistoric sites by Louis Dupree and others suggest that humans were living in what is now Afghanistan at least 50,000 years ago, and that farming communities in the area were among the earliest in the world.
An important site of early historical activities, many believe that Afghanistan compares to Egypt in terms of the historical value of its archaeological sites.
The country sits at a unique nexus point where numerous civilizations have interacted and often fought.
It has been home to various peoples through the ages, among them the ancient Iranian peoples who established the dominant role of Indo-Iranian languages in the region.
At multiple points, the land has been incorporated within large regional empires, among them the Achaemenid Empire, the Macedonian Empire, the Indian Maurya Empire, and the Islamic Empire.
Many empires and kingdoms have also risen to power in Afghanistan, such as the Greco-Bactrians, Kushans, Hephthalites, Kabul Shahis, Saffarids, Samanids, Ghaznavids, Ghurids, Khaljis, Kartids, Timurids, Mughals, and finally the Hotak and Durrani dynasties that marked the political origins of the modern state.
Archaeological exploration done in the 20th century suggests that the geographical area of Afghanistan has been closely connected by culture and trade with its neighbors to the east, west, and north.
Artifacts typical of the Paleolithic, Mesolithic, Neolithic, Bronze, and Iron ages have been found in Afghanistan.
Urban civilization is believed to have begun as early as 3000 BCE, and the early city of Mundigak (near Kandahar in the south of the country) may have been a colony of the nearby Indus Valley Civilization.
More recent findings established that the Indus Valley Civilisation stretched up towards modern-day Afghanistan, making the ancient civilisation today part of Pakistan, Afghanistan and India.
In more detail, it extended from what today is northwest Pakistan to northwest India and northeast Afghanistan.
An Indus Valley site has been found on the Oxus River at Shortugai in northern Afghanistan.
There are several smaller IVC colonies to be found in Afghanistan as well.
After 2000 BCE, successive waves of semi-nomadic people from Central Asia began moving south into Afghanistan; among them were many Indo-European-speaking Indo-Iranians.
These tribes later migrated further into South Asia, Western Asia, and toward Europe via the area north of the Caspian Sea.
The region at the time was referred to as Ariana.
The religion Zoroastrianism is believed by some to have originated in what is now Afghanistan between 1800 and 800 BCE, as its founder Zoroaster is thought to have lived and died in Balkh.
Ancient Eastern Iranian languages may have been spoken in the region around the time of the rise of Zoroastrianism.
By the middle of the 6th century BCE, the Achaemenids overthrew the Medes and incorporated Arachosia, Aria, and Bactria within its eastern boundaries.
An inscription on the tombstone of Darius I of Persia mentions the Kabul Valley in a list of the 29 countries that he had conquered.
Alexander the Great and his Macedonian forces arrived to Afghanistan in 330 BCE after defeating Darius III of Persia a year earlier in the Battle of Gaugamela.
Following Alexander's brief occupation, the successor state of the Seleucid Empire controlled the region until 305 BCE, when they gave much of it to the Maurya Empire as part of an alliance treaty.
The Mauryans controlled the area south of the Hindu Kush until they were overthrown in about 185 BCE.
Their decline began 60 years after Ashoka's rule ended, leading to the Hellenistic reconquest by the Greco-Bactrians.
Much of it soon broke away from them and became part of the Indo-Greek Kingdom.
They were defeated and expelled by the Indo-Scythians in the late 2nd century BCE.
During the first century BCE, the Parthian Empire subjugated the region, but lost it to their Indo-Parthian vassals.
In the mid-to-late first century CE the vast Kushan Empire, centered in Afghanistan, became great patrons of Buddhist culture, making Buddhism flourish throughout the region.
The Kushans were overthrown by the Sassanids in the 3rd century CE, though the Indo-Sassanids continued to rule at least parts of the region.
They were followed by the Kidarite who, in turn, were replaced by the Hephthalites.
By the 6th century CE, the successors to the Kushans and Hepthalites established a small dynasty called Kabul Shahi.
Much of the northeastern and southern areas of the country remained dominated by Buddhist culture.
Arab Muslims brought Islam to Herat and Zaranj in 642 CE and began spreading eastward; some of the native inhabitants they encountered accepted it while others revolted.
The land was collectively recognized by the Arabs as al-Hind due to its cultural connection with Greater India.
Before Islam was introduced, people of the region were mostly Buddhists and Zoroastrians, but there were also Surya and Nana worshipers, Jews, and others.
The Zunbils and Kabul Shahi were first conquered in 870 CE by the Saffarid Muslims of Zaranj.
Later, the Samanids extended their Islamic influence south of the Hindu Kush.
It is reported that Muslims and non-Muslims still lived side by side in Kabul before the Ghaznavids rose to power in the 10th century.
By the 11th century, Mahmud of Ghazni defeated the remaining Hindu rulers and effectively Islamized the wider region, with the exception of Kafiristan.
Afghanistan became one of the main centers in the Muslim world during this Islamic Golden Age.
The Ghaznavid dynasty was overthrown by the Ghurids, who expanded and advanced the already powerful Islamic empire.
In 1219 AD, Genghis Khan and his Mongol army overran the region.
His troops are said to have annihilated the Khorasanian cities of Herat and Balkh as well as Bamyan.
The destruction caused by the Mongols forced many locals to return to an agrarian rural society.
Mongol rule continued with the Ilkhanate in the northwest while the Khalji dynasty administered the Afghan tribal areas south of the Hindu Kush until the invasion of Timur, who established the Timurid Empire in 1370.
In the early 16th century, Babur arrived from Fergana and captured Kabul from the Arghun dynasty.
In 1526, he invaded Delhi in India to replace the Lodi dynasty with the Mughal Empire.
Between the 16th and 18th century, the Khanate of Bukhara, Safavids, and Mughals ruled parts of the territory.
Before the 19th century, the northwestern area of Afghanistan was referred to by the regional name Khorasan.
Two of the four capitals of Khorasan (Herat and Balkh) are now located in Afghanistan, while the regions of Kandahar, Zabulistan, Ghazni, Kabulistan, and Afghanistan formed the frontier between Khorasan and Hindustan.
In 1709, Mirwais Hotak, a local Ghilzai tribal leader, successfully rebelled against the Safavids.
He defeated Gurgin Khan and made Afghanistan independent.
Mirwais died of a natural cause in 1715 and was succeeded by his brother Abdul Aziz, who was soon killed by Mirwais' son Mahmud for treason.
Mahmud led the Afghan army in 1722 to the Persian capital of Isfahan, captured the city after the Battle of Gulnabad and proclaimed himself King of Persia.
The Afghan dynasty was ousted from Persia by Nader Shah after the 1729 Battle of Damghan.
In 1738, Nader Shah and his forces captured Kandahar, the last Hotak stronghold, from Shah Hussain Hotak, at which point the incarcerated 16-year-old Ahmad Shah Durrani was freed and made the commander of an Afghan regiment.
Soon after the Persian and Afghan forces invaded India.
By 1747, the Afghans chose Durrani as their head of state.
Durrani and his Afghan army conquered much of present-day Afghanistan, Pakistan, the Khorasan and Kohistan provinces of Iran, and Delhi in India.
He defeated the Indian Maratha Empire, and one of his biggest victories was the 1761 Battle of Panipat.
In October 1772, Durrani died of a natural cause and was buried at a site now adjacent to the Shrine of the Cloak in Kandahar.
He was succeeded by his son, Timur Shah, who transferred the capital of Afghanistan from Kandahar to Kabul in 1776.
After Timur's death in 1793, the Durrani throne passed down to his son Zaman Shah, followed by Mahmud Shah, Shuja Shah and others.
The Afghan Empire was under threat in the early 19th century by the Persians in the west and the Sikh Empire in the east.
Fateh Khan, leader of the Barakzai tribe, had installed 21 of his brothers in positions of power throughout the empire.
After his death, they rebelled and divided up the provinces of the empire between themselves.
During this turbulent period, Afghanistan had many temporary rulers until Dost Mohammad Khan declared himself emir in 1826.
The Punjab region was lost to Ranjit Singh, who invaded Khyber Pakhtunkhwa and in 1834 captured the city of Peshawar.
In 1837, during the Battle of Jamrud near the Khyber Pass, Akbar Khan and the Afghan army failed to capture the Jamrud fort from the Sikh Khalsa Army, but killed Sikh Commander Hari Singh Nalwa, thus ending the Afghan-Sikh Wars.
By this time the British were advancing from the east and the first major conflict during "The Great Game" was initiated.
In 1838, the British marched into Afghanistan and arrested Dost Mohammad, sent him into exile in India and replaced him with the previous ruler, Shah Shuja.
Following an uprising, the 1842 retreat from Kabul of British-Indian forces and the annihilation of Elphinstone's army, and the Battle of Kabul that led to its recapture, the British placed Dost Mohammad Khan back into power and withdrew their military forces from Afghanistan.
In 1878, the Second Anglo-Afghan War was fought over perceived Russian influence, Abdur Rahman Khan replaced Ayub Khan, and Britain gained control of Afghanistan's foreign relations as part of the Treaty of Gandamak of 1879.
In 1893, Mortimer Durand made Amir Abdur Rahman Khan sign a controversial agreement in which the ethnic Pashtun and Baloch territories were divided by the Durand Line.
This was a standard divide and rule policy of the British and would lead to strained relations, especially with the later new state of Pakistan.
Shia-dominated Hazarajat and pagan Kafiristan remained politically independent until being conquered by Abdur Rahman Khan in 1891-1896.
After the Third Anglo-Afghan War and the signing of the Treaty of Rawalpindi on 19 August 1919, King Amanullah Khan declared Afghanistan a sovereign and fully independent state.
He moved to end his country's traditional isolation by establishing diplomatic relations with the international community and, following a 1927–28 tour of Europe and Turkey, introduced several reforms intended to modernize his nation.
A key force behind these reforms was Mahmud Tarzi, an ardent supporter of the education of women.
He fought for Article 68 of Afghanistan's 1923 constitution, which made elementary education compulsory.
The institution of slavery was abolished in 1923.
Some of the reforms that were actually put in place, such as the abolition of the traditional burqa for women and the opening of a number of co-educational schools, quickly alienated many tribal and religious leaders.
Faced with overwhelming armed opposition, Amanullah Khan was forced to abdicate in January 1929 after Kabul fell to rebel forces led by Habibullah Kalakani.
Prince Mohammed Nadir Shah, Amanullah's cousin, in turn defeated and killed Kalakani in November 1929, and was declared King Nadir Shah.
He abandoned the reforms of Amanullah Khan in favor of a more gradual approach to modernisation but was assassinated in 1933 by Abdul Khaliq, a fifteen-year-old Hazara student.
Mohammed Zahir Shah, Nadir Shah's 19-year-old son, succeeded to the throne and reigned from 1933 to 1973.
Until 1946, Zahir Shah ruled with the assistance of his uncle, who held the post of Prime Minister and continued the policies of Nadir Shah.
Another of Zahir Shah's uncles, Shah Mahmud Khan, became Prime Minister in 1946 and began an experiment allowing greater political freedom, but reversed the policy when it went further than he expected.
He was replaced in 1953 by Mohammed Daoud Khan, the king's cousin and brother-in-law.
Daoud Khan sought a closer relationship with the Soviet Union and a more distant one towards Pakistan.
The King built close relationships with the Axis powers in the 1930s - but Afghanistan remained neutral and was neither a participant in World War II nor aligned with either power bloc in the Cold War thereafter.
However, it was a beneficiary of the latter rivalry as both the Soviet Union and the United States vied for influence by building Afghanistan's main highways, airports, and other vital infrastructure.
On per capita basis, Afghanistan received more Soviet development aid than any other country.
Afghanistan had therefore good relations with both Cold War enemies.
In 1973, while King Zahir Shah was on an official overseas visit, Daoud Khan launched a bloodless coup and became the first President of Afghanistan, abolishing the monarchy.
In the meantime, Zulfikar Ali Bhutto got neighboring Pakistan involved in Afghanistan.
Some experts suggest that Bhutto paved the way for the April 1978 Saur Revolution.
In April 1978, the People's Democratic Party of Afghanistan (PDPA) seized power in the Saur Revolution, a coup d'état against then-President Mohammed Daoud Khan.
The PDPA declared the establishment of the Democratic Republic of Afghanistan, with its first President named as Nur Muhammad Taraki.
Opposition to PDPA reforms, such as its land redistribution policy and modernization of (traditional Islamic) civil and marriage laws, led to unrest which aggravated to rebellion and revolt around October 1978, first in eastern Afghanistan (see Initiation of the insurgency in Afghanistan 1978).
That uprising quickly expanded into a civil war waged by guerrilla mujahideen against regime forces countrywide.
The Pakistani government provided these rebels with covert training centers, while the Soviet Union sent thousands of military advisers to support the PDPA regime.
As early as mid-1979 (see "CIA activities in Afghanistan"), the United States were supporting Afghan "mujahideen" and foreign "Afghan Arab" fighters through Pakistan's ISI.
Meanwhile, increasing friction between the competing factions of the PDPA — the dominant Khalq and the more moderate Parcham — resulted (in July–August 1979) in the dismissal of Parchami cabinet members and the arrest of Parchami military officers under the pretext of a Parchami coup.
In September 1979, President Taraki was assassinated in a coup within the PDPA orchestrated by fellow Khalq member Hafizullah Amin, who assumed the presidency.
The Soviet Union was displeased with Amin's government, and decided to intervene and invade the country on 27 December 1979, killing Amin that same day.
A Soviet-organized regime, led by Parcham's Babrak Karmal but inclusive of both factions (Parcham and Khalq), filled the vacuum.
Soviet troops in more substantial numbers were deployed to stabilize Afghanistan under Karmal, and as a result the Soviets were now directly involved in what had been a domestic war in Afghanistan (of mujahideen against PDPA government), which war from December 1979 until 1989 is therefore also known as the Soviet–Afghan War.
The United States, supporting the Afghan "mujahideen" and foreign "Afghan Arab" fighters since mid-1979 through Pakistan's ISI, and Saudi Arabia, from now on delivered for billions in cash and weapons, including two thousand FIM-92 Stinger surface-to-air missiles, to Pakistan as support for the anti-Soviet mujahideen.
The PDPA prohibited usury, declared equality of the sexes, and introduced women to political life.
During this war from 1979 until 1989, Soviet forces, their Afghan proxies and rebels killed between 562,000 and 2 million Afghans, and displaced about 6 million people who subsequently fled Afghanistan, mainly to Pakistan and Iran.
Many countryside villages were bombed and some cities such as Herat and Kandahar were also damaged from air bombardment.
Pakistan's North-West Frontier Province functioned as an organisational and networking base for the anti-Soviet Afghan resistance, with the province's influential Deobandi ulama playing a major supporting role in promoting the 'jihad'.
Meanwhile, the central Afghan region of Hazarajat, which in this period was free of Soviet or PDPA government presence, experienced an internal civil war from 1980 to 1984.
<br>Faced with mounting international pressure and numerous casualties, the Soviets withdrew from Afghanistan in 1989, but continued to support Afghan President Mohammad Najibullah until 1992.
Mujahideen (Islamic resistance) forces in October 1978 (see above) had started a guerrilla or civil war against the PDPA's government of Afghanistan.
After the Soviet invasion, December 1979, replacing one PDPA President for another PDPA President, the mujahideen proclaimed to be battling the hostile PDPA "puppet regime".
In 1987, Mohammad Najibullah had become Afghan President, and after the Soviet withdrawal in 1989 he was still sponsored by the Soviet Union, and fought by the mujahideen.
President Najibullah therefore tried to build support for his government by moving away from socialism to pan-Afghan nationalism, abolishing the one-party state, portraying his government as Islamic, and in 1990 removing all signs of communism.
Nevertheless, Najibullah did not win any significant support.
In March 1989, two mujahideen groups launched an attack on Jalalabad, instigated by the Pakistani Inter-Services Intelligence (ISI) who wanted to see a mujahideen Islamic government established in Afghanistan, but the attack failed after three months.
With the dissolution of the Soviet Union in December 1991 and the ending of Russian support, President Najibullah was left without foreign aid.
In March 1991, mujahideen forces attacked and conquered the city of Khost.
In March 1992, President Najibullah agreed to step aside and make way for a mujahideen coalition government.
Mujahideen leaders came together in Peshawar, Pakistan, to negotiate such a government, but mujahideen Hezbi Islami's leader Gulbuddin Hekmatyar, presumably supported by ISI, refused to meet other leaders.
On 16 April 1992, four Afghani government Generals ousted President Najibullah.
Little later, Hezbi Islami invaded Kabul.
This ignited war in Kabul on 25 April with rivalling groups Jamiat and Junbish in which soon two more mujahideen groups mingled; all groups except Jamiat were supported by an Islamic foreign government (Saudi Arabia, Iran, Uzbekistan) or intelligence agency (Pakistan's ISI).
In 1992–95, Kabul was heavily bombarded and considerably destroyed, by Hezbi Islami, Jamiat, Junbish, Hizb-i-Wahdat, and Ittihad; in that period, half a million Kabuli fled to Pakistan.
In January–June 1994, 25,000 people died in Kabul due to fighting between an alliance of Dostum's (Junbish) with Hekmatyar's (Hezbi Islami) against Massoud's (Jamiat) forces.
Also other cities turned into battleground.
In 1993–95, (sub-)commanders of Jamiat, Junbish, Hezbi Islami and Hizb-i-Wahdat descended to rape, murder and extortion.
The Taliban emerged in September 1994 as a movement and militia of Pashtun students ("talib") from Islamic madrassas (schools) in Pakistan,
pledged to rid Afghanistan of 'warlords and criminals',
and soon had military support from Pakistan.
In November 1994 the Taliban took control of Kandahar city after forcing local Pashtun
leaders who had tolerated complete lawlessness.
The Taliban in early 1995 attempted to capture Kabul but were repelled by forces under Massoud.
Taliban, having grown stronger, in September 1996 attacked and occupied Kabul after Massoud and Hekmatyar had withdrawn their troops from Kabul.
In late September 1996, the Taliban, in control of Kabul and most of Afghanistan, proclaimed their Islamic Emirate of Afghanistan.
They imposed a strict form of Sharia, similar to that found in Saudi Arabia.
According to Physicians for Human Rights (PHR) in 1998, "no other regime in the world has methodically and violently forced half of its population into virtual house arrest, prohibiting them on pain of physical punishment from showing their faces, seeking medical care without a male escort, or attending school" The brutality of the Taliban's totalitarian regime was comparable to those of Stalin's Russia or the Khmer Rouge rule of Cambodia.
After the fall of Kabul to the Taliban, Massoud and Dostum formed the Northern Alliance.
The Taliban defeated Dostum's forces during the Battles of Mazar-i-Sharif (1997–98).
Pakistan's Chief of Army Staff, Pervez Musharraf, began sending thousands of Pakistanis to help the Taliban defeat the Northern Alliance.<ref name="Ahmed Rashid/The Telegraph"></ref> From 1996 to 2001, the al-Qaeda network of Osama bin Laden and Ayman al-Zawahiri was also operating inside Afghanistan.
This and the fact that around one million Afghans were internally displaced made the United States worry.
From 1990 to September 2001, around 400,000 Afghans died in the internal mini-wars.
On 9 September 2001, Massoud was assassinated by two Arab suicide attackers in Panjshir province of Afghanistan.
Two days later, the September 11 attacks were carried out in the United States.
The US government suspected Osama bin Laden as the perpetrator of the attacks, and demanded that the Taliban hand him over.
The Taliban offered to hand over Bin Laden to a third country for trial, but not directly to the US.
Washington refused that offer.
Instead, the US launched the October 2001 Operation Enduring Freedom.
The majority of Afghans supported the American invasion of their country.
During the initial invasion, US and UK forces bombed al-Qaeda training camps.
The United States began working with the Northern Alliance to remove the Taliban from power.
In December 2001, after the Taliban government was overthrown in the Battle of Tora Bora, the Afghan Interim Administration under Hamid Karzai was formed, in which process the Taliban were typecast as 'the bad guys' and left out.
The International Security Assistance Force (ISAF) was established by the UN Security Council to help assist the Karzai administration and provide basic security.
Taliban forces meanwhile began regrouping inside Pakistan, while more coalition troops entered Afghanistan and began rebuilding the war-torn country.
Shortly after their fall from power, the Taliban began an insurgency to regain control of Afghanistan.
Over the next decade, ISAF and Afghan troops led many offensives against the Taliban, but failed to fully defeat them.
Afghanistan remains one of the poorest countries in the world due to a lack of foreign investment, government corruption, and the Taliban insurgency.
Meanwhile, the Afghan government was able to build some democratic structures, and the country changed its name to the Islamic Republic of Afghanistan.
Attempts were made, often with the support of foreign donor countries, to improve the country's economy, healthcare, education, transport, and agriculture.
ISAF forces also began to train the Afghan National Security Forces.
In the decade following 2002, over five million Afghans were repatriated, including some who were deported from Western countries.
By 2009, a Taliban-led shadow government began to form in parts of the country.
In 2010, President Karzai attempted to hold peace negotiations with the Taliban leaders, but the rebel group refused to attend until mid-2015 when the Taliban supreme leader finally decided to back the peace talks.
After the May 2011 death of Osama bin Laden in Pakistan, many prominent Afghan figures were assassinated.
Afghanistan–Pakistan border skirmishes intensified and many large scale attacks by the Pakistan-based Haqqani Network also took place across Afghanistan.
The United States blamed rogue elements within the Pakistani government for the increased attacks.
In September 2014 Ashraf Ghani became President after the 2014 presidential election where for the first time in Afghanistan's history power was democratically transferred.
On 28 December 2014, NATO formally ended ISAF combat operations in Afghanistan and officially transferred full security responsibility to the Afghan government and the NATO-led Operation Resolute Support was formed the same day as a successor to ISAF.
However, thousands of NATO troops have remained in the country to train and advise Afghan government forces and continue their fight against the Taliban, which remains by far the largest single group fighting against the Afghan government and foreign troops.
Hundreds of thousands of insurgents, Afghan civilians and government forces have been made casualty by the war.
A landlocked mountainous country with plains in the north and southwest, Afghanistan is located within South Asia and Central Asia.
It is part of the US-coined Greater Middle East Muslim world, which lies between latitudes and , and longitudes and .
The country's highest point is Noshaq, at above sea level.
It has a continental climate with harsh winters in the central highlands, the glaciated northeast (around Nuristan), and the Wakhan Corridor, where the average temperature in January is below , and hot summers in the low-lying areas of the Sistan Basin of the southwest, the Jalalabad basin in the east, and the Turkestan plains along the Amu River in the north, where temperatures average over in July.
Despite having numerous rivers and reservoirs, large parts of the country are dry.
The endorheic Sistan Basin is one of the driest regions in the world.
Aside from the usual rainfall, Afghanistan receives snow during the winter in the Hindu Kush and Pamir Mountains, and the melting snow in the spring season enters the rivers, lakes, and streams.
However, two-thirds of the country's water flows into the neighboring countries of Iran, Pakistan, and Turkmenistan.
The state needs more than to rehabilitate its irrigation systems so that the water is properly managed.
The northeastern Hindu Kush mountain range, in and around the Badakhshan Province of Afghanistan, is in a geologically active area where earthquakes may occur almost every year.
They can be deadly and destructive sometimes, causing landslides in some parts or avalanches during the winter.
The last strong earthquakes were in 1998, which killed about 6,000 people in Badakhshan near Tajikistan.
This was followed by the 2002 Hindu Kush earthquakes in which over 150 people were killed and over 1,000 injured.
A 2010 earthquake left 11 Afghans dead, over 70 injured, and more than 2,000 houses destroyed.
The country's natural resources include: coal, copper, iron ore, lithium, uranium, rare earth elements, chromite, gold, zinc, talc, barite, sulfur, lead, marble, precious and semi-precious stones, natural gas, and petroleum, among other things.
In 2010, US and Afghan government officials estimated that untapped mineral deposits located in 2007 by the US Geological Survey are worth at least .
At over , Afghanistan is the world's 41st largest country, slightly bigger than France and smaller than Burma, about the size of Texas in the United States.
It borders Pakistan in the south and east; Iran in the west; Turkmenistan, Uzbekistan, and Tajikistan in the north; and China in the far east.
The population of Afghanistan was estimated at 29.2 million in 2017.
Of this, 15 million are males and 14.2 million females.
About 22% of them are urbanite and the remaining 78% live in rural areas.
An additional 3 million or so Afghans are temporarily housed in neighboring Pakistan and Iran, most of whom were born and raised in those two countries.
This makes the total Afghan population at around 33,332,025, and its current growth rate is 2.34%.
This population is expected to reach 82 million by 2050 if current population trends continue.
The only city with over a million residents is its capital, Kabul.
Due to a lack of census there is no clear indication of what the largest cities in the country are, with various national and international estimates and without always acknowledging the differentiation of city municipalities and urban areas that go beyond city limits.
After Kabul the other five large cities are Kandahar, Herat, Mazar-i-Sharif, Kunduz and Jalalabad.
Other major cities include Lashkar Gah, Taloqan, Khost, Sheberghan, and Ghazni.
Afghanistan's population is divided into several ethnolinguistic groups, which are listed in the chart below:

Dari and Pashto are the official languages of Afghanistan; bilingualism is very common.
Dari, which is a variety of and mutually intelligible with Persian (and very often called 'Farsi' by some Afghans like in Iran) functions as the lingua franca in Kabul as well as in much of the northern and northwestern parts of the country.
Pashto is the native tongue of the Pashtuns, although many of them are also fluent in Dari while some non-Pashtuns are fluent in Pashto.
There are a number of smaller regional languages, they include Uzbek, Turkmen, Balochi, Pashayi, and Nuristani.
Uzbek, Turkmen, Pashayi, Nuristani, Balochi and Pamiri declared third official in areas where the majority speaks them.
A number of Afghans are also fluent in Urdu, English, and other foreign languages.
An estimated 99.7% of the Afghan population is Muslim.
There has never been a nationwide census of any kind in Afghanistan, so the proportions of different religious groups are estimates by different organisations.
Thousands of Afghan Sikhs and Hindus are also found in the major cities.
There was a small Jewish community in Afghanistan who had emigrated to Israel and the United States by the end of the twentieth century; at least one Jew, Zablon Simintov, remained.
There is also at least one known Christian, current First Lady of Afghanistan Rula Ghani, apart from Christian foreigners.
The only Christian Church with a noteworthy number of members in Afghanistan is the Christian Church International.
Afghanistan is an Islamic republic consisting of three branches, the executive, legislative, and judicial.
The nation is led by President Ashraf Ghani with Abdul Rashid Dostum and Sarwar Danish as vice presidents.
Abdullah Abdullah serves as the chief executive officer (CEO).
The National Assembly is the legislature, a bicameral body having two chambers, the House of the People and the House of Elders.
The Supreme Court is led by Chief Justice Said Yusuf Halem, the former Deputy Minister of Justice for Legal Affairs.
According to Transparency International, Afghanistan remains in the top most corrupt countries list.
A January 2010 report published by the United Nations Office on Drugs and Crime revealed that bribery consumed an amount equal to 23% of the GDP of the nation.
A number of government ministries are believed to be rife with corruption, and while then-President Karzai vowed to tackle the problem in 2009 by stating that "individuals who are involved in corruption will have no place in the government", top government officials were stealing and misusing hundreds of millions of dollars through the Kabul Bank.
The 2004 Afghan presidential election was relatively peaceful, in which Hamid Karzai won in the first round with 55.4% of the votes.
However, the 2009 presidential election was characterized by lack of security, low voter turnout, and widespread electoral fraud.
The vote, along with elections for 420 provincial council seats, took place in August 2009, but remained unresolved during a lengthy period of vote counting and fraud investigation.
Two months later, under international pressure, a second round run-off vote between Karzai and remaining challenger Abdullah was announced, but a few days later Abdullah announced that he would not participate in 7 November run-off because his demands for changes in the electoral commission had not been met.
The next day, officials of the election commission cancelled the run-off and declared Hamid Karzai as President for another five-year term.
In the 2005 parliamentary election, among the elected officials were former mujahideen, Islamic fundamentalists, warlords, communists, reformists, and several Taliban associates.
In the same period, Afghanistan reached to the 30th highest nation in terms of female representation in the National Assembly.
The last parliamentary election was held in September 2010, but due to disputes and investigation of fraud, the swearing-in ceremony took place in late January 2011.
The 2014 presidential election ended with Ashraf Ghani winning by 56.44% votes.
Afghanistan is administratively divided into 34 provinces ("wilayats").
Each province is the size of a U.S.
county, having a governor and a capital.
The country is further divided into nearly 400 provincial districts, each of which normally covers a city or a number of villages.
Each district is represented by a district governor.
The provincial governors are appointed by the President of Afghanistan and the district governors are selected by the provincial governors.
The provincial governors are representatives of the central government in Kabul and are responsible for all administrative and formal issues within their provinces.
There are also provincial councils that are elected through direct and general elections for a period of four years.
The functions of provincial councils are to take part in provincial development planning and to participate in the monitoring and appraisal of other provincial governance institutions.
According to article 140 of the constitution and the presidential decree on electoral law, mayors of cities should be elected through free and direct elections for a four-year term.
However, due to huge election costs, mayoral and municipal elections have never been held.
Instead, mayors have been appointed by the government.
In the capital city of Kabul, the mayor is appointed by the President of Afghanistan.
The following is a list of all the 34 provinces in alphabetical order:

Afghanistan became a member of the United Nations in 1946.
It enjoys cordial relations with a number of NATO and allied nations, particularly the United States, Canada, United Kingdom, Germany, Australia, and Turkey.
In 2012, the United States and Afghanistan signed their Strategic Partnership Agreement in which Afghanistan became a major non-NATO ally.
Afghanistan also has friendly diplomatic relations with neighboring China, Iran, Pakistan, Tajikistan, Turkmenistan, and Uzbekistan, including with regional states such as Bangladesh, India, Japan, Kazakhstan, Nepal, Russia, South Korea, the UAE, and so forth.
The Afghan Ministry of Foreign Affairs continues to develop diplomatic relations with other countries around the world.
The United Nations Assistance Mission in Afghanistan (UNAMA) was established in 2002 in order to help the country recover from the decades of war and neglect.
Today, a number of NATO member states deploy about 20,000 troops in Afghanistan as part of the Resolute Support Mission.
Its main purpose is to train the Afghan National Security Forces.
The Afghan Armed Forces are under the Ministry of Defense, which includes the Afghan Air Force (AAF) and the Afghan National Army (ANA).
The Afghan Defense University houses various educational establishments for the Afghan Armed Forces, including the National Military Academy of Afghanistan.
The National Directorate of Security (NDS) is Afghanistan's domestic intelligence agency, which operates similar to that of the U.S.
Department of Homeland Security or UK's Scotland Yard.
The Afghan National Police (ANP) is under the Ministry of Interior Affairs and serves as a single law enforcement agency all across the country.
The Afghan National Civil Order Police is the main branch of the ANP, which is divided into five Brigades, each commanded by a Brigadier General.
These brigades are stationed in Kabul, Gardez, Kandahar, Herat, and Mazar-i-Sharif.
There is one Chief of Police in every province.
All parts of Afghanistan are considered dangerous due to militant activities and terrorism-related incidents.
Kidnapping for ransom and robberies are common in major cities.
Every year hundreds of Afghan police are killed in the line of duty.
The Afghan Border Police (ABP) is responsible for protecting the nation's airports and borders, especially the disputed Durand Line border, which is often used by terrorists and criminals for their illegal activities.
Drugs from Afghanistan are smuggled to neighboring countries by various nationals but mostly by Afghans, Iranians, Pakistanis, Tajikistanis, Turkmenistanis and Uzbekistanis.
The Afghan Ministry of Counter Narcotics is responsible for the monitoring and eradication of the illegal drug business.
Afghanistan's GDP is around $64 billion with an exchange rate of $18.4 billion, and its GDP per capita is $2,000.
Despite having $1 trillion or more in mineral deposits, it remains as one of the least developed countries.
The country imports over $6 billion worth of goods but exports only $658 million, mainly fruits and nuts.
It has less than $1.5 billion in external debt.
Agricultural production is the backbone of Afghanistan's economy.
The country is known for producing some of the finest pomegranates, grapes, apricots, melons, and several other fresh and dry fruits.
It is also known as the world's largest producer of opium.
Sources indicate that as much as 11% or more of the nation's economy is derived from the cultivation and sale of opium.
While the nation's current account deficit is largely financed with donor money, only a small portion is provided directly to the government budget.
The rest is provided to non-budgetary expenditure and donor-designated projects through the United Nations system and non-governmental organizations.
The Afghan Ministry of Finance is focusing on improved revenue collection and public sector expenditure discipline.
For example, government revenues increased 31% to $1.7 billion from March 2010 to March 2011.
Da Afghanistan Bank serves as the central bank of the nation and the "Afghani" (AFN) is the national currency, with an exchange rate of about 60 Afghanis to 1 US dollar.
A number of local and foreign banks operate in the country, including the Afghanistan International Bank, New Kabul Bank, Azizi Bank, Pashtany Bank, Standard Chartered Bank, and the First Micro Finance Bank.
One of the main drivers for the current economic recovery is the return of over 5 million expatriates, who brought with them fresh energy, entrepreneurship and wealth-creating skills as well as much needed funds to start up businesses.
Many Afghans are now involved in construction, which is one of the largest industries in the country.
Some of the major national construction projects include the New Kabul City next to the capital, the Aino Mena project in Kandahar, and the Ghazi Amanullah Khan Town near Jalalabad.
Similar development projects have also begun in Herat, Mazar-e-Sharif, and other cities.
An estimated 400,000 people enter the labor market each year.
A number of small companies and factories began operating in different parts of the country, which not only provide revenues to the government but also create new jobs.
Improvements to the business environment have resulted in more than $1.5 billion in telecom investment and created more than 100,000 jobs since 2003.
Afghan rugs are becoming popular again, allowing many carpet dealers around the country to hire more workers.
Afghanistan is a member of WTO, SAARC, ECO, and OIC.
It holds an observer status in SCO.
Foreign Minister Zalmai Rassoul told the media in 2011 that his nation's "goal is to achieve an Afghan economy whose growth is based on trade, private enterprise and investment".
Experts believe that this will revolutionize the economy of the region.
In June 2012, India advocated for private investments in the resource rich country and the creation of a suitable environment therefor.
Telecommunications company Roshan is the largest private employer in the country .
Michael E. O'Hanlon of the Brookings Institution estimated that if Afghanistan generates about $10 billion per year from its mineral deposits, its gross national product would double and provide long-term funding for Afghan security forces and other critical needs.
The United States Geological Survey (USGS) estimated in 2006 that northern Afghanistan has an average (bbl) of crude oil, 15.7 trillion cubic feet ( bn m) of natural gas, and of natural gas liquids.
In 2011, Afghanistan signed an oil exploration contract with China National Petroleum Corporation (CNPC) for the development of three oil fields along the Amu Darya river in the north.
The country has significant amounts of lithium, copper, gold, coal, iron ore, and other minerals.
The Khanashin carbonatite in Helmand Province contains of rare earth elements.
In 2007, a 30-year lease was granted for the Aynak copper mine to the China Metallurgical Group for $3 billion, making it the biggest foreign investment and private business venture in Afghanistan's history.
The state-run Steel Authority of India won the mining rights to develop the huge Hajigak iron ore deposit in central Afghanistan.
Government officials estimate that 30% of the country's untapped mineral deposits are worth at least .
One official asserted that "this will become the backbone of the Afghan economy" and a Pentagon memo stated that Afghanistan could become the "Saudi Arabia of lithium".
In a 2011 news story, the "CSM" reported, "The United States and other Western nations that have borne the brunt of the cost of the Afghan war have been conspicuously absent from the bidding process on Afghanistan's mineral deposits, leaving it mostly to regional powers."
Air transport in Afghanistan is provided by the national carrier, Ariana Afghan Airlines (AAA), and by private companies such as Afghan Jet International, East Horizon Airlines, Kam Air, Pamir Airways, and Safi Airways.
Airlines from a number of countries also provide flights in and out of the country.
These include Air India, Emirates, Gulf Air, Iran Aseman Airlines, Pakistan International Airlines, and Turkish Airlines.
The country has four international airports: Hamid Karzai International Airport (formerly Kabul International Airport), Kandahar International Airport, Herat International Airport, and Mazar-e Sharif International Airport.
There are also around a dozen domestic airports with flights to Kabul and other major cities.
, the country has three rail links, one a line from Mazar-i-Sharif to the Uzbekistan border; a long line from Toraghundi to the Turkmenistan border (where it continues as part of Turkmen Railways); and a short link from Aqina across the Turkmen border to Kerki, which is planned to be extended further across Afghanistan.
These lines are used for freight only and there is no passenger service as of yet.
A rail line between Khaf, Iran and Herat, western Afghanistan, intended for both freight and passengers, is under construction and due to open in late 2018.
About of the line will lie on the Afghan side.
There are various proposals for the construction of additional rail lines in the country.
Traveling by bus in Afghanistan remains dangerous due to militant activities.
The buses are usually older model Mercedes-Benz and owned by private companies.
Serious traffic accidents are common on Afghan roads and highways, particularly on the Kabul–Kandahar and the Kabul–Jalalabad Road.
Newer automobiles have recently become more widely available after the rebuilding of roads and highways.
They are imported from the United Arab Emirates through Pakistan and Iran.
, vehicles more than 10 years old are banned from being imported into the country.
The development of the nation's road network is a major boost for the economy due to trade with neighboring countries.
Postal services in Afghanistan are provided by the publicly owned Afghan Post and private companies such as FedEx, DHL, and others.
According to the Human Development Index, Afghanistan is the 15th least developed country in the world.
The average life expectancy is estimated to be around 60 years.
The country's maternal mortality rate is 396 deaths/100,000 live births and its infant mortality rate is 66 to 112.8 deaths in every 1,000 live births.
The Ministry of Public Health plans to cut the infant mortality rate to 400 for every 100,000 live births before 2020.
The country has more than 3,000 midwives, with an additional 300 to 400 being trained each year.
There are over 100 hospitals in Afghanistan, with the most advanced treatments being available in Kabul.
The French Medical Institute for Children and Indira Gandhi Children's Hospital in Kabul are the leading children's hospitals in the country.
Some of the other main hospitals in Kabul include the Jamhuriat Hospital and the under-construction Jinnah Hospital.
In spite of all this, many Afghans travel to Pakistan and India for advanced treatment.
It was reported in 2006 that nearly 60% of the Afghan population lives within a two-hour walk of the nearest health facility.
Disability rate is also high in Afghanistan due to the decades of war.
It was reported recently that about 80,000 people are missing limbs.
Non-governmental charities such as Save the Children and Mahboba's Promise assist orphans in association with governmental structures.
Demographic and Health Surveys is working with the Indian Institute of Health Management Research and others to conduct a survey in Afghanistan focusing on maternal death, among other things.
Education in Afghanistan includes K–12 and higher education, which is overseen by the Ministry of Education and the Ministry of Higher Education.
There are over 16,000 schools in the country and roughly 9 million students.
Of this, about 60% are males and 40% females.
Over 174,000 students are enrolled in different universities around the country.
About 21% of these are females.
Former Education Minister Ghulam Farooq Wardak had stated that construction of 8,000 schools is required for the remaining children who are deprived of formal learning.
The top universities in Afghanistan are the American University of Afghanistan (AUAF) followed by Kabul University (KU), both of which are located in Kabul.
The National Military Academy of Afghanistan, modeled after the United States Military Academy at West Point, is a four-year military development institution dedicated to graduating officers for the Afghan Armed Forces.
The Afghan Defense University was constructed near Qargha in Kabul.
Major universities outside of Kabul include Kandahar University in the south, Herat University in the northwest, Balkh University and Kunduz University in the north, Nangarhar University and Khost University in the east.
The United States is building six faculties of education and five provincial teacher training colleges around the country, two large secondary schools in Kabul, and one school in Jalalabad.
The literacy rate of the entire population is 38.2% (males 52% and females 24.2%).
In 2010, the United States began establishing a number of Lincoln learning centers in Afghanistan.
They are set up to serve as programming platforms offering English language classes, library facilities, programming venues, internet connectivity, and educational and other counseling services.
A goal of the program is to reach at least 4,000 Afghan citizens per month per location.
The Afghan National Security Forces are provided with mandatory literacy courses.
In addition to this, Baghch-e-Simsim (based on the American Sesame Street) serves as a means to attract Afghan children into learning.
In 2017, Kazakhstan launched an official development assistance program (ODA) to Afghanistan that involved providing training and education to the Afghan women in Kazakh universities.
The project aims to strengthen the economic independence of Afghan women by providing education from Kazakhstan’s top educational institutions in public administration and healthcare.
Afghanistan is mostly a tribal society with different regions of the country having its own subculture.
Their history is traced back to at least the time of the Achaemenid Empire in 500 BCE.
In the southern and eastern region, the people live according to the Pashtun culture by following Pashtunwali (Pashtun way).
The Pashtuns (and Baloch) are largely connected to the culture of South Asia.
The remaining Afghans are culturally Persian and Turkic.
Some non-Pashtuns who live in proximity with Pashtuns have adopted Pashtunwali in a process called Pashtunization, while some Pashtuns have been Persianized.
Those who have lived in Pakistan and Iran over the last 30 years have been further influenced by the cultures of those neighboring nations.
Afghans are regarded with mingled apprehension and condescension, for their high regard for personal honor, for their tribe loyalty and for their readiness to use force to settle disputes.
As tribal warfare and internecine feuding has been one of their chief occupations since time immemorial, this individualistic trait has made it difficult for foreigners to conquer them.
One writer considers the tribal system to be the best way of organizing large groups of people in a country that is geographically difficult, and in a society that, from a materialistic point of view, has an uncomplicated lifestyle.
There are various Afghan tribes, and an estimated 2–3 million nomads.
The nation has a complex history that has survived either in its current cultures or in the form of various languages and monuments.
However, many of its historic monuments have been damaged in modern times.
The two famous Buddhas of Bamiyan were destroyed by the Taliban, who regarded them as idolatrous.
Despite that, archaeologists are still finding Buddhist relics in different parts of the country, some of them dating back to the 2nd century.
This indicates that Buddhism was widespread in Afghanistan.
Other historical places include the cities of Herat, Kandahar, Ghazni, Mazar-i-Sharif, and Zaranj.
The Minaret of Jam in the Hari River valley is a UNESCO World Heritage site.
A cloak reputedly worn by Islam's prophet Muhammad is kept inside the Shrine of the Cloak in Kandahar, a city founded by Alexander and the first capital of Afghanistan.
The citadel of Alexander in the western city of Herat has been renovated in recent years and is a popular attraction for tourists.
In the north of the country is the Shrine of Ali, believed by many to be the location where Ali was buried.
The National Museum of Afghanistan is located in Kabul.
Afghanistan has around 150 radio stations and over 50 television stations, which includes the state-owned RTA TV and various private channels such as TOLO and Shamshad TV.
The first Afghan newspaper was published in 1906 and there are hundreds of print outlets today.
By the 1920s, Radio Kabul was broadcasting local radio services.
Television programs began airing in the early 1970s.
Voice of America, BBC, and Radio Free Europe/Radio Liberty (RFE/RL) broadcast in both of Afghanistan's official languages.
Since 2002, press restrictions have been gradually relaxed and private media diversified.
Freedom of expression and the press is promoted in the 2004 constitution and censorship is banned, although defaming individuals or producing material contrary to the principles of Islam is prohibited.
The Afghan government cited the growth in the media sector as one of its achievements.
In 2017, Reporters Without Borders ranked Afghanistan 120th in the Press Freedom Index out of 180 countries, a better rating than all its neighbors.
According to "Freedom of the Press" as of 2015, Afghanistan is "partly free", whereas most countries in Asia are "not free".
The city of Kabul has been home to many musicians who were masters of both traditional and modern Afghan music.
Traditional music is especially popular during the Nowruz (New Year) and National Independence Day celebrations.
Ahmad Zahir, Nashenas, Ustad Sarahang, Sarban, Ubaidullah Jan, Farhad Darya, and Naghma are some of the notable Afghan musicians, but there are many others.
Afghans have long been accustomed to watching Indian Bollywood films and listening to its filmi songs.
Many Bollywood film stars have roots in Afghanistan, including Salman Khan, Saif Ali Khan, Shah Rukh Khan (SRK), Aamir Khan, Feroz Khan, Kader Khan, Naseeruddin Shah, Zarine Khan, Celina Jaitly, and a number of others.
Several Bollywood films have been shot inside Afghanistan, including "Dharmatma", "Khuda Gawah", "Escape from Taliban", and "Kabul Express".
Telecommunication services in Afghanistan are provided by Afghan Telecom, Afghan Wireless, Etisalat, MTN Group, and Roshan.
The country uses its own space satellite called Afghansat 1, which provides services to millions of phone, internet and television subscribers.
By 2001 following years of civil war, telecommunications was virtually a non-existent sector, but by 2016 it had grown to a $2 billion industry, with 22 million mobile phone subscribers and 5 million internet users.
The sector employs at least 120,000 people nationwide.
Afghanistan has a wide varying landscape allowing for many different crops.
Afghan food is largely based upon cereals like wheat, maize, barley and rice, which are the nation's chief crops.
Fresh and dried fruits is the most important part of Afghan diet.
Afghanistan is well known for its fine fruits, especially pomegranates, grapes, and its extra-sweet jumbo-size melons.
Classic Persian and Pashto poetry are a cherished part of Afghan culture.
Thursdays are traditionally "poetry night" in the city of Herat when men, women and children gather and recite both ancient and modern poems.
Poetry has always been one of the major educational pillars in the region, to the level that it has integrated itself into culture.
Some notable poets include Rumi, Rabi'a Balkhi, Sanai, Jami, Khushal Khan Khattak, Rahman Baba, Khalilullah Khalili, and Parween Pazhwak.
Afghanistan's sports teams are increasingly celebrating titles at international events.
Its basketball team won the first team sports title at the 2010 South Asian Games.
Later that year, the country's cricket team followed as it won the 2009–10 ICC Intercontinental Cup.
In 2012, the country's 3x3 basketball team won the gold medal at the 2012 Asian Beach Games.
In 2013, Afghanistan's football team followed as it won the SAFF Championship.
Cricket and association football are the most popular sports in the country.
The Afghan national cricket team, which was formed in the last decade, participated in the 2009 ICC World Cup Qualifier, 2010 ICC World Cricket League Division One and the 2010 ICC World Twenty20.
It won the ACC Twenty20 Cup in 2007, 2009, 2011 and 2013.
The team eventually made it to play in the 2015 Cricket World Cup.
The Afghanistan Cricket Board (ACB) is the official governing body of the sport and is headquartered in Kabul.
The Alokozay Kabul International Cricket Ground serves as the nation's main cricket stadium.
There are a number of other stadiums throughout the country, including the Ghazi Amanullah Khan International Cricket Stadium near Jalalabad.
Domestically, cricket is played between teams from different provinces.
The Afghanistan national football team has been competing in international football since 1941.
The national team plays its home games at the Ghazi Stadium in Kabul, while football in Afghanistan is governed by the Afghanistan Football Federation.
The national team has never competed or qualified for the FIFA World Cup, but has recently won an international football trophy in 2013.
The country also has a national team in the sport of futsal, a 5-a-side variation of football.
Other popular sports in Afghanistan include basketball, volleyball, taekwondo, and bodybuilding.
Buzkashi is a traditional sport, mainly among the northern Afghans.
It is similar to polo, played by horsemen in two teams, each trying to grab and hold a goat carcass.
The Afghan Hound (a type of running dog) originated in Afghanistan and was originally used in hunting.
Books

Articles



</doc>
<doc id="738" url="https://en.wikipedia.org/wiki?curid=738" title="Albania">
Albania

Albania ( ; ; ), officially the Republic of Albania (, ), is a country in Southeast Europe on the Adriatic and Ionian Sea within the Mediterranean Sea.
It is bounded by Montenegro to the northwest, Kosovo to the northeast, the Republic of Macedonia to the east and Greece to the south and southeast.
Geographically, the country displays varied climatic, geological, hydrological and morphological conditions, defined in an area of .
It possesses remarkable diversity with the landscape ranging from the snow-capped mountains in the Albanian Alps as well as the Korab, Skanderbeg, Pindus and Ceraunian Mountains to the hot and sunny coasts of the Albanian Adriatic and Ionian Sea along the Mediterranean Sea.
Historically, the area of Albania was populated by various Illyrian, Thracian and Ancient Greek tribes as well as several Greek colonies established in the Illyrian coast.
The area was annexed in the 3rd century by Romans and became an integral part of the Roman provinces of Dalmatia, Macedonia and Illyricum.
The autonomous Principality of Arbër emerged in 1190, established by archon Progon in the Krujë, within the Byzantine Empire.
In the late thirteenth century, Charles of Anjou conquered Albanian territories from the Byzantines and established the medieval Kingdom of Albania, which at its maximal extension was extending from Durrës along the coast to Butrint in the south.
In the mid-fifteenth century, it was conquered by the Ottomans.
The modern nation state of Albania emerged in 1912 following the defeat of the Ottomans in the Balkan Wars.
The modern Kingdom of Albania was invaded by Italy in 1939, which formed Greater Albania, before becoming a Nazi German protectorate in 1943.
After the defeat of Nazi Germany, a Communist state titled the People's Socialist Republic of Albania was founded under the leadership of Enver Hoxha and the Party of Labour.
The country experienced widespread social and political transformations in the communist era, as well as isolation from much of the international community.
In the aftermath of the Revolutions of 1991, the Socialist Republic was dissolved and the fourth Republic of Albania was established.
Politically, the country is a unitary parliamentary constitutional republic and developing country with an upper-middle income economy dominated by the tertiary sector followed by the secondary and primary sector.
It went through a process of transition, following the end of communism in 1990, from a centralized to a market-based economy.
It also provides universal health care and free primary and secondary education to its citizens.
The country is a member of the United Nations, World Bank, UNESCO, NATO, WTO, COE, OSCE and OIC.
It is also an official candidate for membership in the European Union.
In addition it is one of the founding members of the Energy Community, including the Organization of the Black Sea Economic Cooperation and Union for the Mediterranean.
The term Albania is the medieval Latin name of the country.
It may be derived from the Illyrian tribe of Albani () recorded by Ptolemy, the geographer and astronomer from Alexandria, who drafted a map in 150 AD, which shows the city of Albanopolis located northeast of the city of Durrës.
The term may have a continuation in the name of a medieval settlement called Albanon or Arbanon, although it is not certain that this was the same place.
In his history written in the 10th century, the Byzantine historian Michael Attaliates was the first to refer to Albanoi as having taken part in a revolt against Constantinople in 1043 and to the Arbanitai as subjects of the Duke of Dyrrachium.
During the Middle Ages, the Albanians called their country ' and referred to themselves as '.
Nowadays, Albanians call their country '.
As early as the 17th century the placename and the ethnic demonym ' gradually replaced "".
The two terms are popularly interpreted as "Land of the Eagles" and "Children of the Eagles".
The first traces of human presence in Albania, dating to the Middle Paleolithic and Upper Paleolithic eras, were found in the village of Xarrë close to Sarandë and Dajti near Tiranë.
The objects found in a cave near Xarrë include flint and jasper objects and fossilized animal bones, while those found at Mount Dajt comprise bone and stone tools similar to those of the Aurignacian culture.
The Paleolithic finds of Albania show great similarities with objects of the same era found at Crvena Stijena in Montenegro and north-western Greece.
Several Bronze Age artefacts from tumulus burials have been unearthed in central and southern Albania that show close connection with sites in south-western Macedonia and Lefkada, Greece.
Archaeologists have come to the conclusion that these regions were inhabited from the middle of the third millennium BC by Indo-European people who spoke a Proto-Greek language.
A part of this population later moved to Mycenae around 1600 BC and founded the Mycenaean civilisation there.
In ancient times, the territory of modern Albania was mainly inhabited by a number of Illyrian tribes.
The Illyrian tribes never collectively regarded themselves as 'Illyrians', and it is unlikely that they used any collective nomenclature for themselves.
The name Illyrians seems to be the name applied to a specific Illyrian tribe, which was the first to come in contact with the ancient Greeks during the Bronze Age, causing the name Illyrians to be applied "pars pro toto" to all people of similar language and customs.
The territory known as Illyria corresponded roughly to the area east of the Adriatic sea, extending in the south to the mouth of the Vjosë river.
The first account of the Illyrian groups comes from "Periplus of the Euxine Sea", an ancient Greek text written in the middle of the 4th century BC.
The south was inhabited by the Greek tribe of the Chaonians, whose capital was at Phoenice, while numerous colonies, such as Apollonia, Epidamnos and Amantia, were established by Greek city-states on the coast by the 7th century BC.
The west was inhabited by the Thracian tribe of the Bryges.
The Illyrian tribe of the Ardiaei centered in Montenegro ruled over much of nowadays Albania.
The Ardiaean Kingdom reached its greatest extent under Agron, son of Pleuratus II.
Agron extended his rule over other neighboring tribes as well.
After Agron's death in 230 BC, his wife Teuta inherited the Ardiaean kingdom.
Teuta's forces extended their operations further southward into the Ionian Sea.
In 229 BC, Rome declared war on the kingdom for extensively plundering Roman ships.
The war ended in Illyrian defeat in 227 BC.
Teuta was eventually succeeded by Gentius in 181 BC.
Gentius clashed with the Romans in 168 BC, initiating the Third Illyrian War.
The conflict resulted in Roman conquest of the region by 167 BC.
After that the Roman split the region into three administrative divisions.
After the Roman Empire was divided into East and West in the 4th century, the territory of Albania remained within the Eastern Roman Empire.
In the centuries that followed, the Balkan Peninsula suffered from the Barbarian invasions.
The Illyrians are mentioned for the last time in a text from the 7th century.
Towards the end of the 12th and beginning of the 13th centuries, Serbs and Venetians started to take possession over the territory.
The ethnogenesis of the Albanians is uncertain however the first undisputed mention of Albanians dates back in historical records from 1079 or 1080 in a work by Michael Attaliates, who referred to the Albanoi as having taken part in a revolt against Constantinople.
At this point the Albanians were fully christianized.
The first semi-autonomous Albanian polity was formed in 1190, when archon Progon of Kruja established the Principality of Arbanon with the capital in Krujë within the Byzantine Empire.
Progon, was succeeded by his sons Gjin and Dhimitri, the latter which attained the height of the realm.
Following the death of Dhimiter, the last member of the Progon family, the principality came under the Albanian-Greek Gregory Kamonas and later Golem of Kruja.
In the 13th century, the principality was dissolved.
Arbanon is considered to be the first sketch of an Albanian state, that retained a semi-autonomous status as the western extremity of the Byzantine Empire, under the Byzantine Doukai of Epirus or Laskarids of Nicaea.
Few years after the dissolution of Arbanon, Charles of Anjou concluded an agreement with the Albanian rulers, promising to protect them and their ancient liberties.
In 1272, he established the Kingdom of Albania and conquered regions back from the Despotate of Epirus.
The kingdom claimed all of central Albania territory from Dyrrhachium along the Adriatic Sea coast down to Butrint.
A catholic political structure was a basis for the papal plans of spreading Catholicism in the Balkan Peninsula.
This plan found also the support of Helen of Anjou, a cousin of Charles of Anjou, who was at that time ruling territories in North Albania.
Around 30 Catholic churches and monasteries were built during her rule mainly in Northern Albania.
Internal power struggles within the Byzantine Empire in the fourteenth century, enabled Serbs' most powerful medieval ruler, Stefan Dusan, to establish a short-lived empire that included all of Albania except Durrës.
In 1367, various Albanian rulers established the Despotate of Arta.
During that time, several Albanian principalities were created, most notable amongst them the Balsha, Thopia, Kastrioti, Muzaka and Arianiti.
In the first half of the 14th century, the Ottoman Empire invaded most of Albania and the League of Lezhë was held under Skanderbeg as a ruler, who became the national hero of the Albanian medieval history.
The Ottoman invasion of the territory of Albania marked a new era in its history and introduced enormous changes in the political and cultural environment of the area.
The Ottomans erected their garrisons across the south of Albania in 1415 and occupied the majority in 1431 although they reached Albanian coast for first time at 1385.
Upon their arrival, Islam was introduced in the country as a second religion resulting a massive emigration of Christian Albanians to other Christian European countries such as the Arbëreshë to Italy while Muslim Albanians gradually settled to Turkey and other part of the Ottoman Empire such as Algeria, Egypt and Iraq.
In 1443, a great and longstanding revolt broke out under the lead of Gjergj Kastrioti Skanderbeg that lasted until 1479, many times defeating major Ottoman armies led by Murad II and Mehmed II.
Skanderbeg united initially the princes of Albania and later on established a centralized authority over most of the non-conquered territories becoming the ruling "Lord of Albania".
Skanderbeg pursued relentlessly but rather unsuccessfully to create a European coalition against the Ottomans.
He thwarted every attempt by the Ottomans to regain Albania which they envisioned as a springboard for the invasion of Italy and Western Europe.
His unequal fight against the mightiest power of the time, won the esteem of Europe as well as some financial and military aid from Naples, Venice, Sicily and the Papacy.
When the Ottomans were gaining a firm foothold in the region, Albanian towns were organised into four principal sanjaks.
The government fostered trade by settling a sizeable Jewish colony of refugees fleeing persecution in Spain.
The city of Vlorë saw passing through its ports imported merchandise from Europe such as velvets, cotton goods, mohairs, carpets, spices and leather from Bursa and Constantinople.
Some citizens of Vlorë even had business associates throughout Europe.
As Muslims, some Albanians attained important political and military positions within the empire and culturally contributed to the wider Muslim world.
Enjoying this privileged position, Muslim Albanians held various high administrative positions with over two dozen Albanian Grand Viziers among others Köprülü Mehmed Pasha, Köprülü Fazıl Ahmed and Muhammad Ali Pasha.
The most significant impact on the Albanians was the gradual Islamisation process of a large majority of the population, although it became widespread only in the 17th century.
The process was an incremental one, commencing from the arrival of the Ottomans.
Timar holders, the bedrock of early Ottoman control in Southeast Europe, were not necessarily converts to Islam, and occasionally rebelled, with the most famous of these being Skanderbeg.
Mainly Catholic Albanians converted in the 17th century, while the Orthodox Albanians followed suit mainly in the following century.
Initially confined to the main city centres of Elbasan and Shkodër, by this period the countryside was also embracing the new religion.
The motives for conversion according to some scholars were diverse, depending on the context.
The lack of source material does not help when investigating such issues.
The origins of the Albanian Renaissance can be traced back to around the 19th century that was a very difficult period for Albania.
During the period, the modern culture of Albania flourished especially in literature and art as well inspired by romanticism and enlightenment.
The victory of Russia over the Ottoman Empire following the Russian-Ottoman War resulted the execution of the Treaty of San Stefano that overlooked to assign Albanian-populated regions to the Slavic neighbors.
The United Kingdom as well as Austria-Hungary consequently, blocked the arrangement and caused the Treaty of Berlin.
At this point, Albanians started to organize themselves with the aim to protect and unite the Albanian-populated regions into a unitary nation.
This led to the formation of the League of Prizren in the old town of Prizren.
At first the Ottoman authorities supported the League, whose initial position was based on the religious solidarity of Muslim landlords and people connected with the Ottoman administration.
The Ottomans favoured and protected the Muslim solidarity and called for defense of Muslim lands, including present-day Bosnia and Herzegovina.
This was the reason for naming the league "The Committee of the Real Muslims".
The league issued a decree known as Kararname that contained a proclamation that the people from northern Albania, Epirus and Bosnia are willing to defend the territorial integrity of the Ottoman Empire by all possible means against the troops of the kingdoms of Bulgaria, Serbia and Montenegro.
However, it was signed by 47 Muslim deputies of the league on 18 June 1878.
Approximately 300 Muslims participated in the assembly, including delegates from Bosnia and mutasarrif of the Sanjak of Prizren as representatives of the central authorities and no delegates from Vilayet of Scutari.
The Ottomans cancelled their support when the league, under the influence of Abdyl Bey Frashëri, became focused on working toward the Albanian autonomy and requested merging of four Ottoman vilayets, which included Kosovo, Scutari, Monastir and Ioannina into a new vilayet within the empire, the Albanian Vilayet.
The league used military force to prevent the annexing areas of Plav and Gusinje assigned to Montenegro by the Congress of Berlin.
After several successful battles with Montenegrin troops such as in Novsice, under the pressure of the great powers, the league was forced to retreat from their contested regions of Plav and Gusinje and later on, the league was defeated by the Ottoman army sent by the Sultan.
The independence of Albania from the Ottoman Empire was proclaimed on 28 November 1912 by Ismail Qemali in Vlorë.
Immediately after, the leaders of the Assembly of Vlorë established the senate as well as the first government of the country on 4 December 1912 that consisted of only ten members.
The country's independence was recognized by the Conference of London on 29 July 1913.
The treaty delineated the borders of the country and its neighbors leaving many ethnic Albanians outside Albania.
This population was largely divided between Montenegro and Serbia in the north and east and Greece in the south.
Headquartered in Vlorë, the International Commission of Control was established on 15 October 1913 to take care of the administration of newly established Albania, until its own political institutions were in order.
The International Gendarmerie was established as the first law enforcement agency of the Principality of Albania.
In November, the first gendarmerie members arrived in the country.
Prince of Albania Wilhelm of Wied "(Princ Vilhelm Vidi)" was selected as the first prince of the principality.
On 7 March, he arrived in the provisional capital of Durrës and started to organise his government, appointing Turhan Pasha Përmeti to form the first Albanian cabinet.
In November 1913, the Albanian pro-Ottoman forces had offered the throne of Albania to the Ottoman war Minister of Albanian origin, Ahmed Izzet Pasha.
The pro-Ottoman peasants believed that, the new regime of the Principality of Albania was a tool of the six Christian Great Powers and local landowners, that owned half of the arable land.
In February 1914, the Autonomous Republic of Northern Epirus was proclaimed in Gjirokastër by the local Greek population against incorporation to Albania.
This initiative was short lived and in 1921, the southern provinces were finally incorporated to the Albanian Principality.
Meanwhile, the revolt of Albanian peasants against the new Albanian regime erupted under the leadership of the group of Muslim clerics gathered around Essad Pasha Toptani, who proclaimed himself the savior of Albania and Islam.
In order to gain support of the Mirdita Catholic volunteers from the northern part of Albania, Prince Wied appointed their leader, Prênk Bibë Doda, to be the foreign minister of the Principality of Albania.
In May and June 1914, the International Gendarmerie was joined by Isa Boletini and his men, mostly from Kosovo, and northern Mirdita Catholics, were defeated by the rebels who captured most of Central Albania by the end of August 1914.
The regime of Prince Wied collapsed and later he left the country on 3 September 1914.
Following the end of the government of Fan Noli, the parliament adopted a new constitution and proclaimed the country as a parliamentary republic in which Zog I of Albania served as the head of state for a seven years term.
Immediately after, Tirana was endorsed officially as the country's permanent capital.
The politics of Zogu was authoritarian and conservative with the primary aim of which was the maintenance of stability and order.
He was forced to adopt a policy of cooperation with Italy where a pact had been signed between both countries, whereby Italy gained a monopoly on shipping and trade concessions.
In 1928, the country was eventually replaced by another monarchy with a strong support by the fascist regime of Italy however, both maintained close relations until the Italian invasion of the country.
Zogu remained a conservative but initiated reforms and placed great emphasis on the development of infrastructure.
In an attempt at social modernization, the custom of adding one's region to one's name was dropped.
He also made donations of land to international organisations for the building of schools and hospitals.
The armed forces were trained and supervised by instructors from Itala, as a counterweight, he kept British officers in the Gendarmerie despite strong Italian pressure to remove them.
After being militarily occupied by Italy from 1939 until 1943, the Kingdom of Albania was a protectorate and a dependency of the Kingdom of Italy governed by Victor Emmanuel III and his government.
In October 1940, Albania served as a staging ground for an unsuccessful Italian invasion of Greece.
A counterattack resulted in a sizable portion of southern Albania coming under Greek military control until April 1941 when Greece capitulated during the German invasion.
In April 1941, territories of Yugoslavia with substantial Albanian population were annexed to Albania inclusively western Macedonia, a strip of eastern Montenegro, the town of Tutin in central Serbia and most of Kosovo.
Germans started to occupy the country in September 1943 subsequently announced that they would recognize the independence of a neutral Albania and set about organizing a new government, military and law enforcement.
Balli Kombëtar, which had fought against Italy, formed a neutral government and side by side with the Germans fought against the communist-led National Liberation Movement of Albania.
During the last years of the war, the country fell into a civil war-like state between the communists and nationalists.
The communist however defeated the last anti-communist forces in the south in 1944.
Before the end of November, the main German troops had withdrawn from Tirana, and the communists took control by attacking it.
The partisans entirely liberated the country from German occupation on 29 November 1944.
A provisional government, which the communists had formed at Berat in October, administered Albania with Enver Hoxha as the head of government.
By the end of the Second World War, the main military and political force of the nation, the Communist party sent forces to northern Albania against the nationalists to eliminate its rivals.
They faced open resistance in Nikaj-Mërtur, Dukagjin and Kelmend led by Prek Cali.
On 15 January 1945, a clash took place between partisans of the first Brigade and nationalist forces at the Tamara Bridge, resulting in the defeat of the nationalist forces.
About 150 Kelmendi people were killed or tortured.
This event was the starting point of many other issues which took place during Enver Hoxha's dictatorship.
Class struggle was strictly applied, human freedom and human rights were denied.
The Kelmend region was almost isolated by both the border and by a lack of roads for another 20 years, the institution of agricultural cooperatives brought about economic decline.
Many Kelmendi people fled, some were executed trying to cross the border.
In the aftermath of World War II and the defeat of Nazi Germany, the country became initially a satellite state of the Soviet Union and Enver Hoxha emerged consequently as the leader of the newly established People's Republic of Albania.
At this point, the country started to develop foreign relations with other communist countries among others with the People's Republic of China.
During this period, the country experienced an increasing industrialisation, a rapid collectivisation and an economic growth which led to a severe increase in living standards.
The government called for the development of infrastructure and most notably the introduction of a railway system that completely revamped the transportation.
The new land reform laws were passed granting ownership of the land to the workers and peasants who tilled it.
Agriculture became cooperative and production increased significantly, leading to the country's becoming agriculturally self-sufficient.
In the field of education, illiteracy was eliminated among the country's adult population.
The average annual increase in the country's national income was 29% and 56% higher than the world and European average, respectively.. The nation incurred large debts initially with Yugoslavia until 1948, then the Soviet Union until 1961 and China from the middle of the 1950s.
The constitution of the communist regime did not allow taxes on individuals, instead, taxes were imposed on cooperatives and other organizations, with much the same effect.
Today a secular state without any official religion, religious freedoms and practices were severely curtailed during the communism with all forms of worship being outlawed.
In 1945, the Agrarian Reform Law meant that large swaths of property owned by religious groups were nationalized, mostly the waqfs along with the estates of mosques, tekkes, monasteries and dioceses.
Many believers, along with the ulema and many priests, were arrested and executed.
In 1949, a new Decree on Religious Communities required that all their activities be sanctioned by the state alone.
After hundreds of mosques and dozens of Islamic libraries, containing priceless manuscripts were destroyed, Hoxha proclaimed Albania the world's first atheist state in 1967.
The churches had not been spared either, and many were converted into cultural centers for young people.
A 1967 law banned all fascist, religious, warmongerish, antisocialist activity and propaganda.
Preaching religion carried a three to ten-year prison sentence.
Nonetheless, many Albanians continued to practice their beliefs secretly.
The anti-religious policy of Hoxha attained its most fundamental legal and political expression a decade later: "The state recognizes no religion", states the 1976 constitution, "and supports and carries out atheistic propaganda in order to implant a scientific materialistic world outlook in people".
After forty years of communism and isolation as well as the revolutions of 1989, people and most notably students started to become politically active and consequently to campaign against the government that led to the transformation of the existing order.
Following the popular support in the first multi-party elections of 1991, the communists retained a stronghold in the parliament until the victory in the general elections of 1992 led by the Democratic Party.
Considerable economic and financial resources have been devoted to the Ponzi pyramid schemes that were widely supported by the government.
The schemes swept up somewhere between one sixth and one third of the population of the country.
Despite the warnings of the International Monetary Fund, his excellency Sali Berisha defended the schemes as large investment firms, leading more people to redirect their remittances and sell their homes and cattle for cash to deposit in the schemes.
The schemes began to collapse in late 1996, leading many of the investors to join initially peaceful protests against the government, requesting their money back.
The protests turned violent in February 1997 as government forces responded with fire.
In March, the Police and Republican Guard deserted, leaving their armories open.
These were promptly emptied by militias and criminal gangs.
The resulting crisis caused a wave of evacuations of foreign nationals and refugees.
The crisis led both Aleksandër Meksi and Sali Berisha to resign from office in the wake of the general election.
In April 1997, Operation Alba, a UN peacekeeping force led by Italy, entered the country with two goals exclusively to assist with the evacuation of expatriates and to secure the ground for international organizations.
The main international organization, that was involved, was the Western European Union's multinational Albanian Police element, which worked with the government to restructure the judicial system and simultaneously the Albanian Police.
When the communism collapsed in 1990, Albania rediscovered foreign policy after decades of isolationism and began to develop closer ties considerably with other countries of Western Europe and the United States.
At this point, its top foreign policy ambition was achieving integration into modern economic and security organizations.
Previously a member of the Warsaw Pact, the newly established democratic country broadly pursued an integrationist agenda in becoming a member of the NATO.
The organisation invited Albania and Croatia to join the alliance at the 2008 Bucharest summit.
In April 2014, it became a full member of the organisation and was among the first Southeast European countries to join the partnership for peace programme.
Albania applied to join the European Union, becoming an official candidate for accession to the European Union in June 2014.
Although Albania received candidate status for the European Union membership in 2014 (based on its 2009 application), the European Union has twice rejected full membership.
The European Parliament warned the Government leaders in early 2017 that the 2017 parliamentary elections in June must be free and fair before negotiations could begin to admit the country into the union.
On 23 June 2013, the eighth parliamentary elections took place, won by Edi Rama of the Socialist Party.
During his tenure as Prime Minister, Albania has implemented numerous reforms focused on the modernizing the economy and democratizing of state institutions inclusively the judiciary and law enforcement of the country.
Unemployment has been steadily reduced ranking 4th in terms of lowest unemployment rate in the Balkans.
Rama has placed gender equality at the center of its agenda, since 2017 almost 50% of the ministers are female, making it the largest number of women serving in the country's history.
Albania is defined in an area of and located on the Balkan Peninsula in Southern and Southeast Europe.
It is bounded by Montenegro to the northwest, Kosovo to the northeast, the Republic of Macedonia to the east and Greece to the southeast and south.
Its coastline faces the Adriatic Sea to the northwest and the Ionian Sea to the southwest around the Mediterranean Sea.
The country lies mostly between latitudes 42° and 39° N, and longitudes 21° and 19° E. Its northernmost point is Vërmosh at 42° 35' 34" northern latitude; the southernmost is Konispol at 39° 40' 0" northern latitude; the westernmost point is Sazan Island at 19° 16' 50" eastern longitude; and the easternmost point is Vërnik at 21° 1' 26" eastern longitude.
The highest point of the country is Mount Korab at above the Adriatic.
The lowest point is the Adriatic Sea at .
The distance from the east to west is only , while from the north to south about .
For a small country, much of Albania rises into mountains and hills that run in different directions across the length and breadth of the country.
The most extensive mountain ranges are the Albanian Alps in the north, the Korab Mountains in the east, the Pindus Mountains in the southeast, the Ceraunian Mountains in the southwest and the Skanderbeg Mountains in the center.
One of the most remarkable features about the country is the presence of numerous important lakes.
The Lake of Shkodër is the largest lake in Southern Europe and located in northwest.
In the southeast rises the Lake of Ohrid that is one of the oldest continuously existing lakes in the world.
Further south extend the Large and Small Lake of Prespa that are among the highest positioned lakes in the Balkans.
Rivers originate mostly in the east of Albania and discharge into the Adriatic Sea in the west.
The longest river in the country, measured from its mouth to its source, is probably the Drin that starts at the confluence of its two headwaters, the Black and White Drin.
Though of particular concern is the Vjosë that represents one of the last intact large river systems in Europe.
The climate in the country is highly variable and diverse owing to the differences in latitude, longitude and altitude.
Albania experiences predominantly a mediterranean and continental climate, with four distinct seasons.
Defined by the Köppen classification, it accommodates five major climatic types ranging from mediterranean and subtropical in the western halft to oceanic, continental and subarctic in the eastern half of Albania.
The warmest areas of the country are immediately located along the Adriatic and Ionian Sea Coasts.
On the contrary, the coldest areas are positioned within the northern and eastern highlands.
The mean monthly temperature ranges between in winter to in summer.
The highest temperature of was recorded in Kuçovë on 18 July 1973.
The lowest temperature of was registered in the village of Shtyllë, Librazhd on 9 January 2017.
Rainfall varies from season to season and from year to year.
The country receives most of precipitation in winter months and less in summer months.
The average precipitation is about .
The mean annual precipitation ranges between and depending on geographical location.
The northwestern and southeastern highlands receive the higher amount of precipitation, whilst the northeastern and southwestern highlands as well as the western lowlands the smaller amount.
The Albanian Alps in the far north of the country are considered to be among the wettest regions of Europe receiving at least of rain annually.
An expedition from the University of Colorado discovered four glaciers within these mountains at a relatively low altitude of , which is almost unique for such a southerly latitude.
Snowfall occurs regularly in winter in the highlands of the country, particularly on the mountains in the north and east, including the Albanian Alps and Korab Mountains.
Moreover, snow also falls on the coastal areas in the southwest almost every winter such as in the Ceraunian Mountains, where it can lie even beyond March.
A biodiversity hotspot, Albania possesses an exceptionally rich and contrasting biodiversity thanks to its geographical location at the center of the Mediterranean Sea and the great diversity in its climatic, geological and hydrological conditions.
Its biodiversity is conserved in 14 national parks, 1 marine park, 4 ramsar sites, 1 biosphere reserve and 786 protected areas of different categories.
Due to remoteness, the mountains and hills are endowed with forests, trees and grasses that are essential to the lives for a wide variety of animals among other for two of the most iconic endangered species of the country, the lynx and brown bear, as well as the wildcat, gray wolf, red fox, golden jackal and last but not least for the egyptian vulture and golden eagle, the national animal of the country.
The estuaries, wetlands and lakes are particularly important for the greater flamingo, pygmy cormorant and the extremely rare and perhaps the most iconic bird of the country, the dalmatian pelican.
Of particular importance are the mediterranean monk seal, loggerhead sea turtle and green sea turtle that use to nest on the country's coastal waters and shores.
In terms of phytogeography, Albania is part of the Boreal Kingdom and stretches specifically within the Illyrian province of the Circumboreal and Mediterranean Region.
Its territory can be conventionally subdivided into four terrestrial ecoregions of the Palearctic ecozone inclusively the Illyrian deciduous forests, Balkan mixed forests, Pindus Mountains mixed forests and Dinaric Mountains mixed forests.
Approximately 3,500 different species of plants can be found in Albania which refer mostly to a Mediterranean and Eurasian character.
The country has a rich tradition of herbal and medicinal practices.
At least 300 plants growing locally are used in the preparation of herbs and medicines.
The trees within the forests are mostly made up of fir, oak, beech and pine.
In the 2010 Environmental Performance Index, Albania was ranked 23th out of 163 countries in the world.
In 2012, the country advanced from 23th to 15th, whereas it had the highest ranking in South and Eastern Europe and Central Asia.
The country was the 24th greenest country in the world according to the 2005 Environmental Sustainability Index.
Nevertheless, for 2016, the country was ranked the 13th best performing country on the Happy Planet Index by the United Nations.
The sovereign state of Albania is a unitary state defined in a total area of .
It is apportioned into 12 counties containing their own council and administration.
The counties are the country's primary administrative divisions and further subdivided into 61 municipalities.
They are responsible for geographical, economic, social and cultural purposes inside the counties.
The counties were created on 31 July 2000 to replace the 36 former districts.
The government introduced the new administrative divisions to be implemented in 2015, whereby municipalities were reduced to 61, while the rurals were abolished.
The defunct municipalities are known as neighborhoods or villages.
There are overall 2980 villages or communities in the entire country, formerly known as localities.
The municipalities are the first level of local governance, responsible for local needs and law enforcement.
The largest county in Albania, by population, is Tirana County with over 800,000 people followed by Fier County with over 300,000 people.
The smallest county, by population, is Gjirokastër County with over 70,000 people.
The largest in the county, by area, is Korçë County encompassing of the southeast of Albania followed by Shkodër County with in the northwest of Albania.
The smallest county, by area, is Durrës County with an area of in the west of Albania.
Albania is a parliamentary constitutional republic and sovereign state whose politics operate under a framework laid out in the constitution wherein the president functions as the head of state and the prime minister as the head of government.
The sovereignty is vested in the Albanian people and exercised by the Albanian people through their representatives or directly.
The government is based on the separation and balancing of powers among the legislative, judiciary and executive.
The legislative power is held by the parliament and is elected every four years by a system of party-list proportional representation by the Albanian people on the basis of free, equal, universal and periodic suffrage by secret ballot.
The civil law, codified and based on the Napoleonic Code, is divided between courts with regular civil and criminal jurisdiction and administrative courts.
The judicial power is vested in the supreme court, constitutional court, appeal court and administrative court.
Law enforcement in the country is primarily the responsibility of the Albanian Police, the main and largest state law enforcement agency.
It carries out nearly all general police duties including criminal investigation, patrol activity, traffic policing and border control.
The executive power is exercised by the president and prime minister whereby the power of the president is very limited.
The president is the commander-in-chief of the military and the representative of the unity of the Albanian people.
The tenure of the president depends on the confidence of the parliament and is elected for a five-year term by the parliament by a majority of three-fifths of all its members.
The prime minister, appointed by the president and approved by the parliament, is authorized to constitute the cabinet.
The cabinet is composed primarily of the prime minister inclusively its deputies and ministers.
In the time since the end of communism and isolationism, Albania has extended its responsibilities and position in continental and international affairs, developing and establishing friendly relations with other countries around the world.
Its main objectives are the accession into the European Union, the international recognition of Kosovo and the expulsion of Cham Albanians as well as helping and protecting the rights of the Albanians in Kosovo, Montenegro, Macedonia, Greece, Serbia, Italy and Diaspora.
The admission of Albania into the North Atlantic Treaty Organization was considered by Albanian politicians as a primary ambition for the country.
The country has been extensively engaged with the organization and has maintained its position as a stability factor and a strong ally of the United States and European Union in the troubled and divided region of the Balkans.
Albania and Kosovo are culturally, socially and economically very closely rooted due to the Albanian majority population in Kosovo.
In 1998, the country contributed in supporting allied efforts to end the humanitarian tragedy in Kosovo and secure the peace after the NATO bombing of Yugoslavia.
Albania enjoys friendly and close ties with the United States ever after it supported the country's independence and its democracy.
In present day, the two countries have maintained close economic and defense relations and have signed a number of agreements and treaties.
In 2007, Albania welcomed George W. Bush who became the first President of the United States ever to visit the country.
Albania has been an active member of the United Nations since 1955.
They country took on membership for the United Nations Economic and Social Council from 2005 to 2007 as well as in 2012.
It served as vice president of the ECOSOC in 2006 and 2013.
In 2014, it also joined the United Nations Human Rights Council from 2015 to 2017 and was elected vice president in 2015.
Albania is a full member of numerous international organizations inclusively the Council of Europe, International Organization for Migration, World Health Organization, Union for the Mediterranean, Organisation of Islamic Cooperation, Organization for Security and Cooperation in Europe, International Monetary Fund, World Trade Organization and La Francophonie.
The Albanian Armed Forces consist of Land, Air and Naval Forces and constitute the military and paramilitary forces of the country.
They are led by a commander-in-chief under the supervision of the Ministry of Defense and by the President as the supreme commander during wartime however, in times of peace its powers are executed through the Prime Minister and the Defence Minister.
The chief purpose of the armed forces of Albania is the defence of the independence, the sovereignty and the territorial integrity of the country, as well as the participation in humanitarian, combat, non-combat and peace support operations.
Military service is voluntary since 2010 with the age of 19 being the legal minimum age for the duty.
Albania has committed to increase the participations in multinational operations.
Since the fall of communism, the country has participated in six international missions but participated in only one United Nations mission in Georgia sending of 3 military observers.
Since February 2008, Albania has participated officially in NATO's Operation Active Endeavor in the Mediterranean Sea.
It was invited to join NATO on 3 April 2008, and it became a full member on 2 April 2009.
Albania reduced the number of active troops from 65,000 in 1988 to 14,500 in 2009.
The military now consists mainly of a small fleet of aircraft and sea vessels.
In the 1990s, the country scrapped enormous amounts of obsolete hardware from China, such as tanks and SAM systems.
Increasing the military budget was one of the most important conditions for NATO integration.
Military spending has generally been low.
As of 1996 military spending was an estimated 1.5% of the country's GDP, only to peak in 2009 at 2% and fall again to 1.5%.
The transition from a socialist planned economy to a capitalist mixed economy in Albania has been largely successful.
The country has a developing mixed economy classified by the World Bank as an upper-middle income economy.
In 2016, it had the 4th lowest unemployment rate in the Balkans with an estimated value of 14.7%.
Its largest trading partners are Italy, Greece, China, Spain, Kosovo and the United States.
The lek (ALL) is the country's currency and is pegged at approximately 132,51 lek per euro.
The cities of Tirana and Durrës constitute the economic and financial heart of Albania due to their high population, good infrastructure and geographical location.
The country's most important infrastructure facilities take course through both of the cities, connecting the north to the south as well as the west to the east.
Among the largest companies are the petroleum Taçi Oil, Albpetrol, ARMO and Kastrati, the mineral AlbChrome, the cement Antea, the investment BALFIN Group and the technology Albtelecom, Vodafone, Telekom Albania and others.
In 2012, Albania's GDP per capita stood at 30% of the European Union average, while GDP (PPP) per capita was 35%.
Albania were one of three countries in Europe to record an economic growth in the first quarter of 2010 after the global financial crisis.
The International Monetary Fund predicted 2.6% growth for Albania in 2010 and 3.2% in 2011.
According to the Forbes , the Gross Domestic Product (GDP) was growing at 2.8%.
The country had a trade balance of −9.7% and unemployment rate of 14.7%.
The Foreign direct investment has increased significantly in recent years as the government has embarked on an ambitious program to improve the business climate through fiscal and legislative reforms.
The economy is expected to expand in the near term, driven by a recovery in consumption and robust investments.
Growth is projected to be 3.2% in 2016, 3.5% in 2017, and 3.8% in 2018.
Agriculture in the country is based on small to medium-sized family-owned dispersed units.
It remains a significant sector of the economy of Albania.
It employs 41% of the population, and about 24.31% of the land is used for agricultural purposes.
One of the earliest farming sites in Europe has been found in the southeast of the country.
As part of the pre-accession process of Albania to the European Union, farmers are being aided through IPA funds to improve Albanian agriculture standards.
Albania produces significant amounts of fruits (apples, olives, grapes, oranges, lemons, apricots, peaches, cherries, figs, sour cherries, plums, and strawberries), vegetables (potatoes, tomatoes, maize, onions, and wheat), sugar beets, tobacco, meat, honey, dairy products, traditional medicine and aromatic plants.
Further, the country is a worldwide significant producer of salvia, rosemary and yellow gentian.
The country's proximity to the Ionian Sea and the Adriatic Sea give the underdeveloped fishing industry great potential.
The World Bank and European Community economists report that, Albania's fishing industry has good potential to generate export earnings because prices in the nearby Greek and Italian markets are many times higher than those in the Albanian market.
The fish available off the coasts of the country are carp, trout, sea bream, mussels and crustaceans.
Albania has one of Europe's longest histories of viticulture.
The today's region was one of the few places where vine was naturally grown during the ice age.
The oldest found seeds in the region are 4,000 to 6,000 years old.
In 2009, the nation produced an estimated 17,500 tonnes of wine.
During the communism, the production area expanded to some .
The secondary sector of Albania have undergone many changes and diversification, since the collapse of the communist regime in the country.
It is very diversified, from electronics, manufacturing, textiles, to food, cement, mining, and energy.
The Antea Cement plant in Fushë-Krujë is considered as one of the largest industrial greenfield investments in the country.
Albanian oil and gas is represents of the most promising albeit strictly regulated sectors of its economy.
Albania has the second largest oil deposits in the Balkan peninsula after Romania, and the largest oil reserves in Europe.
The Albpetrol company is owned by the Albanian state and monitors the state petroleum agreements in the country.
The textile industry has seen an extensive expansion by approaching companies from the European Union (EU) in Albania.
According to the Institute of Statistics (INSTAT) , the textile production marked an annual growth of 5.3% and an annual turnover of around 1.5 billion euros.
Albania is a significant minerals producer and is ranked among the world's leading chromium producers and exporters.
The nation is also a notable producer of copper, nickel and coal.
The Batra mine, Bulqizë mine, and Thekna mine are among the most recognised Albanian mines that are still in operation.
The tertiary sector represents the fastest growing sector of the country's economy.
36% of the population work in the service sector which contributes to 65% of the country's GDP.
Ever since the end of the 20th century, the banking industry is a major component of the tertiary sector and remains in good conditions overall due to privatization and the commendable monetary policy.
Previously one of the most isolated and controlled countries in the world, telecommunication industry represents nowadays another major contributor to the sector.
It developed largely through privatization and subsequent investment by both domestic and foreign investors.
Albanian Mobile, Plus, Eagle, Vodafone and Telekom Albania are the leading telecommunications provider in the country.
Tourism is recognised as an industry of national importance and has been steadily increasing since the beginnings of the 21st century.
It directly accounted for 8.4% of GDP in 2016 though including indirect contributions pushes the proportion to 26%.
In the same year, the country received approximately 4.74 million visitors mostly from across Europe and the United States as well.
The increase of foreign visitors has been dramatic.
Albania had only 500,000 visitors in 2005, while in 2012 had an estimated 4.2 million, an increase of 740 percent in only 7 years.
In 2015, tourism in summer increased by 25 percent in contrast the previous year, accouring to the country's tourism agency.
In 2011, Lonely Planet named as a top travel destination, while The New York Times placed Albania as number 4 global touristic destination in 2014.
The bulk of the tourist industry is concentrated along the Adriatic and Ionian Sea in the west of the country.
However, the Albanian Riviera in the southwest has the most scenic and pristine beaches, and is often called the pearl of the Albanian coast.
Its coastline has a considerable length of .
The coast has a particular character because it is rich in varieties of virgin beaches, capes, coves, covered bays, lagoons, small gravel beaches, sea caves and many landforms.
Some parts of this seaside are very clean ecologically, which represent in this prospective unexplored areas, which are very rare within the Mediterranean.
Other attractions include the mountainous areas such as the Albanian Alps, Ceraunian Mountains and Korab Mountains but also the historical cities of Berat, Durrës, Gjirokastër, Sarandë, Shkodër and Korçë.
Transportation in Albania has undergone significant changes and improvements in the past two decades.
Improvements to the public transport, road and rail infrastructure, water and air travel have all led to a vast improvement in transportation.
The international airport of Tirana serves as the premier gateway to the country carrying almost 2 million passengers per year with connections to many destinations around Europe.
The country plans to increase the number of airports especially in the south with possible locations in Sarandë, Gjirokastër and Vlorë.
The roads of Albania are well maintained and still under construction.
The A1 is the longest motorway of the country and represents a major transportation corridor in Albania.
It will prospectively link Durrës on the Adriatic Sea across Pristina in Kosovo with the Pan-European Corridor X in Serbia.
The A2 is part of the Adriatic–Ionian Corridor as well as the Pan-European Corridor VIII and connects Fier with Vlorë.
The A3 is currently under construction and will connect, after its completion, Tirana and Elbasan with the Pan-European Corridor VIII.
When all three corridors are completed, Albania will have an estimated of highway linking it with all of its neighboring countries.
Durrës is the busiest and largest seaport in the country followed by Vlorë, Shëngjin and Sarandë.
As of 2014, it is as one of the largest passenger ports on the Adriatic Sea with annual passenger volume of approximately 1.5 million.
The ports serve a system of ferries connecting numerous islands and coastal cities in Croatia, Greece and Italy.
The rail network is administered by the national railway company Hekurudha Shqiptare which was extensively promoted by the dictator Enver Hoxha.
There has been a considerable increase in car ownership and bus usage while rail use decreased since the end of communism.
However a new railway line from Tirana and its airport to Durrës is currently planned.
The location of this railway, connecting the most populated urban areas in Albania, makes it an important economic development project.
In the country, education is secular, free, compulsory and based on three levels of education among others primary, secondary and tertiary education.
The academic year is apportioned into two semesters beginning in September or October and ending in June or July.
The use of the Albanian language serves as the primary language of instruction in all educational institutions across the country.
Compulsory primary education is divided into two levels, elementary and secondary school, from grade one to five and six to nine, respectively.
Pupils are required to attend school from the age of six until they turn 16.
Upon successful completion of primary education, all pupils are entitled to attend high schools with specializing in any particular field inclusively arts, sports, languages, sciences and technology.
The country's tertiary education, an optional stage of formal learning following secondary education, has undergone a thorough reformation and restructuring in compliance with the principles of the Bologna Process.
There is a significant number of private and public institutions of higher education well dispersed in the major cities of Albania.
Studies in tertiary education are organized at three successive levels among other bachelor, master and doctorate.
The study of a first foreign language is mandatory and are taught most often at elementary and bilingual schools in the county.
The languages which are taught in schools are English, Italian, French and German.
The country has a school life expectancy of 16 years and a literacy rate of 98.7%, with 99.2% for males and 98.3% for females.
Albania is mostly dependent on hydroelectricity.
Almost 94.8% of the country's electricity consumption comes from hydroelectrical stations and ranks 7th in the world by percentage.
There are six hydroelectric power stations, including Fierza, Koman, Skavica and Vau i Dejës situated within the Drin River.
Further, there are two stations under construction, namely Banjë and Moglicë located in the Devoll River.
Both are expected to be completed between 2016 and 2018.
Albania has considerably large deposits of oil.
It has the 10th largest oil reserves in europe and the 58th in the world.
The country's main petroleum deposits are located around the Albanian Adriatic Sea Coast and Myzeqe Plain within the Western Lowlands, where the country's largest reserve is located.
Although, Patos-Marinza, also located within the area, is the largest onshore oil field in Europe.
In 2015, of natural gas pipelines and of oil pipelines spanned the country's territory.
The planned Trans Adriatic Pipeline, a major trans Adriatic Sea gas pipeline, will delivers natural gas from Azerbaijan to Albania and Western Europe through Italy and will be completed in 2020.
Further, Albania and Croatia have discussed the possibility of jointly building a nuclear power plant at Lake Shkodër, close to the border with Montenegro, a plan that has gathered criticism from Montenegro due to seismicity in the area.
In 2009, the company Enel announced plans to build an 800 MW coal-fired power plant in the country, to diversify electricity sources.
With the political and economic changings in 1993, human resources in sciences and technology have drastically decreased.
As of various reports, during 1991 to 2005 approximately 50% of the professors and scientists of the universities and science institutions in the country have left Albania.
In 2009, the government approved the National Strategy for Science, Technology and Innovation in Albania covering the period 2009 to 2015.
It aims to triple public spending on research and development to 0.6% of GDP and augment the share of GDE from foreign sources, including the framework programmes for research of the European Union, to the point where it covers 40% of research spending, among others.
Albania has an estimated 257 media outlets, including 66 radio stations and 67 television stations, with 65 national and more than 50 cable television stations.
Radio began officially in 1938 with the founding of Radio Televizioni Shqiptar, while television broadcasting began in 1960.
4 regional radio stations serve in the four extremities of the country.
The international service broadcasts radio programmes in Albanian including seven other languages through medium wave and short wave, using the theme from the song "Keputa një gjethe dafine" as its signature tune.
The international television service through satellite was launched since 1993 and aims at Albanian communities in the neighboring countries and the Albanian diaspora.
Nowadays, the country has organized several shows as a part of worldwide series like "Dancing with the Stars", "Big Brother", "Got Talent", "The Voice" and "X Factor".
The constitution of Albania guarantees equal, free and universal health care for all its citizens.
The health care system of the country is currently organized in three levels, among others primary, secondary and tertiary healthcare, and is in a process of modernisation and development.
The life expectancy at birth in Albania is at 77.8 years and ranks 37th in the world outperforming several developed countries.
The average healthy life expectancy is at 68.8 years and ranks as well 37th in the world.
The country's infant mortality rate is estimated at 12 per 1,000 live births in 2015.
In 2000, the country had the 55th best healthcare performance in the world, as defined by the World Health Organization.
Cardiovascular disease is the principal cause of death in the country accounting 52% of total deaths.
Accidents, injuries, malignant and respiratory diseases are other major causes of death.
Neuropsychiatric disease has increased also due to recent demographic, social and economic changes in the country.
In 2009, the country had a fruit and vegetable supply of 886 grams per capita per day, the fifth highest supply in Europe.
In comparison to other developed and developing countries, Albania has a relatively low rate of obesity probably thanks to the health benefits of the Mediterranean diet.
According to World Health Organisation data from 2016, 21.7% of adults in the country are clinically obese, with a Body Mass Index (BMI) score of 25 or more.
The population of Albania, as defined by Institute of Statistics, was estimated in 2016 to be approximately 2,886,026.
The country's total fertility rate of 1.51 children born per woman is one of the lowest in the world.
Its population density stands at 259 inhabitants per square kilometre.
The overall life expectancy at birth is 78.5 years; 75.8 years for males and 81.4 years for females.
The country is the 8th most populous country in the Balkans and ranks as the 137th most populous country in the world.
The population of the country rose steadily from 2,5 million in 1979 until 1989, when it peaked at 3.1 million.
It is forecasted that the population should not reach its peak number of 1989 until 2031, depending on the actual birth rate and the level of net migration.
The explanation for the recent population decrease is the fall of communism in Albania.
It was marked by large economic mass emigration from Albania to Greece, Italy and the United States.
40 years of isolation from the world, combined with its disastrous economic, social and political situation, had caused this exodus.
The external migration was prohibited outright during the communism, while internal migration was quite limited, hence this was a new phenomenon.
At least, 900,000 people left Albania during this period, about 600,000 of them settling in Greece.
The migration affected the country's internal population distribution.
It decreased particularly in the north and south, while it increased in the center within the cities of Tirana and Durrës.
According to the Institute of Statistics (INSTAT) , the population of Albania is 2,893,005.
About 53.4% of the country's population is living in cities.
The three largest counties by population account for half of the total population.
Almost 30% of the total population is found in Tirana County followed by Fier County with 11% and Durrës County with 10%.
Over 1 million people are concentrated in Tirana and Durrës, making it the largest urban area in Albania.
Tirana is one of largest cities in the Balkan Peninsula and ranks 7th with a population about 800,000.
The second largest city in the country by population is Durrës, with a population of 201.110, followed by Vlorë with a population of 141.513.
Issues of ethnicity are a delicate topic and subject to debate.
Contrary to official statistics that show an over 97 per cent Albanian majority in the country, minority groups (such as Greeks, Macedonians, Montenegrins, Roma and Aromanians) have frequently disputed the official numbers, asserting a higher percentage of the country's population.
According to the disputed 2011 census, ethnic affiliation was as follows: Albanians 2,312,356 (82.6% of the total), Greeks 24,243 (0.9%), Macedonians 5,512 (0.2%), Montenegrins 366 (0.01%), Aromanians 8,266 (0.30%), Romani 8,301 (0.3%), Balkan Egyptians 3,368 (0.1%), other ethnicities 2,644 (0.1%), no declared ethnicity 390,938 (14.0%), and not relevant 44,144 (1.6%).
On the quality of the specific data the Advisory Committee on the Framework Convention for the Protection of National Minorities stated that "the results of the census should be viewed with the utmost caution and calls on the authorities not to rely exclusively on the data on nationality collected during the census in determining its policy on the protection of national minorities.".
Albania recognizes nine national or cultural minorities: Greek, Macedonian, Wallachian, Montenegrin, Serb, Roma, Egyptian, Bosnian and Bulgarian peoples.
Other Albanian minorities are Gorani, Aromanians and Jews.
Regarding the Greeks, "it is difficult to know how many Greeks there are in Albania".
The estimates vary between 60,000 and 300,000 ethnic Greeks in Albania.
According to Ian Jeffries, most of Western sources put the number at around 200,000.
The 300,000 mark is supported by Greek government as well.<ref name="RFE/RL Research Report: Weekly Analyses from the RFE/RL Research Institute"></ref> The CIA World Factbook estimates the Greek minority at 0.9% of the total population.
The US State Department uses 1.17% for Greeks and 0.23% for other minorities.
The latter questions the validity of the census data about the Greek minority, due to the fact that measurements have been affected by boycott.
Macedonian and some Greek minority groups have sharply criticized Article 20 of the Census law, according to which a $1,000 fine will be imposed on anyone who will declare an ethnicity other than what is stated on his or her birth certificate.
This is claimed to be an attempt to intimidate minorities into declaring Albanian ethnicity, according to them the Albanian government has stated that it will jail anyone who does not participate in the census or refuse to declare his or her ethnicity.
Genc Pollo, the minister in charge has declared that: "Albanian citizens will be able to freely express their ethnic and religious affiliation and mother tongue.
However, they are not forced to answer these sensitive questions".
The amendments criticized do not include jailing or forced declaration of ethnicity or religion; only a fine is envisioned which can be overthrown by court.
Greek representatives form part of the Albanian parliament and the government has invited Albanian Greeks to register, as the only way to improve their status.
On the other hand, nationalists, various organizations and political parties in Albania have expressed their concern that the census might artificially increase the numbers of the Greek minority, which might be then exploited by Greece to threaten Albania's territorial integrity.
Albanian is the official language of the Republic of Albania.
Its standard spoken and written form is revised and merged from the two main dialects, Gheg and Tosk, though it is notably based more on the Tosk dialect.
The Shkumbin river is the rough dividing line between the two dialects.
Also a dialect of Greek that preserves features now lost in standard modern Greek is spoken in areas inhabited by the Greek minority.
Other languages spoken by ethnic minorities in Albania include Aromanian, Serbian, Macedonian, Bosnian, Bulgarian, Gorani, and Roma.
Macedonian is official in the Pustec Municipality in East Albania.
According to the 2011 population census, 2,765,610 or 98.767% of the population declared Albanian as their mother tongue ("mother tongue is defined as the first or main language spoken at home during childhood").
Greek is the second most-spoken language in the country, with 0.5 to 3% of the population speaking it as first language, and with two-thirds of mainly Albanian families having at least one member that speaks Greek, most having learned it in the post communist era (1992–present) due to private schools or migration to Greece.
Outside of the small designated "minority area" in the south the teaching of Greek was banned during the communist era.
As of 2003 Greek is offered at over 100 private tutoring centers all over Albania and at a private school in Tirana, the first of its kind outside Greece.
In recent years, the shrinking number of pupils in schools dedicated to the Greek minority has caused problems for teachers.
The Greek language is spoken by an important percentage in the southern part of the country, due to cultural and economic links with adjacent Greece.
In a 2017 study carried out by Instat, the Albanian government statistical agency, 39.9% of the 25–64 years old is able to use at least one foreign language, with English first at 40.0%, followed by Italian with 27.8% and Greek with 22.9%.
Among young people aged 25 or less, English, German and Turkish have seen a rising interest after 2000.
Italian and French have had a stable interest, while Greek has lost most of the interest.
The trends are linked with cultural and economic factors.
The young people have shown a growing interest in German language in recent years.
Some of them go to Germany for studying or various experiences.
Albania and Germany have agreements for cooperating in helping young people of the two countries know both cultures better.
Due to a sharp rise in economic relations with Turkey, interest in learning Turkish, in particular among young people, has been growing on a yearly basis.
Young people, attracted by economic importance of Turkish investments and common values between the two nations, gain from cultural and academic collaboration of universities.
In 2011 Turkish-owned Epoka University, where Turkish along with English and French is taught, was chosen the best foreign-owned university in Albania.
Albania is a secular state without an official religion, with the freedom of religion being a constitutional right.
The 2011 census, for the first time since 1930, included an optional open-ended question on religion; the census recorded a majority of Muslims (58.79%), which include Sunni (56.70%) and Bektashi Muslims (2.09%).
Christians, making up 16.92% of the population, include Roman Catholics (10.03%), Orthodox (6.75%) and evangelical Protestants (0.14%).
Atheists accounted for 2.5% of the population and 5.49% were non-affiliated believers, while 13.79% preferred not to answer.
The preliminary results of the 2011 census seemed to give widely different results, with 70% of respondents refusing to declare belief in any of the listed faiths.
The Albanian Orthodox Church officially refused to recognize the results, claiming that 24% of the total population adhered to its faith.
Some Muslim Community officials expressed unhappiness with the data claiming that many Muslims were not counted and that the number of adherents numbered some 70% of the Albanian population.
The Albanian Catholic Bishops Conference also cast doubts on the census, complaining that many of its believers were not contacted.
The Muslim Albanians are spread throughout the country.
Orthodox and Bektashis are mostly found in the south, whereas Catholics mainly live in the north.
In 2008, there were 694 Catholic churches and 425 orthodox churches, 568 mosques and 70 bektashi tekkes in the country.
Religious tolerance is one of the most important values of the tradition of the Albanians.
It is widely accepted that Albanians generally value a peaceful coexistence among the believers of different religious communities in the country.
During an official visit in Tirana, Pope Francis hailed Albania as model of religious harmony, due to the long tradition of religious coexistence and tolerance.
The country is ranked among the least religious countries in the world.
Furthermore, religion plays an important role in the lives of only 39% of the country's population.
In the WIN/Gallup International Report of 2016, 56% of the Albanian people considered themselves religious, 30% considered themselves non-religious, while 9% defined themselves as convinced atheists; 80% believed in God and 40% believed in life after death.
However, 40% believed in hell, while 42% believed in heaven.
During classical times, there are thought to have been about seventy Christian families in Durrës, as early as the time of the Apostles.
The Archbishopric of Durrës was purportedly founded by Paul the Apostle, while preaching in Illyria and Epirus.
Meanwhile, in medieval times, the Albanian people first appeared within historical records from the Byzantines.
At this point, they were mostly Christianized.
Islam arrived for the first time in the late 9th century to the region, when Arabs raided parts of the eastern banks of the Adriatic Sea.
It later emerged as the majority religion, during the centuries of Ottoman Period, though a significant Christian minority remained.
During modern times, the Albanian republican, monarchic and later communist regimes followed a systematic policy of separating religion from official functions and cultural life.
The country has never had an official religion either as a republic or as a kingdom.
In the 20th century, the clergy of all faiths was weakened under the monarchy and ultimately eradicated during the 1950s and 1960s, under the state policy of obliterating all organized religion from the territories of Albania.
The communist regime persecuted and suppressed religious observance and institutions and entirely banned religion.
The country was then officially declared to be the world's first atheist state.
Although, the country's religious freedom has returned, since the end of communism.
Islam survived communist era persecution and reemerged in the modern era as a practiced religion in Albania.
Some smaller Christian sects in Albania include Evangelicals and several Protestant communities including Seventh-day Adventist Church, Church of Jesus Christ of Latter-day Saints and Jehovah's Witnesses.
The first recorded Protestant of Albania was Said Toptani, who traveled around Europe and returned to Tirana in 1853, where he preached Protestantism.
Due to that, he was arrested and imprisoned by the Ottoman authorities in 1864.
First evangelical Protestants appeared in the 19th century and the Evangelical Alliance was founded in 1892.
Nowadays, it has 160 member congregations from different Protestant denominations.
Albania was the only country in Europe where the Jewish population increased significantly during the Holocaust.
Following the mass emigration to Israel, since the fall of communism, only 200 Albanian Jews are left in the country.
Albania shares many symbols associated with its history, culture and belief.
These include the colours red and black, animals such as the golden eagle living across the country, costumes such as the fustanella, plis and opinga which are worn to special events and celebrations, plants such as the olive and red poppy growing as well across the country.
The flag of Albania is a red flag with a black double-headed eagle in the centre.
The red colour symbolizes the bravery, strength and valour of the Albanian people and the black colour appears as a symbol of freedom and heroism.
Moreover, the eagle has been used by Albanians since the Middle Ages including the establishment of the Principality of Arbër and by numerous noble ruling families such as the Kastrioti, Muzaka, Thopia and Dukagjini.
Gjergj Kastrioti Skënderbeu, who fought and began a rebellion against the Ottoman Empire which halted Ottoman advance into Europe for nearly 25 years, placed the double-headed eagle on his flag and seal.
The country's national motto, Ti Shqipëri, më jep nder, më jep emrin Shqipëtar, finds its origins in the early 19th century.
The first to express this motto was Naim Frashëri in his poem Ti Shqipëri më jep nder.
This poem is notable as refers to the values and ambitions of the Albanian people to remain separate from and independent of, those neighbouring countries around it, which have tried to dominate it.
The duart e kryqëzuara, also referred to as the eagle gesture, is a gesture performed particularly by the Albanian people around the world in order to visually illustrate the double-headed eagle, the symbol of Albania.
Albanian cuisine has evolved over the centuries and has been strongly influenced by the geography and history of Albania.
Previously home to the Illyrians and Ancient Greeks and then subsequently conquered and occupied by the Byzantines, Ottomans and most recently the Italians, it has borrowed elements and styles from those cultures.
The cooking traditions vary especially between the north and the south, due to differing topography and climate, that provides excellent growth conditions for various herbs, vegetables and fruits.
One of the most characteristic element is olive oil, which is the major type of oil used for cooking produced from olive trees prominent throughout the south of the country.
Albanians uses a wide range of ingredients, including a wider availability of vegetables such as beans, tomatoes, peppers, potatoes, onions and garlics, as well as cereals such as wheat, corn and rye.
Herbs and spices include oregano, mint, lavender and basil.
Widely used meat varieties are lamb, beef, veal, chicken and other poultry and pork.
Seafood specialities are particularly popular along the Albanian Adriatic and Ionian Sea Coasts in the west.
Tavë Kosi is a national dish and is prepared with baked lamb as well as eggs and yogurt, while garlic, oregano and other herbs can be added as well.
Petulla, a traditionally fried dough made from wheat or buckwheat flour, is as well a popular speciality and is served with powdered sugar or feta cheese and fruit jams.
Another dish, called Fërgese, is a vegetarian dish made of green and red peppers along with skinned tomatoes and onions; it often served as a side dish to meat dishes.
Also, popular is Flia, consisting of multiple crepe-like layers brushed with cream and served with sour cream.
Krofne, similar to Berliner Pfannkuchen, are filled with jam, marmalade chocolate and often eaten during the cold winter months.
Desserts include Walnut Stuffed Figs that is made of walnuts or candied walnuts and fig syrup.
Although fig is an important agricultural product of Albania.
Bakllavë is also a widely consumed dessert and traditionally eaten during Christmas, Easter and Ramadan.
Tea is enjoyed both at home or outside at cafés, bars or restaurants.
Çaj Mali is enormously beloved and is part of a daily routine for most of the Albanians.
It is cultivated around the south of Albania and noted for its medicinal properties.
Black tea with a slice of lemon and sugar, milk or honey is also a popular type of tea.
Withal, coffee is by far one of the most consumed beverages in Albania, with several types of filter and instant coffee.
Drinking coffee is very much a part of the people's lifestyle.
Wine drinking is popular throughout the Albanians.
The country has a long and ancient history of wine production, as it belongs to the old world of wine producing countries.
Its wine is characterized for its sweet taste and traditionally indigenous varieties.
In 2016, Albania surpassed Spain by becoming the country with the most coffee houses per capita in the world.
In fact, there are 654 coffee houses per 100,000 inhabitants in Albania, a country with only 2.5 million inhabitants.
This is due to coffee houses closing down in Spain due to the economic crisis, and the fact that as many cafes open as they close in Albania.
In addition, the fact that it was one of the easiest ways to make a living after the fall of communism in Albania, together with the country's Ottoman legacy further reinforce the strong dominance of coffee culture in Albania.
The architecture of Albania reflects the legacy of various civilizations tracing back to the classical antiquity.
Major cities in Albania have evolved from within the castle to include dwellings, religious and commercial structures, with constant redesigning of town squares and evolution of building techniques.
Nowadays, the cities and towns reflect a whole spectrum of various architectural styles.
In the 20th century, many historical as well as sacred buildings bearing the ancient influence were demolished during the communism.
Ancient architecture is found throughout Albania and most visible in Byllis, Amantia, Phoenice, Apollonia, Butrint, Antigonia, Shkodër and Durrës.
Considering the long period of rule of the Byzantine Empire, they introduced castles, citadels, churches and monasteries with spectacular wealth of visible murals and frescos.
Perhaps the best known examples can be found in the southern Albanian cities and surroundigs of Korçë, Berat, Voskopojë and Gjirokastër.
Involving the introduction of Ottoman architecture there was a development of mosques and other Islamic buildings, particularly seen in Berat and Gjirokastër.
A productive period of Historicism, Art Nouveau and Neoclassicism merged into the 19th century, best exemplified in Korçë.
The 20th century brought new architectural styles such as the modern Italian style, which is present in Tirana such as the Skanderbeg Square and Ministries.
It is also present in Shkodër, Vlorë, Sarandë and Durrës.
Moreover, other towns received their present-day Albania-unique appearance through various cultural or economic influences.
Socialist classicism arrived during the communism in Albania after the Second World War.
At this period many socialist-styled complexes, wide roads and factories were constructed, while town squares were redesigned and numerous of historic and important buildings demolished.
Notable examples of that style include the Mother Teresa Square, Pyramid of Tirana, Palace of Congresses and so on.
Two Albanian archaeological sites are included in the list of UNESCO World Heritage Sites.
These include the ancient remains of Butrint and the medieval Historic Centres of Berat and Gjirokastër.
Furthermore, the natural and cultural heritage of Ohrid, the royal Illyrian tombs, the remains of Apollonia, the ancient Amphitheatre of Durrës and the Fortress of Bashtovë has been included on the tentative list of Albania.
The Albanian folk music is a prominent section of the national identity and continues to play a major part in Albanian music.
Although, it can be divided into two stylistic groups, as performed by the northern Ghegs and southern Labs and Tosks.
The northern and southern traditions are contrasted by the rugged tone of the north and the relaxed form of the south.
Many of the songs are about events from history and culture, including the traditional themes about honour, hospitality, treachery and revenge.
The first compilation of Albanian folk music was made by two Himariot musicians Neço Muka and Koço Çakali in Paris, during their interpretations with the song Diva Tefta Tashko Koço.
Several gramophone compilations were recorded in those years by this genial trio of artists which eventually led to the recognition of the Himariot Isopolyphonic Music as an UNESCO World Cultural Heritage.
The contemporary music artists Rita Ora, Bebe Rexha, Era Istrefi, Dua Lipa, Bleona, Elvana Gjata, Ermonela Jaho and Inva Mula have achieved international recognition for their music.
Sporano Ermonela Jaho has been described by The Economist as the World's most acclaimed Soprano.
One widely recognized musician from Elbasan is Saimir Pirgu, an Albanian international opera singer.
He was nominated for the 2017 Grammy Award.
Every cultural and geographical region of Albania has its own specific variety of costume that vary in style, material, color, shape, detail and form.
Presently, the national costumes are most often worn with connection to special events and celebrations, mostly at ethnic festivals, religious holidays, weddings and by dancing groups as well.
Some conservative old men and women mainly from the high northern as well as southern regions and wear traditional clothing in their daily lives.
The clothing was made mainly of products from the local agriculture and livestock such as leather, wool, linen, hemp fiber and silk.
Nowadays, the traditional textiles are still embroidered in very collaborate ancient patterns.
The visual arts tradition of Albania has been shaped by the many cultures, which have flourished on its territory.
Once the Byzantines, the Ottomans ruled over Albania for nearly five centuries, which greatly affected the country's artwork and artistic forms.
After Albania's joining with the Ottoman Empire in 1478, Ottoman influenced art forms such as mosaic and mural paintings became prevalent and no real artistic change occurred until the independence in 1912.
Following mosaics and murals from Antiquity and the Middle Ages, the first paintings were icons Byzantine traditions.
Albanian earliest icons date from the late 13th century and generally estimated that their artistic peak reached in the 18th century.
Among the most prominent representatives of the Albanian iconographic art were Onufri and David Selenica.
The museums of Berat, Korçë and Tirana houses good collections remaining icons.
By the end of the Ottoman rule, the painting was limited mostly to folk art and ornate mosques.
Paintings and sculpture arose in the first half of the twentieth century and reached a modest peak in the 1930s and 1940s, when the first organized art exhibitions at national level.
Contemporary Albanian artwork captures the struggle of everyday Albanians, however new artists are utilizing different artistic styles to convey this message.
Albanian artists continue to move art forward, while their art still remains distinctively Albanian in content.
Though among Albanian artist post-modernism was fairly recently introduced, there is a number of artists and works known internationally.
Among the most prominent Albanian post-modernist are considered Anri Sala, Sislej Xhafa, and Helidon Gjergji.
The Albanian language comprises its own branch of the Indo-European language family.
The language is considered an isolate within the Indo-European.
The only other languages that are the sole surviving member of a branch of Indo-European are Armenian and Greek.
It was proved to be an Indo-European language in 1854 by the German philologist Franz Bopp.
Albanian is often held to be related to the Illyrian languages, a language spoken in the Balkans during classical times.
Scholars argue that Albanian derives from Illyrian while some others claim that it derives from Daco-Thracian.
(Illyrian and Daco-Thracian, however, might have been closely related languages; see Thraco-Illyrian.)
The cultural renaissance was first of all expressed through the development of the Albanian language in the area of church texts and publications, mainly of the Catholic region in the northern of Albania, but also of the Orthodox in the south.
The Protestant reforms invigorated hopes for the development of the local language and literary tradition, when cleric Gjon Buzuku brought into the Albanian language the Catholic liturgy, trying to do for the Albanian language, what Martin Luther did for the German language.
Meshari ("The Missal") written by Gjon Buzuku was published in 1555 and is considered as one of the first literary work of written Albanian during the Middle Ages.
The refined level of the language and the stabilised orthography must be the result of an earlier tradition of written Albanian, a tradition that is not well understood.
However, there is some fragmented evidence, pre-dating Buzuku, which indicates that Albanian was written from at least the 14th century.
The earliest evidence dates from 1332 AD with a Latin report from the French Dominican Guillelmus Adae, Archbishop of Antivari, who wrote that Albanians used Latin letters in their books although their language was quite different from Latin.
Other significant examples include: a baptism formula ("Unte paghesont premenit Atit et Birit et spertit senit") from 1462, written in Albanian within a Latin text by the Bishop of Durrës, Pal Engjëlli; a glossary of Albanian words of 1497 by Arnold von Harff, a German who had travelled through Albania, and a 15th-century fragment of the Bible from the Gospel of Matthew, also in Albanian, but written in Greek letters.
Albanian writings from these centuries must not have been religious texts only, but historical chronicles too.
They are mentioned by the humanist Marin Barleti, who in his book Siege of Shkodër ("Rrethimi i Shkodrës") from 1504, confirms that he leafed through such chronicles written in the language of the people ("in vernacula lingua") as well as his famous biography of Skanderbeg Historia de vita et gestis Scanderbegi Epirotarum principis ("History of Skanderbeg") from 1508.
The "History of Skanderbeg" is still the foundation of Skanderbeg studies and is considered an Albanian cultural treasure, vital to the formation of Albanian national self-consciousness.
During the 16th and the 17th centuries, the catechism ("E mbësuame krishterë") (Christian Teachings) from 1592 written by Lekë Matrënga, ("Doktrina e krishterë") (The Christian Doctrine) from 1618 and ("Rituale romanum") 1621 by Pjetër Budi, the first writer of original Albanian prose and poetry, an apology for George Castriot (1636) by Frang Bardhi, who also published a dictionary and folklore creations, the theological-philosophical treaty "Cuneus Prophetarum" (The Band of Prophets) (1685) by Pjetër Bogdani, the most universal personality of Albanian Middle Ages, were published in Albanian.
The most famous Albanian writer in the 20th and 21st century is probably Ismail Kadare.
He has been mentioned as a possible recipient of the Nobel Prize in Literature several times.
Cinematography became popular in the 20th century, when foreign films and documentaries were shown in the cities of Shkodër and Korçë.
The first public showing to occur in Albania was a little-known title, Paddy the Reliable a comical story.
The first Albanian films were mostly documentaries; the first was about the Monastir Congress that sanctioned the Albanian alphabet in 1908.
During communism, the Albanian Film Institute that later became known as Kinostudio Shqipëria e Re was founded with Soviet assistance, focusing mostly on propaganda of wartime struggles.
After 1945, the communist government founded the Kinostudio Shqipëria e Re in 1952.
This was followed by the first Albanian epic film, the Great Warrior Skanderbeg, a cooperation with Soviet artists chronicling the life and fight of the Albanian national hero Skanderbeg.
In addition the film was awarded the International Prize at the 1954 Cannes Film Festival.
By 1990, about 200 movies had been produced, and Albania had over 450 theaters.
With the economic transition after the collapse of communism in the 1990s, the Kinostudio was broken up and privatised.
A new National Center of Cinematography was established, while cities built modern cinema theatres showing mostly American movies.
The Tirana International Film Festival was established in 2003 and has become the premier and largest film festival in the country as well as in the Balkans.
Durrës hosts the International Film Summerfest of Durrës, the second largest international film festival in the country which takes place every year in late August or early September in Durrës Amphitheatre.
Notable Albanian film directors include Andamion Murataj, Besim Sahatçiu, Xhanfize Keko, Dhimitër Anagnosti, Kujtim Çashku, Luljeta Hoxha, Saim Kokona, Saimir Kumbaro, Kristaq Mitro, Leon Qafzezi and Gjergj Xhuvani.
Famous actors in Albania include Nik Xhelilaj, Klement Tinaj, Masiela Lusha, Blerim Destani, Aleksandër Moisiu, Tinka Kurti, Pjetër Malota, Sandër Prosi and Margarita Xhepa.
There are internationally renowned actors in the Albanian diaspora, such as the Albanian-Americans Eliza Dushku, Jim and John Belushi, Kosovo-Albanians Bekim Fehmiu and Arta Dobroshi and Turkish-Albanian Barish Arduç.
Albania participated at the Olympic Games in 1972 for the first time.
The country made their Winter Olympic Games debut in 2006.
Albania missed the next four games, two of them due to the 1980 and 1984 boycotts, but returned for the 1992 games in Barcelona.
Since then, Albania have participated in all games.
Albania normally competes in events that include swimming, athletics, weightlifting, shooting and wrestling.
The country have been represented by the National Olympic Committee of Albania since 1972.
The nation has participated at the Mediterranean Games since the games of 1987 in Syria.
The Albanian athletes have won a total of 43 (8 gold, 17 silver and 18 bronze) medals from 1987 to 2013.
Popular sports in Albania include Football, Weightlifting, Basketball, Volleyball, Tennis, Swimming, Rugby, and Gymnastics.
Football is by far the most popular sport in Albania.
It is governed by the Football Association of Albania (, F.SH.F.
), which was created in 1930 and has membership in FIFA and UEFA.
Football arrived in Albania early in the 20th century when the inhabitants of the northern city of Shkodër were surprised to see a strange game being played by students at a Christian mission.
The Albania national football team, ranking 51st in the World in 2017 (highest 22nd on 22 August 2015) have won the 1946 Balkan Cup and the Malta Rothmans International Tournament 2000, but had never participated in any major UEFA or FIFA tournament, until UEFA Euro 2016, Albania's first ever appearance at the continental tournament and at a major men's football tournament.
Albania scored their first ever goal in a major tournament and secured their first ever win in European Championship when they beat Romania by 1–0 in a UEFA Euro 2016 match on 19 June 2016.
The most successful football clubs in the country are Skënderbeu, KF Tirana, Dinamo Tirana, Partizani and Vllaznia.
Weightlifting is one of the most successful individual sport for the Albanians, with the national team winning medals at the European Weightlifting Championships and the rest international competitions.
Albanian weightlifters have won a total of 16 medals at the European Championships with 1 of them being gold, 7 silver and 8 bronze.
In the World Weightlifting Championships, the Albanian weightlifting team has won in 1972 a gold in 2002 a silver and in 2011 a bronze medal.
Historically, the Albanian people have established several communities in many regions throughout Southern Europe.
The Albanian diaspora has been formed since the late Middle Ages, when they emigrated to places such as Italy, especially in Sicily and Calabria, and Greece to escape either various socio-political difficulties or the Ottoman conquest of Albania.
Following the fall of communism, large numbers of Albanians have migrated to countries such as Australia, Canada, France, Germany, Greece, Italy, Scandinavia, Switzerland, United Kingdom and the United States.
Albanian minorities are present in the neighboring countries of Macedonia, Montenegro and Serbia.
In Kosovo, Albanians make up the largest ethnic group in the country.
Altogether, the number of ethnic Albanian living abroad its territory is estimated to be higher than the total population inside the territory of Albania.
</doc>
<doc id="740" url="https://en.wikipedia.org/wiki?curid=740" title="Allah">
Allah

Allah (; , ) is the Arabic word for God in Abrahamic religions.
In the English language, the word generally refers to God in Islam.
The word is thought to be derived by contraction from "al-ilāh", which means "the god", and is related to "El" and "Elah", the Hebrew and Aramaic words for God.
The word "Allah" has been used by Arabic people of different religions since pre-Islamic times.
More specifically, it has been used as a term for God by Muslims (both Arab and non-Arab) and Arab Christians.
It is also often, albeit not exclusively, used in this way by Bábists, Bahá'ís, Mandaeans, Indonesian and Maltese Christians, and Mizrahi Jews.
Similar usage by Christians and Sikhs in West Malaysia has recently led to political and legal controversies.
The etymology of the word "Allāh" has been discussed extensively by classical Arab philologists.
Grammarians of the Basra school regarded it as either formed "spontaneously" ("murtajal") or as the definite form of "lāh" (from the verbal root "lyh" with the meaning of "lofty" or "hidden").
Others held that it was borrowed from Syriac or Hebrew, but most considered it to be derived from a contraction of the Arabic definite article "al-" "the" and ' "deity, god" to ' meaning ""the deity"", or ""the God"".
The majority of modern scholars subscribe to the latter theory, and view the loanword hypothesis with skepticism.
Cognates of the name "Allāh" exist in other Semitic languages, including Hebrew and Aramaic.
The corresponding Aramaic form is "Elah" (), but its emphatic state is "Elaha" ().
It is written as ("ʼĔlāhā") in Biblical Aramaic and ("ʼAlâhâ") in Syriac as used by the Assyrian Church, both meaning simply "God".
Biblical Hebrew mostly uses the plural (but functional singular) form "Elohim" (), but more rarely it also uses the singular form "Eloah" ().
Regional variants of the word "Allah" occur in both pagan and Christian pre-Islamic inscriptions.
Different theories have been proposed regarding the role of Allah in pre-Islamic polytheistic cults.
Some authors have suggested that polytheistic Arabs used the name as a reference to a creator god or a supreme deity of their pantheon.
The term may have been vague in the Meccan religion.
According to one hypothesis, which goes back to Julius Wellhausen, Allah (the supreme deity of the tribal federation around Quraysh) was a designation that consecrated the superiority of Hubal (the supreme deity of Quraysh) over the other gods.
However, there is also evidence that Allah and Hubal were two distinct deities.
According to that hypothesis, the Kaaba was first consecrated to a supreme deity named Allah and then hosted the pantheon of Quraysh after their conquest of Mecca, about a century before the time of Muhammad.
Some inscriptions seem to indicate the use of Allah as a name of a polytheist deity centuries earlier, but we know nothing precise about this use.
Some scholars have suggested that Allah may have represented a remote creator god who was gradually eclipsed by more particularized local deities.
There is disagreement on whether Allah played a major role in the Meccan religious cult.
No iconic representation of Allah is known to have existed.
Muhammad's father's name was meaning "the slave of Allāh".
The Aramaic word for "God" in the language of Assyrian Christians is "ʼĔlāhā", or "Alaha".
Arabic-speakers of all Abrahamic faiths, including Christians and Jews, use the word "Allah" to mean "God".
The Christian Arabs of today have no other word for "God" than "Allah".
(Even the Arabic-descended Maltese language of Malta, whose population is almost entirely Roman Catholic, uses "Alla" for "God".)
Arab Christians, for example, use the terms ' () for God the Father, ' () for God the Son, and "" () for God the Holy Spirit.
(See God in Christianity for the Christian concept of God.)
Arab Christians have used two forms of invocations that were affixed to the beginning of their written works.
They adopted the Muslim ', and also created their own Trinitized ' as early as the 8th century.
The Muslim ' reads: "In the name of God, the Compassionate, the Merciful."
The Trinitized ' reads: "In the name of Father and the Son and the Holy Spirit, One God."
The Syriac, Latin and Greek invocations do not have the words "One God" at the end.
This addition was made to emphasize the monotheistic aspect of Trinitarian belief and also to make it more palatable to Muslims.
According to Marshall Hodgson, it seems that in the pre-Islamic times, some Arab Christians made pilgrimage to the Kaaba, a pagan temple at that time, honoring Allah there as God the Creator.
Some archaeological excavation quests have led to the discovery of ancient pre-Islamic inscriptions and tombs made by Arab Christians in the ruins of a church at Umm el-Jimal in Northern Jordan, which contained references to Allah as the proper name of God, and some of the graves contained names such as "Abd Allah" which means "the servant/slave of Allah".
The name Allah can be found countless times in the reports and the lists of names of Christian martyrs in South Arabia, as reported by antique Syriac documents of the names of those martyrs from the era of the Himyarite and Aksumite kingdoms

A Christian leader named Abd Allah ibn Abu Bakr ibn Muhammad was martyred in Najran in 523, as he had worn a ring that said "Allah is my lord".
In an inscription of Christian martyrion dated back to 512, references to Allah can be found in both Arabic and Aramaic, which called him "Allah" and "Alaha", and the inscription starts with the statement "By the Help of Allah".
In pre-Islamic Gospels, the name used for God was "Allah", as evidenced by some discovered Arabic versions of the New Testament written by Arab Christians during the pre-Islamic era in Northern and Southern Arabia.
Pre-Islamic Arab Christians have been reported to have raised the battle cry ""Ya La Ibad Allah"" (O slaves of Allah) to invoke each other into battle.
"Allah" was also mentioned in pre-Islamic Christian poems by some Ghassanid and Tanukhid poets in Syria and Northern Arabia.
In Islam, "Allah" is the unique, omnipotent and only deity and creator of the universe and is equivalent to God in other Abrahamic religions.
According to Islamic belief, Allah is the most common word to represent God, and humble submission to his will, divine ordinances and commandments is the pivot of the Muslim faith.
"He is the only God, creator of the universe, and the judge of humankind."
"He is unique (') and inherently one ('), all-merciful and omnipotent."
The Qur'an declares "the reality of Allah, His inaccessible mystery, His various names, and His actions on behalf of His creatures."
In Islamic tradition, there are 99 Names of God ("" lit.
meaning: 'the best names' or 'the most beautiful names'), each of which evoke a distinct characteristic of Allah.
All these names refer to Allah, the supreme and all-comprehensive divine name.
Among the 99 names of God, the most famous and most frequent of these names are "the Merciful" ("al-Raḥmān") and "the Compassionate" ("").
Most Muslims use the untranslated Arabic phrase "" (meaning 'if God wills') after references to future events.
Muslim discursive piety encourages beginning things with the invocation of "" (meaning 'in the name of God').
There are certain phrases in praise of God that are favored by Muslims, including "" (Holiness be to God), "" (Praise be to God), "" (There is no deity but God) and "" (God is greater) as a devotional exercise of remembering God (dhikr).
In a Sufi practice known as "dhikr Allah" (lit.
remembrance of God), the Sufi repeats and contemplates on the name "Allah" or other divine names while controlling his or her breath.
According to Gerhard Böwering, in contrast with pre-Islamic Arabian polytheism, God in Islam does not have associates and companions, nor is there any kinship between God and jinn.
Pre-Islamic pagan Arabs believed in a blind, powerful, inexorable and insensible fate over which man had no control.
This was replaced with the Islamic notion of a powerful but provident and merciful God.
According to Francis Edward Peters, "The Qur’ān insists, Muslims believe, and historians affirm that Muhammad and his followers worship the same God as the Jews ().
The Qur’an's Allah is the same Creator God who covenanted with Abraham".
Peters states that the Qur'an portrays Allah as both more powerful and more remote than Yahweh, and as a universal deity, unlike Yahweh who closely follows Israelites.
In order to pronounce the word Allah correctly, one has to focus on the second “l” (ل) in Allah (الله).
When the word Allah is preceded by the vowel “a” (فَتْحة) or the vowel “u” (ضَمّة), then the Lām is pronounced in a distinct heavy manner – with Tafkhīm.
This heavy Lām is thus articulated with the entire body of the tongue rather than its tip alone.
For example, verse 58:22: “man haddaAllah” (ِمَنْ حَادَّ الله) which means: those who oppose Allah.
If, however, the preceding vowel is “i” (كَسْرة), then the Lām in Allah is light, such as in the Basmala: Bismillahi… (ِبِسْمِ الله الرَّحْمٰنِ الرَّحِيمِ).
So if a Muslim says “Bismillahi”, he should not pronounce the Lām with a heavy emphasis – instead, just with the tip of the tongue.
The history of the name "Allāh" in English was probably influenced by the study of comparative religion in the 19th century; for example, Thomas Carlyle (1840) sometimes used the term Allah but without any implication that Allah was anything different from God.
However, in his biography of Muḥammad (1934), Tor Andræ always used the term "Allah", though he allows that this "conception of God" seems to imply that it is different from that of the Jewish and Christian theologies.
Languages which may not commonly use the term "Allah" to denote God may still contain popular expressions which use the word.
For example, because of the centuries long Muslim presence in the Iberian Peninsula, the word "ojalá" in the Spanish language and "oxalá" in the Portuguese language exist today, borrowed from Arabic (Arabic: إن شاء الله).
This phrase literally means 'if God wills' (in the sense of "I hope so").
The German poet Mahlmann used the form "Allah" as the title of a poem about the ultimate deity, though it is unclear how much Islamic thought he intended to convey.
Some Muslims leave the name "Allāh" untranslated in English.
The word has also been applied to certain living human beings as personifications of the term and concept.
Christians in Malaysia and Indonesia use "Allah" to refer to God in the Malaysian and Indonesian languages (both of them standardized forms of the Malay language).
Mainstream Bible translations in the language use "Allah" as the translation of Hebrew "Elohim" (translated in English Bibles as "God").
This goes back to early translation work by Francis Xavier in the 16th century.
The first dictionary of Dutch-Malay by Albert Cornelius Ruyl, Justus Heurnius, and Caspar Wiltens in 1650 (revised edition from 1623 edition and 1631 Latin edition) recorded "Allah" as the translation of the Dutch word "".
Ruyl also translated the Gospel of Matthew in 1612 into the Malay language (an early Bible translation into a non-European language,
made a year after the publication of the King James Version), which was printed in the Netherlands in 1629.
Then he translated the Gospel of Mark, published in 1638.
The government of Malaysia in 2007 outlawed usage of the term "Allah" in any other but Muslim contexts, but the Malayan High Court in 2009 revoked the law, ruling it unconstitutional.
While "Allah" had been used for the Christian God in Malay for more than four centuries, the contemporary controversy was triggered by usage of "Allah" by the Roman Catholic newspaper "The Herald".
The government appealed the court ruling, and the High Court suspended implementation of its verdict until the hearing of the appeal.
In October 2013 the court ruled in favor of the government's ban.
In early 2014 the Malaysian government confiscated more than 300 bibles for using the word to refer to the Christian God in Peninsular Malaysia.
However, the use of "Allah" is not prohibited in the two Malaysian states of Sabah and Sarawak.
The main reason it is not prohibited in these two states is that usage has been long-established and local Alkitab (Bibles) have been widely distributed freely in East Malaysia without restrictions for years.
Both states also do not have similar Islamic state laws as those in West Malaysia.
In reaction to some media criticism, the Malaysian government has introduced a "10-point solution" to avoid confusion and misleading information.
The 10-point solution is in line with the spirit of the 18- and 20-point agreements of Sarawak and Sabah.
""
in other languages that use Arabic script is spelled in the same way.
This includes Urdu, Persian/Dari, Uyghur among others.
The word ' is always written without an to spell the ' vowel.
This is because the spelling was settled before Arabic spelling started habitually using ' to spell '.
However, in vocalized spelling, a small diacritic "" is added on top of the "" to indicate the pronunciation.
One exception may be in the pre-Islamic Zabad inscription, where it ends with an ambiguous sign that may be a lone-standing "h" with a lengthened start, or may be a non-standard conjoined ":-

Many Arabic type fonts feature special ligatures for Allah.
Unicode has a code point reserved for ', = U+FDF2, in the Arabic Presentation Forms-A block, which exists solely for "compatibility with some older, legacy character sets that encoded presentation forms directly"; this is discouraged for new text.
Instead, the word ' should be represented by its individual Arabic letters, while modern font technologies will render the desired ligature.
The calligraphic variant of the word used as the Coat of arms of Iran is encoded in Unicode, in the Miscellaneous Symbols range, at code point U+262B (☫).
</doc>
<doc id="742" url="https://en.wikipedia.org/wiki?curid=742" title="Algorithms (journal)">
Algorithms (journal)

Algorithms is a monthly peer-reviewed open-access scientific journal of mathematics, covering design, analysis, and experiments on algorithms.
The journal is published by MDPI and was established in 2008.
The founding editor-in-chief was Kazuo Iwama.
Its current editor-in-chief is Henning Fernau (University of Trier).
The journal is abstracted and indexed in:
Journals with similar scope and "algorithm" in their names include


</doc>
<doc id="746" url="https://en.wikipedia.org/wiki?curid=746" title="Azerbaijan">
Azerbaijan

Azerbaijan ( ; ), officially the Republic of Azerbaijan ( ), is a country in the South Caucasus region of Eurasia at the crossroads of Eastern Europe and Western Asia.
It is bounded by the Caspian Sea to the east, Russia to the north, Georgia to the northwest, Armenia to the west and Iran to the south.
The exclave of Nakhchivan is bound by Armenia to the north and east, Iran to the south and west, and has an 11 km long border with Turkey in the northwest.
The Azerbaijan Democratic Republic proclaimed its independence in 1918 and became the first democratic state in the Muslim-oriented world.
The country was incorporated into the Soviet Union in 1920 as the Azerbaijan Soviet Socialist Republic.
The modern Republic of Azerbaijan proclaimed its independence on 30 August 1991, prior to the official dissolution of the USSR in December 1991.
In September 1991, the Armenian majority of the disputed Nagorno-Karabakh region seceded to form the Republic of Artsakh.
The region and seven adjacent districts outside it became "de facto" independent with the end of the Nagorno-Karabakh War in 1994.
These regions are internationally recognized as part of Azerbaijan pending a solution to the status of the Nagorno-Karabakh, found through negotiations facilitated by the OSCE.
The sovereign state of Azerbaijan is a unitary semi-presidential republic.
It's a member state of the Council of Europe, the OSCE, and the NATO Partnership for Peace (PfP) program.
It is one of six independent Turkic states and an active member of the Turkic Council and the TÜRKSOY community.
Azerbaijan has diplomatic relations with 158 countries and holds membership in 38 international organizations.
It is one of the founding members of GUAM, the Commonwealth of Independent States (CIS) and the Organisation for the Prohibition of Chemical Weapons.
A member of the United Nations since 1992 after its independence, Azerbaijan was elected to membership in the newly established Human Rights Council by the United Nations General Assembly on 9 May 2006.
Its term of office began on 19 June 2006.
Azerbaijan is also a member state of the Non-Aligned Movement, holds observer status in World Trade Organization, and is a correspondent at the International Telecommunication Union.
The Constitution of Azerbaijan does not declare an official religion and all major political forces in the country are secularist.
However, the majority of the population are of Muslim background.
More than 89% of the population is Shia.
Most Azerbaijanis, however, do not actively practice any religion, with 53% stating religion has little to no importance in their lives, according to Pew Research Center and Gallup polls.
Alcohol are also permitted.
Azerbaijan has a high level of human development which ranks on par with most Eastern European countries.
It has a high rate of economic development and literacy, as well as a low rate of unemployment.
However, the ruling party, the New Azerbaijan Party, has been accused of authoritarianism and human rights abuses.
According to a modern etymology, the term "Azerbaijan" derives from that of "Atropates", a Persian satrap under the Achaemenid Empire, who was later reinstated as the satrap of Media under Alexander the Great.
The original etymology of this name is thought to have its roots in the once-dominant Zoroastrianism.
In the Avesta's "Frawardin Yasht" ("Hymn to the Guardian Angels"), there is a mention of "âterepâtahe ashaonô fravashîm ýazamaide", which literally translates from Avestan as "we worship the fravashi of the holy Atropatene."
The name "Atropates" itself is the Greek transliteration of an Old Iranian, probably Median, compounded name with the meaning "Protected by the (Holy) Fire" or "The Land of the (Holy) Fire".
The Greek name was mentioned by Diodorus Siculus and Strabo.
Over the span of millennia, the name evolved to (Middle Persian), then to , , (New Persian) and present-day "Azerbaijan".
The name "Azerbaijan" was first adopted for the area of the present-day Republic of Azerbaijan by the government of Musavat in 1918, after the collapse of the Russian Empire, when the independent Azerbaijan Democratic Republic was established.
Until then, the designation had been used exclusively to identify the adjacent region of contemporary northwestern Iran, while the area of the Azerbaijan Democratic Republic was formerly referred to as "Arran" and "Shirvan".
On that basis Iran protested the newly adopted country name.
During the Soviet rule, the country was also spelled in English from the Russian transliteration as "Azerbaydzhan" ().
The earliest evidence of human settlement in the territory of Azerbaijan dates back to the late Stone Age and is related to the Guruchay culture of Azokh Cave.
The Upper Paleolithic and late Bronze Age cultures are attested in the caves of Tağılar, Damcılı, Zar, Yataq-yeri and in the necropolises of Leylatepe and Saraytepe.
Early settlements included the Scythians in the 9th century BC.
Following the Scythians, Iranian Medes came to dominate the area to the south of the Aras.
The Medes forged a vast empire between 900–700 BC, which was integrated into the Achaemenid Empire around 550 BC.
The area was conquered by the Achaemenids leading to the spread of Zoroastrianism.
Later it became part of Alexander the Great's Empire and its successor, the Seleucid Empire.
During this period, Zoroastrianism spread in the Caucasus and Atropatene.
Caucasian Albanians, the original inhabitants of northeastern Azerbaijan, ruled that area from around the 4th century BC, and established an independent kingdom.
The Sasanian Empire turned Caucasian Albania into a vassal state in 252, while King Urnayr officially adopted Christianity as the state religion in the 4th century.
Despite Sassanid rule, Albania remained an entity in the region until the 9th century, while fully subordinate to Sassanid Iran, and retained its monarchy.
Despite being one of the chief vassals of the Sasanian emperor, the Albanian king had only a semblance of authority, and the Sasanian marzban (military governor) held most civil, religious, and military authority.
In the first half of the 7th century, Caucasian Albania, as a vassal of the Sasanians, came under nominal Muslim rule due to the Muslim conquest of Persia.
The Umayyad Caliphate repulsed both the Sasanians and Byzantines from Transcaucasia and turned Caucasian Albania into a vassal state after Christian resistance led by King Javanshir, was suppressed in 667.
The power vacuum left by the decline of the Abbasid Caliphate was filled by numerous local dynasties such as the Sallarids, Sajids, and Shaddadids.
At the beginning of the 11th century, the territory was gradually seized by waves of Oghuz Turks from Central Asia.
The first of these Turkic dynasties established was the Seljuk Empire, who entered the area now known as Azerbaijan by 1067.
The pre-Turkic population that lived on the territory of modern Azerbaijan spoke several Indo-European and Caucasian languages, among them Armenian and an Iranian language, Old Azeri, which was gradually replaced by a Turkic language, the early precursor of the Azerbaijani language of today.
Some linguists have also stated that the Tati dialects of Iranian Azerbaijan and the Republic of Azerbaijan, like those spoken by the Tats, are descended from Old Azeri.
Locally, the possessions of the subsequent Seljuk Empire were ruled by Eldiguzids, technically vassals of the Seljuk sultans, but sometimes "de facto" rulers themselves.
Under the Seljuks, local poets such as Nizami Ganjavi and Khaqani gave rise to a blossoming of Persian literature on the territory of present-day Azerbaijan.
The local dynasty of the Shirvanshahs became a vassal state of Timur's Empire, and assisted him in his war with the ruler of the Golden Horde Tokhtamysh.
Following Timur's death, two independent and rival states emerged: Kara Koyunlu and Aq Qoyunlu.
The Shirvanshahs returned, maintaining a high degree of autonomy as local rulers and vassals from 861, for numerous centuries to come.
In 1501, the Safavid dynasty of Iran subdued the Shirvanshahs, and gained its possessions.
In the course of the next century, the Safavids converted the formerly Sunni population to Shia Islam, as they did with the population in what is modern-day Iran.
The Safavids allowed the Shirvanshahs to remain in power, under Safavid suzerainty, until 1538, when Safavid king Tahmasp I (r.
1524–1576) completely deposed them, and made the area into the Safavid province of Shirvan.
The Sunni Ottomans briefly managed to occupy parts of present-day Azerbaijan as a result of the Ottoman-Safavid War of 1578–1590; by the early 17th century, they were ousted by Safavid Iranian ruler Abbas I (r.
1588–1629).
In the wake of the demise of the Safavid Empire, Baku and its environs were briefly occupied by the Russians as a consequence of the Russo-Persian War of 1722–1723.
Despite brief intermissions such as these by Safavid Iran's neighboring rivals, the land of what is today Azerbaijan remained under Iranian rule from the earliest advent of the Safavids up to the course of the 19th century.
After the Safavids, the area was ruled by the Iranian Afsharid dynasty.
After the death of Nader Shah (r.
1736–1747), many of his former subjects capitalized on the eruption of instability.
Numerous self-ruling khanates with various forms of autonomy emerged in the area.
The rulers of these khanates were directly related to the ruling dynasties of Iran, and were vassals and subjects of the Iranian shah.
The khanates exercised control over their affairs via international trade routes between Central Asia and the West.
Thereafter, the area was under the successive rule of the Iranian Zands and Qajars.
From the late 18th century, Imperial Russia switched to a more aggressive geo-political stance towards its two neighbors and rivals to the south, namely Iran and the Ottoman Empire.
Russia now actively tried to gain possession of the Caucasus region which was, for the most part, in the hands of Iran.
In 1804, the Russians invaded and sacked the Iranian town of Ganja, sparking the Russo-Persian War of 1804–1813.
The militarily superior Russians ended the Russo-Persian War of 1804–1813 with a victory.
Following Qajar Iran's loss in the 1804–1813 war, it was forced to concede suzerainty over most of the khanates, along with Georgia and Dagestan to the Russian Empire, per the Treaty of Gulistan.
The area to the north of the river Aras, amongst which territory lies the contemporary Republic of Azerbaijan, was Iranian territory until it was occupied by Russia in the 19th century.
About a decade later, in violation of the Gulistan treaty, the Russians invaded Iran's Erivan Khanate.
This sparked the final bout of hostilities between the two, the Russo-Persian War of 1826–1828.
The resulting Treaty of Turkmenchay, forced Qajar Iran to cede sovereignty over the Erivan Khanate, the Nakhchivan Khanate and the remainder of the Lankaran Khanate, comprising the last parts of the soil of the contemporary Azerbaijani Republic that were still in Iranian hands.
After incorporation of all Caucasian territories from Iran into Russia, the new border between the two was set at the Aras River, which, upon the Soviet Union's disintegration, subsequently became part of the border between Iran and the Azerbaijan Republic.
Qajar Iran was forced to cede its Caucasian territories to Russia in the 19th century, which thus included the territory of the modern-day Azerbaijan Republic, while as a result of that cession, the Azerbaijani ethnic group is nowadays parted between two nations: Iran and Azerbaijan.
Nevertheless, the number of ethnic Azerbaijanis in Iran far outnumber those in neighbouring Azerbaijan.
After the collapse of the Russian Empire during World War I, the short-lived Transcaucasian Democratic Federative Republic was declared, constituting the present-day republics of Azerbaijan, Georgia, and Armenia.
It was followed by the March Days massacres that took place between 30 March and 2 April 1918 in the city of Baku and adjacent areas of the Baku Governorate of the Russian Empire.
When the republic dissolved in May 1918, the leading Musavat party declared independence as the Azerbaijan Democratic Republic (ADR), adopting the name of "Azerbaijan" for the new republic; a name that prior to the proclamation of the ADR was solely used to refer to the adjacent northwestern region of contemporary Iran.
The ADR was the first modern parliamentary republic in the Muslim world.
Among the important accomplishments of the Parliament was the extension of suffrage to women, making Azerbaijan the first Muslim nation to grant women equal political rights with men.
Another important accomplishment of ADR was the establishment of Baku State University, which was the first modern-type university founded in the Muslim East.
On 13 October 1921, the Soviet republics of Russia, Armenia, Azerbaijan, and Georgia signed an agreement with Turkey known as the Treaty of Kars.
The previously independent Republic of Aras would also become the Nakhichevan Autonomous Soviet Socialist Republic within the Azerbaijan SSR by the treaty of Kars.
On the other hand, Armenia was awarded the region of Zangezur and Turkey agreed to return Gyumri (then known as Alexandropol).
During World War II, Azerbaijan played a crucial role in the strategic energy policy of the Soviet Union, with 80 percent of the Soviet Union's oil on the Eastern Front being supplied by Baku.
By the Decree of the Supreme Soviet of the USSR in February 1942, the commitment of more than 500 workers and employees of the oil industry of Azerbaijan were awarded orders and medals.
Operation Edelweiss carried out by the German Wehrmacht targeted Baku because of its importance as the energy (petroleum) dynamo of the USSR.
A fifth of all Azerbaijanis fought in the Second World War from 1941 to 1945.
Approximately 681,000 people with over 100,000 of them women went to the front, while the total population of Azerbaijan was 3.4 million at the time.
Some 250,000 people from Azerbaijan were killed on the front.
More than 130 Azerbaijanis were named Heroes of the Soviet Union.
Azerbaijani Major-General Azi Aslanov was twice awarded the Hero of the Soviet Union.
Following the politics of "glasnost", initiated by Mikhail Gorbachev, civil unrest and ethnic strife grew in various regions of the Soviet Union, including Nagorno-Karabakh, an autonomous region of the Azerbaijan SSR.
The disturbances in Azerbaijan, in response to Moscow's indifference to an already heated conflict, resulted in calls for independence and secession, which culminated in the Black January events in Baku.
Later in 1990, the Supreme Council of the Azerbaijan SSR dropped the words "Soviet Socialist" from the title, adopted the "Declaration of Sovereignty of the Azerbaijan Republic" and restored the flag of the Azerbaijan Democratic Republic as the state flag.
As a consequence of the failed coup which occurred in August in Moscow, on 18 October 1991, the Supreme Council of Azerbaijan adopted a Declaration of Independence which was affirmed by a nationwide referendum in December 1991, while the Soviet Union officially ceased to exist on 26 December 1991.
The country now celebrates its Independence Day on 18 October.
The early years of independence were overshadowed by the Nagorno-Karabakh war with the ethnic Armenian majority of Nagorno-Karabakh backed by Armenia.
By the end of the hostilities in 1994, Armenians controlled up to 20 percent of Azerbaijani territory, including Nagorno-Karabakh itself.
During the war many atrocities were committed including the massacres at Malibeyli and Gushchular, the Garadaghly massacre, the Agdaban and the Khojaly massacres.
Furthermore, an estimated 30,000 people have been killed and more than a million people have been displaced.
Four United Nations Security Council Resolutions (822, 853, 874, and 884) demand for "the immediate withdrawal of all Armenian forces from all occupied territories of Azerbaijan."
Many Russians and Armenians left Azerbaijan during the 1990s.
According to the 1970 census, there were 510,000 ethnic Russians and 484,000 Armenians in Azerbaijan.
In 1993, democratically elected president Abulfaz Elchibey was overthrown by a military insurrection led by Colonel Surat Huseynov, which resulted in the rise to power of the former leader of Soviet Azerbaijan, Heydar Aliyev.
In 1994, Surat Huseynov, by that time the prime minister, attempted another military coup against Heydar Aliyev, but he was arrested and charged with treason.
A year later, in 1995, another coup was attempted against Aliyev, this time by the commander of the OMON special unit, Rovshan Javadov.
The coup was averted, resulting in the killing of the latter and disbanding of Azerbaijan's OMON units.
At the same time, the country was tainted by rampant corruption in the governing bureaucracy.
In October 1998, Aliyev was reelected for a second term.
Despite the much improved economy, particularly with the exploitation of the Azeri-Chirag-Guneshli oil field and Shah Deniz gas field, Aliyev's presidency was criticized due to suspected election frauds and corruption.
Ilham Aliyev, Heydar Aliyev's son, became chairman of the New Azerbaijan Party as well as President of Azerbaijan when his father died in 2003.
He was reelected to a third term as president in October 2013.
Geographically Azerbaijan is located in the South Caucasus region of Eurasia, straddling Western Asia and Eastern Europe.
It lies between latitudes 38° and 42° N, and longitudes 44° and 51° E. The total length of Azerbaijan's land borders is , of which 1,007 kilometers are with Armenia, 756 kilometers with Iran, 480 kilometers with Georgia, 390 kilometers with Russia and 15 kilometers with Turkey.
The coastline stretches for , and the length of the widest area of the Azerbaijani section of the Caspian Sea is .
The territory of Azerbaijan extends from north to south, and from west to east.
Three physical features dominate Azerbaijan: the Caspian Sea, whose shoreline forms a natural boundary to the east; the Greater Caucasus mountain range to the north; and the extensive flatlands at the country's center.
There are also three mountain ranges, the Greater and Lesser Caucasus, and the Talysh Mountains, together covering approximately 40% of the country.
The highest peak of Azerbaijan is Mount Bazardüzü (4,466 m), while the lowest point lies in the Caspian Sea (−28 m).
Nearly half of all the mud volcanoes on Earth are concentrated in Azerbaijan, these volcanoes were also among nominees for the New7Wonders of Nature.
The main water sources are surface waters.
However, only 24 of the 8,350 rivers are greater than in length.
All the rivers drain into the Caspian Sea in the east of the country.
The largest lake is Sarysu (67 km²), and the longest river is Kur (1,515 km), which is transboundary with Armenia.
Azerbaijan's four main islands in the Caspian Sea have a combined area of over thirty square kilometers.
Since the independence of Azerbaijan in 1991, the Azerbaijani government has taken measures to preserve the environment of Azerbaijan.
National protection of the environment accelerated after 2001 when the state budget increased due to new revenues provided by the Baku-Tbilisi-Ceyhan pipeline.
Within four years protected areas doubled and now make up eight percent of the country's territory.
Since 2001 the government has set up seven large reserves and almost doubled the sector of the budget earmarked for environmental protection.
Azerbaijan is home to a vast variety of landscapes.
Over half of Azerbaijan's land mass consists of mountain ridges, crests, yailas, and plateaus which rise up to hypsometric levels of 400–1000 meters (including the Middle and Lower lowlands), in some places (Talis, Jeyranchol-Ajinohur and Langabiz-Alat foreranges) up to 100–120 meters, and others from 0–50 meters and up (Qobustan, Absheron).
The rest of Azerbaijan's terrain consist of plains and lowlands.
Hypsometric marks within the Caucasus region vary from about −28 meters at the Caspian Sea shoreline up to 4,466 meters (Bazardüzü peak).
The formation of climate in Azerbaijan is influenced particularly by cold arctic air masses of Scandinavian anticyclone, temperate of Siberian anticyclone, and Central Asian anticyclone.
Azerbaijan's diverse landscape affects the ways air masses enter the country.
The Greater Caucasus protects the country from direct influences of cold air masses coming from the north.
That leads to the formation of subtropical climate on most foothills and plains of the country.
Meanwhile, plains and foothills are characterized by high solar radiation rates.
9 out of 11 existing climate zones are present in Azerbaijan.
Both the absolute minimum temperature ( ) and the absolute maximum temperature ( ) were observed in Julfa and Ordubad – regions of Nakhchivan Autonomous Republic.
The maximum annual precipitation falls in Lankaran () and the minimum in Absheron ().
Rivers and lakes form the principal part of the water systems of Azerbaijan, they were formed over a long geological timeframe and changed significantly throughout that period.
This is particularly evidenced by remnants of ancient rivers found throughout the country.
The country's water systems are continually changing under the influence of natural forces and human introduced industrial activities.
Artificial rivers (canals) and ponds are a part of Azerbaijan's water systems.
In terms of water supply, Azerbaijan is below the average in the world with approximately per year of water per square kilometer.
All big water reservoirs are built on Kur.
The hydrography of Azerbaijan basically belongs to the Caspian Sea basin.
There are 8,350 rivers of various lengths within Azerbaijan.
Only 24 rivers are over 100 kilometers long.
The Kura and Aras are the major rivers in Azerbaijan, they run through the Kura-Aras Lowland.
The rivers that directly flow into the Caspian Sea, originate mainly from the north-eastern slope of the Major Caucasus and Talysh Mountains and run along the Samur–Devechi and Lankaran lowlands.
Yanar Dag, translated as "burning mountain", is a natural gas fire which blazes continuously on a hillside on the Absheron Peninsula on the Caspian Sea near Baku, which itself is known as the "land of fire."
Flames jet out into the air from a thin, porous sandstone layer.
It is a tourist attraction to visitors to the Baku area.
The first reports on the richness and diversity of animal life in Azerbaijan can be found in travel notes of Eastern travelers.
Animal carvings on architectural monuments, ancient rocks and stones survived up to the present times.
The first information on the flora and fauna of Azerbaijan was collected during the visits of naturalists to Azerbaijan in the 17th century.
There are 106 species of mammals, 97 species of fish, 363 species of birds, 10 species of amphibians and 52 species of reptiles which have been recorded and classified in Azerbaijan.
The national animal of Azerbaijan is the Karabakh horse, a mountain-steppe racing and riding horse endemic to Azerbaijan.
The Karabakh horse has a reputation for its good temper, speed, elegance and intelligence.
It is one of the oldest breeds, with ancestry dating to the ancient world.
However, today the horse is an endangered species.
Azerbaijan's flora consists of more than 4,500 species of higher plants.
Due to the unique climate in Azerbaijan, the flora is much richer in the number of species than the flora of the other republics of the South Caucasus.
About 67 percent of the species growing in the whole Caucasus can be found in Azerbaijan.
The structural formation of Azerbaijan's political system was completed by the adoption of the new Constitution on 12 November 1995.
According to Article 23 of the Constitution, the state symbols of the Azerbaijan Republic are the flag, the coat of arms, and the national anthem.
The state power in Azerbaijan is limited only by law for internal issues, but for international affairs is additionally limited by the provisions of international agreements.
The government of Azerbaijan is based on the separation of powers among the legislative, executive, and judicial branches.
The legislative power is held by the unicameral National Assembly and the Supreme National Assembly in the Nakhchivan Autonomous Republic.
Parliamentary elections are held every five years, on the first Sunday of November.
The Yeni Azerbaijan Party, and independents loyal to the ruling government, currently hold almost all of the Parliament's 125 seats.
During the 2010 Parliamentary election, the opposition parties, Musavat and Azerbaijani Popular Front Party, failed to win a single seat.
European observers found numerous irregularities in the run-up to the election and on election day.
The executive power is held by the President, who is elected for a seven-year term by direct elections, and the Prime Minister.
The president is authorized to form the Cabinet, a collective executive body, accountable to both the President and the National Assembly.
The Cabinet of Azerbaijan consists primarily of the prime minister, his deputies, and ministers.
The president does not have the right to dissolve the National Assembly, but has the right to veto its decisions.
To override the presidential veto, the parliament must have a majority of 95 votes.
The judicial power is vested in the Constitutional Court, Supreme Court, and the Economic Court.
The president nominates the judges in these courts.
The European Commission for the Efficiency of Justice (CEPEJ) report refers to the Azerbaijani justice model on the selection of new judges as best practice that reflects the particular features and the course of development towards ensuring the independence and quality of the judiciary in a new democracy.
The Security Council is the deliberative body under the president, and he organizes it according to the Constitution.
It was established on 10 April 1997.
The administrative department is not a part of the president's office but manages the financial, technical and pecuniary activities of both the president and his office.
Although Azerbaijan has held several elections since regaining its independence and it has many of the formal institutions of democracy, it remains classified as "not free" (on border with "partly free") by Freedom House.
In recent years, large numbers of Azerbaijani journalists, bloggers, lawyers, and human rights activists have been rounded up and jailed for their criticism of President Aliyev and government authorities.
A resolution adopted by the European Parliament in September 2015 described Azerbaijan as "having suffered the greatest decline in democratic governance in all of Eurasia over the past ten years," noting as well that its dialogue with the country on human rights has "not made any substantial progress."
On 17 March 2016, the President of Azerbaijan signed a decree pardoning more than a dozen of the persons regarded as political prisoners by some NGOs.
This decree was welcomed as a positive step by the US State Department.
On 16 March 2017 another pardon decree was signed, which led to the release of additional persons regarded as political prisoners.
Azerbaijan has been harshly criticized for bribing foreign officials and diplomats in order to promote its causes abroad and legitimize its elections at home, a practice which has been termed as Caviar diplomacy.
However, on 6 March 2017, ESISC (European Strategic Intelligence and Security Center) published a report called “The Armenian Connection” where it attacked human rights NGOs and research organisations criticising human rights violations and corruption in Azerbaijan.
ESISC in that report asserted that "Caviar diplomacy" report elaborated by ESI aimed to create climate of suspicion based on slander to form a network of MPs that would engage in a political war against Azerbaijan, and that the network composed of European PMs, Armenian officials and some NGOs: Human Rights Watch, Amnesty International, "Human Rights House Foundation", "Open Dialog, European Stability Initiative, and Helsinki Committee for Human Rights, was financed by the Soros Foundation.
According to Robert Coalson (Radio Free Europe), ESISC is a part of Baku's lobbying efforts to extend to the use of front think tanks to shift public opinion.
Freedom Files Analytical Centre said that "The report is written in the worst traditions of authoritarian propaganda".
The short-lived Azerbaijan Democratic Republic succeeded in establishing diplomatic relations with six countries, sending diplomatic representatives to Germany and Finland.
The process of international recognition of Azerbaijan's independence from the collapsing Soviet Union lasted roughly one year.
The most recent country to recognize Azerbaijan was Bahrain, on 6 November 1996.
Full diplomatic relations, including mutual exchanges of missions, were first established with Turkey, Pakistan, the United States, Iran and Israel.
Azerbaijan has placed a particular emphasis on its "special relationship" with Turkey.
Azerbaijan has diplomatic relations with 158 countries so far and holds membership in 38 international organizations.
It holds observer status in the Non-Aligned Movement and World Trade Organization and is a correspondent at the International Telecommunication Union.
On 9 May 2006 Azerbaijan was elected to membership in the newly established Human Rights Council by the United Nations General Assembly.
The term of office began on 19 June 2006.
Azerbaijan for the first time elected as a non-permanent member of the UN Security Council in 2011 with the support of 155 countries.
Foreign policy priorities of Azerbaijan include, first of all, the restoration of its territorial integrity; elimination of the consequences of occupation of Nagorno-Karabakh and seven other regions of Azerbaijan surrounding Nagorno-Karabakh; integration into European and Euro-Atlantic structure; contribution to international security; cooperation with international organizations; regional cooperation and bilateral relations; strengthening of defense capability; promotion of security by domestic policy means; strengthening of democracy; preservation of ethnic and religious tolerance; scientific, educational, and cultural policy and preservation of moral values; economic and social development; enhancing internal and border security; and migration, energy, and transportation security policy.
The Azerbaijani government, in late 2007, stated that the long-standing dispute over the Armenian-occupied territory of Nagorno-Karabakh is almost certain to spark a new war if it remains unresolved.
The Government is in the process of increasing its military budget.
Azerbaijan is an active member of international coalitions fighting international terrorism.
Azerbaijan was one of the first countries to offer support after the September 11 attacks.
The country is contributing to peacekeeping efforts in Kosovo, Afghanistan and Iraq.
Azerbaijan is an active member of NATO's Partnership for Peace program.
It also maintains good relations with the European Union and could potentially one day apply for membership.
Azerbaijan is divided into 10 economic regions; 66 rayons ("rayonlar", singular "rayon") and 77 cities ("şəhərlər", singular "şəhər") of which 12 are under the direct authority of the republic.
Moreover, Azerbaijan includes the Autonomous Republic ("muxtar respublika") of Nakhchivan.
The President of Azerbaijan appoints the governors of these units, while the government of Nakhchivan is elected and approved by the parliament of Nakhchivan Autonomous Republic.
"Note: The cities under the direct authority of the republic in italics".
The history of the modern Azerbaijan army dates back to Azerbaijan Democratic Republic in 1918, when the National Army of the newly formed Azerbaijan Democratic Republic was created on 26 June 1918.
When Azerbaijan gained independence after the dissolution of the Soviet Union, the Armed Forces of the Republic of Azerbaijan were created according to the Law on the Armed Forces of 9 October 1991.
The original date of the establishment of the short-lived National Army is celebrated as Army Day (26 June) in today's Azerbaijan.
As of 2002, Azerbaijan had 95,000 active personnel in its armed forces.
There are also 17,000 paramilitary troops.
The armed forces have three branches: the Land Forces, the Air Forces and the Navy.
Additionally the armed forces embrace several military sub-groups that can be involved in state defense when needed.
These are the Internal Troops of the Ministry of Internal Affairs and the State Border Service, which includes the Coast Guard as well.
The Azerbaijan National Guard is a further paramilitary force.
It operates as a semi-independent entity of the Special State Protection Service, an agency subordinate to the President.
Azerbaijan adheres to the Treaty on Conventional Armed Forces in Europe and has signed all major international arms and weapons treaties.
Azerbaijan closely cooperates with NATO in programs such as Partnership for Peace and Individual Partnership Action Plan.
Azerbaijan has deployed 151 of its Peacekeeping Forces in Iraq and another 184 in Afghanistan.
The defense budget of Azerbaijan for 2011 was set at US$3.1 billion.
In addition to that, $1.36 billion was planned to be used for the needs of the defense industry, which bring up the total military budget to 4.6 billion.
Azerbaijani President Ilham Aliyev said on 26 June 2011 that the defence spending reached $3.3 billion that year.
Azerbaijan's defense budget for 2013 is $3.7 billion.
Azerbaijani defense industry manufactures small arms, artillery systems, tanks, armors and noctovision devices, aviation bombs, pilotless vehicles, various military vehicles and military planes and helicopters.
After gaining independence in 1991, Azerbaijan became a member of the International Monetary Fund, the World Bank, the European Bank for Reconstruction and Development, the Islamic Development Bank and the Asian Development Bank.
The banking system of Azerbaijan consists of the Central Bank of Azerbaijan, commercial banks and non-banking credit organizations.
The National (now Central) Bank was created in 1992 based on the Azerbaijan State Savings Bank, an affiliate of the former State Savings Bank of the USSR.
The Central Bank serves as Azerbaijan's central bank, empowered to issue the national currency, the Azerbaijani manat, and to supervise all commercial banks.
Two major commercial banks are UniBank and the state-owned International Bank of Azerbaijan, run by Dr. Jahangir Hajiyev.
Pushed up by spending and demand growth, the 2007 Q1 inflation rate reached 16.6%.
Nominal incomes and monthly wages climbed 29% and 25% respectively against this figure, but price increases in non-oil industry encouraged inflation.
Azerbaijan shows some signs of the so-called "Dutch disease" because of its fast-growing energy sector, which causes inflation and makes non-energy exports more expensive.
In the early 2000s the chronically high inflation was brought under control.
This led to the launch of a new currency, the new Azerbaijani manat, on 1 January 2006, to cement the economic reforms and erase the vestiges of an unstable economy.
In 2008, Azerbaijan was cited as one of the top 10 reformers by the World Bank's Doing Business Report.
Azerbaijan is also ranked 57th in the Global Competitiveness Report for 2010–2011, above other CIS countries.
By 2012 the GDP of Azerbaijan had increased 20-fold from to its 1995 level.
According to World Bank's Doing Business report 2019, Azerbaijan improved its position in the Ease of doing business rank from 57 to 25.
As a result of implementing a record number of reforms mainly involving institutional changes among the 10 top improvers, to do business in Azerbaijan became easier, such as time and cost to get construction permit reduced significantly (time by 80 days and cost by 12.563 AZN), process of connecting electricity grid rationalized, as well as getting credit simplified.
Two-thirds of Azerbaijan is rich in oil and natural gas.
The history of the oil industry of Azerbaijan dates back to the ancient period.
Arabian historian and traveler Ahmed Al-Belaruri discussed the economy of the Absheron peninsula in anntiquity, mentioning its oil in particular.
The region of the Lesser Caucasus accounts for most of the country's gold, silver, iron, copper, titanium, chromium, manganese, cobalt, molybdenum, complex ore and antimony.
In September 1994, a 30-year contract was signed between the State Oil Company of Azerbaijan Republic (SOCAR) and 13 oil companies, among them Amoco, BP, ExxonMobil, Lukoil and Equinor.
As Western oil companies are able to tap deepwater oilfields untouched by the Soviet exploitation, Azerbaijan is considered one of the most important spots in the world for oil exploration and development.
Meanwhile, the State Oil Fund of Azerbaijan was established as an extra-budgetary fund to ensure macroeconomic stability, transparency in the management of oil revenue, and safeguarding of resources for future generations.
Azeriqaz, a sub-company of SOCAR, intends to ensure full gasification of the country by 2021.
Azerbaijan is one of the sponsors of the East–West and North–South energy transport corridors.
Baku–Tbilisi–Kars railway line will connect the Caspian region with Turkey, is expected to be completed in July 2017.
The Trans-Anatolian Natural Gas Pipeline (TANAP) and Trans-Adriatic Pipeline (TAP) will deliver natural gas from Azerbaijan's Shah Deniz gas to Turkey and Europe.
Azerbaijan extended the agreement on development of ACG until 2050 according to the amended PSA signed on 14 September 2017 by SOCAR and co-venturers (BP, Chevron, INPEX, Equinor, ExxonMobil, TP, ITOCHU and ONGC Videsh).
Azerbaijan has the largest agricultural basin in the region.
About 54.9 percent of Azerbaijan is agricultural land.
At the beginning of 2007 there were 4,755,100 hectares of utilized agricultural area.
In the same year the total wood resources counted 136 million m³.
Azerbaijan's agricultural scientific research institutes are focused on meadows and pastures, horticulture and subtropical crops, green vegetables, viticulture and wine-making, cotton growing and medicinal plants.
In some areas it is profitable to grow grain, potatoes, sugar beets, cotton and tobacco.
Livestock, dairy products, and wine and spirits are also important farm products.
The Caspian fishing industry concentrates on the dwindling stocks of sturgeon and beluga.
In 2002 the Azerbaijani merchant marine had 54 ships.
Some products previously imported from abroad have begun to be produced locally.
Among them are Coca-Cola by Coca-Cola Bottlers LTD, beer by Baki-Kastel, parquet by Nehir and oil pipes by EUPEC Pipe Coating Azerbaijan.
Tourism is an important part of the economy of Azerbaijan.
The country was a well-known tourist spot in the 1980s.
However, the fall of the Soviet Union, and the Nagorno-Karabakh War during the 1990s, damaged the tourist industry and the image of Azerbaijan as a tourist destination.
It was not until the 2000s that the tourism industry began to recover, and the country has since experienced a high rate of growth in the number of tourist visits and overnight stays.
In the recent years, Azerbaijan has also become a popular destination for religious, spa, and health care tourism.
During winter, the Shahdag Mountain Resort offers skiing with state of the art facilities.
The government of Azerbaijan has set the development of Azerbaijan as an elite tourist destination as a top priority.
It is a national strategy to make tourism a major, if not the single largest, contributor to the Azerbaijani economy.
These activities are regulated by the Ministry of Culture and Tourism of Azerbaijan.
There are 63 countries which have visa-free score.
E-visa – for a visit of foreigners of visa-required countries to the Republic of Azerbaijan.
According to Travel and Tourism Competitiveness Report 2015 of the World Economic Forum, Azerbaijan holds 84th place.
According to a report by the World Travel and Tourism Council, Azerbaijan was among the top ten countries showing the strongest growth in visitor exports between 2010 and 2016, In addition, Azerbaijan placed first (46.1%) among countries with the fastest-developing travel and tourism economies, with strong indicators for inbound international visitor spending last year.
The convenient location of Azerbaijan on the crossroad of major international traffic arteries, such as the Silk Road and the south–north corridor, highlights the strategic importance of transportation sector for the country's economy.
The transport sector in the country includes roads, railways, aviation, and maritime transport.
Azerbaijan is also an important economic hub in the transportation of raw materials.
The Baku–Tbilisi–Ceyhan pipeline (BTC) became operational in May 2006 and extends more than 1,774 kilometers through the territories of Azerbaijan, Georgia and Turkey.
The BTC is designed to transport up to 50 million tons of crude oil annually and carries oil from the Caspian Sea oilfields to global markets.
The South Caucasus Pipeline, also stretching through the territory of Azerbaijan, Georgia and Turkey, became operational at the end of 2006 and offers additional gas supplies to the European market from the Shah Deniz gas field.
Shah Deniz is expected to produce up to 296 billion cubic meters of natural gas per year.
Azerbaijan also plays a major role in the EU-sponsored Silk Road Project.
In 2002, the Azerbaijani government established the Ministry of Transport with a broad range of policy and regulatory functions.
In the same year, the country became a member of the Vienna Convention on Road Traffic.
Priorities are upgrading the transport network and improving transportation services in order to better facilitate the development of other sectors of the economy.
The construction of Kars–Tbilisi–Baku railway in 2012 is to improve transportation between Asia and Europe by connecting the railways of China and Kazakhstan in the east to the European railway system in the west via Turkey.
In 2010 Broad-gauge railways and electrified railways stretched for and respectively.
By 2010, there were 35 airports and one heliport.
In the 21st century, a new oil and gas boom helped to improve the situation in Azerbaijan's science and technology sectors, and the government launched a campaign aimed at modernization and innovation.
The government estimates that profits from the information technology and communication industry will grow and become comparable to those from oil production.
Azerbaijan has a large and steadily growing Internet sector, mostly uninfluenced by the global financial crisis; rapid growth is forecast for at least five more years.
The country has also been making progress in developing its telecoms sector.
The Ministry of Communications & Information Technologies (MCIT), as well as being an operator through its role in Aztelekom, is both a policy-maker and regulator.
Public pay phones are available for local calls and require the purchase of a token from the telephone exchange or some shops and kiosks.
Tokens allow a call of indefinite duration.
, there were 1,397,000 main telephone lines and 1,485,000 internet users.
There are four GSM providers: Azercell, Bakcell, Azerfon (Nar Mobile), Nakhtel mobile network operators and one CDMA.
In the 21st century a number of prominent Azerbaijani geodynamics and geotectonics scientists, inspired by the fundamental works of Elchin Khalilov and others, designed hundreds of earthquake prediction stations and earthquake-resistant buildings that now constitute the bulk of The Republican Center of Seismic Service.
The Azerbaijan National Aerospace Agency launched its first satellite AzerSat 1 into orbit on 7 February 2013 from Guiana Space Centre in French Guiana at orbital positions 46° East.
The satellite covers Europe and a significant part of Asia and Africa and serves the transmission of TV and radio broadcasting as well as the internet.
The launching of a satellite into orbit is Azerbaijan's first step in realizing its goal of becoming a nation with its own space industry, capable of successfully implementing more projects in the future.
As of 2016, 53.1% of Azerbaijan's total population of 9,705,600 was urban, with the remaining 46.9% being rural.
50.2% of the total population was female.
The sex ratio in the same year was 0.99 males per female.
The 2011 population growth-rate was 0.85%, compared to 1.09% worldwide.
A significant factor restricting population growth is a high level of migration.
In 2011 Azerbaijan saw a migration of −1.14/1,000 people.
The Azerbaijani diaspora is found in 42 countries and in turn there are many centers for ethnic minorities inside Azerbaijan, including the German cultural society "Karelhaus", Slavic cultural center, Azerbaijani-Israeli community, Kurdish cultural center, International Talysh Association, Lezgin national center "Samur", Azerbaijani-Tatar community, Crimean Tatars society, etc.
The ethnic composition of the population according to the 2009 population census: 91.60% Azerbaijanis, 2.02% Lezgians, 1.35% Armenians (almost all Armenians live in the break-away region of Nagorno-Karabakh), 1.34% Russians, 1.26% Talysh, 0.56% Avars, 0.43% Turks, 0.29% Tatars, 0.28% Tats, 0.24% Ukrainians, 0.14% Tsakhurs, 0.11% Georgians, 0.10% Jews, 0.07% Kurds, other 0.21%.
Iranian Azerbaijanis are by far the largest minority in Iran.
The number of ethnic Azerbaijanis in Iran furthermore far outnumber those in neighboring Azerbaijan.
The CIA World Factbook estimates Iranian Azerbaijanis as constituting at least 16% of Iran's population.
In total, Azerbaijan has 78 cities, 63 city districts, and one special legal status city.
These are followed by 261 urban-type settlements and 4248 villages.
The official language is Azerbaijani (Turkic language), which is spoken by approximately 92% of the population as a mother tongue.
It belongs to the Turkic language family.
Russian and Armenian (only in Nagorno-Karabakh) are also spoken, and each are the mother tongue of around 1.5% of the population respectively.
Russian and English play significant roles as second or third languages of education and communication.
There are a dozen other minority languages spoken natively in the country.
Avar, Budukh, Georgian, Juhuri, Khinalug, Kryts, Lezgian, Rutul, Talysh, Tat, Tsakhur, and Udi are all spoken by small minorities.
Some of these language communities are very small and their numbers are decreasing.
Armenian is almost exclusively spoken in the break-away Nagorno-Karabakh region.
Around 97% of the population are Muslims.
65-75% of the Muslims are Shia Muslims (the rest being Sunni Muslims,) and the Republic of Azerbaijan has the second highest Shia population percentage in the world.
Other faiths are practised by the country's various ethnic groups.
Under article 48 of its Constitution, Azerbaijan is a secular state and ensures religious freedom.
In a 2006–2008 Gallup poll, only 21% of respondents from Azerbaijan stated that religion is an important part of their daily lives.
This makes Azerbaijan the least religious Muslim-majority country in the world.
Of the nation's religious minorities, the estimated 280,000 Christians (3.1%) are mostly Russian and Georgian Orthodox and Armenian Apostolic (almost all Armenians live in the break-away region of Nagorno-Karabakh).
In 2003, there were 250 Roman Catholics.
Other Christian denominations as of 2002 include Lutherans, Baptists and Molokans.
There is also a small Protestant community.
Azerbaijan also has an ancient Jewish population with a 2,000-year history; Jewish organizations estimate that 12,000 Jews remain in Azerbaijan.
Azerbaijan also is home to members of the Bahá'í, Hare Krishna and Jehovah's Witnesses communities, as well as adherents of the other religious communities.
Some religious communities have been unofficially restricted from religious freedom.
A U.S.
State Department report on the matter mentions detention of members of certain Muslim and Christian groups, and many groups have difficulty registering with the SCWRA.
A relatively high percentage of Azerbaijanis have obtained some form of higher education, most notably in scientific and technical subjects.
In the Soviet era, literacy and average education levels rose dramatically from their very low starting point, despite two changes in the standard alphabet, from Perso-Arabic script to Latin in the 1920s and from Roman to Cyrillic in the 1930s.
According to Soviet data, 100 percent of males and females (ages nine to forty-nine) were literate in 1970.
According to the United Nations Development Program Report 2009, the literacy rate in Azerbaijan is 99.5 percent.
Since independence, one of the first laws that Azerbaijan's Parliament passed to disassociate itself from the Soviet Union was to adopt a modified-Latin alphabet to replace Cyrillic.
Other than that the Azerbaijani system has undergone little structural change.
Initial alterations have included the reestablishment of religious education (banned during the Soviet period) and curriculum changes that have reemphasized the use of the Azerbaijani language and have eliminated ideological content.
In addition to elementary schools, the education institutions include thousands of preschools, general secondary schools, and vocational schools, including specialized secondary schools and technical schools.
Education through the eighth grade is compulsory.
The culture of Azerbaijan has developed as a result of many influences.
Today, national traditions are well preserved in the country despite Western influences, including globalized consumer culture.
Some of the main elements of the Azerbaijani culture are: music, literature, folk dances and art, cuisine, architecture, cinematography and Novruz Bayram.
The latter is derived from the traditional celebration of the New Year in the ancient Iranian religion of Zoroastrianism.
Novruz is a family holiday.
The profile of Azerbaijan's population consists, as stated above, of Azerbaijanis, as well as other nationalities or ethnic groups, compactly living in various areas of the country.
Azerbaijani national and traditional dresses are the Chokha and Papakhi.
There are radio broadcasts in Russian, Georgian, Kurdish, Lezgian and Talysh languages, which are financed from the state budget.
Some local radio stations in Balakan and Khachmaz organize broadcasts in Avar and Tat.
In Baku several newspapers are published in Russian, Kurdish ("Dengi Kurd"), Lezgian ("Samur") and Talysh languages.
Jewish society "Sokhnut" publishes the newspaper "Aziz".
Music of Azerbaijan builds on folk traditions that reach back nearly a thousand years.
For centuries Azerbaijani music has evolved under the badge of monody, producing rhythmically diverse melodies.
Azerbaijani music has a branchy mode system, where chromatization of major and minor scales is of great importance.
Among national musical instruments there are 14 string instruments, eight percussion instruments and six wind instruments.
According to "The Grove Dictionary of Music and Musicians", "in terms of ethnicity, culture and religion the Azerbaijani are musically much closer to Iran than Turkey."
Mugham, meykhana and ashiq art are among the many musical traditions of Azerbaijan.
Mugham is usually a suite with poetry and instrumental interludes.
When performing mugham, the singers have to transform their emotions into singing and music.
In contrast to the mugham traditions of Central Asian countries, Azerbaijani mugham is more free-form and less rigid; it is often compared to the improvised field of jazz.
UNESCO proclaimed the Azerbaijani mugham tradition a Masterpiece of the Oral and Intangible Heritage of Humanity on 7 November 2003.
Meykhana is a kind of traditional Azerbaijani distinctive folk unaccompanied song, usually performed by several people improvising on a particular subject.
Ashiq combines poetry, storytelling, dance and vocal and instrumental music into a traditional performance art that stands as a symbol of Azerbaijani culture.
It is a mystic troubadour or traveling bard who sings and plays the saz.
This tradition has its origin in the Shamanistic beliefs of ancient Turkic peoples.
Ashiqs' songs are semi-improvised around common bases.
Azerbaijan's ashiq art was included in the list of Intangible Cultural Heritage by the UNESCO on 30 September 2009.
Since the mid-1960s, Western-influenced Azerbaijani pop music, in its various forms, that has been growing in popularity in Azerbaijan, while genres such as rock and hip hop are widely produced and enjoyed.
Azerbaijani pop and Azerbaijani folk music arose with the international popularity of performers like Alim Qasimov, Rashid Behbudov, Vagif Mustafazadeh, Muslim Magomayev, Shovkat Alakbarova and Rubaba Muradova.
Azerbaijan is an enthusiastic participant in the Eurovision Song Contest.
Azerbaijan made its debut appearance at the 2008 Eurovision Song Contest.
The country's entry gained third place in 2009 and fifth the following year.
Ell and Nikki won the first place at the Eurovision Song Contest 2011 with the song "Running Scared", entitling Azerbaijan to host the contest in 2012, in Baku.
They have qualified for every Grand Final up until the 2018 edition of the contest, entering with X My Heart by singer Aisel

There are dozens of Azerbaijani folk dances.
They are performed at formal celebrations and the dancers wear national clothes like the Chokha, which is well-preserved within the national dances.
Most dances have a very fast rhythm.
The national dance shows the characteristics of the Azerbaijani nation.
Among the medieval authors born within the territorial limits of modern Azerbaijani Republic was Persian poet and philosopher Nizami, called Ganjavi after his place of birth, Ganja, who was the author of the Khamseh ("The Quintuplet"), composed of five romantic poems, including "The Treasure of Mysteries," "Khosrow and Shīrīn," and "Leyli and Mejnūn."
The earliest known figure in Azerbaijani literature was Izzeddin Hasanoglu, who composed a divan consisting of Persian and Turkic ghazals.
In Persian ghazals he used his pen-name, while his Turkic ghazals were composed under his own name of Hasanoghlu.
Classical literature in Azerbaijani was formed in the 14th century based on the various Early Middle Ages dialects of Tabriz and Shirvan.
Among the poets of this period were Gazi Burhanaddin, Haqiqi (pen-name of Jahan-shah Qara Qoyunlu), and Habibi.
The end of the 14th century was also the period of starting literary activity of Imadaddin Nesimi, one of the greatest Turkic Hurufi mystical poets of the late 14th and early 15th centuries and one of the most prominent early divan masters in Turkic literary history, who also composed poetry in Persian and Arabic.
The divan and ghazal styles were further developed by poets Qasim al-Anvar, Fuzuli and Khatai (pen-name of Safavid Shah Ismail I).
The Book of Dede Korkut consists of two manuscripts copied in the 16th century, was not written earlier than the 15th century.
It is a collection of 12 stories reflecting the oral tradition of Oghuz nomads.
The 16th-century poet, Muhammed Fuzuli produced his timeless philosophical and lyrical "Qazals" in Arabic, Persian, and Azerbaijani.
Benefiting immensely from the fine literary traditions of his environment, and building upon the legacy of his predecessors, Fizuli was destined to become the leading literary figure of his society.
His major works include "The Divan of Ghazals" and "The Qasidas".
In the same century, Azerbaijani literature further flourished with the development of Ashik () poetic genre of bards.
During the same period, under the pen-name of Khatāī ( for "sinner") Shah Ismail I wrote about 1400 verses in Azerbaijani, which were later published as his "Divan".
A unique literary style known as "qoshma" ( for "improvization") was introduced in this period, and developed by Shah Ismail and later by his son and successor, Shah Tahmasp I.

In the span of the 17th and 18th centuries, Fizuli's unique genres as well Ashik poetry were taken up by prominent poets and writers such as Qovsi of Tabriz, Shah Abbas Sani, Agha Mesih Shirvani, Nishat, Molla Vali Vidadi, Molla Panah Vagif, Amani, Zafar and others.
Along with Turks, Turkmens and Uzbeks, Azerbaijanis also celebrate the Epic of Koroglu (from for "blind man's son"), a legendary folk hero.
Several documented versions of Koroglu epic remain at the Institute for Manuscripts of the National Academy of Sciences of Azerbaijan.
Modern literature in Azerbaijan is based on the Shirvani dialect mainly, while in Iran it is based on the Tabrizi one.
The first newspaper in Azerbaijani, "Akinchi" was published in 1875.
In the mid-19th century, it was taught in the schools of Baku, Ganja, Shaki, Tbilisi, and Yerevan.
Since 1845, it has also been taught in the University of Saint Petersburg in Russia.
Azerbaijanis have a rich and distinctive culture, a major part of which is decorative and applied art.
This form of art is represented by a wide range of handicrafts, such as chasing, jeweler, engraving in metal, carving in wood, stone and bone, carpet-making, lasing, pattern weaving and printing, knitting and embroidery.
Each of these types of decorative art, evidence of the endowments of the Azerbaijan nation, is very much in favor here.
Many interesting facts pertaining to the development of arts and crafts in Azerbaijan were reported by numerous merchants, travelers and diplomats who had visited these places at different times.
The Azerbaijani carpet is a traditional handmade textile of various sizes, with dense texture and a pile or pile-less surface, whose patterns are characteristic of Azerbaijan's many carpet-making regions.
In November 2010 the Azerbaijani carpet was proclaimed a Masterpiece of Intangible Heritage by UNESCO.
Azerbaijan has been since the ancient times known as a center of a large variety of crafts.
The archeological dig on the territory of Azerbaijan testifies to the well developed agriculture, stock raising, metal working, pottery, ceramics, and carpet-weaving that date as far back as to the 2nd millennium BC.
Archeological sites in Dashbulaq, Hasansu, Zayamchai, and Tovuzchai uncovered from the BTC pipeline have revealed early Iron Age artifacts.
Azerbaijani carpets can be categorized under several large groups and a multitude of subgroups.
Scientific research of the Azerbaijani carpet is connected with the name of Latif Kerimov, a prominent scientist and artist.
It was his classification that related the four large groups of carpets with the four geographical zones of Azerbaijan, Guba-Shirvan, Ganja-Kazakh, Karabakh and Tabriz.
The traditional cuisine is famous for an abundance of vegetables and greens used seasonally in the dishes.
Fresh herbs, including mint, cilantro (coriander), dill, basil, parsley, tarragon, leeks, chives, thyme, marjoram, green onion, and watercress, are very popular and often accompany main dishes on the table.
Climatic diversity and fertility of the land are reflected in the national dishes, which are based on fish from the Caspian Sea, local meat (mainly mutton and beef), and an abundance of seasonal vegetables and greens.
Saffron-rice plov is the flagship food in Azerbaijan and black tea is the national beverage.
Azerbaijanis often use traditional armudu (pear-shaped) glass as they have very strong tea culture.
Popular traditional dishes include "bozbash" (lamb soup that exists in several regional varieties with the addition of different vegetables), qutab (fried turnover with a filling of greens or minced meat) and dushbara (sort of dumplings of dough filled with ground meat and flavor).
Azerbaijani architecture typically combines elements of East and West.
Azerbaijiani architecture has heavy influences from Persian architecture.
Many ancient architectural treasures such as the Maiden Tower and Palace of the Shirvanshahs in the Walled City of Baku survive in modern Azerbaijan.
Entries submitted on the UNESCO World Heritage tentative list include the Ateshgah of Baku, Momine Khatun Mausoleum, Hirkan National Park, Binegadi National Park, Lökbatan Mud Volcano, Baku Stage Mountain, Caspian Shore Defensive Constructions, Shusha National Reserve, Ordubad National Reserve and the Palace of Shaki Khans.
Among other architectural treasures are Quadrangular Castle in Mardakan, Parigala in Yukhary Chardaglar, a number of bridges spanning the Aras River, and several mausoleums.
In the 19th and early 20th centuries, little monumental architecture was created, but distinctive residences were built in Baku and elsewhere.
Among the most recent architectural monuments, the Baku subways are noted for their lavish decor.
The task for modern Azerbaijani architecture is diverse application of modern aesthetics, the search for an architect's own artistic style and inclusion of the existing historico-cultural environment.
Major projects such as Heydar Aliyev Cultural Center, Flame Towers, Baku Crystal Hall, Baku White City and SOCAR Tower have transformed the country's skyline and promotes its contemporary identity.
Azerbaijani art includes one of the oldest art objects in the world, which were discovered as Gamigaya Petroglyphs in the territory of Ordubad Rayon are dated back to the 1st to 4th centuries BC.
About 1500 dislodged and carved rock paintings with images of deer, goats, bulls, dogs, snakes, birds, fantastic beings and also people, carriages and various symbols had been found out on basalt rocks.
Norwegian ethnographer and adventurer Thor Heyerdahl was convinced that people from the area went to Scandinavia in about 100 AD and took their boat building skills with them, and transmuted them into the Viking boats in Northern Europe.
Over the centuries, Azerbaijani art has gone through many stylistic changes.
Azerbaijani painting is traditionally characterized by a warmth of colour and light, as exemplified in the works of Azim Azimzade and Bahruz Kangarli, and a preoccupation with religious figures and cultural motifs.
Azerbaijani painting enjoyed preeminence in Caucasus for hundreds of years, from the Romanesque and Ottoman periods, and through the Soviet and Baroque periods, the latter two of which saw fruition in Azerbaijan.
Other notable artists who fall within these periods include Sattar Bahlulzade, Togrul Narimanbekov, Tahir Salahov, Alakbar Rezaguliyev, Mirza Gadim Iravani, Mikayil Abdullayev and Boyukagha Mirzazade.
The film industry in Azerbaijan dates back to 1898.
In fact, Azerbaijan was among the first countries involved in cinematography.
Therefore, it's not surprising that this apparatus soon showed up in Baku – at the start of the 20th century, this bay town on the Caspian was producing more than 50 percent of the world's supply of oil.
Just like today, the oil industry attracted foreigners eager to invest and to work.
In 1919, during the Azerbaijan Democratic Republic, a documentary "The Celebration of the Anniversary of Azerbaijani Independence" was filmed on the first anniversary of Azerbaijan's independence from Russia, 27 May, and premiered in June 1919 at several theatres in Baku.
After the Soviet power was established in 1920, Nariman Narimanov, Chairman of the Revolutionary Committee of Azerbaijan, signed a decree nationalizing Azerbaijan's cinema.
This also influenced the creation of Azerbaijani animation.
In 1991, after Azerbaijan gained its independence from the Soviet Union, the first Baku International Film Festival East-West was held in Baku.
In December 2000, the former President of Azerbaijan, Heydar Aliyev, signed a decree proclaiming 2 August to be the professional holiday of filmmakers of Azerbaijan.
Today Azerbaijani filmmakers are again dealing with issues similar to those faced by cinematographers prior to the establishment of the Soviet Union in 1920.
Once again, both choice of content and sponsorship of films are largely left up to the initiative of the filmmaker.
There are three state-owned television channels: AzTV, Idman TV and Medeniyyet TV.
One public channel and 6 private channels: İctimai Television, ANS TV, Space TV, Lider TV, Azad Azerbaijan TV, Xazar TV and Region TV.
The Constitution of Azerbaijan claims to guarantee freedom of speech, but this is denied in practice.
After several years of decline in press and media freedom, in 2014 the media environment in Azerbaijan deteriorated rapidly under a governmental campaign to silence any opposition and criticism, even while the country led the Committee of Ministers of the Council of Europe (May–November 2014).
Spurious legal charges and impunity in violence against journalists have remained the norm.
All foreign broadcasts are banned in the country.
According to the 2013 Freedom House Freedom of the Press report, Azerbaijan's press freedom status is "not free," and Azerbaijan ranks 177th out of 196 countries.
Radio Free Europe/Radio Liberty and Voice of America are banned in Azerbaijan.
During the last few years, three journalists were killed and several prosecuted in trials described as unfair by international human rights organizations.
Azerbaijan has the biggest number of journalists imprisoned in Europe and Central Asia in 2015, according to the Committee to Protect Journalists, and is the 5th most censored country in the world, ahead of Iran and China.
A report by an Amnesty International researcher in October 2015 points to '...the severe deterioration of human rights in Azerbaijan over the past few years.
Sadly Azerbaijan has been allowed to get away with unprecedented levels of repression and in the process almost wipe out its civil society'.
Amnesty's 2015/16 annual report on the country stated ' ... persecution of political dissent continued.
Human rights organizations remained unable to resume their work.
At least 18 prisoners of conscience remained in detention at the end of the year.
Reprisals against independent journalists and activists persisted both in the country and abroad, while their family members also faced harassment and arrests.
International human rights monitors were barred and expelled from the country.
Reports of torture and other ill-treatment persisted.'
The Guardian reported in April 2017 that "Azerbaijan’s ruling elite operated a secret $2.9bn (£2.2bn) scheme to pay prominent Europeans, buy luxury goods and launder money through a network of opaque British companies ... Leaked data shows that the Azerbaijani leadership, accused of serial human rights abuses, systemic corruption and rigging elections, made more than 16,000 covert payments from 2012 to 2014.
Some of this money went to politicians and journalists, as part of an international lobbying operation to deflect criticism of Azerbaijan’s president, Ilham Aliyev, and to promote a positive image of his oil-rich country."
There was no suggestion that all recipients were aware of the source of the money as it arrived via a disguised route.
Freestyle wrestling has been traditionally regarded as Azerbaijan's national sport, in which Azerbaijan won up to fourteen medals, including four golds since joining the National Olympic Committee.
Currently, the most popular sports include football and wrestling.
Football is the most popular sport in Azerbaijan, and the Association of Football Federations of Azerbaijan with 9,122 registered players, is the largest sporting association in the country.
The national football team of Azerbaijan demonstrates relatively low performance in the international arena compared to the nation football clubs.
The most successful Azerbaijani football clubs are Neftchi Baku, Qarabağ, and Gabala.
In 2012, Neftchi Baku became the first Azerbaijani team to advance to the group stage of a European competition, beating APOEL of Cyprus 4–2 on aggregate in the play-off round of the 2012-13 UEFA Europa League.
In 2014, Qarabağ became the second Azerbaijani club advancing to the group stage of UEFA Europa League.
In 2017, after beating Copenhagen 2–2(a) in the play-off round of the UEFA Champions League, Qarabağ became the first Azerbaijani club to reach the Group stage.
Futsal is another popular sport in Azerbaijan.
The Azerbaijan national futsal team reached fourth place in the 2010 UEFA Futsal Championship, while domestic club Araz Naxçivan clinched bronze medals at the 2009–10 UEFA Futsal Cup and 2013–14 UEFA Futsal Cup.
Azerbaijan was the main sponsor of Spanish football club Atlético de Madrid during seasons 2013/2014 and 2014/2015, a partnership that the club described should 'promote the image of Azerbaijan in the world'.
Backgammon also plays a major role in Azerbaijani culture.
The game is very popular in Azerbaijan and is widely played among the local public.
There are also different variations of backgammon developed and analyzed by Azerbaijani experts.
Azerbaijan Women's Volleyball Super League is one of strongest women leagues in world.
Its women's national team came fourth at the 2005 European Championship.
Over the last years, clubs like Rabita Baku and Azerrail Baku achieved great success at European cups.
Azerbaijani volleyball players include likes of Valeriya Korotenko, Oksana Parkhomenko, Inessa Korkmaz, Natalya Mammadova and Alla Hasanova.
Other well-known Azerbaijani athletes are Namig Abdullayev, Toghrul Asgarov, Rovshan Bayramov, Sharif Sharifov, Mariya Stadnik and Farid Mansurov in wrestling, Nazim Huseynov, Elnur Mammadli, Elkhan Mammadov and Rustam Orujov in judo, Rafael Aghayev in karate, Magomedrasul Majidov and Aghasi Mammadov in boxing, Nizami Pashayev in Olympic weightlifting, Azad Asgarov in pankration, Eduard Mammadov in kickboxing, and K-1 fighter Zabit Samedov.
Azerbaijan has a Formula One race-track, made in June 2012, and the country hosted its first Formula One Grand Prix on 19 June 2016 and the Azerbaijan Grand Prix in 2017 and 2018.
Other annual sporting events held in the country are the Baku Cup tennis tournament and the Tour d'Azerbaïdjan cycling race.
Azerbaijan hosted several major sport competitions since the late 2000s, including the 2013 F1 Powerboat World Championship, 2012 FIFA U-17 Women's World Cup, 2011 AIBA World Boxing Championships, 2010 European Wrestling Championships, 2009 Rhythmic Gymnastics European Championships, 2014 European Taekwondo Championships, 2014 Rhythmic Gymnastics European Championships, 2016 World Chess Olympiad.
On 8 December 2012, Baku was selected to host the 2015 European Games, the first to be held in competition's history.
Baku is also set to host the fourth Islamic Solidarity Games in 2017.
General information

Major government resources

Major news media

Tourism


</doc>
<doc id="748" url="https://en.wikipedia.org/wiki?curid=748" title="Amateur astronomy">
Amateur astronomy

Amateur astronomy is a hobby whose participants enjoy observing or imaging celestial objects in the sky using the unaided eye, binoculars, or telescopes.
Even though scientific research may not be their primary goal, some amateur astronomers make contributions in doing citizen science, such as by monitoring variable stars, double stars or occultations of stars by the Moon or asteroids, or by discovering transient astronomical events, such as comets, galactic novae or supernovae in other galaxies.
Amateur astronomers do not use the field of astronomy as their primary source of income or support, and usually have no professional degree in astrophysics or advanced academic training in the subject.
Most amateurs are beginners or hobbyists, while others have a high degree of experience in astronomy and may often assist and work alongside professional astronomers.
Many astronomers have studied the sky throughout history in an amateur framework; however, since the beginning of the twentieth century, professional astronomy has become an activity clearly distinguished from amateur astronomy and associated activities.
Amateur astronomers typically view the sky at night, when most celestial objects and astronomical events are visible, but others observe during the daytime by viewing sunspots or solar eclipses.
Some just look at the sky using nothing more than their eyes or binoculars, but more dedicated amateurs often use portable telescopes or telescopes situated in their private or club observatories.
Amateurs can also join as members of amateur astronomical societies, which can advise, educate or guide them towards ways of finding and observing celestial objects; or even promoting the science of astronomy among the general public.
Collectively, amateur astronomers observe a variety of celestial objects and phenomena.
Common targets of amateur astronomers include the Moon, planets, stars, comets, meteor showers, and a variety of deep sky objects such as star clusters, galaxies, and nebulae.
Many amateurs like to specialise in observing particular objects, types of objects, or types of events which interest them.
One branch of amateur astronomy, amateur astrophotography, involves the taking of photos of the night sky.
Astrophotography has become more popular with the introduction of far easier to use equipment including, digital cameras, DSLR cameras and relatively sophisticated purpose built high quality CCD cameras.
Most amateur astronomers work at visible wavelengths, but a small minority experiment with wavelengths outside the visible spectrum.
An early pioneer of radio astronomy was Grote Reber, an amateur astronomer who constructed the first purpose built radio telescope in the late 1930s to follow up on the discovery of radio wavelength emissions from space by Karl Jansky.
Non-visual amateur astronomy includes the use of infrared filters on conventional telescopes, and also the use of radio telescopes.
Some amateur astronomers use home-made radio telescopes, while others use radio telescopes that were originally built for astronomical research but have since been made available for use by amateurs.
The One-Mile Telescope is one such example.
Amateur astronomers use a range of instruments to study the sky, depending on a combination of their interests and resources.
Methods include simply looking at the night sky with the naked eye, using binoculars, and using a variety of optical telescopes of varying power and quality, as well as additional sophisticated equipment, such as cameras, to study light from the sky in both the visual and non-visual parts of the spectrum.
Commercial telescopes are available, new and used, but it is also common for amateur astronomers to build (or commission the building of) their own custom telescopes.
Some people even focus on amateur telescope making as their primary interest within the hobby of amateur astronomy.
Although specialized and experienced amateur astronomers tend to acquire more specialized and more powerful equipment over time, relatively simple equipment is often preferred for certain tasks.
Binoculars, for instance, although generally of lower power than the majority of telescopes, also tend to provide a wider field of view, which is preferable for looking at some objects in the night sky.
Amateur astronomers also use star charts that, depending on experience and intentions, may range from simple planispheres through to detailed charts of very specific areas of the night sky.
A range of astronomy software is available and used by amateur astronomers, including software that generates maps of the sky, software to assist with astrophotography, observation scheduling software, and software to perform various calculations pertaining to astronomical phenomena.
Amateur astronomers often like to keep records of their observations, which usually takes the form of an observing log.
Observing logs typically record details about which objects were observed and when, as well as describing the details that were seen.
Sketching is sometimes used within logs, and photographic records of observations have also been used in recent times.
The information gathered is used to help studies and interactions between amateur astronomers in yearly gatherings.
Although not professional information or credible, it is a way for the hobby lovers to share their new sightings and experiences.
The Internet is an essential tool of amateur astronomers.
The popularity of imaging among amateurs has led to large numbers of web sites being written by individuals about their images and equipment.
Much of the social interaction of amateur astronomy occurs on mailing lists or discussion groups.
Discussion group servers host numerous astronomy lists.
A great deal of the commerce of amateur astronomy, the buying and selling of equipment, occurs online.
Many amateurs use online tools to plan their nightly observing sessions, using tools such as the Clear Sky Chart.
While a number of interesting celestial objects are readily identified by the naked eye, sometimes with the aid of a star chart, many others are so faint or inconspicuous that technical means are necessary to locate them.
Although many methods are used in amateur astronomy, most are variations of a few specific techniques.
Star hopping is a method often used by amateur astronomers with low-tech equipment such as binoculars or a manually driven telescope.
It involves the use of maps (or memory) to locate known landmark stars, and "hopping" between them, often with the aid of a finderscope.
Because of its simplicity, star hopping is a very common method for finding objects that are close to naked-eye stars.
More advanced methods of locating objects in the sky include telescope mounts with "setting circles", which assist with pointing telescopes to positions in the sky that are known to contain objects of interest, and "GOTO telescopes", which are fully automated telescopes that are capable of locating objects on demand (having first been calibrated).
The advent of mobile applications for use in smartphones has led to the creation of many dedicated apps.
These apps allow any user to easily locate celestial objects of interest by simply pointing the smartphone device in that direction in the sky.
These apps make use of the inbuilt hardware in the phone, such as GPS location and gyroscope.
Useful information about the pointed object like celestial coordinates, the name of the object, its constellation, etc.
are provided for a quick reference.
Some paid versions give more information.
These apps are gradually getting into regular use during observing, for the alignment process of telescopes.
Setting circles are angular measurement scales that can be placed on the two main rotation axes of some telescopes.
Since the widespread adoption of digital setting circles, any classical engraved setting circle is now specifically identified as an "analog setting circle" (ASC).
By knowing the coordinates of an object (usually given in equatorial coordinates), the telescope user can use the setting circle to align (i.e., point) the telescope in the appropriate direction before looking through its eyepiece.
A computerized setting circle is called a "digital setting circle" (DSC).
Although digital setting circles can be used to display a telescope's RA and Dec coordinates, they are not simply a digital read-out of what can be seen on the telescope's analog setting circles.
As with go-to telescopes, digital setting circle computers (commercial names include Argo Navis, Sky Commander, and NGC Max) contain databases of tens of thousands of celestial objects and projections of planet positions.
To find a celestial object in a telescope equipped with a DSC computer, one does not need to look up the specific RA and Dec coordinates in a book or other resource, and then adjust the telescope to those numerical readings.
Rather, the object is chosen from the electronic database, which causes distance values and arrow markers to appear in the display that indicate the distance and direction to move the telescope.
The telescope is moved until the two angular distance values reach zero, indicating that the telescope is properly aligned.
When both the RA and Dec axes are thus "zeroed out", the object should be in the eyepiece.
Many DSCs, like go-to systems, can also work in conjunction with laptop sky programs.
Computerized systems provide the further advantage of computing coordinate precession.
Traditional printed sources are subtitled by the "epoch" year, which refers to the positions of celestial objects at a given time to the nearest year (e.g., J2005, J2007).
Most such printed sources have been updated for intervals of only about every fifty years (e.g., J1900, J1950, J2000).
Computerized sources, on the other hand, are able to calculate the right ascension and declination of the "epoch of date" to the exact instant of observation.
GOTO telescopes have become more popular since the 1980s as technology has improved and prices have been reduced.
With these computer-driven telescopes, the user typically enters the name of the item of interest and the mechanics of the telescope point the telescope towards that item automatically.
They have several notable advantages for amateur astronomers intent on research.
For example, GOTO telescopes tend to be faster for locating items of interest than star hopping, allowing more time for studying of the object.
GOTO also allows manufacturers to add equatorial tracking to mechanically simpler alt-azimuth telescope mounts, allowing them to produce an overall less expensive product.
GOTO telescopes usually have to be calibrated using alignment stars in order to provide accurate tracking and positioning.
However, several telescope manufacturers have recently developed telescope systems that are calibrated with the use of built-in GPS, decreasing the time it takes to set up a telescope at the start of an observing session.
With the development of fast Internet in the last part of the 20th century along with advances in computer controlled telescope mounts and CCD cameras "Remote Telescope" astronomy is now a viable means for amateur astronomers not aligned with major telescope facilities to partake in research and deep sky imaging.
This enables anyone to control a telescope a great distance away in a dark location.
The observer can image through the telescope using CCD cameras.
The digital data collected by the telescope is then transmitted and displayed to the user by means of the Internet.
An example of a digital remote telescope operation for public use via the Internet is the Bareket observatory, and there are telescope farms in New Mexico, Australia and Atacama in Chile.
Amateur astronomers engage in many imaging techniques including film, DSLR, LRGB, and CCD astrophotography.
Because CCD imagers are linear, image processing may be used to subtract away the effects of light pollution, which has increased the popularity of astrophotography in urban areas.
Narrowband filters may also be used to minimize light pollution.
Scientific research is most often not the "main" goal for many amateur astronomers, unlike professional astronomers.
Work of scientific merit is possible, however, and many amateurs successfully contribute to the knowledge base of professional astronomers.
Astronomy is sometimes promoted as one of the few remaining sciences for which amateurs can still contribute useful data.
To recognize this, the Astronomical Society of the Pacific annually gives Amateur Achievement Awards for significant contributions to astronomy by amateurs.
The majority of scientific contributions by amateur astronomers are in the area of data collection.
In particular, this applies where large numbers of amateur astronomers with small telescopes are more effective than the relatively small number of large telescopes that are available to professional astronomers.
Several organizations, such as the American Association of Variable Star Observers, exist to help coordinate these contributions.
Amateur astronomers often contribute toward activities such as monitoring the changes in brightness of variable stars and supernovae, helping to track asteroids, and observing occultations to determine both the shape of asteroids and the shape of the terrain on the apparent edge of the Moon as seen from Earth.
With more advanced equipment, but still cheap in comparison to professional setups, amateur astronomers can measure the light spectrum emitted from astronomical objects, which can yield high-quality scientific data if the measurements are performed with due care.
A relatively recent role for amateur astronomers is searching for overlooked phenomena (e.g., Kreutz Sungrazers) in the vast libraries of digital images and other data captured by Earth and space based observatories, much of which is available over the Internet.
In the past and present, amateur astronomers have played a major role in discovering new comets.
Recently however, funding of projects such as the Lincoln Near-Earth Asteroid Research and Near Earth Asteroid Tracking projects has meant that most comets are now discovered by automated systems long before it is possible for amateurs to see them.
There are a large number of amateur astronomical societies around the world, that serve as a meeting point for those interested in amateur astronomy.
Members range from active observers with their own equipment to "armchair astronomers" who are simply interested in the topic.
Societies range widely in their goals and activities, which may depend on a variety of factors such as geographic spread, local circumstances, size, and membership.
For example, a small local society located in dark countryside may focus on practical observing and star parties, whereas a large one based in a major city might have numerous members but be limited by light pollution and thus hold regular indoor meetings with guest speakers instead.
Major national or international societies generally publish their own journal or newsletter, and some hold large multi-day meetings akin to a scientific conference or convention.
They may also have sections devoted to particular topics, such as observing the Moon or amateur telescope making.
</doc>
<doc id="751" url="https://en.wikipedia.org/wiki?curid=751" title="Aikido">
Aikido

Aikido's techniques include: irimi (entering), and tenkan (turning) movements (that redirect the opponent's attack momentum), various types of throws and joint locks.
Aikido derives mainly from the martial art of Daitō-ryū Aiki-jūjutsu, but began to diverge from it in the late 1920s, partly due to Ueshiba's involvement with the Ōmoto-kyō religion.
Ueshiba's early students' documents bear the term "aiki-jūjutsu".
Ueshiba's senior students have different approaches to aikido, depending partly on when they studied with him.
Today aikido is found all over the world in a number of styles, with broad ranges of interpretation and emphasis.
However, they all share techniques formulated by Ueshiba and most have concern for the well-being of the attacker.
The word "aikido" is formed of three kanji:

The term "aiki" does not readily appear in the Japanese language outside the scope of budō.
This has led to many possible interpretations of the word.
The term is also found in martial arts such as judo and kendo, and in various non-martial arts, such as Japanese calligraphy (), flower arranging () and tea ceremony ().
Therefore, from a purely literal interpretation, aikido is the "Way of combining forces" or "Way of unifying energy", in which the term "aiki" refers to the martial arts principle or tactic of blending with an attacker's movements for the purpose of controlling their actions with minimal effort.
One applies by understanding the rhythm and intent of the attacker to find the optimal position and timing to apply a counter-technique.
Aikido was created by Morihei Ueshiba ( , 14 December 1883 – 26 April 1969), referred to by some aikido practitioners as ("Great Teacher").
The term "aikido" was coined in the twentieth century.
Ueshiba envisioned aikido not only as the synthesis of his martial training, but as an expression of his personal philosophy of universal peace and reconciliation.
During Ueshiba's lifetime and continuing today, aikido has evolved from the "aiki" that Ueshiba studied into a variety of expressions by martial artists throughout the world.
Ueshiba developed aikido primarily during the late 1920s through the 1930s through the synthesis of the older martial arts that he had studied.
The core martial art from which aikido derives is Daitō-ryū aiki-jūjutsu, which Ueshiba studied directly with Takeda Sōkaku, the reviver of that art.
Additionally, Ueshiba is known to have studied Tenjin Shin'yō-ryū with Tozawa Tokusaburō in Tokyo in 1901, Gotōha Yagyū Shingan-ryū under Nakai Masakatsu in Sakai from 1903 to 1908, and judo with Kiyoichi Takagi ( , 1894–1972) in Tanabe in 1911.
The art of Daitō-ryū is the primary technical influence on aikido.
Along with empty-handed throwing and joint-locking techniques, Ueshiba incorporated training movements with weapons, such as those for the spear (), short staff (), and perhaps the .
However, aikido derives much of its technical structure from the art of swordsmanship ().
Ueshiba moved to Hokkaidō in 1912, and began studying under Takeda Sokaku in 1915.
His official association with Daitō-ryū continued until 1937.
However, during the latter part of that period, Ueshiba had already begun to distance himself from Takeda and the Daitō-ryū.
At that time Ueshiba was referring to his martial art as "Aiki Budō".
It is unclear exactly when Ueshiba began using the name "aikido", but it became the official name of the art in 1942 when the Greater Japan Martial Virtue Society () was engaged in a government sponsored reorganization and centralization of Japanese martial arts.
After Ueshiba left Hokkaidō in 1919, he met and was profoundly influenced by Onisaburo Deguchi, the spiritual leader of the Ōmoto-kyō religion (a neo-Shinto movement) in Ayabe.
One of the primary features of Ōmoto-kyō is its emphasis on the attainment of utopia during one's life.
This was a great influence on Ueshiba's martial arts philosophy of extending love and compassion especially to those who seek to harm others.
Aikido demonstrates this philosophy in its emphasis on mastering martial arts so that one may receive an attack and harmlessly redirect it.
In an ideal resolution, not only is the receiver unharmed, but so is the attacker.
In addition to the effect on his spiritual growth, the connection with Deguchi gave Ueshiba entry to elite political and military circles as a martial artist.
As a result of this exposure, he was able to attract not only financial backing but also gifted students.
Several of these students would found their own styles of aikido.
Aikido was first brought to the rest of the world in 1951 by Minoru Mochizuki with a visit to France where he introduced aikido techniques to judo students.
He was followed by Tadashi Abe in 1952, who came as the official Aikikai Hombu representative, remaining in France for seven years.
Kenji Tomiki toured with a delegation of various martial arts through 15 continental states of the United States in 1953.
Later that year, Koichi Tohei was sent by Aikikai Hombu to Hawaii for a full year, where he set up several dojo.
This trip was followed by several further visits and is considered the formal introduction of aikido to the United States.
The United Kingdom followed in 1955; Italy in 1964 by Hiroshi Tada; and Germany in 1965 by Katsuaki Asai.
Designated "Official Delegate for Europe and Africa" by Morihei Ueshiba, Masamichi Noro arrived in France in September 1961.
Seiichi Sugano was appointed to introduce aikido to Australia in 1965.
Today there are aikido dojo throughout the world.
The largest aikido organization is the Aikikai Foundation, which remains under the control of the Ueshiba family.
However, aikido has many styles, mostly formed by Morihei Ueshiba's major students.
The earliest independent styles to emerge were Yoseikan Aikido, begun by Minoru Mochizuki in 1931, Yoshinkan Aikido, founded by Gozo Shioda in 1955, and Shodokan Aikido, founded by Kenji Tomiki in 1967.
The emergence of these styles pre-dated Ueshiba's death and did not cause any major upheavals when they were formalized.
Shodokan Aikido, however, was controversial, since it introduced a unique rule-based competition that some felt was contrary to the spirit of aikido.
After Ueshiba's death in 1969, two more major styles emerged.
Significant controversy arose with the departure of the Aikikai Hombu Dojo's chief instructor Koichi Tohei, in 1974.
Tohei left as a result of a disagreement with the son of the founder, Kisshomaru Ueshiba, who at that time headed the Aikikai Foundation.
The disagreement was over the proper role of "ki" development in regular aikido training.
After Tohei left, he formed his own style, called Shin Shin Toitsu Aikido, and the organization that governs it, the Ki Society ("Ki no Kenkyūkai").
A final major style evolved from Ueshiba's retirement in Iwama, Ibaraki and the teaching methodology of long term student Morihiro Saito.
It is unofficially referred to as the "Iwama style", and at one point a number of its followers formed a loose network of schools they called Iwama Ryu.
Although Iwama style practitioners remained part of the Aikikai until Saito's death in 2002, followers of Saito subsequently split into two groups.
One remained with the Aikikai and the other formed the independent Shinshin Aikishuren Kai in 2004 around Saito's son Hitohiro Saito.
Today, the major styles of aikido are each run by a separate governing organization, have their own in Japan, and have an international breadth.
The study of "ki" is an important component of aikido, and its study defies categorization as either "physical" or "mental" training, as it encompasses both.
The "kanji" for "ki" normally is written as .
It was written as until the writing reforms after World War II, and this older form still is seen on occasion.
The character for "ki" is used in everyday Japanese terms, such as , or .
"Ki" has many meanings, including "ambience", "mind", "mood", and "intention", however, in traditional martial arts it is often used to refer to "life energy".
Gōzō Shioda's Yoshinkan Aikido, considered one of the "hard styles", largely follows Ueshiba's teachings from before World War II, and surmises that the secret to "ki" lies in timing and the application of the whole body's strength to a single point.
In later years, Ueshiba's application of "ki" in aikido took on a softer, more gentle feel.
This was his Takemusu Aiki and many of his later students teach about "ki" from this perspective.
Koichi Tohei's Ki Society centers almost exclusively around the study of the empirical (albeit subjective) experience of "ki" with students ranked separately in aikido techniques and "ki" development.
In aikido, as in virtually all Japanese martial arts, there are both physical and mental aspects of training.
The physical training in aikido is diverse, covering both general physical fitness and conditioning, as well as specific techniques.
Because a substantial portion of any aikido curriculum consists of throws, beginners learn how to safely fall or roll.
The specific techniques for attack include both strikes and grabs; the techniques for defense consist of throws and pins.
After basic techniques are learned, students study freestyle defense against multiple opponents, and techniques with weapons.
Physical training goals pursued in conjunction with aikido include controlled relaxation, correct movement of joints such as hips and shoulders, flexibility, and endurance, with less emphasis on strength training.
In aikido, pushing or extending movements are much more common than pulling or contracting movements.
This distinction can be applied to general fitness goals for the aikido practitioner.
In aikido, specific muscles or muscle groups are not isolated and worked to improve tone, mass, or power.
Aikido-related training emphasizes the use of coordinated whole-body movement and balance similar to yoga or pilates.
For example, many dojos begin each class with , which may include stretching and ukemi (break falls).
Aikido training is based primarily on two partners practicing pre-arranged forms ("kata") rather than freestyle practice.
The basic pattern is for the receiver of the technique ("uke") to initiate an attack against the person who applies the technique—the "tori", or "shite" (depending on aikido style), also referred to as "nage" (when applying a throwing technique), who neutralises this attack with an aikido technique.
Both halves of the technique, that of "uke" and that of "tori", are considered essential to aikido training.
Both are studying aikido principles of blending and adaptation.
"Tori" learns to blend with and control attacking energy, while "uke" learns to become calm and flexible in the disadvantageous, off-balance positions in which "tori" places them.
This "receiving" of the technique is called "ukemi".
"Uke" continuously seeks to regain balance and cover vulnerabilities (e.g., an exposed side), while "tori" uses position and timing to keep "uke" off-balance and vulnerable.
In more advanced training, "uke" will sometimes apply to regain balance and pin or throw "tori".
Aikido techniques are usually a defense against an attack, so students must learn to deliver various types of attacks to be able to practice aikido with a partner.
Although attacks are not studied as thoroughly as in striking-based arts, sincere attacks (a strong strike or an immobilizing grab) are needed to study correct and effective application of technique.
Many of the of aikido resemble cuts from a sword or other grasped object, which indicate its origins in techniques intended for armed combat.
Other techniques, which explicitly appear to be punches ("tsuki"), are practiced as thrusts with a knife or sword.
Kicks are generally reserved for upper-level variations; reasons cited include that falls from kicks are especially dangerous, and that kicks (high kicks in particular) were uncommon during the types of combat prevalent in feudal Japan.
Some basic strikes include:

Beginners in particular often practice techniques from grabs, both because they are safer and because it is easier to feel the energy and lines of force of a hold than a strike.
Some grabs are historically derived from being held while trying to draw a weapon; a technique could then be used to free oneself and immobilize or strike the attacker who is grabbing the defender.
The following are examples of some basic grabs:

The following are a sample of the basic or widely practiced throws and pins.
Many of these techniques derive from Daitō-ryū Aiki-jūjutsu, but some others were invented by Morihei Ueshiba.
The precise terminology for some may vary between organisations and styles, so what follows are the terms used by the Aikikai Foundation.
Note that despite the names of the first five techniques listed, they are not universally taught in numeric order.
Aikido makes use of body movement ("tai sabaki") to blend with "uke".
For example, an "entering" ("irimi") technique consists of movements inward towards "uke", while a technique uses a pivoting motion.
Additionally, an technique takes place in front of "uke", whereas an technique takes place to their side; a technique is applied with motion to the front of "uke", and a version is applied with motion towards the rear of "uke", usually by incorporating a turning or pivoting motion.
Finally, most techniques can be performed while in a seated posture ("seiza").
Techniques where both "uke" and "tori" are standing are called "tachi-waza", techniques where both start off in "seiza" are called "suwari-waza", and techniques performed with "uke" standing and "tori" sitting are called "hanmi handachi" ().
Thus, from fewer than twenty basic techniques, there are thousands of possible implementations.
For instance, "ikkyō" can be applied to an opponent moving forward with a strike (perhaps with an "ura" type of movement to redirect the incoming force), or to an opponent who has already struck and is now moving back to reestablish distance (perhaps an "omote-waza" version).
Specific aikido "kata" are typically referred to with the formula "attack-technique(-modifier)".
For instance, "katate-dori ikkyō" refers to any "ikkyō" technique executed when "uke" is holding one wrist.
This could be further specified as "katate-dori ikkyō omote", referring to any forward-moving "ikkyō" technique from that grab.
"Atemi" () are strikes (or feints) employed during an aikido technique.
Some view "atemi" as attacks against "vital points" meant to cause damage in and of themselves.
For instance, Gōzō Shioda described using "atemi" in a brawl to quickly down a gang's leader.
Others consider "atemi", especially to the face, to be methods of distraction meant to enable other techniques.
A strike, whether or not it is blocked, can startle the target and break their concentration.
The target may become unbalanced in attempting to avoid the blow, for example by jerking the head back, which may allow for an easier throw.
Many sayings about "atemi" are attributed to Morihei Ueshiba, who considered them an essential element of technique.
Weapons training in aikido traditionally includes the short staff ("jō") although its techniques resemble closely the use of the bayonet or Jūkendō, the wooden sword ("bokken"), and the knife ("tantō").
Some schools incorporate firearm-disarming techniques.
Both weapon-taking and weapon-retention are taught.
Some schools, such as the Iwama style of Morihiro Saito, usually spend substantial time practicing with both "bokken" and "jō", under the names of "aiki-ken", and "aiki-jō", respectively.
The founder developed many of the empty-handed techniques from traditional sword, spear and bayonet movements.
Consequently, the practice of the weapons arts gives insight into the origin of techniques and movements, and reinforces the concepts of distance, timing, foot movement, presence and connectedness with one's training partner(s).
One feature of aikido is training to defend against multiple attackers, often called "taninzudori", or "taninzugake".
Freestyle practice with multiple attackers called "randori" () is a key part of most curricula and is required for the higher level ranks.
"Randori" exercises a person's ability to intuitively perform techniques in an unstructured environment.
Strategic choice of techniques, based on how they reposition the student relative to other attackers, is important in "randori" training.
For instance, an "ura" technique might be used to neutralise the current attacker while turning to face attackers approaching from behind.
In Shodokan Aikido, "randori" differs in that it is not performed with multiple persons with defined roles of defender and attacker, but between two people, where both participants attack, defend, and counter at will.
In this respect it resembles judo "randori".
In applying a technique during training, it is the responsibility of "tori" to prevent injury to "uke" by employing a speed and force of application that is commensurate with their partner's proficiency in "ukemi".
Injuries (especially those to the joints), when they do occur in aikido, are often the result of "tori" misjudging the ability of "uke" to receive the throw or pin.
A study of injuries in the martial arts showed that the type of injuries varied considerably from one art to the other.
Soft tissue injuries are one of the most common types of injuries found within aikido, as well as joint strain and stubbed fingers and toes.
Several deaths from head-and-neck injuries, caused by aggressive "shihōnage" in a senpai/kōhai hazing context, have been reported.
Aikido training is mental as well as physical, emphasizing the ability to relax the mind and body even under the stress of dangerous situations.
This is necessary to enable the practitioner to perform the bold enter-and-blend movements that underlie aikido techniques, wherein an attack is met with confidence and directness.
Morihei Ueshiba once remarked that one "must be willing to receive 99% of an opponent's attack and stare death in the face" in order to execute techniques without hesitation.
As a martial art concerned not only with fighting proficiency but with the betterment of daily life, this mental aspect is of key importance to aikido practitioners.
Aikido practitioners (commonly called "aikidōka" outside Japan) generally progress by promotion through a series of "grades" ("kyū"), followed by a series of "degrees" ("dan"), pursuant to formal testing procedures.
Some aikido organizations use belts to distinguish practitioners' grades, often simply white and black belts to distinguish "kyu" and "dan" grades, though some use various belt colors.
Testing requirements vary, so a particular rank in one organization is not comparable or interchangeable with the rank of another.
Some dojos do not allow students to take the test to obtain a "dan" rank unless they are 16 or older.
The uniform worn for practicing aikido ("aikidōgi") is similar to the training uniform ("keikogi") used in most other modern martial arts; simple trousers and a wraparound jacket, usually white.
Both thick ("judo-style"), and thin ("karate-style") cotton tops are used.
Aikido-specific tops are available with shorter sleeves which reach to just below the elbow.
Most aikido systems add a pair of wide pleated black or indigo trousers called a "hakama" (used also in Naginatajutsu, kendo, and iaido).
In many schools, its use is reserved for practitioners with ("dan") ranks or for instructors, while others allow all practitioners to wear a "hakama" regardless of rank.
The most common criticism of aikido is that it suffers from a lack of realism in training.
The attacks initiated by "uke" (and which "tori" must defend against) have been criticized as being "weak", "sloppy", and "little more than caricatures of an attack".
Weak attacks from "uke" allow for a conditioned response from "tori", and result in underdevelopment of the skills needed for the safe and effective practice of both partners.
To counteract this, some styles allow students to become less compliant over time but, in keeping with the core philosophies, this is after having demonstrated proficiency in being able to protect themselves and their training partners.
Shodokan Aikido addresses the issue by practising in a competitive format.
Such adaptations are debated between styles, with some maintaining that there is no need to adjust their methods because either the criticisms are unjustified, or that they are not training for self-defense or combat effectiveness, but spiritual, fitness or other reasons.
Another criticism pertains to the shift in training focus after the end of Ueshiba's seclusion in Iwama from 1942 to the mid-1950s, as he increasingly emphasized the spiritual and philosophical aspects of aikido.
As a result, strikes to vital points by "tori", entering ("irimi") and initiation of techniques by "tori", the distinction between "omote" (front side) and "ura" (back side) techniques, and the use of weapons, were all de-emphasized or eliminated from practice.
Some Aikido practitioners feel that lack of training in these areas leads to an overall loss of effectiveness.
Conversely, some styles of aikido receive criticism for not placing enough importance on the spiritual practices emphasized by Ueshiba.
According to Minoru Shibata of Aikido Journal, "O-Sensei's aikido was not a continuation and extension of the old and has a distinct discontinuity with past martial and philosophical concepts."
That is, that aikido practitioners who focus on aikido's roots in traditional "jujutsu" or "kenjutsu" are diverging from what Ueshiba taught.
Such critics urge practitioners to embrace the assertion that "[Ueshiba's] transcendence to the spiritual and universal reality were the fundamentals of the paradigm that he demonstrated."
</doc>
<doc id="752" url="https://en.wikipedia.org/wiki?curid=752" title="Art">
Art

Art is a diverse range of human activities in creating visual, auditory or performing artifacts (artworks), expressing the author's imaginative, conceptual idea, or technical skill, intended to be appreciated for their beauty or emotional power.
In their most general form these activities include the production of works of art, the criticism of art, the study of the history of art, and the aesthetic dissemination of art.
The three classical branches of art are painting, sculpture and architecture.
Music, theatre, film, dance, and other performing arts, as well as literature and other media such as interactive media, are included in a broader definition of the arts.
Until the 17th century, "art" referred to any skill or mastery and was not differentiated from crafts or sciences.
In modern usage after the 17th century, where aesthetic considerations are paramount, the fine arts are separated and distinguished from acquired skills in general, such as the decorative or applied arts.
Though the definition of what constitutes art is disputed and has changed over time, general descriptions mention an idea of imaginative or technical skill stemming from human agency and creation.
The nature of art and related concepts, such as creativity and interpretation, are explored in a branch of philosophy known as aesthetics.
In the perspective of the history of art, artistic works have existed for almost as long as humankind: from early pre-historic art to contemporary art; however, some theories restrict the concept of "artistic works" to modern Western societies.
One early sense of the definition of "art" is closely related to the older Latin meaning, which roughly translates to "skill" or "craft," as associated with words such as "artisan."
English words derived from this meaning include "artifact", "artificial", "artifice", "medical arts", and "military arts".
However, there are many other colloquial uses of the word, all with some relation to its etymology.
Over time, philosophers like Plato, Aristotle, Socrates and Kant, among others, questioned the meaning of art.
Several dialogues in Plato tackle questions about art: Socrates says that poetry is inspired by the muses, and is not rational.
He speaks approvingly of this, and other forms of divine madness (drunkenness, eroticism, and dreaming) in the "Phaedrus "(265a–c), and yet in the ""Republic"" wants to outlaw Homer's great poetic art, and laughter as well.
In "Ion", Socrates gives no hint of the disapproval of Homer that he expresses in the "Republic".
The dialogue "Ion" suggests that Homer's "Iliad" functioned in the ancient Greek world as the Bible does today in the modern Christian world: as divinely inspired literary art that can provide moral guidance, if only it can be properly interpreted.
With regards to the literary art and the musical arts, Aristotle considered epic poetry, tragedy, comedy, dithyrambic poetry and music to be mimetic or imitative art, each varying in imitation by medium, object, and manner.
For example, music imitates with the media of rhythm and harmony, whereas dance imitates with rhythm alone, and poetry with language.
The forms also differ in their object of imitation.
Comedy, for instance, is a dramatic imitation of men worse than average; whereas tragedy imitates men slightly better than average.
Lastly, the forms differ in their manner of imitation—through narrative or character, through change or no change, and through drama or no drama.
Aristotle believed that imitation is natural to mankind and constitutes one of mankind's advantages over animals.
The second, and more recent, sense of the word "art" as an abbreviation for "creative art" or "fine art" emerged in the early 17th century.
Fine art refers to a skill used to express the artist's creativity, or to engage the audience's aesthetic sensibilities, or to draw the audience towards consideration of more refined or "finer" work of art.
Within this latter sense, the word "art" may refer to several things: (i) a study of a creative skill, (ii) a process of using the creative skill, (iii) a product of the creative skill, or (iv) the audience's experience with the creative skill.
The creative arts ("art" as discipline) are a collection of disciplines which produce "artworks" ("art" as objects) that are compelled by a personal drive (art as activity) and convey a message, mood, or symbolism for the perceiver to interpret (art as experience).
Art is something that stimulates an individual's thoughts, emotions, beliefs, or ideas through the senses.
Works of art can be explicitly made for this purpose or interpreted on the basis of images or objects.
For some scholars, such as Kant, the sciences and the arts could be distinguished by taking science as representing the domain of knowledge and the arts as representing the domain of the freedom of artistic expression.
Often, if the skill is being used in a common or practical way, people will consider it a craft instead of art.
Likewise, if the skill is being used in a commercial or industrial way, it may be considered commercial art instead of fine art.
On the other hand, crafts and design are sometimes considered applied art.
Some art followers have argued that the difference between fine art and applied art has more to do with value judgments made about the art than any clear definitional difference.
However, even fine art often has goals beyond pure creativity and self-expression.
The purpose of works of art may be to communicate ideas, such as in politically, spiritually, or philosophically motivated art; to create a sense of beauty (see aesthetics); to explore the nature of perception; for pleasure; or to generate strong emotions.
The purpose may also be seemingly nonexistent.
The nature of art has been described by philosopher Richard Wollheim as "one of the most elusive of the traditional problems of human culture".
Art has been defined as a vehicle for the expression or communication of emotions and ideas, a means for exploring and appreciating formal elements for their own sake, and as "mimesis" or representation.
Art as mimesis has deep roots in the philosophy of Aristotle.
Leo Tolstoy identified art as a use of indirect means to communicate from one person to another.
Benedetto Croce and R.G.
Collingwood advanced the idealist view that art expresses emotions, and that the work of art therefore essentially exists in the mind of the creator.
The theory of art as form has its roots in the philosophy of Kant, and was developed in the early twentieth century by Roger Fry and Clive Bell.
More recently, thinkers influenced by Martin Heidegger have interpreted art as the means by which a community develops for itself a medium for self-expression and interpretation.
George Dickie has offered an institutional theory of art that defines a work of art as any artifact upon which a qualified person or persons acting on behalf of the social institution commonly referred to as "the art world" has conferred "the status of candidate for appreciation".
Larry Shiner has described fine art as "not an essence or a fate but something we have made.
Art as we have generally understood it is a European invention barely two hundred years old."
Art may be characterized in terms of mimesis (its representation of reality), narrative (storytelling), expression, communication of emotion, or other qualities.
During the Romantic period, art came to be seen as "a special faculty of the human mind to be classified with religion and science".
The oldest documented forms of art are visual arts, which include creation of images or objects in fields including today painting, sculpture, printmaking, photography, and other visual media.
Sculptures, cave paintings, rock paintings and petroglyphs from the Upper Paleolithic dating to roughly 40,000 years ago have been found, but the precise meaning of such art is often disputed because so little is known about the cultures that produced them.
The oldest art objects in the world—a series of tiny, drilled snail shells about 75,000 years old—were discovered in a South African cave.
Containers that may have been used to hold paints have been found dating as far back as 100,000 years.
Etched shells by "Homo erectus" from 430,000 and 540,000 years ago were discovered in 2014.
Many great traditions in art have a foundation in the art of one of the great ancient civilizations: Ancient Egypt, Mesopotamia, Persia, India, China, Ancient Greece, Rome, as well as Inca, Maya, and Olmec.
Each of these centers of early civilization developed a unique and characteristic style in its art.
Because of the size and duration of these civilizations, more of their art works have survived and more of their influence has been transmitted to other cultures and later times.
Some also have provided the first records of how artists worked.
For example, this period of Greek art saw a veneration of the human physical form and the development of equivalent skills to show musculature, poise, beauty, and anatomically correct proportions.
In Byzantine and Medieval art of the Western Middle Ages, much art focused on the expression of subjects about Biblical and religious culture, and used styles that showed the higher glory of a heavenly world, such as the use of gold in the background of paintings, or glass in mosaics or windows, which also presented figures in idealized, patterned (flat) forms.
Nevertheless, a classical realist tradition persisted in small Byzantine works, and realism steadily grew in the art of Catholic Europe.
Renaissance art had a greatly increased emphasis on the realistic depiction of the material world, and the place of humans in it, reflected in the corporeality of the human body, and development of a systematic method of graphical perspective to depict recession in a three-dimensional picture space.
In the east, Islamic art's rejection of iconography led to emphasis on geometric patterns, calligraphy, and architecture.
Further east, religion dominated artistic styles and forms too.
India and Tibet saw emphasis on painted sculptures and dance, while religious painting borrowed many conventions from sculpture and tended to bright contrasting colors with emphasis on outlines.
China saw the flourishing of many art forms: jade carving, bronzework, pottery (including the stunning terracotta army of Emperor Qin), poetry, calligraphy, music, painting, drama, fiction, etc.
Chinese styles vary greatly from era to era and each one is traditionally named after the ruling dynasty.
So, for example, Tang dynasty paintings are monochromatic and sparse, emphasizing idealized landscapes, but Ming dynasty paintings are busy and colorful, and focus on telling stories via setting and composition.
Japan names its styles after imperial dynasties too, and also saw much interplay between the styles of calligraphy and painting.
Woodblock printing became important in Japan after the 17th century.
The western Age of Enlightenment in the 18th century saw artistic depictions of physical and rational certainties of the clockwork universe, as well as politically revolutionary visions of a post-monarchist world, such as Blake's portrayal of Newton as a divine geometer, or David's propagandistic paintings.
This led to Romantic rejections of this in favor of pictures of the emotional side and individuality of humans, exemplified in the novels of Goethe.
The late 19th century then saw a host of artistic movements, such as academic art, Symbolism, impressionism and fauvism among others.
The history of twentieth-century art is a narrative of endless possibilities and the search for new standards, each being torn down in succession by the next.
Thus the parameters of impressionism, Expressionism, Fauvism, Cubism, Dadaism, Surrealism, etc.
cannot be maintained very much beyond the time of their invention.
Increasing global interaction during this time saw an equivalent influence of other cultures into Western art.
Thus, Japanese woodblock prints (themselves influenced by Western Renaissance draftsmanship) had an immense influence on impressionism and subsequent development.
Later, African sculptures were taken up by Picasso and to some extent by Matisse.
Similarly, in the 19th and 20th centuries the West has had huge impacts on Eastern art with originally western ideas like Communism and Post-Modernism exerting a powerful influence.
Modernism, the idealistic search for truth, gave way in the latter half of the 20th century to a realization of its unattainability.
Theodor W. Adorno said in 1970, "It is now taken for granted that nothing which concerns art can be taken for granted any more: neither art itself, nor art in relationship to the whole, nor even the right of art to exist."
Relativism was accepted as an unavoidable truth, which led to the period of contemporary art and postmodern criticism, where cultures of the world and of history are seen as changing forms, which can be appreciated and drawn from only with skepticism and irony.
Furthermore, the separation of cultures is increasingly blurred and some argue it is now more appropriate to think in terms of a global culture, rather than of regional ones.
In "The Origin of the Work of Art", Martin Heidegger, a German philosopher and a seminal thinker, describes the essence of art in terms of the concepts of being and truth.
He argues that art is not only a way of expressing the element of truth in a culture, but the means of creating it and providing a springboard from which "that which is" can be revealed.
Works of art are not merely representations of the way things are, but actually produce a community's shared understanding.
Each time a new artwork is added to any culture, the meaning of what it is to exist is inherently changed.
The creative arts are often divided into more specific categories, typically along perceptually distinguishable categories such as media, genre, styles, and form.
Art form refers to the elements of art that are independent of its interpretation or significance.
It covers the methods adopted by the artist and the physical composition of the artwork, primarily non-semantic aspects of the work (i.e., figurae), such as color, contour, dimension, medium, melody, space, texture, and value.
Form may also include visual design principles, such as arrangement, balance, contrast, emphasis, harmony, proportion, proximity, and rhythm.
In general there are three schools of philosophy regarding art, focusing respectively on form, content, and context.
Extreme Formalism is the view that all aesthetic properties of art are formal (that is, part of the art form).
Philosophers almost universally reject this view and hold that the properties and aesthetics of art extend beyond materials, techniques, and form.
Unfortunately, there is little consensus on terminology for these informal properties.
Some authors refer to subject matter and content – i.e., denotations and connotations – while others prefer terms like meaning and significance.
Extreme Intentionalism holds that authorial intent plays a decisive role in the meaning of a work of art, conveying the content or essential main idea, while all other interpretations can be discarded.
It defines the subject as the persons or idea represented, and the content as the artist's experience of that subject.
For example, the composition of Napoleon I on his Imperial Throne is partly borrowed from the Statue of Zeus at Olympia.
As evidenced by the title, the subject is Napoleon, and the content is Ingres's representation of Napoleon as "Emperor-God beyond time and space".
Similarly to extreme formalism, philosophers typically reject extreme intentionalism, because art may have multiple ambiguous meanings and authorial intent may be unknowable and thus irrelevant.
Its restrictive interpretation is "socially unhealthy, philosophically unreal, and politically unwise".
Finally, the developing theory of post-structuralism studies art's significance in a cultural context, such as the ideas, emotions, and reactions prompted by a work.
The cultural context often reduces to the artist's techniques and intentions, in which case analysis proceeds along lines similar to formalism and intentionalism.
However, in other cases historical and material conditions may predominate, such as religious and philosophical convictions, sociopolitical and economic structures, or even climate and geography.
Art criticism continues to grow and develop alongside art.
Art can connote a sense of trained ability or mastery of a medium.
Art can also simply refer to the developed and efficient use of a language to convey meaning with immediacy and or depth.
Art can be defined as an act of expressing feelings, thoughts, and observations.
There is an understanding that is reached with the material as a result of handling it, which facilitates one's thought processes.
A common view is that the "art", particular in its elevated sense, requires a certain level of creative expertise by the artist, whether this be a demonstration of technical ability, an originality in stylistic approach, or a combination of these two.
Traditionally skill of execution was viewed as a quality inseparable from art and thus necessary for its success; for Leonardo da Vinci, art, neither more nor less than his other endeavors, was a manifestation of skill.
Rembrandt's work, now praised for its ephemeral virtues, was most admired by his contemporaries for its virtuosity.
At the turn of the 20th century, the adroit performances of John Singer Sargent were alternately admired and viewed with skepticism for their manual fluency, yet at nearly the same time the artist who would become the era's most recognized and peripatetic iconoclast, Pablo Picasso, was completing a traditional academic training at which he excelled.
A common contemporary criticism of some modern art occurs along the lines of objecting to the apparent lack of skill or ability required in the production of the artistic object.
In conceptual art, Marcel Duchamp's "Fountain" is among the first examples of pieces wherein the artist used found objects ("ready-made") and exercised no traditionally recognised set of skills.
Tracey Emin's "My Bed", or Damien Hirst's "The Physical Impossibility of Death in the Mind of Someone Living" follow this example and also manipulate the mass media.
Emin slept (and engaged in other activities) in her bed before placing the result in a gallery as work of art.
Hirst came up with the conceptual design for the artwork but has left most of the eventual creation of many works to employed artisans.
Hirst's celebrity is founded entirely on his ability to produce shocking concepts.
The actual production in many conceptual and contemporary works of art is a matter of assembly of found objects.
However, there are many modernist and contemporary artists who continue to excel in the skills of drawing and painting and in creating "hands-on" works of art.
Art has had a great number of different functions throughout its history, making its purpose difficult to abstract or quantify to any single concept.
This does not imply that the purpose of Art is "vague", but that it has had many unique, different reasons for being created.
Some of these functions of Art are provided in the following outline.
The different purposes of art may be grouped according to those that are non-motivated, and those that are motivated (Lévi-Strauss).
The non-motivated purposes of art are those that are integral to being human, transcend the individual, or do not fulfill a specific external purpose.
In this sense, Art, as creativity, is something humans must do by their very nature (i.e., no other species creates art), and is therefore beyond utility.
Motivated purposes of art refer to intentional, conscious actions on the part of the artists or creator.
These may be to bring about political change, to comment on an aspect of society, to convey a specific emotion or mood, to address personal psychology, to illustrate another discipline, to (with commercial arts) sell a product, or simply as a form of communication.
The functions of art described above are not mutually exclusive, as many of them may overlap.
For example, art for the purpose of entertainment may also seek to sell a product, i.e. the movie or video game.
Since ancient times, much of the finest art has represented a deliberate display of wealth or power, often achieved by using massive scale and expensive materials.
Much art has been commissioned by political rulers or religious establishments, with more modest versions only available to the most wealthy in society.
Nevertheless, there have been many periods where art of very high quality was available, in terms of ownership, across large parts of society, above all in cheap media such as pottery, which persists in the ground, and perishable media such as textiles and wood.
In many different cultures, the ceramics of indigenous peoples of the Americas are found in such a wide range of graves that they were clearly not restricted to a social elite, though other forms of art may have been.
Reproductive methods such as moulds made mass-production easier, and were used to bring high-quality Ancient Roman pottery and Greek Tanagra figurines to a very wide market.
Cylinder seals were both artistic and practical, and very widely used by what can be loosely called the middle class in the Ancient Near East.
Once coins were widely used, these also became an art form that reached the widest range of society.
Another important innovation came in the 15th century in Europe, when printmaking began with small woodcuts, mostly religious, that were often very small and hand-colored, and affordable even by peasants who glued them to the walls of their homes.
Printed books were initially very expensive, but fell steadily in price until by the 19th century even the poorest could afford some with printed illustrations.
Popular prints of many different sorts have decorated homes and other places for centuries.
Public buildings and monuments, secular and religious, by their nature normally address the whole of society, and visitors as viewers, and display to the general public has long been an important factor in their design.
Egyptian temples are typical in that the most largest and most lavish decoration was placed on the parts that could be seen by the general public, rather than the areas seen only by the priests.
Many areas of royal palaces, castles and the houses of the social elite were often generally accessible, and large parts of the art collections of such people could often be seen, either by anybody, or by those able to pay a small price, or those wearing the correct clothes, regardless of who they were, as at the Palace of Versailles, where the appropriate extra accessories (silver shoe buckles and a sword) could be hired from shops outside.
Special arrangements were made to allow the public to see many royal or private collections placed in galleries, as with the Orleans Collection mostly housed in a wing of the Palais Royal in Paris, which could be visited for most of the 18th century.
In Italy the art tourism of the Grand Tour became a major industry from the Renaissance onwards, and governments and cities made efforts to make their key works accessible.
The British Royal Collection remains distinct, but large donations such as the Old Royal Library were made from it to the British Museum, established in 1753.
The Uffizi in Florence opened entirely as a gallery in 1765, though this function had been gradually taking the building over from the original civil servants' offices for a long time before.
The building now occupied by the Prado in Madrid was built before the French Revolution for the public display of parts of the royal art collection, and similar royal galleries open to the public existed in Vienna, Munich and other capitals.
The opening of the Musée du Louvre during the French Revolution (in 1793) as a public museum for much of the former French royal collection certainly marked an important stage in the development of public access to art, transferring ownership to a republican state, but was a continuation of trends already well established.
Most modern public museums and art education programs for children in schools can be traced back to this impulse to have art available to everyone.
Museums in the United States tend to be gifts from the very rich to the masses.
(The Metropolitan Museum of Art in New York City, for example, was created by John Taylor Johnston, a railroad executive whose personal art collection seeded the museum.)
But despite all this, at least one of the important functions of art in the 21st century remains as a marker of wealth and social status.
There have been attempts by artists to create art that can not be bought by the wealthy as a status object.
One of the prime original motivators of much of the art of the late 1960s and 1970s was to create art that could not be bought and sold.
It is "necessary to present something more than mere objects" said the major post war German artist Joseph Beuys.
This time period saw the rise of such things as performance art, video art, and conceptual art.
The idea was that if the artwork was a performance that would leave nothing behind, or was simply an idea, it could not be bought and sold.
"Democratic precepts revolving around the idea that a work of art is a commodity impelled the aesthetic innovation which germinated in the mid-1960s and was reaped throughout the 1970s.
Artists broadly identified under the heading of Conceptual art ... substituting performance and publishing activities for engagement with both the material and materialistic concerns of painted or sculptural form ... [have] endeavored to undermine the art object qua object."
In the decades since, these ideas have been somewhat lost as the art market has learned to sell limited edition DVDs of video works, invitations to exclusive performance art pieces, and the objects left over from conceptual pieces.
Many of these performances create works that are only understood by the elite who have been educated as to why an idea or video or piece of apparent garbage may be considered art.
The marker of status becomes understanding the work instead of necessarily owning it, and the artwork remains an upper-class activity.
"With the widespread use of DVD recording technology in the early 2000s, artists, and the gallery system that derives its profits from the sale of artworks, gained an important means of controlling the sale of video and computer artworks in limited editions to collectors."
Art has long been controversial, that is to say disliked by some viewers, for a wide variety of reasons, though most pre-modern controversies are dimly recorded, or completely lost to a modern view.
Iconoclasm is the destruction of art that is disliked for a variety of reasons, including religious ones.
Aniconism is a general dislike of either all figurative images, or often just religious ones, and has been a thread in many major religions.
It has been a crucial factor in the history of Islamic art, where depictions of Muhammad remain especially controversial.
Much art has been disliked purely because it depicted or otherwise stood for unpopular rulers, parties or other groups.
Artistic conventions have often been conservative and taken very seriously by art critics, though often much less so by a wider public.
The iconographic content of art could cause controversy, as with late medieval depictions of the new motif of the Swoon of the Virgin in scenes of the Crucifixion of Jesus.
The "Last Judgment" by Michelangelo was controversial for various reasons, including breaches of decorum through nudity and the Apollo-like pose of Christ.
The content of much formal art through history was dictated by the patron or commissioner rather than just the artist, but with the advent of Romanticism, and economic changes in the production of art, the artists' vision became the usual determinant of the content of his art, increasing the incidence of controversies, though often reducing their significance.
Strong incentives for perceived originality and publicity also encouraged artists to court controversy.
Théodore Géricault's "Raft of the Medusa" (c.
1820), was in part a political commentary on a recent event.
Édouard Manet's "Le Déjeuner sur l'Herbe" (1863), was considered scandalous not because of the nude woman, but because she is seated next to men fully dressed in the clothing of the time, rather than in robes of the antique world.
John Singer Sargent's "Madame Pierre Gautreau (Madam X)" (1884), caused a controversy over the reddish pink used to color the woman's ear lobe, considered far too suggestive and supposedly ruining the high-society model's reputation.
The gradual abandonment of naturalism and the depiction of realistic representations of the visual appearance of subjects in the 19th and 20th centuries led to a rolling controversy lasting for over a century.
In the twentieth century, Pablo Picasso's "Guernica" (1937) used arresting cubist techniques and stark monochromatic oils, to depict the harrowing consequences of a contemporary bombing of a small, ancient Basque town.
Leon Golub's "Interrogation III" (1981), depicts a female nude, hooded detainee strapped to a chair, her legs open to reveal her sexual organs, surrounded by two tormentors dressed in everyday clothing.
Andres Serrano's "Piss Christ" (1989) is a photograph of a crucifix, sacred to the Christian religion and representing Christ's sacrifice and final suffering, submerged in a glass of the artist's own urine.
The resulting uproar led to comments in the United States Senate about public funding of the arts.
Before Modernism, aesthetics in Western art was greatly concerned with achieving the appropriate balance between different aspects of realism or truth to nature and the ideal; ideas as to what the appropriate balance is have shifted to and fro over the centuries.
This concern is largely absent in other traditions of art.
The aesthetic theorist John Ruskin, who championed what he saw as the naturalism of J. M. W. Turner, saw art's role as the communication by artifice of an essential truth that could only be found in nature.
The definition and evaluation of art has become especially problematic since the 20th century.
Richard Wollheim distinguishes three approaches to assessing the aesthetic value of art: the Realist, whereby aesthetic quality is an absolute value independent of any human view; the Objectivist, whereby it is also an absolute value, but is dependent on general human experience; and the Relativist position, whereby it is not an absolute value, but depends on, and varies with, the human experience of different humans.
The arrival of Modernism in the late nineteenth century lead to a radical break in the conception of the function of art, and then again in the late twentieth century with the advent of postmodernism.
Clement Greenberg's 1960 article "Modernist Painting" defines modern art as "the use of characteristic methods of a discipline to criticize the discipline itself".
Greenberg originally applied this idea to the Abstract Expressionist movement and used it as a way to understand and justify flat (non-illusionistic) abstract painting:

Pop artists like Andy Warhol became both noteworthy and influential through work including and possibly critiquing popular culture, as well as the art world.
Artists of the 1980s, 1990s, and 2000s expanded this technique of self-criticism beyond "high art" to all cultural image-making, including fashion images, comics, billboards and pornography.
Duchamp once proposed that art is any activity of any kind- everything.
However, the way that only certain activities are classified today as art is a social construction.
There is evidence that there may be an element of truth to this. ""
is an art history book which examines the construction of the modern system of the arts i.e. Fine Art.
Shiner finds evidence that the older system of the arts before our modern system (fine art) held art to be any skilled human activity i.e. Ancient Greek society did not possess the term art but techne.
Techne can be understood neither as art or craft, the reason being that the distinctions of art and craft are historical products that came later on in human history.
Techne included painting, sculpting and music but also; cooking, medicine, horsemanship, geometry, carpentry, prophecy, and farming etc.
Following Duchamp during the first half of the twentieth century, a significant shift to general aesthetic theory took place which attempted to apply aesthetic theory between various forms of art, including the literary arts and the visual arts, to each other.
This resulted in the rise of the New Criticism school and debate concerning "the intentional fallacy".
At issue was the question of whether the aesthetic intentions of the artist in creating the work of art, whatever its specific form, should be associated with the criticism and evaluation of the final product of the work of art, or, if the work of art should be evaluated on its own merits independent of the intentions of the artist.
In 1946, William K. Wimsatt and Monroe Beardsley published a classic and controversial New Critical essay entitled "The Intentional Fallacy", in which they argued strongly against the relevance of an author's intention, or "intended meaning" in the analysis of a literary work.
For Wimsatt and Beardsley, the words on the page were all that mattered; importation of meanings from outside the text was considered irrelevant, and potentially distracting.
In another essay, "The Affective Fallacy," which served as a kind of sister essay to "The Intentional Fallacy" Wimsatt and Beardsley also discounted the reader's personal/emotional reaction to a literary work as a valid means of analyzing a text.
This fallacy would later be repudiated by theorists from the reader-response school of literary theory.
Ironically, one of the leading theorists from this school, Stanley Fish, was himself trained by New Critics.
Fish criticizes Wimsatt and Beardsley in his essay "Literature in the Reader" (1970).
As summarized by Gaut and Livingston in their essay "The Creation of Art": "Structuralist and post-structuralists theorists and critics were sharply critical of many aspects of New Criticism, beginning with the emphasis on aesthetic appreciation and the so-called autonomy of art, but they reiterated the attack on biographical criticisms's assumption that the artist's activities and experience were a privileged critical topic."
These authors contend that: "Anti-intentionalists, such as formalists, hold that the intentions involved in the making of art are irrelevant or peripheral to correctly interpreting art.
So details of the act of creating a work, though possibly of interest in themselves, have no bearing on the correct interpretation of the work."
Gaut and Livingston define the intentionalists as distinct from formalists stating that: "Intentionalists, unlike formalists, hold that reference to intentions is essential in fixing the correct interpretation of works."
They quote Richard Wollheim as stating that, "The task of criticism is the reconstruction of the creative process, where the creative process must in turn be thought of as something not stopping short of, but terminating on, the work of art itself."
The end of the 20th century fostered an extensive debate known as the linguistic turn controversy, or the "innocent eye debate", and generally referred to as the structuralism-poststructuralism debate in the philosophy of art.
This debate discussed the encounter of the work of art as being determined by the relative extent to which the conceptual encounter with the work of art dominates over the perceptual encounter with the work of art.
Decisive for the linguistic turn debate in art history and the humanities were the works of yet another tradition, namely the structuralism of Ferdinand de Saussure and the ensuing movement of poststructuralism.
In 1981, the artist Mark Tansey created a work of art titled "The Innocent Eye" as a criticism of the prevailing climate of disagreement in the philosophy of art during the closing decades of the 20th century.
Influential theorists include Judith Butler, Luce Irigaray, Julia Kristeva, Michel Foucault and Jacques Derrida.
The power of language, more specifically of certain rhetorical tropes, in art history and historical discourse was explored by Hayden White.
The fact that language is "not" a transparent medium of thought had been stressed by a very different form of philosophy of language which originated in the works of Johann Georg Hamann and Wilhelm von Humboldt.
Ernst Gombrich and Nelson Goodman in his book "Languages of Art: An Approach to a Theory of Symbols" came to hold that the conceptual encounter with the work of art predominated exclusively over the perceptual and visual encounter with the work of art during the 1960s and 1970s.
He was challenged on the basis of research done by the Nobel prize winning psychologist Roger Sperry who maintained that the human visual encounter was not limited to concepts represented in language alone (the linguistic turn) and that other forms of psychological representations of the work of art were equally defensible and demonstrable.
Sperry's view eventually prevailed by the end of the 20th century with aesthetic philosophers such as Nick Zangwill strongly defending a return to moderate aesthetic formalism among other alternatives.
Disputes as to whether or not to classify something as a work of art are referred to as classificatory disputes about art.
Classificatory disputes in the 20th century have included cubist and impressionist paintings, Duchamp's "Fountain", the movies, superlative imitations of banknotes, conceptual art, and video games.
Philosopher David Novitz has argued that disagreement about the definition of art are rarely the heart of the problem.
Rather, "the passionate concerns and interests that humans vest in their social life" are "so much a part of all classificatory disputes about art" (Novitz, 1996).
According to Novitz, classificatory disputes are more often disputes about societal values and where society is trying to go than they are about theory proper.
For example, when the "Daily Mail" criticized Hirst's and Emin's work by arguing "For 1,000 years art has been one of our great civilising forces.
Today, pickled sheep and soiled beds threaten to make barbarians of us all" they are not advancing a definition or theory about art, but questioning the value of Hirst's and Emin's work.
In 1998, Arthur Danto, suggested a thought experiment showing that "the status of an artifact as work of art results from the ideas a culture applies to it, rather than its inherent physical or perceptible qualities.
Cultural interpretation (an art theory of some kind) is therefore constitutive of an object's arthood."
Anti-art is a label for art that intentionally challenges the established parameters and values of art; it is term associated with Dadaism and attributed to Marcel Duchamp just before World War I, when he was making art from found objects.
One of these, "Fountain" (1917), an ordinary urinal, has achieved considerable prominence and influence on art.
Anti-art is a feature of work by Situationist International, the lo-fi Mail art movement, and the Young British Artists, though it is a form still rejected by the Stuckists, who describe themselves as anti-anti-art.
Architecture is often included as one of the visual arts; however, like the decorative arts, or advertising, it involves the creation of objects where the practical considerations of use are essential in a way that they usually are not in a painting, for example.
Somewhat in relation to the above, the word "art" is also used to apply judgments of value, as in such expressions as "that meal was a work of art" (the cook is an artist), or "the art of deception", (the highly attained level of skill of the deceiver is praised).
It is this use of the word as a measure of high quality and high value that gives the term its flavor of subjectivity.
Making judgments of value requires a basis for criticism.
At the simplest level, a way to determine whether the impact of the object on the senses meets the criteria to be considered "art" is whether it is perceived to be attractive or repulsive.
Though perception is always colored by experience, and is necessarily subjective, it is commonly understood that what is not somehow aesthetically satisfying cannot be art.
However, "good" art is not always or even regularly aesthetically appealing to a majority of viewers.
In other words, an artist's prime motivation need not be the pursuit of the aesthetic.
Also, art often depicts terrible images made for social, moral, or thought-provoking reasons.
For example, Francisco Goya's painting depicting the Spanish shootings of 3rd of May 1808 is a graphic depiction of a firing squad executing several pleading civilians.
Yet at the same time, the horrific imagery demonstrates Goya's keen artistic ability in composition and execution and produces fitting social and political outrage.
Thus, the debate continues as to what mode of aesthetic satisfaction, if any, is required to define 'art'.
The assumption of new values or the rebellion against accepted notions of what is aesthetically superior need not occur concurrently with a complete abandonment of the pursuit of what is aesthetically appealing.
Indeed, the reverse is often true, that the revision of what is popularly conceived of as being aesthetically appealing allows for a re-invigoration of aesthetic sensibility, and a new appreciation for the standards of art itself.
Countless schools have proposed their own ways to define quality, yet they all seem to agree in at least one point: once their aesthetic choices are accepted, the value of the work of art is determined by its capacity to transcend the limits of its chosen medium to strike some universal chord by the rarity of the skill of the artist or in its accurate reflection in what is termed the "zeitgeist".
Art is often intended to appeal to and connect with human emotion.
It can arouse aesthetic or moral feelings, and can be understood as a way of communicating these feelings.
Artists express something so that their audience is aroused to some extent, but they do not have to do so consciously.
Art may be considered an exploration of the human condition; that is, what it is to be human.
</doc>
<doc id="764" url="https://en.wikipedia.org/wiki?curid=764" title="Agnostida">
Agnostida

Agnostida is an order of arthropod which first developed near the end of the Early Cambrian period and thrived during the Middle Cambrian.
They are present in the Lower Cambrian fossil record along with trilobites from the Redlichiida, Corynexochida, and Ptychopariida orders.
The last agnostids went extinct in the Late Ordovician.
The Agnostida are divided into two suborders — Agnostina and Eodiscina — which are then subdivided into a number of families.
As a group, agnostids are isopygous, meaning their pygidium is similar in size and shape to their cephalon.
Most agnostid species were eyeless.
The systematic position of the order Agnostida within the class Trilobita remains uncertain, and there has been continuing debate whether they are trilobites or a stem group.
The challenge to the status has focused on Agnostina partly due to the juveniles of one genus have been found with legs differing dramatically from those of adult trilobites, suggesting they are not members of the lamellipedian clade, of which trilobites are a part.
Instead, the limbs of agnostids closely resemble those of stem group crustaceans, although they lack the proximal endite, which defines that group.
They are likely the sister taxon to the crustacean stem lineage, and, as such, part of the clade, Crustaceomorpha.
Other researchers have suggested, based on a cladistic analyses of dorsal exoskeletal features, that Eodiscina and Agnostida are closely united, and the Eodiscina descended from the trilobite order Ptychopariida.
Scientists have long debated whether the agnostids lived a pelagic or a benthic lifestyle.
Their lack of eyes, a morphology not well-suited for swimming, and their fossils found in association with other benthic trilobites suggest a benthic (bottom-dwelling) mode of life.
They are likely to have lived on areas of the ocean floor which received little or no light and fed on detritus which descended from upper layers of the sea to the bottom.
Their wide geographic dispersion in the fossil record is uncharacteristic of benthic animals, suggesting a pelagic existence.
The thoracic segment appears to form a hinge between the head and pygidium allowing for a bivalved ostracodan-type lifestyle.
The orientation of the thoracic appendages appears ill-suited for benthic living.
Recent work suggests that some agnostids were benthic predators, engaging in cannibalism and possibly pack-hunting behavior.
They are sometimes preserved within the voids of other organisms, for instance within empty hyolith conchs, within sponges, worm tubes and under the carapaces of bivalved arthropods, presumably in order to hide from predators or strong storm currents; or maybe whilst scavenging for food.
In the case of the tapering worm tubes "Selkirkia", trilobites are always found with their heads directed towards the opening of the tube, suggesting that they reversed in; the absence of any moulted carapaces suggests that moulting was not their primary reason for seeking shelter.
</doc>
<doc id="765" url="https://en.wikipedia.org/wiki?curid=765" title="Abortion">
Abortion

Abortion is the ending of pregnancy due to removing an embryo or fetus before it can survive outside the uterus.
An abortion that occurs spontaneously is also known as a miscarriage.
When deliberate steps are taken to end a pregnancy, it is called an induced abortion, or less frequently an "induced miscarriage".
The word "abortion" is often used to mean only induced abortions.
A similar procedure after the fetus could potentially survive outside the womb is known as a "late termination of pregnancy" or less accurately as a "late term abortion".
When allowed by law, abortion in the developed world is one of the safest procedures in medicine.
Modern methods use medication or surgery for abortions.
The drug mifepristone in combination with prostaglandin appears to be as safe and effective as surgery during the first and second trimester of pregnancy.
The most common surgical technique involves dilating the cervix and using a suction device.
Birth control, such as the pill or intrauterine devices, can be used immediately following abortion.
When performed legally and safely, induced abortions do not increase the risk of long-term mental or physical problems.
In contrast, unsafe abortions (those performed by unskilled individuals, with hazardous equipment, or in unsanitary facilities) cause 47,000 deaths and 5 million hospital admissions each year.
The World Health Organization recommends safe and legal abortions be available to all women.
Around 56 million abortions are performed each year in the world, with about 45% done unsafely.
Abortion rates changed little between 2003 and 2008, before which they decreased for at least two decades as access to family planning and birth control increased.
, 40% of the world's women had access to legal abortions without limits as to reason.
Countries that permit abortions have different limits on how late in pregnancy abortion is allowed.
Historically, abortions have been attempted using herbal medicines, sharp tools, forceful massage, or through other traditional methods.
Abortion laws and cultural or religious views of abortions are different around the world.
In some areas abortion is legal only in specific cases such as rape, problems with the fetus, poverty, risk to a woman's health, or incest.
In many places there is much debate over the moral, ethical, and legal issues of abortion.
Those who oppose abortion often maintain that an embryo or fetus is a human with a right to life, and so they may compare abortion to murder.
Those who favor the legality of abortion often hold that a woman has a right to make decisions about her own body.
Others favor legal and accessible abortion as a public health measure.
An induced abortion may be classified as "therapeutic" (done in response to a health condition of the women or fetus) or "elective" (chosen for other reasons).
Approximately 205 million pregnancies occur each year worldwide.
Over a third are unintended and about a fifth end in induced abortion.
Most abortions result from unintended pregnancies.
In the United Kingdom, 1 to 2% of abortions are done due to genetic problems in the fetus.
A pregnancy can be intentionally aborted in several ways.
The manner selected often depends upon the gestational age of the embryo or fetus, which increases in size as the pregnancy progresses.
Specific procedures may also be selected due to legality, regional availability, and doctor or a woman's personal preference.
Reasons for procuring induced abortions are typically characterized as either therapeutic or elective.
An abortion is medically referred to as a therapeutic abortion when it is performed to save the life of the pregnant woman; to prevent harm to the woman's physical or mental health; to terminate a pregnancy where indications are that the child will have a significantly increased chance of mortality or morbidity; or to selectively reduce the number of fetuses to lessen health risks associated with multiple pregnancy.
An abortion is referred to as an elective or voluntary abortion when it is performed at the request of the woman for non-medical reasons.
Confusion sometimes arises over the term "elective" because "elective surgery" generally refers to all scheduled surgery, whether medically necessary or not.
Spontaneous abortion, also known as miscarriage, is the unintentional expulsion of an embryo or fetus before the 24th week of gestation.
A pregnancy that ends before 37 weeks of gestation resulting in a live-born infant is known as a "premature birth" or a "preterm birth".
When a fetus dies in utero after viability, or during delivery, it is usually termed "stillborn".
Premature births and stillbirths are generally not considered to be miscarriages although usage of these terms can sometimes overlap.
Only 30% to 50% of conceptions progress past the first trimester.
The vast majority of those that do not progress are lost before the woman is aware of the conception, and many pregnancies are lost before medical practitioners can detect an embryo.
Between 15% and 30% of known pregnancies end in clinically apparent miscarriage, depending upon the age and health of the pregnant woman.
80% of these spontaneous abortions happen in the first trimester.
The most common cause of spontaneous abortion during the first trimester is chromosomal abnormalities of the embryo or fetus, accounting for at least 50% of sampled early pregnancy losses.
Other causes include vascular disease (such as lupus), diabetes, other hormonal problems, infection, and abnormalities of the uterus.
Advancing maternal age and a woman's history of previous spontaneous abortions are the two leading factors associated with a greater risk of spontaneous abortion.
A spontaneous abortion can also be caused by accidental trauma; intentional trauma or stress to cause miscarriage is considered induced abortion or feticide.
Medical abortions are those induced by abortifacient pharmaceuticals.
Medical abortion became an alternative method of abortion with the availability of prostaglandin analogs in the 1970s and the antiprogestogen mifepristone (also known as RU-486) in the 1980s.
The most common early first-trimester medical abortion regimens use mifepristone in combination with a prostaglandin analog (misoprostol or gemeprost) up to 9 weeks gestational age, methotrexate in combination with a prostaglandin analog up to 7 weeks gestation, or a prostaglandin analog alone.
Mifepristone–misoprostol combination regimens work faster and are more effective at later gestational ages than methotrexate–misoprostol combination regimens, and combination regimens are more effective than misoprostol alone.
This regime is effective in the second trimester.
Medical abortion regiments involving mifepristone followed by misoprostol in the cheek between 24 and 48 hours later are effective when performed before 63 days' gestation.
In very early abortions, up to 7 weeks gestation, medical abortion using a mifepristone–misoprostol combination regimen is considered to be more effective than surgical abortion (vacuum aspiration), especially when clinical practice does not include detailed inspection of aspirated tissue.
Early medical abortion regimens using mifepristone, followed 24–48 hours later by buccal or vaginal misoprostol are 98% effective up to 9 weeks gestational age.
If medical abortion fails, surgical abortion must be used to complete the procedure.
Early medical abortions account for the majority of abortions before 9 weeks gestation in Britain, France, Switzerland, and the Nordic countries.
In the United States, the percentage of early medical abortions is around 30% .
Medical abortion regimens using mifepristone in combination with a prostaglandin analog are the most common methods used for second-trimester abortions in Canada, most of Europe, China and India, in contrast to the United States where 96% of second-trimester abortions are performed surgically by dilation and evacuation.
Up to 15 weeks' gestation, suction-aspiration or vacuum aspiration are the most common surgical methods of induced abortion.
"Manual vacuum aspiration" (MVA) consists of removing the fetus or embryo, placenta, and membranes by suction using a manual syringe, while "electric vacuum aspiration" (EVA) uses an electric pump.
These techniques differ in the mechanism used to apply suction, in how early in pregnancy they can be used, and in whether cervical dilation is necessary.
MVA, also known as "mini-suction" and "menstrual extraction", can be used in very early pregnancy, and does not require cervical dilation.
Dilation and curettage (D&C), the second most common method of surgical abortion, is a standard gynecological procedure performed for a variety of reasons, including examination of the uterine lining for possible malignancy, investigation of abnormal bleeding, and abortion.
Curettage refers to cleaning the walls of the uterus with a curette.
The World Health Organization recommends this procedure, also called "sharp curettage," only when MVA is unavailable.
From the 15th week of gestation until approximately the 26th, other techniques must be used.
Dilation and evacuation (D&E) consists of opening the cervix of the uterus and emptying it using surgical instruments and suction.
After the 16th week of gestation, abortions can also be induced by intact dilation and extraction (IDX) (also called intrauterine cranial decompression), which requires surgical decompression of the fetus's head before evacuation.
IDX is sometimes called "partial-birth abortion", which has been federally banned in the United States.
In the third trimester of pregnancy, induced abortion may be performed surgically by intact dilation and extraction or by hysterotomy.
Hysterotomy abortion is a procedure similar to a caesarean section and is performed under general anesthesia.
It requires a smaller incision than a caesarean section and is used during later stages of pregnancy.
First-trimester procedures can generally be performed using local anesthesia, while second-trimester methods may require deep sedation or general anesthesia.
In places lacking the necessary medical skill for dilation and extraction, or where preferred by practitioners, an abortion can be induced by first inducing labor and then inducing fetal demise if necessary.
This is sometimes called "induced miscarriage".
This procedure may be performed from 13 weeks gestation to the third trimester.
Although it is very uncommon in the United States, more than 80% of induced abortions throughout the second trimester are labor induced abortions in Sweden and other nearby countries.
Only limited data are available comparing this method with dilation and extraction.
Unlike D&E, labor induced abortions after 18 weeks may be complicated by the occurrence of brief fetal survival, which may be legally characterized as live birth.
For this reason, labor induced abortion is legally risky in the U.S.
Historically, a number of herbs reputed to possess abortifacient properties have been used in folk medicine.
Among these are: tansy, pennyroyal, black cohosh, and the now-extinct silphium.
However, modern users of these plants often lack knowledge of the proper use and dosage.
The historian of medicine John Riddle has spoken of the "broken chain of knowledge,"
and historian of science Ann Hibner Koblitz has written,

For example, in 1978 one woman in Colorado died and another was seriously injured when they attempted to procure an abortion by taking pennyroyal oil.
Because the indiscriminant use of herbs as abortifacients can cause serious—even lethal—side effects, such as multiple organ failure, such use is not recommended by physicians.
Abortion is sometimes attempted by causing trauma to the abdomen.
The degree of force, if severe, can cause serious internal injuries without necessarily succeeding in inducing miscarriage.
In Southeast Asia, there is an ancient tradition of attempting abortion through forceful abdominal massage.
One of the bas reliefs decorating the temple of Angkor Wat in Cambodia depicts a demon performing such an abortion upon a woman who has been sent to the underworld.
Reported methods of unsafe, self-induced abortion include misuse of misoprostol, and insertion of non-surgical implements such as knitting needles and clothes hangers into the uterus.
These and other methods to terminate pregnancy may be called "induced miscarriage".
Such methods are rarely used in countries where surgical abortion is legal and available.
The health risks of abortion depend principally upon whether the procedure is performed safely or unsafely.
The World Health Organization defines unsafe abortions as those performed by unskilled individuals, with hazardous equipment, or in unsanitary facilities.
Legal abortions performed in the developed world are among the safest procedures in medicine.
In the US, the risk of maternal death from abortion is 0.7 per 100,000 procedures, making abortion about 13 times safer for women than childbirth (8.8 maternal deaths per 100,000 live births).
In the United States from 2000 to 2009, abortion had a lower mortality rate than plastic surgery.
The risk of abortion-related mortality increases with gestational age, but remains lower than that of childbirth through at least 21 weeks' gestation.
Outpatient abortion is as safe and effective from 64 to 70 days' gestation as it is from 57 to 63 days.
Medical abortion is safe and effective for pregnancies earlier than 6 weeks' gestation.
Vacuum aspiration in the first trimester is the safest method of surgical abortion, and can be performed in a primary care office, abortion clinic, or hospital.
Complications, which are rare, can include uterine perforation, pelvic infection, and retained products of conception requiring a second procedure to evacuate.
Infections account for one-third of abortion-related deaths in the United States.
The rate of complications of vacuum aspiration abortion in the first trimester is similar regardless of whether the procedure is performed in a hospital, surgical center, or office.
Preventive antibiotics (such as doxycycline or metronidazole) are typically given before elective abortion, as they are believed to substantially reduce the risk of postoperative uterine infection.
The rate of failed procedures does not appear to vary significantly depending on whether the abortion is performed by a doctor or a mid-level practitioner.
Complications after second-trimester abortion are similar to those after first-trimester abortion, and depend somewhat on the method chosen.
Second-trimester abortions are generally well-tolerated.
There is little difference in terms of safety and efficacy between medical abortion using a combined regimen of mifepristone and misoprostol and surgical abortion (vacuum aspiration) in early first trimester abortions up to 9 weeks gestation.
Medical abortion using the prostaglandin analog misoprostol alone is less effective and more painful than medical abortion using a combined regimen of mifepristone and misoprostol or surgical abortion.
Some purported risks of abortion are promoted primarily by anti-abortion groups,
but lack scientific support.
For example, the question of a link between induced abortion and breast cancer has been investigated extensively.
Major medical and scientific bodies (including the World Health Organization, National Cancer Institute, American Cancer Society, Royal College of OBGYN and American Congress of OBGYN) have concluded that abortion does not cause breast cancer.
In the past even illegality has not automatically meant that the abortions were unsafe.
Referring to the U.S., historian Linda Gordon states: "In fact, illegal abortions in this country have an impressive safety record."
According to Rickie Solinger,

Authors Jerome Bates and Edward Zawadzki describe the case of an illegal abortionist in the eastern U.S.
in the early 20th century who was proud of having successfully completed 13,844 abortions without any fatality.
In 1870s New York City the famous abortionist/midwife Madame Restell (Anna Trow Lohman) appears to have lost very few women among her more than 100,000 patients—a lower mortality rate than the childbirth mortality rate at the time.
In 1936 the prominent professor of obstetrics and gynecology Frederick J. Taussig wrote that a cause of increasing mortality during the years of illegality in the U.S.
was that
Current evidence finds no relationship between most induced abortions and mental-health problems other than those expected for any unwanted pregnancy.
A report by the American Psychological Association concluded that a woman's first abortion is not a threat to mental health when carried out in the first trimester, with such women no more likely to have mental-health problems than those carrying an unwanted pregnancy to term; the mental-health outcome of a woman's second or greater abortion is less certain.
Some older reviews concluded that abortion was associated with an increased risk of psychological problems; however, they did not use an appropriate control group.
Although some studies show negative mental-health outcomes in women who choose abortions after the first trimester because of fetal abnormalities, more rigorous research would be needed to show this conclusively.
Some proposed negative psychological effects of abortion have been referred to by anti-abortion advocates as a separate condition called "post-abortion syndrome", but this is not recognized by medical or psychological professionals in the United States.
Women seeking to terminate their pregnancies sometimes resort to unsafe methods, particularly when access to legal abortion is restricted.
They may attempt to self-abort or rely on another person who does not have proper medical training or access to proper facilities.
This has a tendency to lead to severe complications, such as incomplete abortion, sepsis, hemorrhage, and damage to internal organs.
Unsafe abortions are a major cause of injury and death among women worldwide.
Although data are imprecise, it is estimated that approximately 20 million unsafe abortions are performed annually, with 97% taking place in developing countries.
Unsafe abortions are believed to result in millions of injuries.
Estimates of deaths vary according to methodology, and have ranged from 37,000 to 70,000 in the past decade; deaths from unsafe abortion account for around 13% of all maternal deaths.
The World Health Organization believes that mortality has fallen since the 1990s.
To reduce the number of unsafe abortions, public health organizations have generally advocated emphasizing the legalization of abortion, training of medical personnel, and ensuring access to reproductive-health services.
In response, opponents of abortion point out that abortion bans in no way affect prenatal care for women who choose to carry their fetus to term.
The Dublin Declaration on Maternal Health, signed in 2012, notes, "the prohibition of abortion does not affect, in any way, the availability of optimal care to pregnant women."
A major factor in whether abortions are performed safely or not is the legal standing of abortion.
Countries with restrictive abortion laws have higher rates of unsafe abortion and similar overall abortion rates compared to those where abortion is legal and available.
For example, the 1996 legalization of abortion in South Africa had an immediate positive impact on the frequency of abortion-related complications, with abortion-related deaths dropping by more than 90%.
Similar reductions in maternal mortality have been observed after other countries have liberalized their abortion laws, such as Romania and Nepal.
A 2011 study concluded that in the United States, some state-level anti-abortion laws are correlated with lower rates of abortion in that state.
The analysis, however, did not take into account travel to other states without such laws to obtain an abortion.
In addition, a lack of access to effective contraception contributes to unsafe abortion.
It has been estimated that the incidence of unsafe abortion could be reduced by up to 75% (from 20 million to 5 million annually) if modern family planning and maternal health services were readily available globally.
Rates of such abortions may be difficult to measure because they can be reported variously as miscarriage, "induced miscarriage", "menstrual regulation", "mini-abortion", and "regulation of a delayed/suspended menstruation".
Forty percent of the world's women are able to access therapeutic and elective abortions within gestational limits, while an additional 35 percent have access to legal abortion if they meet certain physical, mental, or socioeconomic criteria.
While maternal mortality seldom results from safe abortions, unsafe abortions result in 70,000 deaths and 5 million disabilities per year.
Complications of unsafe abortion account for approximately an eighth of maternal mortalities worldwide, though this varies by region.
Secondary infertility caused by an unsafe abortion affects an estimated 24 million women.
The rate of unsafe abortions has increased from 44% to 49% between 1995 and 2008.
Health education, access to family planning, and improvements in health care during and after abortion have been proposed to address this phenomenon.
Although it is very uncommon, women undergoing surgical abortion after 18 weeks gestation sometimes give birth to a fetus that may survive briefly.
Longer term survival is possible after 22 weeks.
If medical staff observe signs of life, they may be required to provide care: emergency medical care if the child has a good chance of survival and palliative care if not.
Induced fetal demise before termination of pregnancy after 20–21 weeks gestation is recommended to avoid this.
Death following live birth caused by abortion is given the ; data are identified as either fetus or newborn.
Between 1999 and 2013, in the U.S., the CDC recorded 531 such deaths for newborns, approximately 4 per 100,000 abortions.
There are two commonly used methods of measuring the incidence of abortion:

In many places, where abortion is illegal or carries a heavy social stigma, medical reporting of abortion is not reliable.
For this reason, estimates of the incidence of abortion must be made without determining certainty related to standard error.
The number of abortions performed worldwide seems to have remained stable in recent years, with 41.6 million having been performed in 2003 and 43.8 million having been performed in 2008.
The abortion rate worldwide was 28 per 1000 women, though it was 24 per 1000 women for developed countries and 29 per 1000 women for developing countries.
The same 2012 study indicated that in 2008, the estimated abortion percentage of known pregnancies was at 21% worldwide, with 26% in developed countries and 20% in developing countries.
On average, the incidence of abortion is similar in countries with restrictive abortion laws and those with more liberal access to abortion.
However, restrictive abortion laws are associated with increases in the percentage of abortions performed unsafely.
The unsafe abortion rate in developing countries is partly attributable to lack of access to modern contraceptives; according to the Guttmacher Institute, providing access to contraceptives would result in about 14.5 million fewer unsafe abortions and 38,000 fewer deaths from unsafe abortion annually worldwide.
The rate of legal, induced abortion varies extensively worldwide.
According to the report of employees of Guttmacher Institute it ranged from 7 per 1000 women (Germany and Switzerland) to 30 per 1000 women (Estonia) in countries with complete statistics in 2008.
The proportion of pregnancies that ended in induced abortion ranged from about 10% (Israel, the Netherlands and Switzerland) to 30% (Estonia) in the same group, though it might be as high as 36% in Hungary and Romania, whose statistics were deemed incomplete.
The abortion rate may also be expressed as the average number of abortions a woman has during her reproductive years; this is referred to as "total abortion rate" (TAR).
Abortion rates also vary depending on the stage of pregnancy and the method practiced.
In 2003, the Centers for Disease Control and Prevention (CDC) reported that 26% of reported legal induced abortions in the United States were known to have been obtained at less than 6 weeks' gestation, 18% at 7 weeks, 15% at 8 weeks, 18% at 9 through 10 weeks, 9.7% at 11 through 12 weeks, 6.2% at 13 through 15 weeks, 4.1% at 16 through 20 weeks and 1.4% at more than 21 weeks.
90.9% of these were classified as having been done by "curettage" (suction-aspiration, dilation and curettage, dilation and evacuation), 7.7% by "medical" means (mifepristone), 0.4% by "intrauterine instillation" (saline or prostaglandin), and 1.0% by "other" (including hysterotomy and hysterectomy).
According to the CDC, due to data collection difficulties the data must be viewed as tentative and some fetal deaths reported beyond 20 weeks may be natural deaths erroneously classified as abortions if the removal of the dead fetus is accomplished by the same procedure as an induced abortion.
The Guttmacher Institute estimated there were 2,200 intact dilation and extraction procedures in the US during 2000; this accounts for 0.17% of the total number of abortions performed that year.
Similarly, in England and Wales in 2006, 89% of terminations occurred at or under 12 weeks, 9% between 13 and 19 weeks, and 1.5% at or over 20 weeks.
64% of those reported were by vacuum aspiration, 6% by D&E, and 30% were medical.
There are more second trimester abortions in developing countries such as China, India and Vietnam than in developed countries.
The reasons why women have abortions are diverse and vary across the world.
Some abortions are undergone as the result of societal pressures.
These might include the preference for children of a specific sex or race, disapproval of single or early motherhood, stigmatization of people with disabilities, insufficient economic support for families, lack of access to or rejection of contraceptive methods, or efforts toward population control (such as China's one-child policy).
These factors can sometimes result in compulsory abortion or sex-selective abortion.
An American study in 2002 concluded that about half of women having abortions were using a form of contraception at the time of becoming pregnant.
Inconsistent use was reported by half of those using condoms and three-quarters of those using the birth control pill; 42% of those using condoms reported failure through slipping or breakage.
The Guttmacher Institute estimated that "most abortions in the United States are obtained by minority women" because minority women "have much higher rates of unintended pregnancy".
An additional factor is risk to maternal or fetal health, which was cited as the primary reason for abortion in over a third of cases in some countries and as a significant factor in only a single-digit percentage of abortions in other countries.
In the U.S., the Supreme Court decisions in "Roe v. Wade" and "Doe v. Bolton": "ruled that the state's interest in the life of the fetus became compelling only at the point of viability, defined as the point at which the fetus can survive independently of its mother.
Even after the point of viability, the state cannot favor the life of the fetus over the life or health of the pregnant woman.
Under the right of privacy, physicians must be free to use their "medical judgment for the preservation of the life or health of the mother."
On the same day that the Court decided Roe, it also decided Doe v. Bolton, in which the Court defined health very broadly: "The medical judgment may be exercised in the light of all factors—physical, emotional, psychological, familial, and the woman's age—relevant to the well-being of the patient.
All these factors may relate to health.
This allows the attending physician the room he needs to make his best medical judgment."
Public opinion shifted in America following television personality Sherri Finkbine's discovery during her fifth month of pregnancy that she had been exposed to thalidomide.
Unable to obtain a legal abortion in the United States, she traveled to Sweden.
From 1962 to 1965, an outbreak of German measles left 15,000 babies with severe birth defects.
In 1967, the American Medical Association publicly supported liberalization of abortion laws.
A National Opinion Research Center poll in 1965 showed 73% supported abortion when the mother's life was at risk, 57% when birth defects were present and 59% for pregnancies resulting from rape or incest.
The rate of cancer during pregnancy is 0.02–1%, and in many cases, cancer of the mother leads to consideration of abortion to protect the life of the mother, or in response to the potential damage that may occur to the fetus during treatment.
This is particularly true for cervical cancer, the most common type of which occurs in 1 of every 2,000–13,000 pregnancies, for which initiation of treatment "cannot co-exist with preservation of fetal life (unless neoadjuvant chemotherapy is chosen)".
Very early stage cervical cancers (I and IIa) may be treated by radical hysterectomy and pelvic lymph node dissection, radiation therapy, or both, while later stages are treated by radiotherapy.
Chemotherapy may be used simultaneously.
Treatment of breast cancer during pregnancy also involves fetal considerations, because lumpectomy is discouraged in favor of modified radical mastectomy unless late-term pregnancy allows follow-up radiation therapy to be administered after the birth.
Exposure to a single chemotherapy drug is estimated to cause a 7.5–17% risk of teratogenic effects on the fetus, with higher risks for multiple drug treatments.
Treatment with more than 40 Gy of radiation usually causes spontaneous abortion.
Exposure to much lower doses during the first trimester, especially 8 to 15 weeks of development, can cause intellectual disability or microcephaly, and exposure at this or subsequent stages can cause reduced intrauterine growth and birth weight.
Exposures above 0.005–0.025 Gy cause a dose-dependent reduction in IQ.
It is possible to greatly reduce exposure to radiation with abdominal shielding, depending on how far the area to be irradiated is from the fetus.
The process of birth itself may also put the mother at risk.
"Vaginal delivery may result in dissemination of neoplastic cells into lymphovascular channels, haemorrhage, cervical laceration and implantation of malignant cells in the episiotomy site, while abdominal delivery may delay the initiation of non-surgical treatment."
Since ancient times abortions have been done using herbal medicines, sharp tools, with force, or through other traditional methods.
Induced abortion has long history, and can be traced back to civilizations as varied as China under Shennong (c.
2700 BCE), Ancient Egypt with its Ebers Papyrus (c.
1550 BCE), and the Roman Empire in the time of Juvenal (c.
200 CE).
There is evidence to suggest that pregnancies were terminated through a number of methods, including the administration of abortifacient herbs, the use of sharpened implements, the application of abdominal pressure, and other techniques.
One of the earliest known artistic representations of abortion is in a bas relief at Angkor Wat (c.
1150).
Found in a series of friezes that represent judgment after death in Hindu and Buddhist culture, it depicts the technique of abdominal abortion.
Some medical scholars and abortion opponents have suggested that the Hippocratic Oath forbade Ancient Greek physicians from performing abortions; other scholars disagree with this interpretation, and state that the medical texts of Hippocratic Corpus contain descriptions of abortive techniques right alongside the Oath.
The physician Scribonius Largus wrote in 43 CE that the Hippocratic Oath prohibits abortion, as did Soranus, although apparently not all doctors adhered to it strictly at the time.
According to Soranus' 1st or 2nd century CE work "Gynaecology", one party of medical practitioners banished all abortives as required by the Hippocratic Oath; the other party—to which he belonged—was willing to prescribe abortions, but only for the sake of the mother's health.
Aristotle, in his treatise on government "Politics" (350 BCE), condemns infanticide as a means of population control.
He preferred abortion in such cases, with the restriction "<nowiki>[that it]</nowiki> must be practised on it before it has developed sensation and life; for the line between lawful and unlawful abortion will be marked by the fact of having sensation and being alive".
In Christianity, Pope Sixtus V (1585–90) was the only Pope before 1869 to declare that abortion is homicide regardless of the stage of pregnancy; and his pronouncement of 1588 was reversed three years later by his successor.
Through most of its history the Catholic Church was divided on whether it believed that abortion was murder, and it did not begin vigorously opposing abortion until the 19th century.
In fact, several historians have written that prior to the 19th century most Catholic authors did not regard termination of pregnancy before "quickening" or "ensoulment" as an abortion.
A 1995 survey reported that Catholic women are as likely as the general population to terminate a pregnancy, Protestants are less likely to do so, and Evangelical Christians are the least likely to do so.
Islamic tradition has traditionally permitted abortion until a point in time when Muslims believe the soul enters the fetus, considered by various theologians to be at conception, 40 days after conception, 120 days after conception, or quickening.<ref name="BBC and Islam / Abortion"></ref> However, abortion is largely heavily restricted or forbidden in areas of high Islamic faith such as the Middle East and North Africa.
In Europe and North America, abortion techniques advanced starting in the 17th century.
However, conservatism by most physicians with regards to sexual matters prevented the wide expansion of safe abortion techniques.
Other medical practitioners in addition to some physicians advertised their services, and they were not widely regulated until the 19th century, when the practice (sometimes called "restellism") was banned in both the United States and the United Kingdom.
Church groups as well as physicians were highly influential in anti-abortion movements.
In the US, according to some sources, abortion was more dangerous than childbirth until about 1930 when incremental improvements in abortion procedures relative to childbirth made abortion safer.
However, other sources maintain that in the 19th century early abortions under the hygienic conditions in which midwives usually worked were relatively safe.
In addition, some commentators have written that, despite improved medical procedures, the period from the 1930s until legalization also saw more zealous enforcement of anti-abortion laws, and concomitantly an increasing control of abortion providers by organized crime.
Soviet Russia (1919), Iceland (1935) and Sweden (1938) were among the first countries to legalize certain or all forms of abortion.
In 1935 Nazi Germany, a law was passed permitting abortions for those deemed "hereditarily ill", while women considered of German stock were specifically prohibited from having abortions.
Beginning in the second half of the twentieth century, abortion was legalized in a greater number of countries.
Induced abortion has long been the source of considerable debate.
Ethical, moral, philosophical, biological, religious and legal issues surrounding abortion are related to value systems.
Opinions of abortion may be about fetal rights, governmental authority, and women's rights.
In both public and private debate, arguments presented in favor of or against abortion access focus on either the moral permissibility of an induced abortion, or justification of laws permitting or restricting abortion.
The World Medical Association Declaration on Therapeutic Abortion notes, "circumstances bringing the interests of a mother into conflict with the interests of her unborn child create a dilemma and raise the question as to whether or not the pregnancy should be deliberately terminated."
Abortion debates, especially pertaining to abortion laws, are often spearheaded by groups advocating one of these two positions.
Anti-abortion groups who favor greater legal restrictions on abortion, including complete prohibition, most often describe themselves as "pro-life" while abortion rights groups who are against such legal restrictions describe themselves as "pro-choice".
Generally, the former position argues that a human fetus is a human person with a right to live, making abortion morally the same as murder.
The latter position argues that a woman has certain reproductive rights, especially the right to decide whether or not to carry a pregnancy to term.
Current laws pertaining to abortion are diverse.
Religious, moral, and cultural factors continue to influence abortion laws throughout the world.
The right to life, the right to liberty, the right to security of person, and the right to reproductive health are major issues of human rights that sometimes constitute the basis for the existence or absence of abortion laws.
In jurisdictions where abortion is legal, certain requirements must often be met before a woman may obtain a safe, legal abortion (an abortion performed without the woman's consent is considered feticide).
These requirements usually depend on the age of the fetus, often using a trimester-based system to regulate the window of legality, or as in the U.S., on a doctor's evaluation of the fetus' viability.
Some jurisdictions require a waiting period before the procedure, prescribe the distribution of information on fetal development, or require that parents be contacted if their minor daughter requests an abortion.
Other jurisdictions may require that a woman obtain the consent of the fetus' father before aborting the fetus, that abortion providers inform women of health risks of the procedure—sometimes including "risks" not supported by the medical literature—and that multiple medical authorities certify that the abortion is either medically or socially necessary.
Many restrictions are waived in emergency situations.
China, which has ended their one-child policy, and now has a two child policy, has at times incorporated mandatory abortions as part of their population control strategy.
Other jurisdictions ban abortion almost entirely.
Many, but not all, of these allow legal abortions in a variety of circumstances.
These circumstances vary based on jurisdiction, but may include whether the pregnancy is a result of rape or incest, the fetus' development is impaired, the woman's physical or mental well-being is endangered, or socioeconomic considerations make childbirth a hardship.
In countries where abortion is banned entirely, such as Nicaragua, medical authorities have recorded rises in maternal death directly and indirectly due to pregnancy as well as deaths due to doctors' fears of prosecution if they treat other gynecological emergencies.
Some countries, such as Bangladesh, that nominally ban abortion, may also support clinics that perform abortions under the guise of menstrual hygiene.
This is also a terminology in traditional medicine.
In places where abortion is illegal or carries heavy social stigma, pregnant women may engage in medical tourism and travel to countries where they can terminate their pregnancies.
Women without the means to travel can resort to providers of illegal abortions or attempt to perform an abortion by themselves.
The organization Women on Waves, has been providing education about medical abortions since 1999.
The NGO created a mobile medical clinic inside a shipping container, which then travels on rented ships to countries with restrictive abortion laws.
Because the ships are registered in the Netherlands, Dutch law prevails when the ship is in international waters.
While in port, the organization provides free workshops and education; while in international waters, medical personnel are legally able to prescribe medical abortion drugs and counseling.
Sonography and amniocentesis allow parents to determine sex before childbirth.
The development of this technology has led to sex-selective abortion, or the termination of a fetus based on sex.
The selective termination of a female fetus is most common.
Sex-selective abortion is partially responsible for the noticeable disparities between the birth rates of male and female children in some countries.
The preference for male children is reported in many areas of Asia, and abortion used to limit female births has been reported in Taiwan, South Korea, India, and China.
This deviation from the standard birth rates of males and females occurs despite the fact that the country in question may have officially banned sex-selective abortion or even sex-screening.
In China, a historical preference for a male child has been exacerbated by the one-child policy, which was enacted in 1979.
Many countries have taken legislative steps to reduce the incidence of sex-selective abortion.
At the International Conference on Population and Development in 1994 over 180 states agreed to eliminate "all forms of discrimination against the girl child and the root causes of son preference", conditions also condemned by a PACE resolution in 2011.
The World Health Organization and UNICEF, along with other United Nations agencies, have found that measures to reduce access to abortion are much less effective at reducing sex-selective abortions than measures to reduce gender inequality.
In a number of cases, abortion providers and these facilities have been subjected to various forms of violence, including murder, attempted murder, kidnapping, stalking, assault, arson, and bombing.
Anti-abortion violence is classified by both governmental and scholarly sources as terrorism.
Only a small fraction of those opposed to abortion commit violence.
In the United States, four physicians who performed abortions have been murdered: David Gunn (1993), John Britton (1994), Barnett Slepian (1998), and George Tiller (2009).
Also murdered, in the U.S.
and Australia, have been other personnel at abortion clinics, including receptionists and security guards such as James Barrett, Shannon Lowney, Lee Ann Nichols, and Robert Sanderson.
Woundings (e.g., Garson Romalis) and attempted murders have also taken place in the United States and Canada.
Hundreds of bombings, arsons, acid attacks, invasions, and incidents of vandalism against abortion providers have occurred.
Notable perpetrators of anti-abortion violence include Eric Robert Rudolph, Scott Roeder, Shelley Shannon, and Paul Jennings Hill, the first person to be executed in the United States for murdering an abortion provider.
Legal protection of access to abortion has been brought into some countries where abortion is legal.
These laws typically seek to protect abortion clinics from obstruction, vandalism, picketing, and other actions, or to protect women and employees of such facilities from threats and harassment.
Far more common than physical violence is psychological pressure.
In 2003, Chris Danze organized pro-life organizations throughout Texas to prevent the construction of a Planned Parenthood facility in Austin.
The organizations released the personal information online, of those involved with construction, sending them up to 1200 phone calls a day and contacting their churches.
Some protestors record women entering clinics on camera.
Spontaneous abortion occurs in various animals.
For example, in sheep it may be caused by stress or physical exertion, such as crowding through doors or being chased by dogs.
In cows, abortion may be caused by contagious disease, such as brucellosis or "Campylobacter", but can often be controlled by vaccination.
Eating pine needles can also induce abortions in cows.
In horses, a fetus may be aborted or resorbed if it has lethal white syndrome (congenital intestinal aganglionosis).
Foal embryos that are homozygous for the dominant white gene (WW) are theorized to also be aborted or resorbed before birth.
In many species of sharks and rays, stress induced abortions occur frequently on capture.
Viral infection can cause abortion in dogs.
Cats can experience spontaneous abortion for many reasons, including hormonal imbalance.
A combined abortion and spaying is performed on pregnant cats, especially in Trap-Neuter-Return programs, to prevent unwanted kittens from being born.
Female rodents may terminate a pregnancy when exposed to the smell of a male not responsible for the pregnancy, known as the Bruce effect.
Abortion may also be induced in animals, in the context of animal husbandry.
For example, abortion may be induced in mares that have been mated improperly, or that have been purchased by owners who did not realize the mares were pregnant, or that are pregnant with twin foals.
Feticide can occur in horses and zebras due to male harassment of pregnant mares or forced copulation, although the frequency in the wild has been questioned.
Male gray langur monkeys may attack females following male takeover, causing miscarriage.
</doc>
<doc id="766" url="https://en.wikipedia.org/wiki?curid=766" title="Abstract (law)">
Abstract (law)

In law, an abstract is a brief statement that contains the most important points of a long legal document or of several related legal papers.
The Abstract of Title, used in real estate transactions, is the more common form of abstract.
An abstract of title lists all the owners of a piece of land, a house, or a building before it came into possession of the present owner.
The abstract also records all deeds, wills, mortgages, and other documents that affect ownership of the property.
An abstract describes a chain of transfers from owner to owner and any agreements by former owners that are binding on later owners.
A clear title to property is one that clearly states any obligation in the deed to the property.
It reveals no breaksin the chain of legal ownership.
After the records of the property have been traced and the title has been found clear, it is sometimes guaranteed, or insured.
In a few states, a different system of insuring title of real properties provides for registration of a clear title with public authorities.
After this is accomplished, no abstract of title is necessary.
In the context of patent law and specifically in prior art searches, searching through abstracts is a common way to find relevant prior art document to question to novelty or inventive step (or non-obviousness in United States patent law) of an invention.
Under United States patent law, the abstract may be called "Abstract of the Disclosure".
Certain government bureaucracies, such as a "department of motor vehicles" will issue an abstract of a completed transaction or an updated record intended to serve as a proof of compliance with some administrative requirement.
This is often done in advance of the update of reporting databases and/or the issuance of official documents.
</doc>
<doc id="771" url="https://en.wikipedia.org/wiki?curid=771" title="American Revolutionary War">
American Revolutionary War

The American Revolutionary War (17751783), also known as the American War of Independence, was an 18th-century war between Great Britain and its Thirteen Colonies (allied with France) which declared independence as the United States of America.
After 1765, growing philosophical and political differences strained the relationship between Great Britain and its colonies.
Patriot protests against taxation without representation followed the Stamp Act and escalated into boycotts, which culminated in 1773 with the Sons of Liberty destroying a shipment of tea in Boston Harbor.
Britain responded by closing Boston Harbor and passing a series of punitive measures against Massachusetts Bay Colony.
Massachusetts colonists responded with the Suffolk Resolves, and they established a shadow government which wrested control of the countryside from the Crown.
Twelve colonies formed a Continental Congress to coordinate their resistance, establishing committees and conventions that effectively seized power.
British attempts to disarm the Massachusetts militia in Concord led to open combat on April 19, 1775.
Militia forces then besieged Boston, forcing a British evacuation in March 1776, and Congress appointed George Washington to command the Continental Army.
Concurrently, the Americans failed decisively in an attempt to invade Quebec and raise insurrection against the British.
On July 2, 1776, the Continental Congress voted for independence, issuing its declaration on July 4.
Sir William Howe launched a British counter-offensive, capturing New York City and leaving American morale at a low ebb.
However, victories at Trenton and Princeton restored American confidence.
In 1777, the British launched an invasion from Quebec under John Burgoyne, intending to isolate the New England Colonies.
Instead of assisting this effort, Howe took his army on a separate campaign against Philadelphia, and Burgoyne was decisively defeated at Saratoga in October 1777.
Burgoyne's defeat had drastic consequences.
France formally allied with the Americans and entered the war in 1778, and Spain joined the war the following year as an ally of France but not as an ally of the United States.
In 1780, the Kingdom of Mysore attacked the British in India, and tensions between Great Britain and the Netherlands erupted into open war.
In North America, the British mounted a "Southern strategy" led by Charles Cornwallis which hinged upon a Loyalist uprising, but too few came forward.
Cornwallis suffered reversals at King's Mountain and Cowpens.
He retreated to Yorktown, Virginia, intending an evacuation, but a decisive French naval victory deprived him of an escape.
A Franco-American army led by the Comte de Rochambeau and Washington then besieged Cornwallis' army and, with no sign of relief, he surrendered in October 1781.
Whigs in Britain had long opposed the pro-war Tories in Parliament, and the surrender gave them the upper hand.
In early 1782, Parliament voted to end all offensive operations in North America, but the war continued in Europe and India.
Britain remained under siege in Gibraltar but scored a major victory over the French navy.
On September 3, 1783, the belligerent parties signed the Treaty of Paris in which Great Britain agreed to recognize the sovereignty of the United States and formally end the war.
French involvement had proven decisive, but France made few gains and incurred crippling debts.
Spain made some territorial gains but failed in its primary aim of recovering Gibraltar.
The Dutch were defeated on all counts and were compelled to cede territory to Great Britain.
In India, the war against Mysore and its allies concluded in 1784 without any territorial changes.
Parliament passed the Stamp Act in 1765.
Colonists condemned the tax because their rights as Englishmen protected them from being taxed by a Parliament in which they had no elected representatives.
Parliament argued that the colonies were "represented virtually", an idea that was criticized throughout the Empire.
Parliament did repeal the act in 1766; however, it also affirmed its right to pass laws that were binding on the colonies.
From 1767, Parliament began passing legislation to raise revenue for the salaries of civil officials, ensuring their loyalty while inadvertently increasing resentment among the colonists, and opposition soon became widespread.
Enforcing the acts proved difficult.
The seizure of the sloop "Liberty" in 1768 on suspicions of smuggling triggered a riot.
In response, British troops occupied Boston, and Parliament threatened to extradite colonists to face trial in England.
Tensions rose after the murder of Christopher Seider by a customs official in 1770 and escalated into outrage after British troops fired on civilians in the Boston Massacre.
In 1772, colonists in Rhode Island boarded and burned a customs schooner.
Parliament then repealed all taxes except the one on tea, passing the Tea Act in 1773, attempting to force colonists to buy East India Company tea on which the Townshend duties were paid, thus implicitly agreeing to Parliamentary supremacy.
The landing of the tea was resisted in all colonies, but the governor of Massachusetts permitted British tea ships to remain in Boston Harbor.
So, the Sons of Liberty destroyed the tea chests, an incident that later became known as the "Boston Tea Party".
Parliament then passed punitive legislation.
It closed Boston Harbor until the tea was paid for and revoked the Massachusetts Charter, taking upon themselves the right to directly appoint the Massachusetts Governor's Council.
Additionally, the royal governor was granted powers to undermine local democracy.
Further measures allowed the extradition of officials for trial elsewhere in the Empire, if the governor felt that a fair trial could not be secured locally.
The act's vague reimbursement policy for travel expenses left few with the ability to testify, and colonists argued that it would allow officials to harass them with impunity.
Further laws allowed the governor to billet troops in private property without permission.
The colonists referred to the measures as the "Intolerable Acts", and they argued that both their constitutional rights and their natural rights were being violated, viewing the acts as a threat to all of America.
The acts were widely opposed, driving neutral parties into support of the Patriots and curtailing Loyalist sentiment.
The colonists responded by establishing the Massachusetts Provincial Congress, effectively removing Crown control of the colony outside Boston.
Meanwhile, representatives from twelve colonies convened the First Continental Congress to respond to the crisis.
The Congress narrowly rejected a proposal to create an American parliament to act in concert with the British Parliament; instead, they passed a compact declaring a trade boycott against Britain.
The Congress also affirmed that Parliament had no authority over internal American matters, but they were willing to consent to trade regulations for the benefit of the empire, and they authorized committees and conventions to enforce the boycott.
The boycott was effective, as imports from Britain dropped by 97% in 1775 compared to 1774.
Parliament refused to yield.
In 1775, it declared Massachusetts to be in a state of rebellion and enforced a blockade of the colony.
It then passed legislation to limit colonial trade to the British West Indies and the British Isles.
Colonial ships were barred from the Newfoundland cod fisheries, a measure which pleased Canadiens but damaged New England's economy.
These increasing tensions led to a mutual scramble for ordnance and pushed the colonies toward open war.
Thomas Gage was the British Commander-in-Chief and military governor of Massachusetts, and he received orders on April 14, 1775 to disarm the local militias.
On April 18, 1775, 700 troops were sent to confiscate militia ordnance stored at Concord.
Fighting broke out, forcing the regulars to conduct a fighting withdrawal to Boston.
Overnight, the local militia converged on and laid siege to Boston.
On May 25, 4,500 British reinforcements arrived with generals William Howe, John Burgoyne, and Henry Clinton.
The British seized the Charlestown peninsula on June 17 after a costly frontal assault, leading Howe to replace Gage.
Many senior officers were dismayed at the attack, which had gained them little, while Gage wrote to London stressing the need for a large army to suppress the revolt.
On July 3, George Washington took command of the Continental Army besieging Boston.
Howe made no effort to attack, much to Washington's surprise.
A plan was rejected to assault the city, and the Americans instead fortified Dorchester Heights in early March 1776 with heavy artillery captured from a raid on Fort Ticonderoga.
The British were permitted to withdraw unmolested on March 17, and they sailed to Halifax, Nova Scotia.
Washington then moved his army to New York.
Starting in August 1775, American Privateers began to raid villages in Nova Scotia, first at Saint John, then Charlottetown and Yarmouth.
They continued in 1776 at Canso and then a land assault on Fort Cumberland.
Meanwhile, British officials in Quebec began lobbying Indian tribes to support them, while the Americans urged them to maintain their neutrality.
In April 1775, Congress feared an Anglo-Indian attack from Canada and authorized an invasion of Quebec.
Quebec had a largely Francophone population and had been under British rule for only 12 years, and the Americans expected that they would welcome being liberated from the British.
The Americans attacked Quebec City on December 31 after an arduous march but were defeated.
After a loose siege, the Americans withdrew on May 6.
1776.
A failed counter-attack on June 8 ended American operations in Quebec.
However, the British could not conduct an aggressive pursuit because of American ships on Lake Champlain.
On October 11, the British defeated the American squadron, forcing them to withdraw to Ticonderoga and ending the campaign.
The invasion cost the Patriots their support in British public opinion, while aggressive anti-Loyalist policies diluted Canadian support.
The Patriots continued to view Quebec as a strategic aim, though no further attempts to invade were ever made.
In Virginia, Royal governor Lord Dunmore had attempted to disarm the militia as tensions increased, although no fighting broke out.
He issued a proclamation on November 7, 1775 promising freedom for slaves who fled their Patriot masters to fight for the Crown.
Dunmore's troops were overwhelmed by Patriots at Great Bridge, and Dunmore fled to naval ships anchored off Norfolk.
Subsequent negotiations broke down, so Dunmore ordered the ships to destroy the town.
Fighting broke out on November 19 in South Carolina between Loyalist and Patriot militias, and the Loyalists were subsequently driven out of the colony.
Loyalists were recruited in North Carolina to reassert colonial rule in the South, but they were decisively defeated and Loyalist sentiment was subdued.
A troop of British regulars set out to reconquer South Carolina and launched an attack on Charleston on June 28, 1776, but it failed and effectively left the South in Patriot control until 1780.
The shortage of gunpowder had led Congress to authorize an expedition against the Bahamas colony in the British West Indies in order to secure ordnance there.
On March 3, 1776, the Americans landed after a bloodless exchange of fire, and the local militia offered no resistance.
They confiscated all the supplies that they could load and sailed away on March 17.
The squadron reached New London, Connecticut on April 8, after a brief skirmish with the Royal Navy frigate "HMS Glasgow" on April 6.
After fighting began, Congress launched a final attempt to avert war, which Parliament rejected as insincere.
King George then issued a Proclamation of Rebellion on August 23, 1775, which only served to embolden the colonists in their determination to become independent.
After a speech by the King, Parliament rejected coercive measures on the colonies by 170 votes.
British Tories refused to compromise, while Whigs argued that current policy would drive the colonists towards independence.
Despite opposition, the King himself began micromanaging the war effort.
The Irish Parliament pledged to send troops to America, and Irish Catholics were allowed to enlist in the army for the first time.
Irish Protestants favored the Americans, while Catholics favored the King.
The initial hostilities provided a sobering military lesson for the British, causing them to rethink their views on colonial military capability.
The weak British response gave the Patriots the advantage, and the British lost control over every colony.
The army had been deliberately kept small in England since 1688 to prevent abuses of power by the King.
Parliament secured treaties with small German states for additional troops and sent an army of 32,000 men to America after a year, the largest that it had ever sent outside Europe at the time.
In the colonies, the success of Thomas Paine's pamphlet "Common Sense" had boosted public support for independence.
On July 2, Congress voted in favor of independence with twelve affirmatives and one abstention, issuing its declaration on July 4.
Washington read the declaration to his men and the citizens of New York on July 9, invigorating the crowd to tear down a lead statue of the King and melting it to make bullets.
British Tories criticized the signatories for not extending the same standards of equality to slaves.
Patriots followed independence with the Test Laws, requiring residents to swear allegiance to the state in which they lived, intending to root out neutrals or opponents to independence.
Failure to do so meant possible imprisonment, exile, or even death.
American Tories were barred from public office, forbidden from practising medicine and law, forced to pay increased taxes, or even barred from executing wills or becoming guardians to orphans.
Congress enabled states to confiscate Loyalist property to fund the war.
Some Quakers who remained neutral had their property confiscated.
States later prevented Loyalists from collecting any debts that they were owed.
After regrouping at Halifax, William Howe determined to take the fight to the Americans.
He set sail in June 1776 and began landing troops on Staten Island near the entrance to New York Harbor on July 2.
Due to poor military intelligence, Washington split his army to positions on Manhattan Island and across the East River in western Long Island, and an informal attempt to negotiate peace was rejected by the Americans.
On August 27, Howe outflanked Washington and forced him back to Brooklyn Heights.
Howe restrained his subordinates from pursuit, opting to besiege Washington instead.
Washington withdrew to Manhattan without any losses in men or ordnance.
Following the withdrawal, the Staten Island Peace Conference failed to negotiate peace, as the British delegates did not possess the authority to recognize independence.
Howe then seized control of New York City on September 15, and unsuccessfully engaged the Americans the following day.
He attempted to encircle Washington, but the Americans successfully withdrew.
On October 28, the British fought an indecisive action against Washington, in which Howe declined to attack Washington's army, instead concentrating his efforts upon a hill that was of no strategic value.
Washington's retreat left his forces isolated, and the British captured an American fortification on November 16, taking 3,000 prisoners and amounting to what one historian terms "the most disastrous defeat of the entire war".
Washington's army fell back four days later.
Henry Clinton then captured Newport, Rhode Island, an operation which he opposed, feeling that the 6,000 troops assigned to him could have been better employed in the pursuit of Washington.
The American prisoners were then sent to the infamous prison ships in which more American soldiers and sailors died of disease and neglect than died in every battle of the war combined.
Charles Cornwallis pursued Washington, but Howe ordered him to halt, and Washington marched away unmolested.
The outlook of the American cause was bleak; the army had dwindled to fewer than 5,000 men and would be reduced further when the enlistments expired at the end of the year.
Popular support wavered, morale ebbed away, and Congress abandoned Philadelphia.
Loyalist activity surged in the wake of the American defeat, especially in New York.
News of the campaign was well received in Britain.
Festivities took place in London, public support reached a peak, and the King awarded the Order of the Bath to William Howe.
The successes led to predictions that the British could win within a year.
The American defeat revealed what one writer views as Washington's strategic deficiencies, such as dividing a numerically weaker army in the face of a stronger one, his inexperienced staff misreading the situation, and his troops fleeing in disorder when fighting began.
In the meantime, the British entered winter quarters and were in a good place to resume campaigning.
On December 25, 1776, Washington stealthily crossed the Delaware River, and his army overwhelmed the Hessian garrison at Trenton, New Jersey the following morning, taking 900 prisoners.
The decisive victory rescued the army's flagging morale and gave a new hope to the cause for independence.
Cornwallis marched to retake Trenton, but his efforts were repulsed on January 2.
Washington outmanoeuvred Cornwallis that night, and defeated his rearguard the following day.
The victories proved instrumental in convincing the French and Spanish that the Americans were worthwhile allies, as well as recovering morale in the army.
Washington entered winter quarters at Morristown, New Jersey on January 6, though a protracted guerrilla conflict continued.
While encamped, Howe made no attempt to attack, much to Washington's amazement.
In December 1776, John Burgoyne returned to London to set strategy with Lord George Germain.
Burgoyne's plan was to establish control of the Champlain-George-Hudson route from New York to Quebec, isolating New England.
Efforts could then be concentrated on the southern colonies, where it was believed Loyalist support was in abundance.
Burgoyne's plan was to lead an army along Lake Champlain, while a strategic diversion advanced along the Mohawk River, and both would rendezvous at Albany.
Burgoyne set out on June 14, 1777, quickly capturing Ticonderoga on July 5.
Leaving 1,300 men behind as a garrison, Burgoyne continued the advance.
Progress was slow; the Americans blocked roads, destroyed bridges, dammed streams and denuded the area of food.
Meanwhile, Barry St.
Ledger's diversionary column laid siege to Fort Stanwix.
St.
Ledger withdrew to Quebec on August 22 after his Indian support abandoned him.
On August 16, a Hessian foraging expedition was soundly defeated at Bennington, and more than 700 troops were captured.
Meanwhile, the vast majority of Burgoyne's Indian support abandoned him and Howe informed Burgoyne he would launch his campaign on Philadelphia as planned, and would be unable to render aid.
Burgoyne decided to continue the advance.
On September 19, he attempted to flank the American position, and .
The British won, but at the cost of 600 casualties.
Burgoyne then dug in, but suffered a constant haemorrhage of deserters, and critical supplies were running low.
On October 7, a British reconnaissance in force against the American lines was .
Burgoyne then withdrew with the Americans in pursuit, and by October 13, he was surrounded.
With no hope of relief and supplies exhausted, Burgoyne surrendered on October 17, and 6,222 soldiers became prisoners of the Americans.
The decisive success spurred France to enter the war as an ally of the United States, securing the final elements needed for victory over Britain, that of foreign assistance.
Meanwhile, Howe launched his campaign against Washington, though his initial efforts to bring him to battle in June 1777 failed.
Howe declined to attack Philadelphia overland via New Jersey, or by sea via the Delaware Bay, even though both options would have enabled him to assist Burgoyne if necessary.
Instead, he took his army on a time-consuming route through the Chesapeake Bay, leaving him completely unable to assist Burgoyne.
This decision was so difficult to understand, Howe's critics accused him of treason.
Howe outflanked and defeated Washington on September 11, though he failed to follow-up on the victory and destroy his army.
A British victory at Willistown left Philadelphia defenceless, and Howe captured the city unopposed on September 26.
Howe then moved 9,000 men to Germantown, north of Philadelphia.
Washington launched a surprise attack on Howe's garrison on October 4, which was eventually repulsed.
Again, Howe did not follow-up on his victory, leaving the American army intact and able to fight.
Later, after several days of probing American defences at White Marsh, Howe inexplicably ordered a retreat to Philadelphia, astonishing both sides.
Howe ignored the vulnerable American rear, where an attack could have deprived Washington of his baggage and supplies.
On December 19, Washington's army entered winter quarters at Valley Forge.
Poor conditions and supply problems resulted in the deaths of some 2,500 troops.
Howe, only 20 miles (32 km) away, made no effort to attack, which critics observed could have ended the war.
The Continental Army was put through a new training program, supervised by Baron von Steuben, introducing the most modern Prussian methods of drilling.
Meanwhile, Howe resigned and was replaced by Henry Clinton on May 24, 1778.
Clinton received orders to abandon Philadelphia and fortify New York following France's entry into the war.
On June 18, the British departed Philadelphia, with the reinvigorated Americans in pursuit.
The two armies fought at Monmouth Court House on June 28, with the Americans holding the field, greatly boosting morale and confidence.
By July, both armies were back in the same positions they had been two years prior.
The defeat at Saratoga caused considerable anxiety in Britain over foreign intervention.
The North ministry sought reconciliation with the colonies by consenting to their original demands, although Lord North refused to grant independence.
No positive reply was received from the Americans.
French foreign minister the Comte de Vergennes was strongly anti-British, and he sought a "casus belli" to go to war with Britain following the conquest of Canada in 1763.
The French had covertly supplied the Americans through neutral Dutch ports since the onset of the war, proving invaluable throughout the Saratoga campaign.
The French public favored war, though Vergennes and King Louis XVI were hesitant, owing to the military and financial risk.
The American victory at Saratoga convinced the French that supporting the Patriots was worthwhile, but doing so also brought major concerns.
The King was concerned that Britain's concessions would be accepted, and that she would then reconcile with the Colonies to strike at French and Spanish possessions in the Caribbean.
To prevent this, France formally recognized the United States on February 6, 1778 and followed with a military alliance.
France aimed to expel Britain from the Newfoundland fishery, end restrictions on Dunkirk sovereignty, regain free trade in India, recover Senegal and Dominica, and restore the Treaty of Utrecht provisions pertaining to Anglo-French trade.
Spain was wary of provoking war with Britain before she was ready, so she covertly supplied the Patriots via her colonies in New Spain.
Congress hoped to persuade Spain into an open alliance, so the first American Commission met with the Count of Aranda in 1776.
Spain was still reluctant to make an early commitment, owing to a lack of direct French involvement, the threat against their treasure fleets, and the possibility of war with Portugal, Spain's neighbor and a close ally of Britain.
However, Spain affirmed its desire to support the Americans the following year, hoping to weaken Britain's empire.
In the Spanish-Portuguese War (1776–77), the Portuguese threat was neutralized.
On 12 April 1779, Spain signed the Treaty of Aranjuez with France and went to war against Britain.
Spain sought to recover Gibraltar and Menorca in Europe, as well as Mobile and Pensacola in Florida, and also to expel the British from Central America.
Meanwhile, George III had given up on subduing America while Britain had a European war to fight.
He did not welcome war with France, but he believed that Britain had made all necessary steps to avoid it and cited the British victories over France in the Seven Years' War as a reason to remain optimistic.
Britain tried in vain to find a powerful ally to engage France, leaving it isolated, preventing Britain from focusing the majority of her efforts in one theater, and forcing a major diversion of military resources from America.
Despite this, the King determined never to recognize American independence and to ravage the colonies indefinitely, or until they pleaded to return to the yoke of the Crown.
Mahan argues that Britain's attempt to fight in multiple theaters simultaneously without major allies was fundamentally flawed, citing impossible mutual support, exposing the forces to defeat in detail.
Since the outbreak of the conflict, Britain had appealed to her ally, the neutral Dutch Republic, to loan her the use of the Scots Brigade for service in America, but pro-American sentiment among the Dutch public forced them to deny the request.
Consequently, the British attempted to invoke several treaties for outright Dutch military support, but the Republic still refused.
Moreover, American troops were being supplied with ordnance by Dutch merchants via their West Indies colonies.
French supplies bound for America had also passed through Dutch ports.
The Republic maintained free trade with France following France's declaration of war on Britain, citing a prior concession by Britain on this issue.
Britain responded by confiscating Dutch shipping, and even firing upon it.
Consequently, the Republic joined the First League of Armed Neutrality to enforce their neutral status.
The Republic had also given sanctuary to American privateers and had drafted a treaty of commerce with the Americans.
Britain argued that these actions contravened the Republic's neutral stance and declared war in December 1780.
Soon after France declared war, French and British fleets fought an indecisive action off Ushant on 27 July 1778.
Spain entered the war on 12 April 1779, with a primary goal of capturing Gibraltar, Spanish troops under the Duc de Crillon laid siege to the Rock on 24 June.
The naval blockade, however, was relatively weak, and the British were able to resupply the garrison.
Meanwhile, a plan was formulated for a combined Franco-Spanish invasion of the British mainland, but the expedition failed due to a combination of poor planning, disease, logistical issues, and high financial expenditures.
However, a diversionary Franco-American squadron did meet with some success on 23 September under John Paul Jones.
On 16 January 1780, the Royal Navy under George Rodney scored a major victory over the Spanish, weakening the naval blockade of Gibraltar.
A Franco-Spanish fleet commanded by Luis de Córdova intercepted and decisively defeated a large British convoy off the Azores led by John Moutray on 9 August which was bound for the West Indies.
The defeat was catastrophic for Britain, which lost 52 merchant ships, 5 East Indiamen, 80,000 muskets, equipment for 40,000 troops, 294 guns, and 3,144 men, making it one of the most complete naval captures ever made.
The loss was valued at some £1.5 million (£ in today's money), dealing a severe blow to British commerce.
The French blockaded the lucrative sugar islands of Barbados and Jamaica, intending to damage British trade.
French troops led by the Marquis de Bouillé captured Dominica on 7 September 1778 in order to improve communication among French Caribbean islands and to strike a blow to privateering.
The British defeated a French naval force on 15 December and captured St.
Lucia on 28 December.
Both fleets received reinforcements through the first half of 1779, but the French under the Comte d'Estaing had superiority in the Caribbean and began capturing British territories, seizing St.
Vincent on 18 June and Grenada on 4 July.
The British fleet under John Byron was tactically defeated on July 6, having pursued d'Estaing from Grenada, the worst loss that the Royal Navy had suffered since 1690.
Naval skirmishes continued until 17 April 1780, when British and French fleets clashed indecisively off Martinique.
General Bernardo de Gálvez raised an army in New Orleans and drove the British out of the Gulf of Mexico.
Gálvez captured five British forts in the Lower Mississippi Valley.
They repelled a British and Indian attack in St.
Louis, Missouri and captured the British fort of St.
Joseph in present-day Niles, Michigan.
With reinforcements from Cuba, Mexico, Puerto Rico, Gálvez captured Mobile and Pensacola, the capital of the British colony of West Florida.
At Pensacola, Gálvez commanded a multinational army of over 7,000 black and white soldiers.
These men were born in Spain, Cuba, Mexico, Puerto Rico, Santo Domingo, and other Spanish colonies such as Venezuela.
In Central America, the defense of Guatemala was a priority for Spain.
The British intended to capture the key fortress of San Fernando de Omoa and drive the Spanish from the region.
After inadequate first attempts, 1,200 British troops led by William Dalrymple arrived on 16 October, and they captured the fort on 20 October.
However, the British suffered terribly due to disease and were forced to abandon the fort on 29 November; Spanish troops subsequently reoccupied it.
In 1780, Jamaica's governor John Dalling planned an expedition to cut New Spain in two by capturing Granada, which would subsequently allow them full control of the San Juan River.
A British expedition set out on 3 February 1780, led by John Polson and Horatio Nelson.
They reached Fort San Juan on 17 March and laid siege, capturing it on 29 April.
The British were ravaged by disease and were running low on food due to poor logistics.
They withdrew on 8 November, the expedition having suffered a decisive defeat; some 2,500 troops had perished, making it the costliest British disaster of the war.
The British East India Company moved quickly to capture French possessions in India when they learned about the hostilities with France, and they took Pondicherry on 19 October 1778 after a two-week siege.
The Company resolved to drive the French completely out of India, and they captured the Malabar port of Mahé in 1779 where French ordnance passed through.
Mahé was under the protection of Mysore's ruler Hyder Ali (the Tipu Sultan), and tensions were already inflamed because the British had supported Malabar rebels who had risen against him; so the fall of Mahé precipitated war.
Hyder Ali invaded the Carnatic region in July 1780 and laid siege to Tellicherry and Arcot.
A British relief force of 7,000 men under William Baille was intercepted and destroyed by the Tipu Sultan on 10 September, the worst defeat suffered by a European army in India at the time.
Ali then renewed the siege at Arcot instead of pressing on for a decisive victory against a second British army at Madras, capturing it on 3 November.
The delay allowed British forces to regroup for campaigning the following year.
Henry Clinton withdrew from Philadelphia, consolidating his forces in New York following the British defeat at Saratoga and the entry of France into the war.
French admiral the Comte d'Estaing had been dispatched to North America in April 1778 to assist Washington, and he arrived shortly after Clinton withdrew into New York.
The Franco-American forces felt that New York's defenses were too formidable for the French fleet, and they opted to attack Newport.
This effort was launched on August 29, but it failed when the French opted to withdraw, and this displeased the Americans.
The war then ground down to a stalemate, with the majority of actions fought as large skirmishes, such as those at Chestnut Neck and Little Egg Harbor.
In the summer of 1779, the Americans captured British posts at Stony Point and Paulus Hook.
In July, Clinton unsuccessfully attempted to coax Washington into a decisive engagement by making a major raid into Connecticut.
That month, a large American naval operation attempted to retake Maine, but it resulted in the worst American naval defeat until Pearl Harbor in 1941.
The high frequency of Iroquois raids on the locals compelled Washington to mount a punitive expedition which destroyed a large number of Iroquois settlements, but the effort ultimately failed to stop the raids.
During the winter of 1779–80, the Continental Army suffered greater hardships than at Valley Forge.
Morale was poor; public support was being eroded by the long war; the national currency was virtually worthless; the army was plagued with supply problems; desertion was common; and whole regiments mutinied over the conditions in early 1780.
In 1780, Clinton launched an attempt to retake New Jersey.
On June 7, 6,000 men invaded under Hessian general Wilhelm von Knyphausen, but they met stiff resistance from the local militia.
The British held the field, but Knyphausen feared a general engagement with Washington's main army and withdrew.
Knyphausen and Clinton decided upon a second attempt two weeks later which was soundly defeated at Springfield, effectively ending British ambitions in New Jersey.
Meanwhile, American general Benedict Arnold had defected to the British, and he conspired to betray the key American fortress of West Point by surrendering it to the enemy.
The plot was foiled when British spy master John André was captured, so Arnold fled to British lines in New York.
He attempted to justify his betrayal by appealing to Loyalist public opinion, but the Patriots strongly condemned him as a coward and turncoat.
The war to the west of the Appalachians was largely confined to skirmishing and raids.
An expedition of militia was halted due to adverse weather in February 1778 which had set out to destroy British military supplies in settlements along the Cuyahoga River.
Later in the year, a second campaign was undertaken to seize the Illinois Country from the British.
The Americans captured Kaskaskia on July 4 and then secured Vincennes, although Vincennes was recaptured by Henry Hamilton, the British commander at Detroit.
In early 1779, the Americans counterattacked by undertaking a risky winter march, and they secured the surrender of the British at Vincennes, taking Hamilton prisoner.
On May 25, 1780, the British launched an expedition into Kentucky as part of a wider operation to clear resistance from Quebec to the Gulf coast.
The expedition met with only limited success, though hundreds of settlers were killed or captured.
The Americans responded with a major offensive along the Mad River in August which met with some success, but it did little to abate the Indian raids on the frontier.
French militia attempted to capture Detroit, but it ended in disaster when Miami Indians ambushed and defeated the gathered troops on November 5.
The war in the west had become a stalemate; the Americans did not have the manpower to simultaneously defeat the hostile Indian tribes and occupy their land.
The British turned their attention to conquering the South in 1778, after Loyalists in London assured them of a strong Loyalist base there.
A southern campaign also had the advantage of keeping the Royal Navy closer to the Caribbean, where it would be needed to defend lucrative colonies against the Franco-Spanish fleets.
On December 29, 1778, an expeditionary corps from New York captured Savannah, and British troops then moved inland to recruit Loyalist support.
There was a promising initial turnout in early 1779, but then a large Loyalist militia was defeated at Kettle Creek on February 14 and they had to recognize their dependence upon the British.
The British, however, defeated Patriot militia at Brier Creek on March 3, and then launched an abortive assault on Charleston, South Carolina.
The operation became notorious for its high degree of looting by British troops, enraging both Loyalists and Patriot colonists.
In October, a combined Franco-American effort failed to recapture Savannah.
In May 1780, Henry Clinton captured Charleston, taking over 5,000 prisoners and effectively destroying the Continental Army in the south.
Organized American resistance in the region collapsed when Banastre Tarleton defeated the withdrawing Americans at Waxhaws on May 29.
Clinton returned to New York, leaving Charles Cornwallis in command in Charleston to oversee the southern war effort.
Far fewer Loyalists than expected joined him.
In the interim, the war was carried on by Patriot militias who effectively suppressed Loyalists by winning victories in Fairfield County, Lincolnton, Huck's Defeat, Stanly County, and Lancaster County.
Congress appointed Horatio Gates, victor at Saratoga, to lead the American effort in the south.
He suffered a major defeat at Camden on August 16, 1780, setting the stage for Cornwallis to invade North Carolina.
The British attempted to subjugate the countryside, and Patriot militia continued to fight against them, so Cornwallis dispatched troops to raise Loyalist forces to cover his left flank as he moved north.
This wing of Cornwallis' army was virtually destroyed on October 7, irreversibly breaking Loyalist support in the Carolinas.
Cornwallis subsequently aborted his advance and retreated back into South Carolina.
In the interim, Washington replaced Gates with his trusted subordinate, Nathanael Greene.
Greene was unable to confront the British directly, so he dispatched a force under Daniel Morgan to recruit additional troops.
Morgan then defeated the cream of the British army under Tarleton on January 17, 1781 at Cowpens.
Cornwallis was criticized for having detached a substantial part of his army without adequate support, but he advanced into North Carolina despite the setbacks, gambling that he would receive substantial Loyalist support there.
Greene evaded combat with Cornwallis, instead wearing his army down through a protracted war of attrition.
By March, Greene's army had increased in size enough that he felt confident in facing Cornwallis.
The two armies engaged at Guilford Courthouse on March 15; Greene was beaten, but Cornwallis' army suffered irreplaceable casualties.
Compounding this, far fewer Loyalists were joining than the British had previously expected.
Cornwallis' casualties were such that he was compelled to retreat to Wilmington for reinforcement, leaving the Patriots in control of the interior of the Carolinas and Georgia.
Greene then proceeded to reclaim the South.
The American troops suffered a reversal at Hobkirk's Hill on April 25; nonetheless, they continued to dislodge strategic British posts in the area, capturing Fort Watson and Fort Motte.
Augusta was the last major British outpost in the South outside of Charleston and Savannah, but the Americans reclaimed possession of it on June 6.
A British force clashed with American troops at Eutaw Springs on September 8 in a final effort to stop Greene, but the British casualties were so high that they withdrew to Charleston.
Minor skirmishes continued in the Carolinas until the end of the war, and British troops were effectively confined to Charleston and Savannah for the remainder of the conflict.
Cornwallis had discovered that the majority of American supplies in the Carolinas were passing through Virginia, and he had written to both Lord Germain and Clinton detailing his intentions to invade.
Cornwallis believed that a successful campaign there would cut supplies to Greene's army and precipitate a collapse of American resistance in the South.
Clinton strongly opposed the plan, favoring a campaign farther north in the Chesapeake Bay region.
Lord Germain wrote to Cornwallis to approve his plan and neglected to include Clinton in the decision-making, even though Clinton was Cornwallis' superior officer, and Cornwallis then decided to move into Virginia without informing Clinton.
Clinton, however, had failed to construct a coherent strategy for British operations in 1781, owing to his difficult relationship with his naval counterpart Marriot Arbuthnot.
Following the calamitous operations at Newport and Savannah, French planners realized that closer cooperation with the Americans was required to achieve success.
The French fleet led by the Comte de Grasse had received discretionary orders from Paris to assist joint efforts in the north if naval support was needed.
Washington and the Comte de Rochambeau discussed their options.
Washington pushed for an attack on New York, while Rochambeau preferred a strike in Virginia where the British were less well-established and thus easier to defeat.
Franco-American movements around New York caused Clinton a great deal of anxiety, fearing an attack on the city.
His instructions were vague to Cornwallis during this time, rarely forming explicit orders.
However, Clinton did instruct Cornwallis to establish a fortified naval base and to transfer troops to the north to defend New York.
Cornwallis dug in at Yorktown and awaited the Royal Navy.
Washington still favored an assault on New York, but he acquiesced to the French when they opted to send their fleet to their preferred target of Yorktown.
In August, the combined Franco-American army moved south to coordinate with de Grasse in defeating Cornwallis.
The British lacked sufficient naval resources to effectively counter the French, but they dispatched a fleet under Thomas Graves to assist Cornwallis and attempt to gain naval dominance.
On September 5, the French fleet decisively defeated Graves, giving the French control of the seas around Yorktown and cutting off Cornwallis from reinforcements and relief.
Despite the continued urging of his subordinates, Cornwallis made no attempt to break out and engage the Franco-American army before it had established siege works, expecting that reinforcements would arrive from New York, and the Franco-American army laid siege to Yorktown on September 28.
Cornwallis continued to think that relief was imminent from Clinton, and he abandoned his outer defenses which were immediately occupied by American troops—serving to hasten his subsequent defeat.
The British then failed in an attempt to break out of the siege across the river at Gloucester Point when a storm hit.
Cornwallis and his subordinates were under increasing bombardment and facing dwindling supplies; they agreed that their situation was untenable and negotiated a surrender on October 17, 1781, and 7,685 soldiers became prisoners of the Americans.
The same day as the surrender, 6,000 troops under Clinton had departed New York, sailing to relieve Yorktown.
On 25 November 1781, news arrived in London of the surrender at Yorktown.
The Whig opposition gained traction in Parliament, and a motion was proposed on December 12 to end the war which was defeated by only one vote.
On 27 February 1782, the House voted against further war in America by 19 votes.
Lord Germain was dismissed and a vote of no confidence was passed against North.
The Rockingham Whigs came to power and opened negotiations for peace.
Rockingham died and was succeeded by the Earl of Shelburne.
Despite their defeat, the British still had 30,000 troops garrisoned in New York, Charleston, and Savannah.
Henry Clinton was recalled and was replaced by Guy Carleton who was under orders to suspend offensive operations.
After hostilities with the Dutch began in late 1780, Britain had moved quickly, enforcing a blockade across the North Sea.
Within weeks, the British had captured 200 Dutch merchantmen, and 300 more were holed up in foreign ports, though political turmoil within the Republic and peace negotiations by both sides helped keep conflict to a minimum.
The majority of the Dutch public favored a military alliance with France against Britain; however, the Dutch Stadtholder impeded these efforts, hoping to secure an early peace.
To restore diminishing trade a Dutch squadron under Johan Zoutman escorted a fleet of some 70 merchantmen from the Texel.
Zoutman's ships were intercepted by Sir Hyde Parker, who engaged Zoutman at Dogger Bank on 5 August 1781.
Though the contest was tactically inconclusive, the Dutch fleet did not leave harbor again during the war, and their merchant fleet remained crippled.
On 6 January 1781, a French attempt to capture Jersey to neutralize British privateering failed.
Frustrated in their attempts to capture Gibraltar, a Franco-Spanish force of 14,000 men under the Duc de Mahon invaded Minorca on 19 August.
After a long siege of St.
Philip's, the British garrison under James Murray surrendered on 5 February 1782, securing a primary war goal for the Spanish.
At Gibraltar, a major Franco-Spanish assault on 13 September 1782 was repulsed with heavy casualties.
On 20 October 1782, following a successful resupply of Gibraltar, British ships under Richard Howe successfully refused battle to the Franco-Spanish fleet under Luis de Córdova, denying Córdova dominance at sea.
On 7 February 1783, after 1,322 days of siege, the Franco-Spanish army withdrew, decisively defeated.
Sint Eustatius, a key supply port for the Patriots, was sacked by British forces under George Rodney on 3 February 1781, who plundered the island's wealth.
Few operations were conducted against the Dutch, although several Dutch colonies were captured by the British in 1781.
After the fall of Mobile to Spanish troops under Bernardo de Gálvez, an attempt to capture Pensacola was thwarted due to a hurricane.
Emboldened by the disaster, John Campbell, British commander at Pensacola, decided to recapture Mobile.
Campbell's expeditionary force of around 700 men was defeated on 7 January 1781.
After re-grouping at Havana, Gálvez set out for Pensacola on 13 February.
Arriving on 9 March, siege operations did not begin until 24 March, owing to difficulties in bringing the ships into the bay.
After a 45-day siege, Gálvez decisively defeated the garrison, securing the conquest of West Florida.
In May, Spanish troops captured the Bahamas, although the British bloodlessly recaptured the islands the following year on 18 April.
In the West Indies, on 29–30 April 1781, a Royal Navy squadron under Samuel Hood was narrowly defeated by the French, led by the Comte de Grasse, who continued seizing British territories: Tobago fell on 2 June; Demerara and Essequibo on 22 January 1782; St.
Kitts and Nevis on 12 February, despite a British naval victory on 25 January; and Montserrat on 22 February.
In 1782, the primary strategic goal of the French and Spanish was the capture of Jamaica, whose sugar exports were more valuable to the British than the Thirteen Colonies combined.
On 7 April 1782, de Grasse departed Martinique to rendezvous with Franco-Spanish troops at Saint Domingue and invade Jamaica from the north.
The British under Hood and George Rodney pursued and decisively defeated the French off Dominica between 9–12 April.
The Franco-Spanish plan to conquer Jamaica was in ruins, and the balance of naval power in the Caribbean shifted to the Royal Navy.
In Guatemala, Matías de Gálvez led Spanish troops in an effort to dislocate British settlements along the Gulf of Honduras.
Gálvez captured Roatán on 16 March 1782, and then quickly took Black River.
Following the decisive naval victory at the Saintes, Archibald Campbell, the Royal governor of Jamaica, authorized Edward Despard to re-take Black River, which he did on 22 August.
However, with peace talks opening, and Franco-Spanish resources committed to the siege of Gibraltar, no further offensive operations took place.
Following Dutch entry into the conflict, East India Company troops under Hector Munro captured the Dutch port of Negapatam after a three-week siege on 11 October 1781.
Soon after, British Admiral Edward Hughes captured Trincomalee after a brief engagement on 11 January 1782.
In March 1781, French Admiral Bailli de Suffren was dispatched to India to assist colonial efforts.
Suffren arrived off the Indian coast in February 1782, where he clashed with a British fleet under Hughes, winning a narrow tactical victory.
After landing troops at Porto Novo to assist Mysore, Suffren's fleet clashed with Hughes again Providien on 12 April.
There was no clear victor, though Hughes' fleet came off worse, and he withdrew to the British-held port of Trincomalee.
Hyder Ali wished for the French to capture Negapatam to establish naval dominance over the British, and this task fell to Suffren.
Suffren's fleet clashed with Hughes again off Negapatam on 6 July.
Suffren withdrew to Cuddalore, strategically defeated, and the British remained in control of Negapatam.
Intending to find a more suitable port than Cuddalore, Suffren captured Trincomalee on 1 September, and successfully engaged Hughes two days later.
Meanwhile, Ali's troops loosely blockaded Vellore as the East India Company regrouped.
Company troops under Sir Eyre Coote led a counter-offensive, defeating Ali at Porto Novo on 1 July 1781, Pollilur on 27 August, and Sholinghur on 27 September, expelling the Mysorean troops from the Carnatic.
On 18 February 1782, Tipu Sultan defeated John Braithwaite near Tanjore, taking his entire 1,800-strong force prisoner.
The war had, by this point, reached an uneasy stalemate.
On 7 December 1782, Hyder Ali died, and the rule of Mysore passed to his son, Tipu Sultan.
Sultan advanced along the west coast, laying siege to Mangalore on 20 May 1783.
Meanwhile, on the east coast, an army under James Stuart besieged the French-held port of Cuddalore on 9 June 1783.
On 20 June, key British naval support for the siege was neutralized when Suffren defeated Hughes' fleet off Cuddalore, and though narrow, the victory gave Suffren the opportunity to displace British holdings in India.
On 25 June, the Franco-Mysorean defenders made repeated sorties against British lines, though all assaults failed.
On 30 June, news arrived of a preliminary peace between the belligerent powers, and the siege was effectively over when the French abandoned the siege.
Mangalore remained under siege, and capitulated to Sultan on 30 January 1784.
Little fighting took place thereafter, and Mysore and Britain made peace on 11 March.
Following the surrender at Yorktown, the Whig party came to power in Britain and began opening negotiations for a cessation of hostilities.
While peace negotiations were being undertaken, British troops in America were restricted from launching further offensives.
Prime Minister the Earl of Shelburne was reluctant to accept American independence as a prerequisite for peace, as the British were aware that the French economy was nearly bankrupt, and reinforcements sent to the West Indies could potentially reverse the situation there.
He preferred that the colonies accept Dominion status within the Empire, though a similar offer had been rejected by the Americans in 1778.
Negotiations soon began in Paris.
The Americans initially demanded that Quebec be ceded to them as spoils of war, a proposal that was dropped when Shelburne accepted American demands for recognition of independence.
On April 19, 1782, the Dutch formally recognized the United States as a sovereign power, enhancing American leverage at the negotiations.
Spain initially impeded the negotiations, refusing to enter into peace talks until Gibraltar had been captured.
The Comte de Vergennes proposed that American territory be confined to the east of the Appalachians; Britain would have sovereignty over the area north of the Ohio River, below which an Indian barrier state would be established under Spanish control.
The United States fiercely opposed the proposal.
The Americans skirted their allies, recognizing that more favorable terms would be found in London.
They negotiated directly with Shelburne, who hoped to make Britain a valuable trading partner of America at the expense of France.
To this end, Shelburne offered to cede all the land east of the Mississippi River, north of Florida, and south of Quebec, while also allowing American fishermen access to the rich Newfoundland fishery.
Shelburne was hoping to facilitate the growth of the American population, creating lucrative markets that Britain could exploit at no administrative cost to London.
As Vergennes commented, "the English buy peace rather than make it".
Throughout the negotiations, Britain never consulted her American Indian allies, forcing them to reluctantly accept the treaty.
However, the subsequent tension erupted into conflicts between the Indians and the young United States, the largest being the Northwest Indian War.
Britain continued trying to create an Indian buffer state in the American Midwest as late as 1814 during the War of 1812.
Britain negotiated separate treaties with Spain, France, and the Dutch Republic.
Gibraltar proved to be a stumbling block in the peace talks; Spain offered to relinquish their conquests in West Florida, Menorca, and the Bahamas in exchange for Gibraltar, terms which Shelburne steadfastly refused.
Shelburne instead offered to cede East Florida, West Florida, and Menorca if Spain would relinquish the claim on Gibraltar, terms which were reluctantly accepted.
However, in the long-term, the new territorial gains were of little value to Spain.
France's only net gains were the island of Tobago in the Caribbean and Senegal in Africa, after agreeing to return all other colonial conquests to British sovereignty.
Britain returned Dutch Caribbean territories to Dutch sovereignty, in exchange for free trade rights in the Dutch East Indies and control of the Indian port of Negapatnam.
Preliminary peace articles were signed in Paris on 30 November 1782, while preliminaries between Britain, Spain, France, and the Netherlands continued until September 1783.
The United States Congress of the Confederation ratified the Treaty of Paris on January 14, 1784.
Copies were sent back to Europe for ratification by the other parties involved, the first reaching France in March 1784.
British ratification occurred on April 9, 1784, and the ratified versions were exchanged in Paris on May 12, 1784.
The war formally concluded on September 3, 1783.
The last British troops departed New York City on November 25, 1783, marking the end of British rule in the new United States.
The total loss of life throughout the conflict is largely unknown.
As was typical in wars of the era, diseases such as smallpox claimed more lives than battle.
Between 1775 and 1782, a smallpox epidemic broke out throughout North America, killing 40 people in Boston alone.
Historian Joseph Ellis suggests that Washington's decision to have his troops inoculated against the disease was one of his most important decisions.
Between 25,000 and 70,000 American Patriots died during active military service.
Of these, approximately 6,800 were killed in battle, while at least 17,000 died from disease.
The majority of the latter died while prisoners of war of the British, mostly in the prison ships in New York Harbor.
If the upper limit of 70,000 is accepted as the total net loss for the Patriots, it would make the conflict proportionally deadlier than the American Civil War.
Uncertainty arises due to the difficulties in accurately calculating the number of those who succumbed to disease, as it is estimated at least 10,000 died in 1776 alone.
The number of Patriots seriously wounded or disabled by the war has been estimated from 8,500 to 25,000.
The French suffered approximately 7,000 total dead throughout the conflict; of those, 2,112 were killed in combat in the American theaters of war.
The Dutch suffered around 500 total killed, owing to the minor scale of their conflict with Britain.
British returns in 1783 listed 43,633 rank and file deaths across the British Armed Forces.
A table from 1781 puts total British Army deaths at 9,372 soldiers killed in battle across the Americas; 6,046 in North America (1775–1779), and 3,326 in the West Indies (1778–1780).
In 1784, a British lieutenant compiled a detailed list of 205 British officers killed in action during the war, encompassing Europe, the Caribbean and the East Indies.
Extrapolations based upon this list puts British Army losses in the area of at least 4,000 killed or died of wounds.
Approximately 7,774 Germans died in British service in addition to 4,888 deserters; of the former, it is estimated 1,800 were killed in combat.
Around 171,000 sailors served in the Royal Navy during the war; approximately a quarter of whom had been pressed into service.
Around 1,240 were killed in battle, while an estimated 18,500 died from disease (1776–1780).
The greatest killer at sea was scurvy, a disease caused by vitamin C deficiency.
It was not until 1795 that scurvy was eradicated from the Royal Navy after the Admiralty declared lemon juice and sugar were to be issued among the standard daily rations of sailors.
Around 42,000 sailors deserted during the war.
The impact on merchant shipping was substantial; an estimated 3,386 merchant ships were seized by enemy forces during the war; of those, 2,283 were taken by American privateers alone.
At the start of the war, the economy of the colonies was flourishing, and the free white population enjoyed the highest standard of living in the world.
The Royal Navy enforced a naval blockade during the war to financially cripple the colonies, however, this proved unsuccessful; 90% of the population worked in farming, not in coastal trade, and, as such, the American economy proved resilient enough to withstand the blockade.
Congress had immense difficulties throughout the conflict to efficiently finance the war effort.
As the circulation of hard currency declined, the Americans had to rely on loans from American merchants and bankers, France, Spain and the Netherlands, saddling the young nation with crippling debts.
Congress attempted to remedy this by printing vast amounts of paper money and bills of credit to raise revenue.
The effect was disastrous; inflation skyrocketed, and the paper money became virtually worthless.
The inflation spawned a popular phrase that anything of little value was "not worth a continental".
By 1791, the United States had accumulated a national debt of approximately $75.5 million.
The United States finally solved its debt and currency problems in the 1790s, when Secretary of the Treasury Alexander Hamilton secured legislation by which the national government assumed all of the state debts, and, in addition, created a national bank and a funding system based on tariffs and bond issues that paid off the foreign debts.
Britain spent around £80 million and ended with a national debt of £250 million, (£ in today's money), generating a yearly interest of £9.5 million annually.
The debts piled upon that which it had already accumulated from the Seven Years' War.
Due to wartime taxation upon the British populace, the tax for the average Briton amounted to approximately four shilling in every pound, or 20 percent.
The French spent approximately 1.3 billion livres on aiding the Americans, accumulating a national debt of 3.315.1 billion livres by 1783 on war costs.
Unlike Britain, which had a very efficient taxation system, the French tax system was highly unstable, eventually leading to a financial crisis in 1786.
The debts contributed to a worsening fiscal crisis that ultimately begat the French Revolution at the end of the century.
The debt continued to spiral; on the eve of the French Revolution, the national debt had skyrocketed to 12 billion livres.
Spain had nearly doubled her military spending during the war, from 454 million reales in 1778 to over 700 million in 1779.
Spain more easily disposed of her debts unlike her French ally, partially due to the massive increase in silver mining in her American colonies; production increased approximately 600% in Mexico, and by 250% in Peru and Bolivia.
The population of Great Britain and Ireland in 1780 was approximately 12.6 million, while the Thirteen Colonies held a population of some 2.8 million, including some 500,000 slaves.
Theoretically, Britain had the advantage, however, many factors inhibited the procurement of a large army.
In 1775, the standing British Army, exclusive of militia, comprised 45,123 men worldwide, made up of 38,254 infantry and 6,869 cavalry.
The Army had approximately eighteen regiments of foot, some 8,500 men, stationed in North America.
Standing armies had played a key role in the purge of the Long Parliament in 1648, the maintenance of a military dictatorship under Oliver Cromwell, and the overthrow of James II, and, as such, the Army had been deliberately kept small in peacetime to prevent abuses of power by the King.
Despite this, eighteenth century armies were not easy guests, and were regarded with scorn and contempt by the press and public of the New and Old World alike, derided as enemies of liberty.
An expression ran in the Navy; "A messmate before a shipmate, a shipmate before a stranger, a stranger before a dog, a dog before a soldier".
Parliament suffered chronic difficulties in obtaining sufficient manpower, and found it impossible to fill the quotas they had set.
The Army was a deeply unpopular profession, one contentious issue being pay.
A Private infantryman was paid a wage of just 8d.
per day, the same pay as for a New Model Army infantryman, 130 years earlier.
The rate of pay in the army was insufficient to meet the rising costs of living, turning off potential recruits, as service was nominally for life.
To entice people to enrol, Parliament offered a bounty of £1.10s for every recruit.
As the war dragged on, Parliament became desperate for manpower; criminals were offered military service to escape legal penalties, and deserters were pardoned if they re-joined their units.
After the defeat at Saratoga, Parliament doubled the bounty to £3, and increased it again the following year, to £3.3s, as well as expanding the age limit from 17–45 to 16–50 years of age.
Impressment, essentially conscription by the "press gang", was a favored recruiting method, though it was unpopular with the public, leading many to enlist in local militias to avoid regular service.
Attempts were made to draft such levies, much to the chagrin of the militia commanders.
Competition between naval and army press gangs, and even between rival ships or regiments, frequently resulted in brawls between the gangs in order to secure recruits for their unit.
Men would maim themselves to avoid the press gangs, while many deserted at the first opportunity.
Pressed men were militarily unreliable; regiments with large numbers of such men were deployed to garrisons such as Gibraltar or the West Indies, purely to increase the difficulty in successfully deserting.
By 1781, the Army numbered approximately 121,000 men globally, 48,000 of whom were stationed throughout the Americas.
Of the 171,000 sailors who served in the Royal Navy throughout the conflict, around a quarter were pressed.
This same proportion, approximately 42,000 men, deserted during the conflict.
At its height, the Navy had 94 ships-of-the-line, 104 frigates and 37 sloops in service.
In 1775, Britain unsuccessfully attempted to secure 20,000 mercenaries from Russia, and the use of the Scots Brigade from the Dutch Republic, such was the shortage of manpower.
Parliament managed to negotiate treaties with the princes of German states for large sums of money, in exchange for mercenary troops.
In total, 29,875 troops were hired for British service from six German states; Brunswick (5,723), Hesse-Kassel (16,992), Hesse-Hannau (2,422), Ansbach-Bayreuth (2,353), Waldeck-Pyrmont (1,225) and Anhalt-Zerbst (1,160).
King George III, who also ruled Hanover as a Prince-elector of the Holy Roman Empire, was approached by Parliament to loan the government Hanoverian soldiers for service in the war.
Hanover supplied 2,365 men in five battalions, however, the lease agreement permitted them to only be used in Europe.
Without any major allies, the manpower shortage became critical when France and Spain entered the war, forcing a major diversion of military resources from the Americas.
Recruiting adequate numbers of Loyalist militia in America proved difficult due to high Patriot activity.
To bolster numbers, the British promised freedom and grants of land to slaves who fought for them.
Approximately 25,000 Loyalists fought for the British throughout the war, and provided some of the best troops in the British service; the British Legion, a mixed regiment of 250 dragoons and 200 infantry commanded by Banastre Tarleton, gained a fearsome reputation in the colonies, especially in the South.
Britain had a difficult time appointing a determined senior military leadership in America.
Thomas Gage, Commander-in-Chief of North America at the outbreak of the war, was criticized for being too lenient on the rebellious colonists.
Jeffrey Amherst, who was appointed Commander-in-Chief of the Forces in 1778, refused a direct command in America, due to unwillingness to take sides in the war.
Admiral Augustus Keppel similarly opposed a command, stating; "I cannot draw the sword in such a cause".
The Earl of Effingham resigned his commission when his regiment was posted to America, while William Howe and John Burgoyne were opposed to military solutions to the crisis.
Howe and Henry Clinton both stated they were unwilling participants, and were only following orders.
As was the case in many European armies, except the Prussian Army, officers in British service could purchase commissions to ascend the ranks.
Despite repeated attempts by Parliament to suppress it, the practise was common in the Army.
Values of commissions varied, but were usually in line with social and military prestige, for example, regiments such as the Guards commanded the highest prices.
The lower ranks often regarded the treatment to high-ranking commissions by wealthier officers as "plums for [their] consumption".
Wealthy individuals lacking any formal military education, or practical experience, often found their way into positions of high responsibility, diluting the effectiveness of a regiment.
Though Royal authority had forbade the practise since 1711, it was still permitted for infants to hold commissions.
Young boys, often orphans of deceased wealthy officers, were taken from their schooling and placed in positions of responsibility within regiments.
Logistical organization of eighteenth century armies was chaotic at best, and the British Army was no exception.
No logistical corps existed in the modern sense; while on campaign in foreign territories such as America, horses, wagons, and drivers were frequently requisitioned from the locals, often by impressment or by hire.
No centrally organized medical corps existed.
It was common for surgeons to have no formal medical education, and no diploma or entry examination was required.
Nurses sometimes were apprentices to surgeons, but many were drafted from the women who followed the army.
Army surgeons and doctors were poorly paid and were regarded as social inferiors to other officers.
The heavy personal equipment and wool uniform of the regular infantrymen were wholly unsuitable for combat in America, and the outfit was especially ill-suited to comfort and agile movement.
During the Battle of Monmouth in late June 1778, the temperature exceeded 100°F (37.8°C) and is said to have claimed more lives through heat stroke than through actual combat.
The standard-issue firearm of the British Army was the Land Pattern Musket.
Some officers preferred their troops to fire careful, measured shots (around two per minute), rather than rapid firing.
A bayonet made firing difficult, as its cumbersome shape hampered ramming down the charge into the barrel.
British troops had a tendency to fire impetuously, resulting in inaccurate fire, a trait for which John Burgoyne criticized them during the Saratoga campaign.
Burgoyne instead encouraged bayonet charges to break up enemy formations, which was a preferred tactic in most European armies at the time.
Every battalion in America had organized its own rifle company by the end of the war, although rifles were not formally issued to the army until the Baker Rifle in 1801.
Flintlocks were heavily dependent on the weather; high winds could blow the gunpowder from the flash pan, while heavy rain could soak the paper cartridge, ruining the powder and rendering the musket unable to fire.
Furthermore, flints used in British muskets were of notoriously poor quality; they could only be fired around six times before requiring resharpening, while American flints could fire sixty.
This led to a common expression among the British: "Yankee flint was as good as a glass of grog".
Provisioning troops and sailors proved to be an immense challenge, as the majority of food stores had to be shipped overseas from Britain.
The need to maintain Loyalist support prevented the Army from living off the land.
Other factors also impeded this option; the countryside was too sparsely populated and the inhabitants were largely hostile or indifferent, the network of roads and bridges was poorly developed, and the area which the British controlled was so limited that foraging parties were frequently in danger of being ambushed.
After France entered the war, the threat of the French navy increased the difficulty of transporting supplies to America.
Food supplies were frequently in bad condition.
The climate was also against the British in the southern colonies and the Caribbean, where the intense summer heat caused food supplies to sour and spoil.
Life at sea was little better.
Sailors and passengers were issued a daily food ration, largely consisting of hardtack and beer.
The hardtack was often infested by weevils and was so tough that it earned the nicknames "molar breakers" and "worm castles", and it sometimes had to be broken up with cannon shot.
Meat supplies often spoiled on long voyages.
The lack of fresh fruit and vegetables gave rise to scurvy, one of the biggest killers at sea.
Discipline was harsh in the armed forces, and the lash was used to punish even trivial offences—and not used sparingly.
For instance, two redcoats received 1,000 lashes each for robbery during the Saratoga campaign, while another received 800 lashes for striking a superior officer.
Flogging was a common punishment in the Royal Navy and came to be associated with the stereotypical hardiness of sailors.
Despite the harsh discipline, a distinct lack of self-discipline pervaded all ranks of the British forces.
Soldiers had an intense passion for gambling, reaching such excesses that troops would often wager their own uniforms.
Many drank heavily, and this was not exclusive to the lower ranks; William Howe was said to have seen many "crapulous mornings" while campaigning in New York.
John Burgoyne drank heavily on a nightly basis towards the end of the Saratoga campaign.
The two generals were also reported to have found solace with the wives of subordinate officers to ease the stressful burdens of command.
During the Philadelphia campaign, British officers deeply offended local Quakers by entertaining their mistresses in the houses where they had been quartered.
Some reports indicated that British troops were generally scrupulous in their treatment of non-combatants.
This is in contrast to diaries of Hessian soldiers, who recorded their disapproval of British conduct towards the colonists, such as the destruction of property and the execution of prisoners.
The presence of Hessian soldiers caused considerable anxiety among the colonists, both Patriot and Loyalist, who viewed them as brutal mercenaries.
British soldiers were often contemptuous in their treatment of Hessian troops, despite orders from General Howe that "the English should treat the Germans as brothers".
The order only began to have any real effect when the Hessians learned to speak a minimal degree of English, which was seen as a prerequisite for the British troops to accord them any respect.
During peacetime, the Army's idleness led to it being riddled with corruption and inefficiency, resulting in many administrative difficulties once campaigning began.
The British leadership soon discovered it had overestimated the capabilities of its own troops, while underestimating those of the colonists, causing a sudden re-think in British planning.
The ineffective initial response of British military and civil officials to the onset of the rebellion had allowed the advantage to shift to the colonists, as British authorities rapidly lost control over every colony.
A microcosm of these shortcomings were evident at the Battle of Bunker Hill.
It took ten hours for the British leadership to respond following the sighting of the Americans on the Charlestown Peninsula, giving the colonists ample time to reinforce their defenses.
Rather than opt for a simple flanking attack that would have rapidly succeeded with minimal loss, the British decided on repeated frontal attacks.
The results were telling; the British suffered 1,054 casualties of a force of around 3,000 after repeated frontal assaults.
The British leadership had nevertheless remained excessively optimistic, believing that just two regiments could suppress the rebellion in Massachusetts.
Debate persists over whether a British defeat was a guaranteed outcome.
Ferling argues that the odds were so long, the defeat of Britain was nothing short of a miracle.
Ellis, however, considers that the odds always favored the Americans, and questions whether a British victory by any margin was realistic.
Ellis argues that the British squandered their only opportunities for a decisive success in 1777, and that the strategic decisions undertaken by William Howe underestimated the challenges posed by the Americans.
Ellis concludes that, once Howe failed, the opportunity for a British victory "would never come again".
Conversely, the United States Army's official textbook argues that, had Britain been able to commit 10,000 fresh troops to the war in 1780, a British victory was within the realms of possibility.
Historians such as Ellis and Stewart have observed that, under William Howe's command, the British squandered several opportunities to achieve a decisive victory over the Americans.
Throughout the New York and Philadelphia campaigns, Howe made several strategic errors, errors which cost the British opportunities for a complete victory.
At Long Island, Howe failed to even attempt an encirclement of Washington, and actively restrained his subordinates from mounting an aggressive pursuit of the defeated American army.
At White Plains, he refused to engage Washington's vulnerable army, and instead concentrated his efforts upon a hill which offered the British no strategic advantage.
After securing control of New York, Howe dispatched Henry Clinton to capture Newport, a measure which Clinton was opposed to, on the grounds the troops assigned to his command could have been put to better use in pursuing Washington's retreating army.
Despite the bleak outlook for the revolutionary cause and the surge of Loyalist activity in the wake of Washington's defeats, Howe made no attempt to mount an attack upon Washington while the Americans settled down into winter quarters, much to their surprise.
During planning for the Saratoga campaign, Howe was left with the choice of committing his army to support Burgoyne, or capture Philadelphia, the revolutionary capital.
Howe decided upon the latter, determining that Washington was of a greater threat.
When Howe launched his campaign, he took his army upon a time-consuming route through the Chesapeake Bay, rather than the more sensible choices of overland through New Jersey, or by sea through the Delaware Bay.
The move left him unable to assist Burgoyne even if it was required of him.
The decision so angered Parliament, that Howe was accused by Tories on both sides of the Atlantic of treason.
During the Philadelphia campaign, Howe failed to pursue and destroy the defeated Americans on two occasions; once after the Battle of Brandywine, and again after the Battle of Germantown.
At the Battle of White Marsh, Howe failed to even attempt to exploit the vulnerable American rear, and then inexplicably ordered a retreat to Philadelphia after only minor skirmishes, astonishing both sides.
While the Americans wintered only twenty miles away, Howe made no effort to attack their camp, which critics argue could have ended the war.
Following the conclusion of the campaign, Howe resigned his commission, and was replaced by Henry Clinton on May 24, 1778.
Contrary to Howe's more hostile critics, however, there were strategic factors at play which impeded aggressive action.
Howe may have been dissuaded from pursuing aggressive manoeuvres due to the memory of the grievous losses the British suffered at Bunker Hill.
During the major campaigns in New York and Philadelphia, Howe often wrote of the scarcity of adequate provisions, which hampered his ability to mount effective campaigns.
Howe's tardiness in launching the New York campaign, and his reluctance to allow Cornwallis to vigorously pursue Washington's beaten army, have both been attributed to the paucity of available food supplies.
During the winter of 1776–1777, Howe split his army into scattered cantonments.
This decision dangerously exposed the individual forces to defeat in detail, as the distance between them was such that they could not mutually support each other.
This strategic failure allowed the Americans to achieve victory at the Battle of Trenton, and the concurrent Battle of Princeton.
While a major strategic error to divide an army in such a manner, the quantity of available food supplies in New York was so low that Howe had been compelled to take such a decision.
The garrisons were widely spaced so their respective foraging parties would not interfere with each other's efforts.
Howe's difficulties during the Philadelphia campaign were also greatly exacerbated by the poor quality and quantity of available provisions.
In 1780, the primary British strategy hinged upon a Loyalist uprising in the south, for which Charles Cornwallis was chiefly responsible.
After an encouraging success at Camden, Cornwallis was poised to invade North Carolina.
However, any significant Loyalist support had been effectively destroyed at the Battle of Kings Mountain, and the British Legion, the cream of his army, had been decisively defeated at the Battle of Cowpens.
Following both defeats, Cornwallis was fiercely criticized for detaching a significant portion of his army without adequate mutual support.
Despite the defeats, Cornwallis chose to proceed into North Carolina, gambling his success upon a large Loyalist uprising which never materialized.
As a result, subsequent engagements cost Cornwallis valuable troops he could not replace, as at the Battle of Guilford Courthouse, and the Americans steadily wore his army down in an exhaustive war of attrition.
Cornwallis had thus left the Carolinas ripe for reconquest.
The Americans had largely achieved this aim by the end of 1781, effectively confining the British to the coast, and undoing all the progress they had made in the previous year.
In a last-ditch attempt to win the war in the South, Cornwallis resolved to invade Virginia, in order to cut off the American's supply base to the Carolinas.
Henry Clinton, Cornwallis' superior, strongly opposed the plan, believing the decisive confrontations would take place between Washington in the North.
London had approved Cornwallis plan, however they had failed to include Clinton in the decision-making, despite his seniority over Cornwallis, leading to a muddled strategic direction.
Cornwallis then decided to invade Virginia without informing Clinton of his intentions.
Clinton, however, had wholly failed to construct a coherent strategy for British campaigning that year, owing to his fractious relationship that he shared with Mariot Arbuthnot, his naval counterpart.
As the Franco-American army approached Cornwallis at Yorktown, he made no attempt to sally out and engage before siege lines could be erected, despite the repeated urging of his subordinate officers.
Expecting relief to soon arrive from Clinton, Cornwallis prematurely abandoned all of his outer defences, which were then promptly occupied by the besiegers, serving to hasten the British defeat.
These factors contributed to the eventual surrender of Cornwallis' entire army, and the end of major operations in North America.
Like Howe before him, Clinton's efforts to campaign suffered from chronic supply issues.
In 1778, Clinton wrote to Germain complaining of the lack of supplies, even after the arrival of a convoy from Ireland.
That winter, the supply issue had deteriorated so badly, that Clinton expressed considerable anxiety over how the troops were going to be properly fed.
Clinton was largely inactive in the North throughout 1779, launching few major campaigns.
This inactivity was partially due to the shortage of food.
By 1780, the situation had not improved.
Clinton wrote a frustrated correspondence to Germain, voicing concern that a "fatal consequence will ensue" if matters did not improve.
By October that year, Clinton again wrote to Germain, angered that the troops in New York had not received "an ounce" of that year's allotted stores from Britain.
Suppressing a rebellion in America presented the British with major problems.
The key issue was distance; it could take up to three months to cross the Atlantic, and orders from London were often outdated by the time that they arrived.
The colonies had never been formally united prior to the conflict and there was no centralized area of ultimate strategic importance.
Traditionally, the fall of a capital city often signalled the end of a conflict, yet the war continued unabated even after the fall of major settlements such as New York, Philadelphia (which was the Patriot capital), and Charleston.
Britain's ability to project its power overseas lay chiefly in the power of the Royal Navy, allowing her to control major coastal settlements with relative ease and enforce a strong blockade of colonial ports.
However, the overwhelming majority of the American population was agrarian, not urban.
As a result, the American economy proved resilient enough to withstand the blockade's effects.
The need to maintain Loyalist support prevented the British from using the harsh methods of suppressing revolts that they had used in Scotland and Ireland.
For example, British troops looted and pillaged the locals during an aborted attack on Charleston in 1779, enraging both Patriots and Loyalists.
Neutral colonists were often driven into the ranks of the Patriots when brutal combat broke out between Tories and Whigs across the Carolinas in the later stages of the war.
Conversely, Loyalists were often emboldened when Patriots resorted to intimidating suspected Tories, such as destroying property or tarring and feathering.
The vastness of the American countryside and the limited manpower available meant that the British could never simultaneously defeat the Americans and occupy captured territory.
One British statesman described the attempt as "like trying to conquer a map".
Wealthy Loyalists wielded great influence in London and were successful in convincing the British that the majority view in the colonies was sympathetic toward the Crown.
Consequently, British planners pinned the success of their strategies on popular uprisings of Loyalists.
Historians have estimated that Loyalists made up only 15–20% of the population (vs. 40–45% Patriots) and that they continued to deceive themselves on their level of support as late as 1780.
The British discovered that any significant level of organized Loyalist activity would require the continued presence of British regulars, which presented them with a major dilemma.
The manpower that the British had available was insufficient to both protect Loyalist territory and counter American advances.
The vulnerability of Loyalist militias was repeatedly demonstrated in the South, where they suffered strings of defeats to their Patriot neighbors.
The most crucial juncture of this was at Kings Mountain, and the victory of the Patriot partisans irreversibly crippled Loyalist military capability in the South.
Upon the entry of France and Spain into the conflict, the British were forced to severely limit the number of troops and warships that they sent to North America in order to defend other key territories and the British mainland.
As a result, King George III abandoned any hope of subduing America militarily while he had a European war to contend with.
The small size of Britain's army left them unable to concentrate their resources primarily in one theater as they had done in the Seven Years' War, leaving them at a critical disadvantage.
The British were compelled to disperse troops from the Americas to Europe and the East Indies, and these forces were unable to assist one other as a result, precariously exposing them to defeat.
In North America, the immediate strategic focus of the French, Spanish, and British shifted to Jamaica, whose sugar exports were more valuable to the British than the economy of the Thirteen Colonies combined.
Following the end of the war, Britain had lost some of her most populous colonies.
However, the economic effects of the loss were negligible in the long-term, and she became a global superpower just 32 years after the end of the conflict.
The Americans began the war with significant disadvantages compared to the British.
They had no national government, no national army or navy, no financial system, no banks, no established credit, and no functioning government departments, such as a treasury.
The Congress tried to handle administrative affairs through legislative committees, which proved inefficient.
The state governments were themselves brand new and officials had no administrative experience.
In peacetime the colonies relied heavily on ocean travel and shipping, but that was now shut down by the British blockade and the Americans had to rely on slow overland travel.
However, the Americans had multiple advantages that in the long run outweighed the initial disadvantages they faced.
The Americans had a large prosperous population that depended not on imports but on local production for food and most supplies, while the British were mostly shipped in from across the ocean.
The British faced a vast territory far larger than Britain or France, located at a far distance from home ports.
Most of the Americans lived on farms distant from the seaports—the British could capture any port but that did not give them control over the hinterland.
They were on their home ground, had a smoothly functioning, well organized system of local and state governments, newspapers and printers, and internal lines of communications.
They had a long-established system of local militia, previously used to combat the French and Native Americans, with companies and an officer corps that could form the basis of local militias, and provide a training ground for the national army created by Congress.
Motivation was a major asset.
The Patriots wanted to win; over 200,000 fought in the war; 25,000 died.
The British expected the Loyalists to do much of the fighting, but they did much less than expected.
The British also hired German mercenaries to do much of their fighting.
At the onset of the war, the Americans had no major international allies.
Battles such as the Battle of Bennington, the Battles of Saratoga and even defeats such as the Battle of Germantown proved decisive in gaining the attention and support of powerful European nations such as France and Spain, who moved from covertly supplying the Americans with weapons and supplies, to overtly supporting them militarily, moving the war to a global stage.
The new Continental Army suffered significantly from a lack of an effective training regime, and largely inexperienced officers and sergeants.
The inexperience of its officers was compensated for in part by a few senior officers.
The Americans solved their training dilemma during their stint in Winter Quarters at Valley Forge, where they were relentlessly drilled and trained by General Friedrich Wilhelm von Steuben, a veteran of the famed Prussian General Staff.
He taught the Continental Army the essentials of military discipline, drills, tactics and strategy, and wrote the Revolutionary War Drill Manual.
When the Army emerged from Valley Forge, it proved its ability to equally match the British troops in battle when they fought a successful strategic action at the Battle of Monmouth.
When the war began, the 13 colonies lacked a professional army or navy.
Each colony sponsored local militia.
Militiamen were lightly armed, had little training, and usually did not have uniforms.
Their units served for only a few weeks or months at a time, were reluctant to travel far from home and thus were unavailable for extended operations, and lacked the training and discipline of soldiers with more experience.
If properly used, however, their numbers could help the Continental armies overwhelm smaller British forces, as at the battles of Concord, Bennington and Saratoga, and the siege of Boston.
Both sides used partisan warfare but the Americans effectively suppressed Loyalist activity when British regulars were not in the area.
Seeking to coordinate military efforts, the Continental Congress established a regular army on June 14, 1775, and appointed George Washington as commander-in-chief.
The development of the Continental Army was always a work in progress, and Washington used both his regulars and state militia throughout the war.
Three current branches of the United States Military trace their institutional roots to the American Revolutionary War; the United States Army comes from the Continental Army, formed by a resolution of the Continental Congress on June 14, 1775.
The United States Navy recognizes October 13, 1775 as the date of its official establishment, the passage of the resolution of the Continental Congress at Philadelphia that created the Continental Navy.
The United States Marine Corps links to the Continental Marines of the war, formed by a resolution of the Continental Congress on November 10, 1775.
However, in 1783 both the Continental Navy and Continental Marines were disbanded.
At the beginning of 1776, Washington commanded 20,000 men, with two-thirds enlisted in the Continental Army and the other third in the various state militias.
About 250,000 men served as regulars or as militiamen for the Revolutionary cause in the eight years of the war, but there were never more than 90,000 men under arms at one time.
About 55,000 sailors served aboard American privateers during the war.
They used 1,700 ships, and they captured 2,283 enemy ships.
John Paul Jones became the first great American naval hero, capturing HMS "Drake" on April 24, 1778, the first victory for any American military vessel in British waters.
Armies were small by European standards of the era, largely attributable, on the American side, to limitations such as lack of powder and other logistical capabilities; and, on the British side, to the difficulty of transporting troops across the Atlantic, as well as the dependence on local supplies, which the Patriots tried to cut off.
The largest force Washington commanded was certainly under 17,000, and may have been no more than 13,000 troops, and even the combined American and French forces at the siege of Yorktown amounted to only about 19,000.
By comparison, Duffy notes that in an era when European rulers were generally revising their forces downward, in favor of a size that could be most effectively controlled (the very different perspective of mass conscript armies came later, during the French Revolutionary and then the Napoleonic Wars), the largest army that Frederick the Great ever led into battle was 65,000 men (at Prague in 1757), and at other times he commanded between 23,000 and 50,000 men, considering the latter the most effective number.
General Washington assumed main five main roles during the war.
First, he designed the overall strategy of the war, in cooperation with Congress.
The goal was always independence.
When France entered the war, he worked closely with the soldiers it sent – they were decisive in the great victory at Yorktown in 1781.
Second, he provided leadership of troops against the main British forces in 1775–77 and again in 1781.
He lost many of his battles, but he never surrendered his army during the war, and he continued to fight the British relentlessly until the war's end.
Washington worked hard to develop a successful espionage system to detect British locations and plans.
In 1778, he formed the Culper Ring to spy on enemy movements in New York City.
In 1780 it discovered Benedict Arnold was a traitor.
The British put a low value on intelligence, and its operations were of poor quality until 1780, when it finally inserted some spies with Congress and with Washington's command.
Even then, however, British commanders ignored or downplayed threats that were revealed.
The most serious intelligence failure came in 1781 when top commanders were unaware that The American and French armies at both left the Northeast and marched down to Yorktown, where they outnumbered Cornwallis by more than 2 to 1.
Third, he was charged selecting and guiding the generals.
In June 1776, Congress made its first attempt at running the war effort with the committee known as "Board of War and Ordnance", succeeded by the Board of War in July 1777, a committee which eventually included members of the military.
The command structure of the armed forces was a hodgepodge of Congressional appointees (and Congress sometimes made those appointments without Washington's input) with state-appointments filling the lower ranks.
The results of his general staff were mixed, as some of his favorites never mastered the art of command, such as John Sullivan.
Eventually, he found capable officers such as Nathanael Greene, Daniel Morgan, Henry Knox (chief of artillery), and Alexander Hamilton (chief of staff).
The American officers never equaled their opponents in tactics and maneuver, and they lost most of the pitched battles.
The great successes at Boston (1776), Saratoga (1777), and Yorktown (1781) came from trapping the British far from base with much larger numbers of troops.
Fourth he took charge of training the army and providing supplies, from food to gunpowder to tents.
He recruited regulars and assigned Baron Friedrich Wilhelm von Steuben, a veteran of the Prussian general staff, to train them.
He transformed Washington's army into a disciplined and effective force.
The war effort and getting supplies to the troops were under the purview of Congress, but Washington pressured the Congress to provide the essentials.
There was never nearly enough.
Washington's fifth and most important role in the war effort was the embodiment of armed resistance to the Crown, serving as the representative man of the Revolution.
His long-term strategy was to maintain an army in the field at all times, and eventually this strategy worked.
His enormous personal and political stature and his political skills kept Congress, the army, the French, the militias, and the states all pointed toward a common goal.
Furthermore, he permanently established the principle of civilian supremacy in military affairs by voluntarily resigning his commission and disbanding his army when the war was won, rather than declaring himself monarch.
He also helped to overcome the distrust of a standing army by his constant reiteration that well-disciplined professional soldiers counted for twice as much as poorly trained and led militias.
African Americans—slave and free—served on both sides during the war.
The British recruited slaves belonging to Patriot masters and promised freedom to those who served by act of Lord Dunmore's Proclamation.
Because of manpower shortages, George Washington lifted the ban on black enlistment in the Continental Army in January 1776.
Small all-black units were formed in Rhode Island and Massachusetts; many slaves were promised freedom for serving.
Some of the men promised freedom were sent back to their masters, after the war was over, out of political convenience.
Another all-black unit came from Saint-Domingue with French colonial forces.
At least 5,000 black soldiers fought for the Revolutionary cause.
Tens of thousands of slaves escaped during the war and joined British lines; others simply moved off in the chaos.
For instance, in South Carolina, nearly 25,000 slaves (30% of the enslaved population) fled, migrated or died during the disruption of the war.
This greatly disrupted plantation production during and after the war.
When they withdrew their forces from Savannah and Charleston, the British also evacuated 10,000 slaves belonging to Loyalists.
Altogether, the British evacuated nearly 20,000 blacks at the end of the war.
More than 3,000 of them were freedmen and most of these were resettled in Nova Scotia; other blacks were sold in the West Indies.
Most American Indians east of the Mississippi River were affected by the war, and many tribes were divided over the question of how to respond to the conflict.
A few tribes were on friendly terms with the other Americans, but most Indians opposed the union of the Colonies as a potential threat to their territory.
Approximately 13,000 Indians fought on the British side, with the largest group coming from the Iroquois tribes, who fielded around 1,500 men.
The powerful Iroquois Confederacy was shattered as a result of the conflict, whatever side they took; the Seneca, Onondaga, and Cayuga nations sided with the British.
Members of the Mohawk nation fought on both sides.
Many Tuscarora and Oneida sided with the colonists.
The Continental Army sent the Sullivan Expedition on raids throughout New York to cripple the Iroquois tribes that had sided with the British.
Mohawk leaders Joseph Louis Cook and Joseph Brant sided with the Americans and the British respectively, and this further exacerbated the split.
Early in July 1776, a major action occurred in the fledgling conflict when the Cherokee allies of Britain attacked the western frontier areas of North Carolina.
Their defeat resulted in a splintering of the Cherokee settlements and people, and was directly responsible for the rise of the Chickamauga Cherokee, bitter enemies of the Colonials who carried on a frontier war for decades following the end of hostilities with Britain.
Creek and Seminole allies of Britain fought against Americans in Georgia and South Carolina.
In 1778, a force of 800 Creeks destroyed American settlements along the Broad River in Georgia.
Creek warriors also joined Thomas Brown's raids into South Carolina and assisted Britain during the Siege of Savannah.
Many Indians were involved in the fighting between Britain and Spain on the Gulf Coast and up the Mississippi River—mostly on the British side.
Thousands of Creeks, Chickasaws, and Choctaws fought in major battles such as the Battle of Fort Charlotte, the Battle of Mobile, and the Siege of Pensacola.
Pybus (2005) estimates that about 20,000 slaves defected to or were captured by the British, of whom about 8,000 died from disease or wounds or were recaptured by the Patriots.
The British took some 12,000 at the end of the war; of these 8000 remained in slavery.
Including those who left during the war, a total of about 8000 to 10,000 slaves gained freedom.
About 4000 freed slaves went to Nova Scotia and 1200 blacks remained slaves.
Baller (2006) examines family dynamics and mobilization for the Revolution in central Massachusetts.
He reports that warfare and the farming culture were sometimes incompatible.
Militiamen found that living and working on the family farm had not prepared them for wartime marches and the rigors of camp life.
Rugged individualism conflicted with military discipline and regimentation.
A man's birth order often influenced his military recruitment, as younger sons went to war and older sons took charge of the farm.
A person's family responsibilities and the prevalent patriarchy could impede mobilization.
Harvesting duties and family emergencies pulled men home regardless of the sergeant's orders.
Some relatives might be Loyalists, creating internal strains.
On the whole, historians conclude the Revolution's effect on patriarchy and inheritance patterns favored egalitarianism.
McDonnell (2006) shows a grave complication in Virginia's mobilization of troops was the conflicting interests of distinct social classes, which tended to undercut a unified commitment to the Patriot cause.
The Assembly balanced the competing demands of elite slave-owning planters, the middling yeomen (some owning a few slaves), and landless indentured servants, among other groups.
The Assembly used deferments, taxes, military service substitute, and conscription to resolve the tensions.
Unresolved class conflict, however, made these laws less effective.
There were violent protests, many cases of evasion, and large-scale desertion, so that Virginia's contributions came at embarrassingly low levels.
With the British invasion of the state in 1781, Virginia was mired in class division as its native son, George Washington, made desperate appeals for troops.
These are some of the standard works about the war in general that are not listed above; books about specific campaigns, battles, units, and individuals can be found in those articles.
</doc>
<doc id="772" url="https://en.wikipedia.org/wiki?curid=772" title="Ampere">
Ampere

The ampere (; symbol: A), often shortened to "amp", is the base unit of electric current in the International System of Units (SI).
It is named after André-Marie Ampère (1775–1836), French mathematician and physicist, considered the father of electrodynamics.
The International System of Units defines the ampere in terms of other base units by measuring the electromagnetic force between electrical conductors carrying electric current.
The earlier CGS measurement system had two different definitions of current, one essentially the same as the SI's and the other using electric charge as the base unit, with the unit of charge defined by measuring the force between two charged metal plates.
The ampere was then defined as one coulomb of charge per second.
In SI, the unit of charge, the coulomb, is defined as the charge carried by one ampere during one second.
New definitions, in terms of invariant constants of nature, specifically the elementary charge, will take effect on 20 May 2019.
SI defines ampere as follows:

The ampere is that constant current which, if maintained in two straight parallel conductors of infinite length, of negligible circular cross-section, and placed one metre apart in vacuum, would produce between these conductors a force equal to newtons per metre of length.
Ampère's force law states that there is an attractive or repulsive force between two parallel wires carrying an electric current.
This force is used in the formal definition of the ampere.
The SI unit of charge, the coulomb, "is the quantity of electricity carried in 1 second by a current of 1 ampere".
Conversely, a current of one ampere is one coulomb of charge going past a given point per second:
In general, charge "Q" is determined by steady current "I" flowing for a time "t" as .
Constant, instantaneous and average current are expressed in amperes (as in "the charging current is 1.2 A") and the charge accumulated, or passed through a circuit over a period of time is expressed in coulombs (as in "the battery charge is ").
The relation of the ampere (C/s) to the coulomb is the same as that of the watt (J/s) to the joule.
The ampere was originally defined as one tenth of the unit of electric current in the centimetre–gram–second system of units.
That unit, now known as the abampere, was defined as the amount of current that generates a force of two dynes per centimetre of length between two wires one centimetre apart.
The size of the unit was chosen so that the units derived from it in the MKSA system would be conveniently sized.
The "international ampere" was an early realization of the ampere, defined as the current that would deposit of silver per second from a silver nitrate solution.
Later, more accurate measurements revealed that this current is .
Since power is defined as the product of current and voltage, the ampere can alternatively be expressed in terms of the other units using the relationship I=P/V, and thus 1 ampere equals 1 W/V.
Current can be measured by a multimeter, a device that can measure electrical voltage, current, and resistance.
The standard ampere is most accurately realized using a Kibble balance, but is in practice maintained via Ohm's law from the units of electromotive force and resistance, the volt and the ohm, since the latter two can be tied to physical phenomena that are relatively easy to reproduce, the Josephson junction and the quantum Hall effect, respectively.
At present, techniques to establish the realization of an ampere have a relative uncertainty of approximately a few parts in 10, and involve realizations of the watt, the ohm and the volt.
Rather than a definition in terms of the force between two current-carrying wires, it has been proposed that the ampere should be defined in terms of the rate of flow of elementary charges.
Since a coulomb is approximately equal to elementary charges (such as those carried by protons, or the negative of those carried by electrons), one ampere is approximately equivalent to elementary charges moving past a boundary in one second.
( is the reciprocal of the value of the elementary charge in coulombs.)
The proposed change would define 1 A as being the current in the direction of flow of a particular number of elementary charges per second.
In 2005, the International Committee for Weights and Measures (CIPM) agreed to study the proposed change.
The new definition was discussed at the 25th General Conference on Weights and Measures (CGPM) in 2014 but for the time being was not adopted.
The current drawn by typical constant-voltage energy distribution systems is usually dictated by the power (watt) consumed by the system and the operating voltage.
For this reason the examples given below are grouped by voltage level.
A typical motor vehicle has a 12 V battery.
The various accessories that are powered by the battery might include:

Most Canada, Mexico and United States domestic power suppliers run at 120 V.

Household circuit breakers typically provide a maximum of 15 A or 20 A of current to a given set of outlets.
Most European domestic power supplies run at 230 V, and most Commonwealth domestic power supplies run at 240 V. For the same amount of power (in watts), the current drawn by a particular European or Commonwealth appliance (in Europe or a Commonwealth country) will be less than for an equivalent North American appliance.
Typical circuit breakers will provide 16 A.

The current drawn by a number of typical appliances are:




</doc>
<doc id="775" url="https://en.wikipedia.org/wiki?curid=775" title="Algorithm">
Algorithm

In mathematics and computer science, an algorithm () is an unambiguous specification of how to solve a class of problems.
Algorithms can perform calculation, data processing and automated reasoning tasks.
As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function.
Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing "output" and terminating at a final ending state.
The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.
The concept of algorithm has existed for centuries.
Greek mathematicians used algorithms in, for example, the sieve of Eratosthenes for finding prime numbers and the Euclidean algorithm for finding the greatest common divisor of two numbers.
The word "algorithm" itself derives from the 9th Century mathematician Muḥammad ibn Mūsā al-Khwārizmī, Latinized "Algoritmi".
A partial formalization of what would become the modern concept of algorithm began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert in 1928.
Later formalizations were framed as attempts to define "effective calculability" or "effective method".
Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.
The word 'algorithm' has its roots in Latinizing the name of Muhammad ibn Musa al-Khwarizmi in a first step to "algorismus".
Al-Khwārizmī (, c.
780–850) was a Persian mathematician, astronomer, geographer, and scholar in the House of Wisdom in Baghdad, whose name means 'the native of Khwarezm', a region that was part of Greater Iran and is now in Uzbekistan.
About 825, al-Khwarizmi wrote an Arabic language treatise on the Hindu–Arabic numeral system, which was translated into Latin during the 12th century under the title "Algoritmi de numero Indorum".
This title means "Algoritmi on the numbers of the Indians", where "Algoritmi" was the translator's Latinization of Al-Khwarizmi's name.
Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through another of his books, the Algebra.
In late medieval Latin, "algorismus", English 'algorism', the corruption of his name, simply meant the "decimal number system".
In the 15th century, under the influence of the Greek word ἀριθμός 'number' ("cf."
'arithmetic'), the Latin word was altered to "algorithmus", and the corresponding English term 'algorithm' is first attested in the 17th century; the modern sense was introduced in the 19th century.
In English, it was first used in about 1230 and then by Chaucer in 1391.
English adopted the French term, but it wasn't until the late 19th century that "algorithm" took on the meaning that it has in modern English.
Another early use of the word is from 1240, in a manual titled "Carmen de Algorismo" composed by Alexandre de Villedieu.
It begins thus:
which translates as:
The poem is a few hundred lines long and summarizes the art of calculating with the new style of Indian dice, or Talibus Indorum, or Hindu numerals.
An informal definition could be "a set of rules that precisely defines a sequence of operations."
which would include all computer programs, including programs that do not perform numeric calculations.
Generally, a program is only an algorithm if it stops eventually.
A prototypical example of an algorithm is the Euclidean algorithm to determine the maximum common divisor of two integers; an example (there are others) is described by the flowchart above and as an example in a later section.
No human being can write fast enough, or long enough, or small enough† ( †"smaller and smaller without limit ...you'd be trying to write on molecules, on atoms, on electrons") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation.
But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give "explicit instructions for determining the nth member of the set", for arbitrary finite "n".
Such instructions are to be given quite explicitly, in a form in which "they could be followed by a computing machine", or by a "human who is capable of carrying out only very elementary operations on symbols."
An "enumerably infinite set" is one whose elements can be put into one-to-one correspondence with the integers.
Thus, Boolos and Jeffrey are saying that an algorithm implies instructions for a process that "creates" output integers from an "arbitrary" "input" integer or integers that, in theory, can be arbitrarily large.
Thus an algorithm can be an algebraic equation such as "y = m + n" – two arbitrary "input variables" "m" and "n" that produce an output "y".
But various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):

The concept of "algorithm" is also used to define the notion of decidability.
That notion is central for explaining how formal systems come into being starting from a small set of axioms and rules.
In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to our customary physical dimension.
From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of "algorithm" that suits both concrete (in some sense) and abstract usage of the term.
Algorithms are essential to the way computers process data.
Many computer programs contain algorithms that detail the specific instructions a computer should perform (in a specific order) to carry out a specified task, such as calculating employees' paychecks or printing students' report cards.
Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system.
Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich (2000):

Typically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing.
Stored data are regarded as part of the internal state of the entity performing the algorithm.
In practice, the state is stored in one or more data structures.
For some such computational process, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise.
That is, any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).
Because an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm.
Instructions are usually assumed to be listed explicitly, and are described as starting "from the top" and going "down to the bottom", an idea that is described more formally by "flow of control".
So far, this discussion of the formalization of an algorithm has assumed the premises of imperative programming.
This is the most common conception, and it attempts to describe a task in discrete, "mechanical" means.
Unique to this conception of formalized algorithms is the assignment operation, setting the value of a variable.
It derives from the intuition of "memory" as a scratchpad.
There is an example below of such an assignment.
For some alternate conceptions of what constitutes an algorithm see functional programming and logic programming.
Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters).
Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms.
Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language statements.
Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer but are often used as a way to define or document algorithms.
There is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see more at finite-state machine, state transition table and control table), as flowcharts and drakon-charts (see more at state diagram), or as a form of rudimentary machine code or assembly code called "sets of quadruples" (see more at Turing machine).
Representations of algorithms can be classed into three accepted levels of Turing machine description:

For an example of the simple algorithm "Add m+n" described in all three levels, see Algorithm#Examples.
Algorithm design refers to a method or mathematical process for problem-solving and engineering algorithms.
The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer.
Techniques for designing and implementing algorithm designs are also called algorithm design patterns, such as the template method pattern and decorator pattern.
One of the most important aspects of algorithm design is creating an algorithm that has an efficient run-time, also known as its Big O.

Typical steps in the development of algorithms:

Most algorithms are intended to be implemented as computer programs.
However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.
In computer systems, an algorithm is basically an instance of logic written in software by software developers, to be effective for the intended "target" computer(s) to produce "output" from given (perhaps null) "input".
An optimal algorithm, even running in old hardware, would produce faster results than a non-optimal (higher time complexity) algorithm for the same purpose, running in more efficient hardware; that is why algorithms, like computer hardware, are considered technology.
""Elegant" (compact) programs, "good" (fast) programs ": The notion of "simplicity and elegance" appears informally in Knuth and precisely in Chaitin:

Chaitin prefaces his definition with: "I'll show you can't prove that a program is 'elegant'"—such proof would solve the Halting problem (ibid).
"Algorithm versus function computable by an algorithm": For a given function multiple algorithms may exist.
This is true, even without expanding the available instruction set available to the programmer.
Rogers observes that "It is .
.
.
important to distinguish between the notion of "algorithm", i.e. procedure and the notion of "function computable by algorithm", i.e. mapping yielded by the procedure.
The same function may have several different algorithms".
Unfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)—an elegant program may take more steps to complete a computation than one less elegant.
An example that uses Euclid's algorithm appears below.
"Computers (and computors), models of computation": A computer (or human "computor") is a restricted type of machine, a "discrete deterministic mechanical device" that blindly follows its instructions.
Melzak's and Lambek's primitive models reduced this notion to four elements: (i) discrete, distinguishable "locations", (ii) discrete, indistinguishable "counters" (iii) an agent, and (iv) a list of instructions that are "effective" relative to the capability of the agent.
Minsky describes a more congenial variation of Lambek's "abacus" model in his "Very Simple Bases for Computability".
Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions, unless either a conditional IF–THEN GOTO or an unconditional GOTO changes program flow out of sequence.
Besides HALT, Minsky's machine includes three "assignment" (replacement, substitution) operations: ZERO (e.g.
the contents of location replaced by 0: L ← 0), SUCCESSOR (e.g.
L ← L+1), and DECREMENT (e.g.
L ← L − 1).
Rarely must a programmer write "code" with such a limited instruction set.
But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general "types" of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT.
"Simulation of an algorithm: computer (computor) language": Knuth advises the reader that "the best way to learn an algorithm is to try it .
.
.
immediately take pen and paper and work through an example".
But what about a simulation or execution of the real thing?
The programmer must translate the algorithm into a language that the simulator/computer/computor can "effectively" execute.
Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to take a square root.
If they don't, then the algorithm, to be effective, must provide a set of rules for extracting a square root.
This means that the programmer must know a "language" that is effective relative to the target computing agent (computer/computor).
But what model should be used for the simulation?
Van Emde Boas observes "even if we base complexity theory on abstract instead of concrete machines, arbitrariness of the choice of a model remains.
It is at this point that the notion of "simulation" enters".
When speed is being measured, the instruction set matters.
For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a "modulus" instruction available rather than just subtraction (or worse: just Minsky's "decrement").
"Structured programming, canonical structures": Per the Church–Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT.
Kemeny and Kurtz observe that, while "undisciplined" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in "spaghetti code", a programmer can write structured programs using only these instructions; on the other hand "it is also possible, and not too hard, to write badly structured programs in a structured language".
Tausworthe augments the three Böhm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE.
An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.
"Canonical flowchart symbols": The graphical aide called a flowchart, offers a way to describe and document an algorithm (and a computer program of one).
Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down.
Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie).
The Böhm–Jacopini canonical structures are made of these primitive shapes.
Sub-structures can "nest" in rectangles, but only if a single exit occurs from the superstructure.
The symbols, and their use to build the canonical structures are shown in the diagram.
One of the simplest algorithms is to find the largest number in a list of numbers of random order.
Finding the solution requires looking at every number in the list.
From this follows a simple algorithm, which can be stated in a high-level description of English prose, as:

"High-level description:"

"(Quasi-)formal description:"
Written in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:

Euclid's algorithm to compute the greatest common divisor (GCD) to two numbers appears as Proposition II in Book VII ("Elementary Number Theory") of his "Elements".
Euclid poses the problem thus: "Given two numbers not prime to one another, to find their greatest common measure".
He defines "A number [to be] a multitude composed of units": a counting number, a positive integer not including zero.
To "measure" is to place a shorter measuring length "s" successively ("q" times) along longer length "l" until the remaining portion "r" is less than the shorter length "s".
In modern words, remainder "r" = "l" − "q"×"s", "q" being the quotient, or remainder "r" is the "modulus", the integer-fractional part left over after the division.
For Euclid's method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be “proper”; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (alternately, the two can be equal so their subtraction yields zero).
Euclid's original proof adds a third requirement: the two lengths must not be prime to one another.
Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers' common measure is in fact the "greatest".
While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another, it yields the number "1" for their common measure.
So, to be precise, the following is really Nicomachus' algorithm.
Only a few instruction "types" are required to execute Euclid's algorithm—some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and subtraction.
The following algorithm is framed as Knuth's four-step version of Euclid's and Nicomachus', but, rather than using division to find the remainder, it uses successive subtractions of the shorter length "s" from the remaining length "r" until "r" is less than "s".
The high-level description, shown in boldface, is adapted from Knuth 1973:2–4:

INPUT:

E0: [Ensure "r" ≥ "s".]
E1: [Find remainder]: Until the remaining length "r" in R is less than the shorter length "s" in S, repeatedly subtract the measuring number "s" in S from the remaining length "r" in R.

E2: [Is the remainder zero?
]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.

E3: [Interchange "s" and "r"]: The nut of Euclid's algorithm.
Use remainder "r" to measure what was previously smaller number "s"; L serves as a temporary location.
OUTPUT:

DONE:

The following version of Euclid's algorithm requires only six core instructions to do what thirteen are required to do by "Inelegant"; worse, "Inelegant" requires more "types" of instructions.
The flowchart of "Elegant" can be found at the top of this article.
In the (unstructured) Basic language, the steps are numbered, and the instruction is the assignment instruction symbolized by ←.
The following version can be used with Object Oriented languages:

"How "Elegant" works": In place of an outer "Euclid loop", "Elegant" shifts back and forth between two "co-loops", an A > B loop that computes A ← A − B, and a B ≤ A loop that computes B ← B − A. This works because, when at last the minuend M is less than or equal to the subtrahend S ( Difference = Minuend − Subtrahend), the minuend can become "s" (the new measuring length) and the subtrahend can become the new "r" (the length to be measured); in other words the "sense" of the subtraction reverses.
Does an algorithm do what its author wants it to do?
A few test cases usually suffice to confirm core functionality.
One source uses 3009 and 884.
Knuth suggested 40902, 24140.
Another interesting case is the two relatively prime numbers 14157 and 5950.
But exceptional cases must be identified and tested.
Will "Inelegant" perform properly when R > S, S > R, R = S?
Ditto for "Elegant": B > A, A > B, A = B?
(Yes to all).
What happens when one number is zero, both numbers are zero?
("Inelegant" computes forever in all cases; "Elegant" computes forever when A = 0.)
What happens if "negative" numbers are entered?
Fractional numbers?
If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function.
A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).
"Proof of program correctness by use of mathematical induction": Knuth demonstrates the application of mathematical induction to an "extended" version of Euclid's algorithm, and he proposes "a general method applicable to proving the validity of any algorithm".
Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.
"Elegance (compactness) versus goodness (speed)": With only six core instructions, "Elegant" is the clear winner, compared to "Inelegant" at thirteen instructions.
However, "Inelegant" is "faster" (it arrives at HALT in fewer steps).
Algorithm analysis indicates why this is the case: "Elegant" does "two" conditional tests in every subtraction loop, whereas "Inelegant" only does one.
As the algorithm (usually) requires many loop-throughs, "on average" much time is wasted doing a "B = 0?"
test that is needed only after the remainder is computed.
"Can the algorithms be improved?
": Once the programmer judges a program "fit" and "effective"—that is, it computes the function intended by its author—then the question becomes, can it be improved?
The compactness of "Inelegant" can be improved by the elimination of five steps.
But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm; rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc.
Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13.
Comparison with "Elegant" provides a hint that these steps, together with steps 2 and 3, can be eliminated.
This reduces the number of core instructions from thirteen to eight, which makes it "more elegant" than "Elegant", at nine steps.
The speed of "Elegant" can be improved by moving the "B=0?"
test outside of the two subtraction loops.
This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO).
Now "Elegant" computes the example-numbers faster; whether this is always the case for any given A, B, and R, S would require a detailed analysis.
It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm.
Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, the sorting algorithm above has a time requirement of O("n"), using the big O notation with "n" as the length of the list.
At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input list.
Therefore, it is said to have a space requirement of "O(1)", if the space required to store the input numbers is not counted, or O("n") if it is counted.
Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others.
For example, a binary search algorithm (with cost O(log n) ) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.
The analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation.
In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation.
Usually pseudocode is used for analysis as it is the simplest and most general representation.
However, ultimately, most algorithms are usually implemented on particular hardware/software platforms and their algorithmic efficiency is eventually put to the test using real code.
For the solution of a "one off" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical.
Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.
Empirical testing is useful because it may uncover unexpected interactions that affect performance.
Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.
Empirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.
To illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging.
In general, speed improvements depend on special properties of the problem, which are very common in practical applications.
Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.
There are various ways to classify algorithms, each with its own merits.
One way to classify algorithms is by implementation means.
Another way of classifying algorithms is by their design methodology or paradigm.
There is a certain number of paradigms, each different from the other.
Furthermore, each of these categories includes many different types of algorithms.
Some common paradigms are:


For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:


Every field of science has its own problems and needs efficient algorithms.
Related problems in one field are often studied together.
Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.
Fields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields.
For example, dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields.
Algorithms can be classified by the amount of time they need to complete compared to their input size:


Some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms.
There are also mappings from some problems to other problems.
Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.
The adjective "continuous" when applied to the word "algorithm" can mean:

Algorithms, by themselves, are not usually patentable.
In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute "processes" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson).
However practical applications of algorithms are sometimes patentable.
For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable.
The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW patent.
Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).
Algorithms were used in ancient Greece.
Two examples are the Sieve of Eratosthenes, which was described in Introduction to Arithmetic by Nicomachus, and the Euclidean algorithm, which was first described in Euclid's Elements (c.
300 BC).
Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.
Tally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks or making discrete symbols in clay.
Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p.
16–41).
Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post–Turing machine computations.
The work of the ancient Greek geometers (Euclidean algorithm), the Indian mathematician Brahmagupta, and the Persian mathematician Al-Khwarizmi (from whose name the terms "algorism" and "algorithm" are derived), and Western European mathematicians culminated in Leibniz's notion of the calculus ratiocinator (ca 1680):
"The clock": Bolter credits the invention of the weight-driven clock as "The key invention [of Europe in the Middle Ages]", in particular, the verge escapement that provides us with the tick and tock of a mechanical clock.
"The accurate automatic machine" led immediately to "mechanical automata" beginning in the 13th century and finally to "computational machines"—the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid-19th century.
Lovelace is credited with the first creation of an algorithm intended for processing on a computer – Babbage's analytical engine, the first device considered a real Turing-complete computer instead of just a calculator – and is sometimes called "history's first programmer" as a result, though a full implementation of Babbage's second device would not be realized until decades after her lifetime.
"Logical machines 1870—Stanley Jevons' "logical abacus" and "logical machine"": The technical problem was to reduce Boolean equations when presented in a form similar to what is now known as Karnaugh maps.
Jevons (1880) describes first a simple "abacus" of "slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically .
.
.
More recently, however, I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a "Logical Machine"" His machine came equipped with "certain moveable wooden rods" and "at the foot are 21 keys like those of a piano [etc] .
.
.".
With this machine he could analyze a "syllogism or any other simple logical argument".
This machine he displayed in 1870 before the Fellows of the Royal Society.
Another logician John Venn, however, in his 1881 "Symbolic Logic", turned a jaundiced eye to this effort: "I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines"; see more at Algorithm characterizations.
But not to be outdone he too presented "a plan somewhat analogous, I apprehend, to Prof. Jevon's "abacus" ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described.
I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine".
"Jacquard loom, Hollerith punch cards, telegraphy and telephony—the electromechanical relay": Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and "telephone switching technologies" were the roots of a tree leading to the development of the first computers.
By the mid-19th century the telegraph, the precursor of the telephone, was in use throughout the world, it's discrete and distinguishable encoding of letters as "dots and dashes" a common sound.
By the late 19th century the ticker tape (ca 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S.
census.
Then came the teleprinter (ca.
1910) with its punched-paper use of Baudot code on tape.
"Telephone-switching networks" of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device.
As he worked in Bell Laboratories, he observed the "burdensome' use of mechanical calculators with gears.
"He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device".
Davis (2000) observes the particular importance of the electromechanical relay (with its two "binary states" "open" and "closed"):

"Symbols and rules": In rapid succession, the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888–1889) reduced arithmetic to a sequence of symbols manipulated by rules.
Peano's "The principles of arithmetic, presented by a new method" (1888) was "the first attempt at an axiomatization of mathematics in a symbolic language".
But Heijenoort gives Frege (1879) this kudos: Frege's is "perhaps the most important single work ever written in logic.
... in which we see a " 'formula language', that is a "lingua characterica", a language written with special symbols, "for pure thought", that is, free from rhetorical embellishments ... constructed from specific symbols that are manipulated according to definite rules".
The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910–1913).
"The paradoxes": At the same time a number of disturbing paradoxes appeared in the literature, in particular, the Burali-Forti paradox (1897), the Russell paradox (1902–03), and the Richard Paradox.
The resultant considerations led to Kurt Gödel's paper (1931)—he specifically cites the paradox of the liar—that completely reduces rules of recursion to numbers.
"Effective calculability": In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an "effective method" or "effective calculation" or "effective calculability" (i.e., a calculation that would succeed).
In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J. B. Rosser's λ-calculus a finely honed definition of "general recursion" from the work of Gödel acting on suggestions of Jacques Herbrand (cf.
Gödel's Princeton lectures of 1934) and subsequent simplifications by Kleene.
Church's proof that the Entscheidungsproblem was unsolvable, Emil Post's definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction.
Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his "a- [automatic-] machine"—in effect almost identical to Post's "formulation", J. Barkley Rosser's definition of "effective method" in terms of "a machine".
S. C. Kleene's proposal of a precursor to "Church thesis" that he called "Thesis I", and a few years later Kleene's renaming his Thesis "Church's Thesis" and proposing "Turing's Thesis".
Here is a remarkable coincidence of two men not knowing each other but describing a process of men-as-computers working on computations—and they yield virtually identical definitions.
Emil Post (1936) described the actions of a "computer" (human being) as follows:

His symbol space would be

Alan Turing's work preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing.
Turing's biographer believed that Turing's use of a typewriter-like model derived from a youthful interest: "Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter, and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical'".
Given the prevalence of Morse code and telegraphy, ticker tape machines, and teletypewriters we might conjecture that all were influences.
Turing—his model of computation is now called a Turing machine—begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and "states of mind".
But he continues a step further and creates a machine as a model of computation of numbers.
Turing's reduction yields the following:
"It may be that some of these change necessarily invoke a change of state of mind.
The most general single operation must, therefore, be taken to be one of the following:

A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:

J. Barkley Rosser defined an 'effective [mathematical] method' in the following manner (italicization added):

Rosser's footnote No.
5 references the work of (1) Church and Kleene and their definition of λ-definability, in particular Church's use of it in his "An Unsolvable Problem of Elementary Number Theory" (1936); (2) Herbrand and Gödel and their use of recursion in particular Gödel's use in his famous paper "On Formally Undecidable Propositions of Principia Mathematica and Related Systems I" (1931); and (3) Post (1936) and Turing (1936–37) in their mechanism-models of computation.
Stephen C. Kleene defined as his now-famous "Thesis I" known as the Church–Turing thesis.
But he did this in the following context (boldface in original):

A number of efforts have been directed toward further refinement of the definition of "algorithm", and activity is on-going because of issues surrounding, in particular, foundations of mathematics (especially the Church–Turing thesis) and philosophy of mind (especially arguments about artificial intelligence).
For more, see Algorithm characterizations.
</doc>
<doc id="777" url="https://en.wikipedia.org/wiki?curid=777" title="Annual plant">
Annual plant

An annual plant is a plant that completes its life cycle, from germination to the production of seeds, within one year, and then dies.
Summer annuals germinate during spring or early summer and mature by autumn of the same year.
Winter annuals germinate during the autumn and mature during the spring or summer of the following calendar year.
One seed-to-seed life cycle for an annual can occur in as little as a month in some species, though most last several months.
Oilseed rapa can go from seed-to-seed in about five weeks under a bank of fluorescent lamps.
This style of growing is often used in classrooms for education.
Many desert annuals are therophytes, because their seed-to-seed life cycle is only weeks and they spend most of the year as seeds to survive dry conditions.
In cultivation, many food plants are, or are grown as, annuals, including virtually all domesticated grains.
Some perennials and biennials are grown in gardens as annuals for convenience, particularly if they are not considered cold hardy for the local climate.
Carrot, celery and parsley are true biennials that are usually grown as annual crops for their edible roots, petioles and leaves, respectively.
Tomato, sweet potato and bell pepper are tender perennials usually grown as annuals.
Ornamental perennials commonly grown as annuals are impatiens, mirabilis, wax begonia, snapdragon, "pelargonium", coleus and petunia.
Examples of true annuals include corn, wheat, rice, lettuce, peas, watermelon, beans, zinnia and marigold.
Summer annuals sprout, flower, produce seed, and die, during the warmer months of the year.
The lawn weed crabgrass is a summer annual.
Winter annuals germinate in autumn or winter, live through the winter, then bloom in winter or spring.
The plants grow and bloom during the cool season when most other plants are dormant or other annuals are in seed form waiting for warmer weather to germinate.
Winter annuals die after flowering and setting seed.
The seeds germinate in the autumn or winter when the soil temperature is cool.
Winter annuals typically grow low to the ground, where they are usually sheltered from the coldest nights by snow cover, and make use of warm periods in winter for growth when the snow melts.
Some common winter annuals include henbit, deadnettle, chickweed, and winter cress.
Winter annuals are important ecologically, as they provide vegetative cover that prevents soil erosion during winter and early spring when no other cover exists and they provide fresh vegetation for animals and birds that feed on them.
Although they are often considered to be weeds in gardens, this viewpoint is not always necessary, as most of them die when the soil temperature warms up again in early to late spring when other plants are still dormant and have not yet leafed out.
Even though they do not compete directly with cultivated plants, sometimes winter annuals are considered a pest in commercial agriculture, because they can be hosts for insect pests or fungal diseases (ovary smut – Microbotryum sp) which attack crops being cultivated.
The property that they prevent the soil from drying out can also be problematic for commercial agriculture.
In 2008, it was discovered that the inactivation of only two genes in one species of annual plant leads to the conversion into a perennial plant.
Researchers deactivated the SOC1 and FUL genes in "Arabidopsis thaliana", which control flowering time.
This switch established phenotypes common in perennial plants, such as wood formation.
</doc>
<doc id="779" url="https://en.wikipedia.org/wiki?curid=779" title="Anthophyta">
Anthophyta

The anthophytes were thought to be a clade comprising plants bearing flower-like structures.
The group contained the angiosperms - the extant flowering plants, such as roses and grasses - as well as the Gnetales and the extinct Bennettitales.
Detailed morphological and molecular studies have shown that the group is not actually monophyletic, with proposed floral homologies of the gnetophytes and the angiosperms having evolved in parallel.
This makes it easier to reconcile molecular clock data that suggests that the angiosperms diverged from the gymnosperms around .
Some more recent studies have used the word anthophyte to describe a group which includes the angiosperms and a variety of fossils (glossopterids, "Pentoxylon", Bennettitales, and "Caytonia"), but not the Gnetales.
</doc>
<doc id="780" url="https://en.wikipedia.org/wiki?curid=780" title="Atlas (disambiguation)">
Atlas (disambiguation)

An atlas is a collection of maps, originally named after the Ancient Greek deity.
Atlas may also refer to:

























</doc>
<doc id="782" url="https://en.wikipedia.org/wiki?curid=782" title="Mouthwash">
Mouthwash

Mouthwash, mouth rinse, oral rinse, or mouth bath is a liquid which is held in the mouth passively or swilled around the mouth by contraction of the perioral muscles and/or movement of the head, and may be gargled, where the head is tilted back and the liquid bubbled at the back of the mouth.
Usually mouthwashes are antiseptic solutions intended to reduce the microbial load in the oral cavity, although other mouthwashes might be given for other reasons such as for their analgesic, anti-inflammatory or anti-fungal action.
Additionally, some rinses act as saliva substitutes to neutralize acid and keep the mouth moist in xerostomia (dry mouth).
Cosmetic mouthrinses temporarily control or reduce bad breath and leave the mouth with a pleasant taste.
Rinsing with water or mouthwash after brushing with a fluoride toothpaste can reduce the availability of salivary fluoride.
This can lower the anti-cavity re-mineralization and antibacterial effects of fluoride.
Fluoridated mouthwash may mitigate this effect or in high concentrations increase available fluoride.
A group of experts discussing post brushing rinsing in 2012 found that there although was clear guidance given in many public health advice publications to "spit, avoid rinsing with water/excessive rinsing with water" they believed there was a limited evidence base for best practice.
Common use involves rinsing the mouth with about 20-50 ml (2/3 fl oz) of mouthwash.
The wash is typically swished or gargled for about half a minute and then spat out.
Most companies suggest not drinking water immediately after using mouthwash.
In some brands, the expectorate is stained, so that one can see the bacteria and debris.
Mouthwash should not be used immediately after brushing the teeth so as not to wash away the beneficial fluoride residue left from the toothpaste.
Similarly, the mouth should not be rinsed out with water after brushing.
Patients were told to "spit don't rinse" after toothbrushing as part of a National Health Service campaign in the UK.
Gargling is where the head is tilted back, allowing the mouthwash to sit in the back of the mouth while exhaling, causing the liquid to bubble.
Gargling is practiced in Japan for perceived prevention of viral infection.
One commonly used way is with infusions or tea.
In some cultures, gargling is usually done in private, typically in a bathroom at a sink so the liquid can be rinsed away.
The most common use of mouthwash is commercial antiseptics, which are used at home as part of an oral hygiene routine.
Examples of commercial mouthwashes companies include Cēpacol, Colgate, Corsodyl, Dentyl pH, Listerine, , Oral-B, Sarakan, Scope, Tantum verde, and Biotene.
Mouthwashes combine ingredients to treat a variety of oral conditions.
Variations are common, and mouthwash has no standard formulation so its use and recommendation involves concerns about patient safety.
Some manufacturers of mouthwash state that antiseptic and anti-plaque mouth rinse kill the bacterial plaque that causes cavities, gingivitis, and bad breath.
It is, however, generally agreed that the use of mouthwash does not eliminate the need for both brushing and flossing.
The American Dental Association asserts that regular brushing and proper flossing are enough in most cases, in addition to regular dental check-ups, although they approve many mouthwashes.
For many patients, however, the mechanical methods could be tedious and time-consuming and additionally some local conditions may render them especially difficult.
Chemotherapeutic agents, including mouthrinses, could have a key role as adjuncts to daily home care, preventing and controlling supragingival plaque, gingivitis and oral malodor.
Minor and transient side effects of mouthwashes are very common, such as taste disturbance, tooth staining, sensation of a dry mouth, etc.
Alcohol-containing mouthwashes may make dry mouth and halitosis worse since it dries out the mouth.
Soreness, ulceration and redness may sometimes occur (e.g.
aphthous stomatitis, allergic contact stomatitis) if the person is allergic or sensitive to mouthwash ingredients such as preservatives, coloring, flavors and fragrances.
Such effects might be reduced or eliminated by diluting the mouthwash with water, using a different mouthwash (e.g.
salt water), or foregoing mouthwash entirely.
Prescription mouthwashes are used prior to and after oral surgery procedures such as tooth extraction or to treat the pain associated with mucositis caused by radiation therapy or chemotherapy.
They are also prescribed for aphthous ulcers, other oral ulcers, and other mouth pain.
Magic mouthwashes are prescription mouthwashes compounded in a pharmacy from a list of ingredients specified by a doctor.
Despite a lack of evidence that prescription mouthwashes are more effective in decreasing the pain of oral lesions, many patients and prescribers continue to use them.
There has been only one controlled study to evaluate the efficacy of magic mouthwash; it shows no difference in efficacy among the most common formulation and commercial mouthwashes such as chlorhexidine or a saline/baking soda solution.
Current guidelines suggest that saline solution is just as effective as magic mouthwash in pain relief or shortening of healing time of oral mucositis from cancer therapies.
The first known references to mouth rinsing is in Ayurveda for treatment of gingivitis.
Later, in the Greek and Roman periods, mouth rinsing following mechanical cleansing became common among the upper classes, and Hippocrates recommended a mixture of salt, alum, and vinegar.
The Jewish Talmud, dating back about 1,800 years, suggests a cure for gum ailments containing "dough water" and olive oil.
Before Europeans came to the Americas, Native North American and Mesoamerican cultures used mouthwashes, often made from plants such as "Coptis trifolia".
Indeed, Aztec dentistry was more advanced than European dentistry of the age.
Peoples of the Americas used salt water mouthwashes for sore throats, and other mouthwashes for problems such as teething and mouth ulcers.
Anton van Leeuwenhoek, the famous 17th century microscopist, discovered living organisms (living, because they were mobile) in deposits on the teeth (what we now call dental plaque).
He also found organisms in water from the canal next to his home in Delft.
He experimented with samples by adding vinegar or brandy and found that this resulted in the immediate immobilization or killing of the organisms suspended in water.
Next he tried rinsing the mouth of himself and somebody else with a mouthwash containing vinegar or brandy and found that living organisms remained in the dental plaque.
He concluded—correctly—that the mouthwash either did not reach, or was not present long enough, to kill the plaque organisms.
In 1892, German Richard Seifert invented mouthwash product Odol, which was produced by company founder Karl August Lingner (1861–1916) in Dresden.
That remained the state of affairs until the late 1960s when Harald Loe (at the time a professor at the Royal Dental College in Aarhus, Denmark) demonstrated that a chlorhexidine compound could prevent the build-up of dental plaque.
The reason for chlorhexidine's effectiveness is that it strongly adheres to surfaces in the mouth and thus remains present in effective concentrations for many hours.
Since then commercial interest in mouthwashes has been intense and several newer products claim effectiveness in reducing the build-up in dental plaque and the associated severity of gingivitis, in addition to fighting bad breath.
Many of these solutions aim to control the Volatile Sulfur Compound (VSC)-creating anaerobic bacteria that live in the mouth and excrete substances that lead to bad breath and unpleasant mouth taste.
For example, the number of mouthwash variants in the United States of America has grown from 15 (1970) to 66 (1998) to 113 (2012).
Research in the field of microbiotas shows that only a limited set of microbes cause tooth decay, with most of the bacteria in the human mouth being harmless.
Focused attention on cavity-causing bacteria such as "Streptococcus mutans" has led research into new mouthwash treatments that prevent these bacteria from initially growing.
While current mouthwash treatments must be used with a degree of frequency to prevent this bacteria from regrowing, future treatments could provide a viable long term solution.
Alcohol is added to mouthwash not to destroy bacteria but to act as a carrier agent for essential active ingredients such as menthol, eucalyptol and thymol which help to penetrate plaque.
Sometimes a significant amount of alcohol (up to 27% vol) is added, as a carrier for the flavor, to provide "bite".
Because of the alcohol content, it is possible to fail a breathalyzer test after rinsing although breath alcohol levels return to normal after 10 minutes.
In addition, alcohol is a drying agent, which encourages bacterial activity in the mouth, releasing more malodorous volatile sulfur compounds.
Therefore, alcohol-containing mouthwash may temporarily worsen halitosis in those who already have it, or indeed be the sole cause of halitosis in other individuals.
It is hypothesized that alcohol mouthwashes acts as a carcinogen (cancer-inducing).
Generally, there is no scientific consensus about this.
One review stated:

The same researchers also state that the risk of acquiring oral cancer rises almost five times for users of alcohol-containing mouthwash who neither smoke nor drink (with a higher rate of increase for those who do).
In addition, the authors highlight side effects from several mainstream mouthwashes that included dental erosion and accidental poisoning of children.
The review garnered media attention and conflicting opinions from other researchers.
Yinka Ebo of Cancer Research UK disputed the findings, concluding that "there is still not enough evidence to suggest that using mouthwash that contains alcohol will increase the risk of mouth cancer".
Studies conducted in 1985, 1995, 2003, and 2012 did not support an association between alcohol-containing mouth rinses and oral cancer.
Andrew Penman, chief executive of The Cancer Council New South Wales, called for further research on the matter.
In a March 2009 brief, the American Dental Association said "the available evidence does not support a connection between oral cancer and alcohol-containing mouthrinse".
Many newer brands of mouthwash are alcohol free, not just in response to consumer concerns about oral cancer, but also to cater for religious groups who abstain from alcohol consumption.
In painful oral conditions such as aphthous stomatitis, analgesic mouthrinses (e.g.
benzydamine mouthwash, or "Difflam") are sometimes used to ease pain, commonly used before meals to reduce discomfort while eating.
Acts as a buffer

Betamethasone is sometimes used as an anti-inflammatory, corticosteroid mouthwash.
It may be used for severe inflammatory conditions of the oral mucosa such as the severe forms of aphthous stomatitis.
Cetylpyridinium chloride containing mouthwash (e.g.
0.05%) is used in some specialized mouthwashes for halitosis.
Cetylpyridinium chloride mouthwash has less anti-plaque effect than chlorhexidine and may cause staining of teeth, or sometimes an oral burning sensation or ulceration.
Chlorhexidine digluconate is a chemical antiseptic and is used in a 0.12-0.2% solution as a mouthwash.
However, there is no evidence to support that higher concentrations are more effective in controlling dental plaque and gingivitis.
It has anti-plaque action, but also some anti-fungal action.
It is especially effective against Gram-negative rods.
The proportion of Gram-negative rods increase as gingivitis develops so it is also used to reduce gingivitis.
It is sometimes used as an adjunct to prevent dental caries and to treat gingivitis periodontal disease, although it does not penetrate into periodontal pockets well.
Chlorhexidine mouthwash alone is unable to prevent plaque, so it is not a substitute for regular toothbrushing and flossing.
Instead, chlorhexidine is more effective used as an adjunctive treatment with tooth brushing and flossing.
In the short term, if toothbrushing is impossible due to pain, as may occur in primary herpetic gingivostomatitis, chlorhexidine is used as temporary substitute for other oral hygiene measures.
It is not suited for use in acute necrotizing ulcerative gingivitis, however.
Rinsing with chlorhexidine mouthwash before a tooth extraction reduces the risk of dry socket, a painful condition where the blood clot is lost from an extraction socket and bone is exposed to the oral cavity.
Other uses of chlorhexidine mouthwash include prevention of oral candidiasis in immunocompromised persons, treatment of denture-related stomatitis, mucosal ulceration/erosions and oral mucosal lesions, general burning sensation and many other uses.
Chlorhexidine has good "substantivity" (the ability of a mouthwash to bind to hard and soft tissues in the mouth).
However, chlorhexidine binds to tannins, meaning that prolonged use in persons who consume coffee, tea or red wine is associated with extrinsic staining (i.e. removable staining) of teeth.
Chlorhexidine mouthwash can also cause taste disturbance and/or alteration.
Chlorhexidine is rarely associated with other issues like overgrowth of enterobacteria in persons with leukemia, desquamation and irritation of oral mucosa, salivary gland pain and swelling, and hypersensitivity reactions including anaphylaxis.
A randomized clinical trial conducted in Rabat university in Morocco found better results in plaque inhibition when chlorohexidine with alcohol base 0.12% was used , when compared to an alcohol free 0.1% chlorhexidine mouthrinse.
Chlorhexidine mouthrinses increase staining score of teeth over a period of time.
Hexetidine also has anti-plaque, analgesic, astringent and anti-malodor properties but is considered as an inferior alternative to Chlorhexidine.
In traditional Ayurvedic medicine, the use of oil mouthwashes is called "Kavala" ("oil swishing") or "Gandusha", and this practice has more recently been re-marketed by the complimentary and alternative medicine industry as "oil pulling".
Its promoters claim it works by "pulling out" "toxins", which are known as ama in Ayurvedic medicine, and thereby reducing inflammation.
Ayurvedic literature suggests oil pulling is capable of improving oral and systemic health, including a benefit in conditions such as headaches, migraines, diabetes mellitus, asthma, and acne, as well as whitening teeth.
Oil pulling has received little study and there is little evidence to support claims made by the technique's advocates.
When compared with chlorhexidine in one small study, it was found to be less effective at reducing oral bacterial load, otherwise the health claims of oil pulling have failed scientific verification or have not been investigated.
There is a report of lipid pneumonia caused by accidental inhalation of the oil during oil pulling.
The mouth is rinsed with approximately one tablespoon of oil for 10–20 minutes then spat out.
Sesame oil, coconut oil and ghee are traditionally used, but newer oils such as sunflower oil are also used.
Phenolic compounds include essential oil constituents that have some antibacterial properties, like phenol, thymol, eugenol, eucalyptol or menthol.
Essential oils are oils which have been extracted from plants.
Mouthwashes based on essential oils could be more effective than traditional mouthcare - for anti-gingival treatments.
They have been found effective in reducing halitosis, and are being used in several commercial mouthwashes.
Anti-cavity mouth rinses use fluoride to protect against tooth decay.
Most people using fluoridated toothpastes do not require fluoride-containing mouth rinses, rather fluoride mouthwashes are sometimes used in individuals who are at high risk of dental decay, due to dental caries or people with xerostomia.
Flavoring agents include sweeteners such as sorbitol, sucralose, sodium saccharin, and xylitol, which stimulate salivary function due to their sweetness and taste and helps restore the mouth to a neutral level of acidity.
Xylitol rinses double as a bacterial inhibitor and have been used as substitute for Alcohol to avoid dryness of mouth associated with Alcohol.
Hydrogen peroxide can be used as an oxidizing mouthwash (e.g.
Peroxyl, 1.5%).
It kills anaerobic bacteria, and also has a mechanical cleansing action when it froths as it comes into contact with debris in mouth.
It is often used in the short term to treat acute necrotising ulcerative gingivitis.
Side effects with prolonged use might occur, including hypertrophy of the lingual papillae.
Enzymes and proteins such as Lactoperoxidase, Lysozyme, Lactoferrin have been used in mouthrinses (e.g.
Biotene) to reduce oral bacteria and hence the acid produced by bacteria.
Oral lidocaine is useful for the treatment of mucositis symptoms (inflammation of mucous membranes) that is induced by radiation or chemotherapy.
There is evidence that lidocaine anesthetic mouthwash has the potential to be systemically absorbed when it was tested in patients with oral mucositis who underwent a bone marrow transplant.
Methyl salicylate functions as an anti-septic, anti-inflammatory, analgesic, flavoring, and fragrance Methyl salicylate]] has some anti-plaque action, but less than chlorhexidine.
Methyl salicylate does not not stain teeth.
Nystatin suspension is an antifungal ingredient used for the treatment of oral candidiasis.
A randomized clinical trial found promising results in controlling and reducing dentine hypersensitivity when potassium oxalate mouthrinse was used in conjugation with toothbrushing.
A 2005 study found that gargling three times a day with simple water or with a Povidone-iodine solution was effective in preventing upper respiratory infection and decreasing the severity of symptoms if contracted.
Other sources attribute the benefit to a simple placebo effect.
Sanguinarine-containing mouthwashes are marketed as anti-plaque and anti-malodor.
It is a toxic alkaloid herbal extract, obtained from plants such as "Sanguinaria canadensis" (Bloodroot), "Argemone mexicana" (Mexican Prickly Poppy) and others.
However, its use is strongly associated with development of leukoplakia (a white patch in the mouth), usually in the buccal sulcus.
This type of leukoplakia has been termed "sanguinaria-associated keratosis" and more than 80% of people with leukoplakia in the vestibule of the mouth have used this substance.
Upon stopping contact with the causative substance, the lesions may persist for years.
Although this type of leukoplakia may show dysplasia, the potential for malignant transformation is unknown.
Ironically, elements within the complimentary and alternative medicine industry promote the use of sanguinaria as a therapy for cancer.
Sodium bicarbonate is sometimes combined with salt to make a simple homemade mouthwash, indicated for any of the reasons that a salt water mouthwash might be used.
Pre-mixed mouthwashes of 1% sodium bicarbonate and 1.5% sodium chloride in aqueous solution are marketed, although pharmacists will easily be able to produce such a formulation from the base ingredients when required.
Sodium bicarbonate mouthwash is sometimes used to remove viscous saliva and to aid visualization of the oral tissues during examination of the mouth.
Salt water mouth wash is made by dissolving 0.5–1 teaspoon of table salt into a cup of water, which is as hot as possible without causing discomfort in the mouth.
Saline has a mechanical cleansing action and an antiseptic action as it is a hypertonic solution in relation to bacteria, which undergo lysis.
The heat of the solution produces a therapeutic increase in blood flow (hyperemia) to the surgical site, promoting healing.
Hot salt water mouthwashes also encourage the draining of pus from dental abscesses.
Conversely, if heat is applied on the side of the face (e.g., hot water bottle) rather than inside the mouth, it may cause a dental abscess to drain extra-orally, which is later associated with an area of fibrosis on the face (see cutaneous sinus of dental origin).
Gargling with salt water is said to reduce the symptoms of a sore throat.
Hot salt water mouth baths (or hot salt water mouth washes, sometimes abbreviated to "HSWMW") are also routinely used after oral surgery, to keep food debris out of healing wounds and to prevent infection.
Some oral surgeons consider salt water mouthwashes the mainstay of wound cleanliness after surgery.
In dental extractions, hot salt water mouthbaths should start about 24 hours after a dental extraction.
The term "mouth bath" implies that the liquid is passively held in the mouth rather than vigorously swilled around, which could dislodge a blood clot.
Once the blood clot has stabilized, the mouth wash can be used more vigorously.
These mouthwashes tend to be advised about 6 times per day, especially after meals to remove food from the socket.
Sodium lauryl sulfate (SLS) is used as a foaming agent in many oral hygiene products including many mouthwashes.
Some may suggest that it is probably advisable to use mouthwash at least an hour after brushing with toothpaste when the toothpaste contains SLS, since the anionic compounds in the SLS toothpaste can deactivate cationic agents present in the mouthrinse.
Sucralfate is a mucosal coating agent, composed of an aluminum salt of sulfated sucrose.
It is not recommended for use in the prevention of oral mucositis in head and neck cancer patients receiving radiotherapy or chemoradiation due to a lack of efficacy found in a well-designed, randomized controlled trial.
Tetracycline is an antibiotic which may sometimes be used as a mouthwash in adults (it causes red staining of teeth in children).
It is sometimes use for herpetiforme ulceration (an uncommon type of aphthous stomatitis), but prolonged use may lead to oral candidiasis as the fungal population of the mouth overgrows in the absence of enough competing bacteria.
Similarly, Minocycline mouthwashes of 0.5% concentrations can relieve symptoms of recurrent aphthous stomatitis.
Erythromycin is similar.
4.8% tranexamic acid solution is sometimes used as an antifibrinolytic mouthwash to prevent bleeding during and after oral surgery in persons with coagulopathies (clotting disorders) or who are taking anticoagulants (blood thinners such as warfarin).
Triclosan is a non-ionic chlorinate bisphenol antiseptic found in some mouthwashes.
When used in mouthwash (e.g.
0.03%), there is moderate substantivity, broad spectrum anti-bacterial action, some anti-fungal action and significant anti-plaque effect, especially when combined with copolymer or zinc citrate.
Triclosan does not cause staining of the teeth.
The safety of triclosan has been questioned.
Astringents like zinc chloride provide a pleasant-tasting sensation and shrink tissues.
Zinc when used in combination with other anti-septic agents can limit the build-up of tartar



</doc>
<doc id="783" url="https://en.wikipedia.org/wiki?curid=783" title="Alexander the Great">
Alexander the Great

Alexander III of Macedon (; 20/21 July 356 BC – 10/11 June 323 BC), commonly known as Alexander the Great (, was a king ("basileus") of the ancient Greek kingdom of Macedon and a member of the Argead dynasty.
He was born in Pella in 356 BC and succeeded his father Philip II to the throne at the age of twenty.
He spent most of his ruling years on an unprecedented military campaign through Asia and northeast Africa, and he created one of the largest empires of the ancient world by the age of thirty, stretching from Greece to northwestern India.
He was undefeated in battle and is widely considered one of history's most successful military commanders.
During his youth, Alexander was tutored by Aristotle until age 16.
After Philip's assassination in 336 BC, he succeeded his father to the throne and inherited a strong kingdom and an experienced army.
Alexander was awarded the generalship of Greece and used this authority to launch his father's pan-Hellenic project to lead the Greeks in the conquest of Persia.
In 334 BC, he invaded the Achaemenid Empire (Persian Empire) and began a series of campaigns that lasted ten years.
Following the conquest of Anatolia, Alexander broke the power of Persia in a series of decisive battles, most notably the battles of Issus and Gaugamela.
He subsequently overthrew Persian King Darius III and conquered the Achaemenid Empire in its entirety.
At that point, his empire stretched from the Adriatic Sea to the Indus River.
He endeavored to reach the "ends of the world and the Great Outer Sea" and invaded India in 326 BC, winning an important victory over the Pauravas at the Battle of the Hydaspes.
He eventually turned back at the demand of his homesick troops.
Alexander died in Babylon in 323 BC, the city that he planned to establish as his capital, without executing a series of planned campaigns that would have begun with an invasion of Arabia.
In the years following his death, a series of civil wars tore his empire apart, resulting in the establishment of several states ruled by the Diadochi, Alexander's surviving generals and heirs.
Alexander's legacy includes the cultural diffusion and syncretism which his conquests engendered, such as Greco-Buddhism.
He founded some twenty cities that bore his name, most notably Alexandria in Egypt.
Alexander's settlement of Greek colonists and the resulting spread of Greek culture in the east resulted in a new Hellenistic civilization, aspects of which were still evident in the traditions of the Byzantine Empire in the mid-15th century AD and the presence of Greek speakers in central and far eastern Anatolia until the 1920s.
Alexander became legendary as a classical hero in the mold of Achilles, and he features prominently in the history and mythic traditions of both Greek and non-Greek cultures.
He became the measure against which military leaders compared themselves, and military academies throughout the world still teach his tactics.
He is often ranked among the most influential people in history.
Alexander was born on the sixth day of the ancient Greek month of Hekatombaion, which probably corresponds to 20July 356 BC, although the exact date is disputed, in Pella, the capital of the Kingdom of Macedon.
He was the son of the king of Macedon, Philip II, and his fourth wife, Olympias, the daughter of Neoptolemus I, king of Epirus.
Although Philip had seven or eight wives, Olympias was his principal wife for some time, likely because she gave birth to Alexander.
Several legends surround Alexander's birth and childhood.
According to the ancient Greek biographer Plutarch, on the eve of the consummation of her marriage to Philip, Olympias dreamed that her womb was struck by a thunder bolt that caused a flame to spread "far and wide" before dying away.
Sometime after the wedding, Philip is said to have seen himself, in a dream, securing his wife's womb with a seal engraved with a lion's image.
Plutarch offered a variety of interpretations of these dreams: that Olympias was pregnant before her marriage, indicated by the sealing of her womb; or that Alexander's father was Zeus.
Ancient commentators were divided about whether the ambitious Olympias promulgated the story of Alexander's divine parentage, variously claiming that she had told Alexander, or that she dismissed the suggestion as impious.
On the day Alexander was born, Philip was preparing a siege on the city of Potidea on the peninsula of Chalcidice.
That same day, Philip received news that his general Parmenion had defeated the combined Illyrian and Paeonian armies, and that his horses had won at the Olympic Games.
It was also said that on this day, the Temple of Artemis in Ephesus, one of the Seven Wonders of the World, burnt down.
This led Hegesias of Magnesia to say that it had burnt down because Artemis was away, attending the birth of Alexander.
Such legends may have emerged when Alexander was king, and possibly at his own instigation, to show that he was superhuman and destined for greatness from conception.
In his early years, Alexander was raised by a nurse, Lanike, sister of Alexander's future general Cleitus the Black.
Later in his childhood, Alexander was tutored by the strict Leonidas, a relative of his mother, and by Lysimachus of Acarnania.
Alexander was raised in the manner of noble Macedonian youths, learning to read, play the lyre, ride, fight, and hunt.
When Alexander was ten years old, a trader from Thessaly brought Philip a horse, which he offered to sell for thirteen talents.
The horse refused to be mounted, and Philip ordered it away.
Alexander however, detecting the horse's fear of its own shadow, asked to tame the horse, which he eventually managed.
Plutarch stated that Philip, overjoyed at this display of courage and ambition, kissed his son tearfully, declaring: "My boy, you must find a kingdom big enough for your ambitions.
Macedon is too small for you", and bought the horse for him.
Alexander named it Bucephalas, meaning "ox-head".
Bucephalas carried Alexander as far as India.
When the animal died (because of old age, according to Plutarch, at age thirty), Alexander named a city after him, Bucephala.
When Alexander was 13, Philip began to search for a tutor, and considered such academics as Isocrates and Speusippus, the latter offering to resign from his stewardship of the Academy to take up the post.
In the end, Philip chose Aristotle and provided the Temple of the Nymphs at Mieza as a classroom.
In return for teaching Alexander, Philip agreed to rebuild Aristotle's hometown of Stageira, which Philip had razed, and to repopulate it by buying and freeing the ex-citizens who were slaves, or pardoning those who were in exile.
Mieza was like a boarding school for Alexander and the children of Macedonian nobles, such as Ptolemy, Hephaistion, and Cassander.
Many of these students would become his friends and future generals, and are often known as the 'Companions'.
Aristotle taught Alexander and his companions about medicine, philosophy, morals, religion, logic, and art.
Under Aristotle's tutelage, Alexander developed a passion for the works of Homer, and in particular the "Iliad"; Aristotle gave him an annotated copy, which Alexander later carried on his campaigns.
At age 16, Alexander's education under Aristotle ended.
Philip waged war against Byzantion, leaving Alexander in charge as regent and heir apparent.
During Philip's absence, the Thracian Maedi revolted against Macedonia.
Alexander responded quickly, driving them from their territory.
He colonized it with Greeks, and founded a city named Alexandropolis.
Upon Philip's return, he dispatched Alexander with a small force to subdue revolts in southern Thrace.
Campaigning against the Greek city of Perinthus, Alexander is reported to have saved his father's life.
Meanwhile, the city of Amphissa began to work lands that were sacred to Apollo near Delphi, a sacrilege that gave Philip the opportunity to further intervene in Greek affairs.
Still occupied in Thrace, he ordered Alexander to muster an army for a campaign in southern Greece.
Concerned that other Greek states might intervene, Alexander made it look as though he was preparing to attack Illyria instead.
During this turmoil, the Illyrians invaded Macedonia, only to be repelled by Alexander.
Philip and his army joined his son in 338 BC, and they marched south through Thermopylae, taking it after stubborn resistance from its Theban garrison.
They went on to occupy the city of Elatea, only a few days' march from both Athens and Thebes.
The Athenians, led by Demosthenes, voted to seek alliance with Thebes against Macedonia.
Both Athens and Philip sent embassies to win Thebes' favour, but Athens won the contest.
Philip marched on Amphissa (ostensibly acting on the request of the Amphictyonic League), capturing the mercenaries sent there by Demosthenes and accepting the city's surrender.
Philip then returned to Elatea, sending a final offer of peace to Athens and Thebes, who both rejected it.
As Philip marched south, his opponents blocked him near Chaeronea, Boeotia.
During the ensuing Battle of Chaeronea, Philip commanded the right wing and Alexander the left, accompanied by a group of Philip's trusted generals.
According to the ancient sources, the two sides fought bitterly for some time.
Philip deliberately commanded his troops to retreat, counting on the untested Athenian hoplites to follow, thus breaking their line.
Alexander was the first to break the Theban lines, followed by Philip's generals.
Having damaged the enemy's cohesion, Philip ordered his troops to press forward and quickly routed them.
With the Athenians lost, the Thebans were surrounded.
Left to fight alone, they were defeated.
After the victory at Chaeronea, Philip and Alexander marched unopposed into the Peloponnese, welcomed by all cities; however, when they reached Sparta, they were refused, but did not resort to war.
At Corinth, Philip established a "Hellenic Alliance" (modelled on the old anti-Persian alliance of the Greco-Persian Wars), which included most Greek city-states except Sparta.
Philip was then named "Hegemon" (often translated as "Supreme Commander") of this league (known by modern scholars as the League of Corinth), and announced his plans to attack the Persian Empire.
When Philip returned to Pella, he fell in love with and married Cleopatra Eurydice, the niece of his general Attalus.
The marriage made Alexander's position as heir less secure, since any son of Cleopatra Eurydice would be a fully Macedonian heir, while Alexander was only half-Macedonian.
During the wedding banquet, a drunken Attalus publicly prayed to the gods that the union would produce a legitimate heir.
Alexander fled Macedon with his mother, dropping her off with her brother, King Alexander I of Epirus in Dodona, capital of the Molossians.
He continued to Illyria, where he sought refuge with the Illyrian king and was treated as a guest, despite having defeated them in battle a few years before.
However, it appears Philip never intended to disown his politically and militarily trained son.
Accordingly, Alexander returned to Macedon after six months due to the efforts of a family friend, Demaratus, who mediated between the two parties.
In the following year, the Persian satrap (governor) of Caria, Pixodarus, offered his eldest daughter to Alexander's half-brother, Philip Arrhidaeus.
Olympias and several of Alexander's friends suggested this showed Philip intended to make Arrhidaeus his heir.
Alexander reacted by sending an actor, Thessalus of Corinth, to tell Pixodarus that he should not offer his daughter's hand to an illegitimate son, but instead to Alexander.
When Philip heard of this, he stopped the negotiations and scolded Alexander for wishing to marry the daughter of a Carian, explaining that he wanted a better bride for him.
Philip exiled four of Alexander's friends, Harpalus, Nearchus, Ptolemy and Erigyius, and had the Corinthians bring Thessalus to him in chains.
In summer 336 BC, while at Aegae attending the wedding of his daughter Cleopatra to Olympias's brother, Alexander I of Epirus, Philip was assassinated by the captain of his bodyguards, Pausanias.
As Pausanias tried to escape, he tripped over a vine and was killed by his pursuers, including two of Alexander's companions, Perdiccas and Leonnatus.
Alexander was proclaimed king on the spot by the nobles and army at the age of 20.
Alexander began his reign by eliminating potential rivals to the throne.
He had his cousin, the former Amyntas IV, executed.
He also had two Macedonian princes from the region of Lyncestis killed, but spared a third, Alexander Lyncestes.
Olympias had Cleopatra Eurydice and Europa, her daughter by Philip, burned alive.
When Alexander learned about this, he was furious.
Alexander also ordered the murder of Attalus, who was in command of the advance guard of the army in Asia Minor and Cleopatra's uncle.
Attalus was at that time corresponding with Demosthenes, regarding the possibility of defecting to Athens.
Attalus also had severely insulted Alexander, and following Cleopatra's murder, Alexander may have considered him too dangerous to leave alive.
Alexander spared Arrhidaeus, who was by all accounts mentally disabled, possibly as a result of poisoning by Olympias.
News of Philip's death roused many states into revolt, including Thebes, Athens, Thessaly, and the Thracian tribes north of Macedon.
When news of the revolts reached Alexander, he responded quickly.
Though advised to use diplomacy, Alexander mustered 3,000 Macedonian cavalry and rode south towards Thessaly.
He found the Thessalian army occupying the pass between Mount Olympus and Mount Ossa, and ordered his men to ride over Mount Ossa.
When the Thessalians awoke the next day, they found Alexander in their rear and promptly surrendered, adding their cavalry to Alexander's force.
He then continued south towards the Peloponnese.
Alexander stopped at Thermopylae, where he was recognized as the leader of the Amphictyonic League before heading south to Corinth.
Athens sued for peace and Alexander pardoned the rebels.
The famous encounter between Alexander and Diogenes the Cynic occurred during Alexander's stay in Corinth.
When Alexander asked Diogenes what he could do for him, the philosopher disdainfully asked Alexander to stand a little to the side, as he was blocking the sunlight.
This reply apparently delighted Alexander, who is reported to have said "But verily, if I were not Alexander, I would like to be Diogenes."
At Corinth, Alexander took the title of "Hegemon" ("leader") and, like Philip, was appointed commander for the coming war against Persia.
He also received news of a Thracian uprising.
Before crossing to Asia, Alexander wanted to safeguard his northern borders.
In the spring of 335 BC, he advanced to suppress several revolts.
Starting from Amphipolis, he travelled east into the country of the "Independent Thracians"; and at Mount Haemus, the Macedonian army attacked and defeated the Thracian forces manning the heights.
The Macedonians marched into the country of the Triballi, and defeated their army near the Lyginus river (a tributary of the Danube).
Alexander then marched for three days to the Danube, encountering the Getae tribe on the opposite shore.
Crossing the river at night, he surprised them and forced their army to retreat after the first cavalry skirmish.
News then reached Alexander that Cleitus, King of Illyria, and King Glaukias of the Taulantii were in open revolt against his authority.
Marching west into Illyria, Alexander defeated each in turn, forcing the two rulers to flee with their troops.
With these victories, he secured his northern frontier.
While Alexander campaigned north, the Thebans and Athenians rebelled once again.
Alexander immediately headed south.
While the other cities again hesitated, Thebes decided to fight.
The Theban resistance was ineffective, and Alexander razed the city and divided its territory between the other Boeotian cities.
The end of Thebes cowed Athens, leaving all of Greece temporarily at peace.
Alexander then set out on his Asian campaign, leaving Antipater as regent.
Alexander's army crossed the Hellespont in 334 BC with approximately 48,100 soldiers, 6,100 cavalry and a fleet of 120 ships with crews numbering 38,000, drawn from Macedon and various Greek city-states, mercenaries, and feudally raised soldiers from Thrace, Paionia, and Illyria.
He showed his intent to conquer the entirety of the Persian Empire by throwing a spear into Asian soil and saying he accepted Asia as a gift from the gods.
This also showed Alexander's eagerness to fight, in contrast to his father's preference for diplomacy.
After an initial victory against Persian forces at the Battle of the Granicus, Alexander accepted the surrender of the Persian provincial capital and treasury of Sardis; he then proceeded along the Ionian coast, granting autonomy and democracy to the cities.
Miletus, held by Achaemenid forces, required a delicate siege operation, with Persian naval forces nearby.
Further south, at Halicarnassus, in Caria, Alexander successfully waged his first large-scale siege, eventually forcing his opponents, the mercenary captain Memnon of Rhodes and the Persian satrap of Caria, Orontobates, to withdraw by sea.
Alexander left the government of Caria to a member of the Hecatomnid dynasty, Ada, who adopted Alexander.
From Halicarnassus, Alexander proceeded into mountainous Lycia and the Pamphylian plain, asserting control over all coastal cities to deny the Persians naval bases.
From Pamphylia onwards the coast held no major ports and Alexander moved inland.
At Termessos, Alexander humbled but did not storm the Pisidian city.
At the ancient Phrygian capital of Gordium, Alexander "undid" the hitherto unsolvable Gordian Knot, a feat said to await the future "king of Asia".
According to the story, Alexander proclaimed that it did not matter how the knot was undone and hacked it apart with his sword.
In spring 333 BC, Alexander crossed the Taurus into Cilicia.
After a long pause due to an illness, he marched on towards Syria.
Though outmanoeuvered by Darius' significantly larger army, he marched back to Cilicia, where he defeated Darius at Issus.
Darius fled the battle, causing his army to collapse, and left behind his wife, his two daughters, his mother Sisygambis, and a fabulous treasure.
He offered a peace treaty that included the lands he had already lost, and a ransom of 10,000 talents for his family.
Alexander replied that since he was now king of Asia, it was he alone who decided territorial divisions.
Alexander proceeded to take possession of Syria, and most of the coast of the Levant.
In the following year, 332 BC, he was forced to attack Tyre, which he captured after a long and difficult siege.
The men of military age were massacred and the women and children sold into slavery.
When Alexander destroyed Tyre, most of the towns on the route to Egypt quickly capitulated.
However, Alexander met with resistance at Gaza.
The stronghold was heavily fortified and built on a hill, requiring a siege.
When "his engineers pointed out to him that because of the height of the mound it would be impossible… this encouraged Alexander all the more to make the attempt".
After three unsuccessful assaults, the stronghold fell, but not before Alexander had received a serious shoulder wound.
As in Tyre, men of military age were put to the sword and the women and children were sold into slavery.
Alexander advanced on Egypt in later 332 BC, where he was regarded as a liberator.
He was pronounced son of the deity Amun at the Oracle of Siwa Oasis in the Libyan desert.
Henceforth, Alexander often referred to Zeus-Ammon as his true father, and after his death, currency depicted him adorned with the horns of a ram as a symbol of his divinity.
During his stay in Egypt, he founded Alexandria-by-Egypt, which would become the prosperous capital of the Ptolemaic Kingdom after his death.
Leaving Egypt in 331 BC, Alexander marched eastward into Mesopotamia (now northern Iraq) and again defeated Darius, at the Battle of Gaugamela.
Darius once more fled the field, and Alexander chased him as far as Arbela.
Gaugamela would be the final and decisive encounter between the two.
Darius fled over the mountains to Ecbatana (modern Hamedan), while Alexander captured Babylon.
From Babylon, Alexander went to Susa, one of the Achaemenid capitals, and captured its treasury.
He sent the bulk of his army to the Persian ceremonial capital of Persepolis via the Persian Royal Road.
Alexander himself took selected troops on the direct route to the city.
He then stormed the pass of the Persian Gates (in the modern Zagros Mountains) which had been blocked by a Persian army under Ariobarzanes and then hurried to Persepolis before its garrison could loot the treasury.
On entering Persepolis, Alexander allowed his troops to loot the city for several days.
Alexander stayed in Persepolis for five months.
During his stay a fire broke out in the eastern palace of Xerxes I and spread to the rest of the city.
Possible causes include a drunken accident or deliberate revenge for the burning of the Acropolis of Athens during the Second Persian War by Xerxes.
Even as he watched the city burn, Alexander immediately began to regret his decision.
Plutarch claims that he ordered his men to put out the fires, but that the flames had already spread to most of the city.
Curtius claims that Alexander did not regret his decision until the next morning.
Plutarch recounts an anecdote in which Alexander pauses and talks to a fallen statue of Xerxes as if it were a live person:

Alexander then chased Darius, first into Media, and then Parthia.
The Persian king no longer controlled his own destiny, and was taken prisoner by Bessus, his Bactrian satrap and kinsman.
As Alexander approached, Bessus had his men fatally stab the Great King and then declared himself Darius' successor as Artaxerxes V, before retreating into Central Asia to launch a guerrilla campaign against Alexander.
Alexander buried Darius' remains next to his Achaemenid predecessors in a regal funeral.
He claimed that, while dying, Darius had named him as his successor to the Achaemenid throne.
The Achaemenid Empire is normally considered to have fallen with Darius.
Alexander viewed Bessus as a usurper and set out to defeat him.
This campaign, initially against Bessus, turned into a grand tour of central Asia.
Alexander founded a series of new cities, all called Alexandria, including modern Kandahar in Afghanistan, and Alexandria Eschate ("The Furthest") in modern Tajikistan.
The campaign took Alexander through Media, Parthia, Aria (West Afghanistan), Drangiana, Arachosia (South and Central Afghanistan), Bactria (North and Central Afghanistan), and Scythia.
Spitamenes, who held an undefined position in the satrapy of Sogdiana, in 329 BC betrayed Bessus to Ptolemy, one of Alexander's trusted companions, and Bessus was executed.
However, when, at some point later, Alexander was on the Jaxartes dealing with an incursion by a horse nomad army, Spitamenes raised Sogdiana in revolt.
Alexander personally defeated the Scythians at the Battle of Jaxartes and immediately launched a campaign against Spitamenes, defeating him in the Battle of Gabai.
After the defeat, Spitamenes was killed by his own men, who then sued for peace.
During this time, Alexander adopted some elements of Persian dress and customs at his court, notably the custom of "proskynesis", either a symbolic kissing of the hand, or prostration on the ground, that Persians showed to their social superiors.
The Greeks regarded the gesture as the province of deities and believed that Alexander meant to deify himself by requiring it.
This cost him the sympathies of many of his countrymen, and he eventually abandoned it.
A plot against his life was revealed, and one of his officers, Philotas, was executed for failing to alert Alexander.
The death of the son necessitated the death of the father, and thus Parmenion, who had been charged with guarding the treasury at Ecbatana, was assassinated at Alexander's command, to prevent attempts at vengeance.
Most infamously, Alexander personally killed the man who had saved his life at Granicus, Cleitus the Black, during a violent drunken altercation at Maracanda (modern day Samarkand in Uzbekistan), in which Cleitus accused Alexander of several judgmental mistakes and most especially, of having forgotten the Macedonian ways in favour of a corrupt oriental lifestyle.
Later, in the Central Asian campaign, a second plot against his life was revealed, this one instigated by his own royal pages.
His official historian, Callisthenes of Olynthus, was implicated in the plot, and in the "Anabasis of Alexander", Arrian states that Callisthenes and the pages were then tortured on the rack as punishment, and likely died soon after.
It remains unclear if Callisthenes was actually involved in the plot, for prior to his accusation he had fallen out of favour by leading the opposition to the attempt to introduce proskynesis.
When Alexander set out for Asia, he left his general Antipater, an experienced military and political leader and part of Philip II's "Old Guard", in charge of Macedon.
Alexander's sacking of Thebes ensured that Greece remained quiet during his absence.
The one exception was a call to arms by Spartan king Agis III in 331 BC, whom Antipater defeated and killed in the battle of Megalopolis.
Antipater referred the Spartans' punishment to the League of Corinth, which then deferred to Alexander, who chose to pardon them.
There was also considerable friction between Antipater and Olympias, and each complained to Alexander about the other.
In general, Greece enjoyed a period of peace and prosperity during Alexander's campaign in Asia.
Alexander sent back vast sums from his conquest, which stimulated the economy and increased trade across his empire.
However, Alexander's constant demands for troops and the migration of Macedonians throughout his empire depleted Macedon's strength, greatly weakening it in the years after Alexander, and ultimately led to its subjugation by Rome after the Third Macedonian War (171–168 BC).
After the death of Spitamenes and his marriage to Roxana (Raoxshna in Old Iranian) to cement relations with his new satrapies, Alexander turned to the Indian subcontinent.
He invited the chieftains of the former satrapy of Gandhara (a region presently straddling eastern Afghanistan and northern Pakistan), to come to him and submit to his authority.
Omphis (Indian name Ambhi), the ruler of Taxila, whose kingdom extended from the Indus to the Hydaspes (Jhelum), complied, but the chieftains of some hill clans, including the Aspasioi and Assakenoi sections of the Kambojas (known in Indian texts also as Ashvayanas and Ashvakayanas), refused to submit.
Ambhi hastened to relieve Alexander of his apprehension and met him with valuable presents, placing himself and all his forces at his disposal.
Alexander not only returned Ambhi his title and the gifts but he also presented him with a wardrobe of "Persian robes, gold and silver ornaments, 30 horses and 1,000 talents in gold".
Alexander was emboldened to divide his forces, and Ambhi assisted Hephaestion and Perdiccas in constructing a bridge over the Indus where it bends at Hund (Fox 1973), supplied their troops with provisions, and received Alexander himself, and his whole army, in his capital city of Taxila, with every demonstration of friendship and the most liberal hospitality.
On the subsequent advance of the Macedonian king, Taxiles accompanied him with a force of 5,000 men and took part in the battle of the Hydaspes River.
After that victory he was sent by Alexander in pursuit of Porus, to whom he was charged to offer favourable terms, but narrowly escaped losing his life at the hands of his old enemy.
Subsequently, however, the two rivals were reconciled by the personal mediation of Alexander; and Taxiles, after having contributed zealously to the equipment of the fleet on the Hydaspes, was entrusted by the king with the government of the whole territory between that river and the Indus.
A considerable accession of power was granted him after the death of Philip, son of Machatas; and he was allowed to retain his authority at the death of Alexander himself (323 BC), as well as in the subsequent partition of the provinces at Triparadisus, 321 BC.
In the winter of 327/326 BC, Alexander personally led a campaign against these clans; the Aspasioi of Kunar valleys, the Guraeans of the Guraeus valley, and the Assakenoi of the Swat and Buner valleys.
A fierce contest ensued with the Aspasioi in which Alexander was wounded in the shoulder by a dart, but eventually the Aspasioi lost.
Alexander then faced the Assakenoi, who fought in the strongholds of Massaga, Ora and Aornos.
The fort of Massaga was reduced only after days of bloody fighting, in which Alexander was wounded seriously in the ankle.
According to Curtius, "Not only did Alexander slaughter the entire population of Massaga, but also did he reduce its buildings to rubble."
A similar slaughter followed at Ora.
In the aftermath of Massaga and Ora, numerous Assakenians fled to the fortress of Aornos.
Alexander followed close behind and captured the strategic hill-fort after four bloody days.
After Aornos, Alexander crossed the Indus and fought and won an epic battle against King Porus, who ruled a region lying between the Hydaspes and the Acesines (Chenab), in what is now the Punjab, in the Battle of the Hydaspes in 326 BC.
Alexander was impressed by Porus' bravery, and made him an ally.
He appointed Porus as satrap, and added to Porus' territory land that he did not previously own, towards the south-east, up to the Hyphasis (Beas).
Choosing a local helped him control these lands so distant from Greece.
Alexander founded two cities on opposite sides of the Hydaspes river, naming one Bucephala, in honour of his horse, who died around this time.
The other was Nicaea (Victory), thought to be located at the site of modern-day Mong, Punjab.
East of Porus' kingdom, near the Ganges River, was the Nanda Empire of Magadha, and further east, the Gangaridai Empire of Bengal region of the Indian subcontinent.
Fearing the prospect of facing other large armies and exhausted by years of campaigning, Alexander's army mutinied at the Hyphasis River (Beas), refusing to march farther east.
This river thus marks the easternmost extent of Alexander's conquests.
Alexander tried to persuade his soldiers to march farther, but his general Coenus pleaded with him to change his opinion and return; the men, he said, "longed to again see their parents, their wives and children, their homeland".
Alexander eventually agreed and turned south, marching along the Indus.
Along the way his army conquered the Malhi (in modern-day Multan) and other Indian tribes and Alexander sustained an injury during the siege.
Alexander sent much of his army to Carmania (modern southern Iran) with general Craterus, and commissioned a fleet to explore the Persian Gulf shore under his admiral Nearchus, while he led the rest back to Persia through the more difficult southern route along the Gedrosian Desert and Makran.
Alexander reached Susa in 324 BC, but not before losing many men to the harsh desert.
Discovering that many of his satraps and military governors had misbehaved in his absence, Alexander executed several of them as examples on his way to Susa.
As a gesture of thanks, he paid off the debts of his soldiers, and announced that he would send over-aged and disabled veterans back to Macedon, led by Craterus.
His troops misunderstood his intention and mutinied at the town of Opis.
They refused to be sent away and criticized his adoption of Persian customs and dress and the introduction of Persian officers and soldiers into Macedonian units.
After three days, unable to persuade his men to back down, Alexander gave Persians command posts in the army and conferred Macedonian military titles upon Persian units.
The Macedonians quickly begged forgiveness, which Alexander accepted, and held a great banquet for several thousand of his men at which he and they ate together.
In an attempt to craft a lasting harmony between his Macedonian and Persian subjects, Alexander held a mass marriage of his senior officers to Persian and other noblewomen at Susa, but few of those marriages seem to have lasted much beyond a year.
Meanwhile, upon his return to Persia, Alexander learned that guards of the tomb of Cyrus the Great in Pasargadae had desecrated it, and swiftly executed them.
Alexander admired Cyrus the Great, from an early age reading Xenophon's "Cyropaedia", which described Cyrus's heroism in battle and governance as a king and legislator.
During his visit to Pasargadae Alexander ordered his architect Aristobulus to decorate the interior of the sepulchral chamber of Cyrus' tomb.
Afterwards, Alexander travelled to Ecbatana to retrieve the bulk of the Persian treasure.
There, his closest friend and possible lover, Hephaestion, died of illness or poisoning.
Hephaestion's death devastated Alexander, and he ordered the preparation of an expensive funeral pyre in Babylon, as well as a decree for public mourning.
Back in Babylon, Alexander planned a series of new campaigns, beginning with an invasion of Arabia, but he would not have a chance to realize them, as he died shortly after Hephaestion.
On either 10 or 11 June 323 BC, Alexander died in the palace of Nebuchadnezzar II, in Babylon, at age 32.
There are two different versions of Alexander's death and details of the death differ slightly in each.
Plutarch's account is that roughly 14 days before his death, Alexander entertained admiral Nearchus, and spent the night and next day drinking with Medius of Larissa.
He developed a fever, which worsened until he was unable to speak.
The common soldiers, anxious about his health, were granted the right to file past him as he silently waved at them.
In the second account, Diodorus recounts that Alexander was struck with pain after downing a large bowl of unmixed wine in honour of Heracles, followed by 11 days of weakness; he did not develop a fever and died after some agony.
Arrian also mentioned this as an alternative, but Plutarch specifically denied this claim.
Given the propensity of the Macedonian aristocracy to assassination, foul play featured in multiple accounts of his death.
Diodorus, Plutarch, Arrian and Justin all mentioned the theory that Alexander was poisoned.
Justin stated that Alexander was the victim of a poisoning conspiracy, Plutarch dismissed it as a fabrication, while both Diodorus and Arrian noted that they mentioned it only for the sake of completeness.
The accounts were nevertheless fairly consistent in designating Antipater, recently removed as Macedonian viceroy, and at odds with Olympias, as the head of the alleged plot.
Perhaps taking his summons to Babylon as a death sentence, and having seen the fate of Parmenion and Philotas, Antipater purportedly arranged for Alexander to be poisoned by his son Iollas, who was Alexander's wine-pourer.
There was even a suggestion that Aristotle may have participated.
The strongest argument against the poison theory is the fact that twelve days passed between the start of his illness and his death; such long-acting poisons were probably not available.
However, in a 2003 BBC documentary investigating the death of Alexander, Leo Schep from the New Zealand National Poisons Centre proposed that the plant white hellebore ("Veratrum album"), which was known in antiquity, may have been used to poison Alexander.
In a 2014 manuscript in the journal Clinical Toxicology, Schep suggested Alexander's wine was spiked with "Veratrum album", and that this would produce poisoning symptoms that match the course of events described in the "Alexander Romance".
"Veratrum album" poisoning can have a prolonged course and it was suggested that if Alexander was poisoned, "Veratrum album" offers the most plausible cause.
Another poisoning explanation put forward in 2010 proposed that the circumstances of his death were compatible with poisoning by water of the river Styx (modern-day Mavroneri in Arcadia, Greece) that contained calicheamicin, a dangerous compound produced by bacteria.
Several natural causes (diseases) have been suggested, including malaria and typhoid fever.
A 1998 article in the "New England Journal of Medicine" attributed his death to typhoid fever complicated by bowel perforation and ascending paralysis.
Another recent analysis suggested pyogenic (infectious) spondylitis or meningitis.
Other illnesses fit the symptoms, including acute pancreatitis and West Nile virus.
Natural-cause theories also tend to emphasize that Alexander's health may have been in general decline after years of heavy drinking and severe wounds.
The anguish that Alexander felt after Hephaestion's death may also have contributed to his declining health.
Alexander's body was laid in a gold anthropoid sarcophagus that was filled with honey, which was in turn placed in a gold casket.
According to Aelian, a seer called Aristander foretold that the land where Alexander was laid to rest "would be happy and unvanquishable forever".
Perhaps more likely, the successors may have seen possession of the body as a symbol of legitimacy, since burying the prior king was a royal prerogative.
While Alexander's funeral cortege was on its way to Macedon, Ptolemy seized it and took it temporarily to Memphis.
His successor, Ptolemy II Philadelphus, transferred the sarcophagus to Alexandria, where it remained until at least late Antiquity.
Ptolemy IX Lathyros, one of Ptolemy's final successors, replaced Alexander's sarcophagus with a glass one so he could convert the original to coinage.
The recent discovery of an enormous tomb in northern Greece, at Amphipolis, dating from the time of Alexander the Great has given rise to speculation that its original intent was to be the burial place of Alexander.
This would fit with the intended destination of Alexander's funeral cortege.
Pompey, Julius Caesar and Augustus all visited the tomb in Alexandria, where Augustus, allegedly, accidentally knocked the nose off.
Caligula was said to have taken Alexander's breastplate from the tomb for his own use.
Around AD 200, Emperor Septimius Severus closed Alexander's tomb to the public.
His son and successor, Caracalla, a great admirer, visited the tomb during his own reign.
After this, details on the fate of the tomb are hazy.
The so-called "Alexander Sarcophagus", discovered near Sidon and now in the Istanbul Archaeology Museum, is so named not because it was thought to have contained Alexander's remains, but because its bas-reliefs depict Alexander and his companions fighting the Persians and hunting.
It was originally thought to have been the sarcophagus of Abdalonymus (died 311 BC), the king of Sidon appointed by Alexander immediately following the battle of Issus in 331.
However, more recently, it has been suggested that it may date from earlier than Abdalonymus' death.
Alexander's death was so sudden that when reports of his death reached Greece, they were not immediately believed.
Alexander had no obvious or legitimate heir, his son Alexander IV by Roxane being born after Alexander's death.
According to Diodorus, Alexander's companions asked him on his deathbed to whom he bequeathed his kingdom; his laconic reply was "tôi kratistôi"—"to the strongest".
Another theory is that his successors willfully or erroneously misheard "tôi Kraterôi"—"to Craterus", the general leading his Macedonian troops home and newly entrusted with the regency of Macedonia.
Arrian and Plutarch claimed that Alexander was speechless by this point, implying that this was an apocryphal story.
Diodorus, Curtius and Justin offered the more plausible story that Alexander passed his signet ring to Perdiccas, a bodyguard and leader of the companion cavalry, in front of witnesses, thereby nominating him.
Perdiccas initially did not claim power, instead suggesting that Roxane's baby would be king, if male; with himself, Craterus, Leonnatus, and Antipater as guardians.
However, the infantry, under the command of Meleager, rejected this arrangement since they had been excluded from the discussion.
Instead, they supported Alexander's half-brother Philip Arrhidaeus.
Eventually, the two sides reconciled, and after the birth of Alexander IV, he and Philip III were appointed joint kings, albeit in name only.
Dissension and rivalry soon afflicted the Macedonians, however.
The satrapies handed out by Perdiccas at the Partition of Babylon became power bases each general used to bid for power.
After the assassination of Perdiccas in 321 BC, Macedonian unity collapsed, and 40 years of war between "The Successors" ("Diadochi") ensued before the Hellenistic world settled into four stable power blocs: Ptolemaic Egypt, Seleucid Mesopotamia and Central Asia, Attalid Anatolia, and Antigonid Macedon.
In the process, both Alexander IV and Philip III were murdered.
Diodorus stated that Alexander had given detailed written instructions to Craterus some time before his death.
Craterus started to carry out Alexander's commands, but the successors chose not to further implement them, on the grounds they were impractical and extravagant.
Nevertheless, Perdiccas read Alexander's will to his troops.
Alexander's will called for military expansion into the southern and western Mediterranean, monumental constructions, and the intermixing of Eastern and Western populations.
It included:

Alexander earned the epithet "the Great" due to his unparalleled success as a military commander.
He never lost a battle, despite typically being outnumbered.
This was due to use of terrain, phalanx and cavalry tactics, bold strategy, and the fierce loyalty of his troops.
The Macedonian phalanx, armed with the sarissa, a spear long, had been developed and perfected by Philip II through rigorous training, and Alexander used its speed and maneuverability to great effect against larger but more disparate Persian forces.
Alexander also recognized the potential for disunity among his diverse army, which employed various languages and weapons.
He overcame this by being personally involved in battle, in the manner of a Macedonian king.
In his first battle in Asia, at Granicus, Alexander used only a small part of his forces, perhaps 13,000 infantry with 5,000 cavalry, against a much larger Persian force of 40,000.
Alexander placed the phalanx at the center and cavalry and archers on the wings, so that his line matched the length of the Persian cavalry line, about .
By contrast, the Persian infantry was stationed behind its cavalry.
This ensured that Alexander would not be outflanked, while his phalanx, armed with long pikes, had a considerable advantage over the Persians' scimitars and javelins.
Macedonian losses were negligible compared to those of the Persians.
At Issus in 333 BC, his first confrontation with Darius, he used the same deployment, and again the central phalanx pushed through.
Alexander personally led the charge in the center, routing the opposing army.
At the decisive encounter with Darius at Gaugamela, Darius equipped his chariots with scythes on the wheels to break up the phalanx and equipped his cavalry with pikes.
Alexander arranged a double phalanx, with the center advancing at an angle, parting when the chariots bore down and then reforming.
The advance was successful and broke Darius' center, causing the latter to flee once again.
When faced with opponents who used unfamiliar fighting techniques, such as in Central Asia and India, Alexander adapted his forces to his opponents' style.
Thus, in Bactria and Sogdiana, Alexander successfully used his javelin throwers and archers to prevent outflanking movements, while massing his cavalry at the center.
In India, confronted by Porus' elephant corps, the Macedonians opened their ranks to envelop the elephants and used their sarissas to strike upwards and dislodge the elephants' handlers.
Greek biographer Plutarch () describes Alexander's appearance as:

Greek historian Arrian (Lucius Flavius Arrianus 'Xenophon' ) described Alexander as:

The semi-legendary "Alexander Romance" also suggests that Alexander exhibited heterochromia iridum: that one eye was dark and the other light.
British historian Peter Green provided a description of Alexander's appearance, based on his review of statues and some ancient documents:

Ancient authors recorded that Alexander was so pleased with portraits of himself created by Lysippos that he forbade other sculptors from crafting his image.
Lysippos had often used the contrapposto sculptural scheme to portray Alexander and other characters such as Apoxyomenos, Hermes and Eros.
Lysippos' sculpture, famous for its naturalism, as opposed to a stiffer, more static pose, is thought to be the most faithful depiction.
Some of Alexander's strongest personality traits formed in response to his parents.
His mother had huge ambitions, and encouraged him to believe it was his destiny to conquer the Persian Empire.
Olympias' influence instilled a sense of destiny in him, and Plutarch tells how his ambition "kept his spirit serious and lofty in advance of his years".
However, his father Philip was Alexander's most immediate and influential role model, as the young Alexander watched him campaign practically every year, winning victory after victory while ignoring severe wounds.
Alexander's relationship with his father forged the competitive side of his personality; he had a need to out-do his father, illustrated by his reckless behaviour in battle.
While Alexander worried that his father would leave him "no great or brilliant achievement to be displayed to the world", he also downplayed his father's achievements to his companions.
According to Plutarch, among Alexander's traits were a violent temper and rash, impulsive nature, which undoubtedly contributed to some of his decisions.
Although Alexander was stubborn and did not respond well to orders from his father, he was open to reasoned debate.
He had a calmer side—perceptive, logical, and calculating.
He had a great desire for knowledge, a love for philosophy, and was an avid reader.
This was no doubt in part due to Aristotle's tutelage; Alexander was intelligent and quick to learn.
His intelligent and rational side was amply demonstrated by his ability and success as a general.
He had great self-restraint in "pleasures of the body", in contrast with his lack of self-control with alcohol.
Alexander was erudite and patronized both arts and sciences.
However, he had little interest in sports or the Olympic games (unlike his father), seeking only the Homeric ideals of honour ("timê") and glory ("kudos").
He had great charisma and force of personality, characteristics which made him a great leader.
His unique abilities were further demonstrated by the inability of any of his generals to unite Macedonia and retain the Empire after his death—only Alexander had the ability to do so.
During his final years, and especially after the death of Hephaestion, Alexander began to exhibit signs of megalomania and paranoia.
His extraordinary achievements, coupled with his own ineffable sense of destiny and the flattery of his companions, may have combined to produce this effect.
His delusions of grandeur are readily visible in his will and in his desire to conquer the world, in as much as he is by various sources described as having "boundless ambition", an epithet, the meaning of which has descended into an historical cliché.
He appears to have believed himself a deity, or at least sought to deify himself.
Olympias always insisted to him that he was the son of Zeus, a theory apparently confirmed to him by the oracle of Amun at Siwa.
He began to identify himself as the son of Zeus-Ammon.
Alexander adopted elements of Persian dress and customs at court, notably "proskynesis", a practice of which Macedonians disapproved, and were loath to perform.
This behaviour cost him the sympathies of many of his countrymen.
However, Alexander also was a pragmatic ruler who understood the difficulties of ruling culturally disparate peoples, many of whom lived in kingdoms where the king was divine.
Thus, rather than megalomania, his behaviour may simply have been a practical attempt at strengthening his rule and keeping his empire together.
Alexander married three times: Roxana, daughter of the Sogdian nobleman Oxyartes of Bactria, out of love; and the Persian princesses Stateira II and Parysatis II, the former a daughter of Darius III and latter a daughter of Artaxerxes III, for political reasons.
He apparently had two sons, Alexander IV of Macedon by Roxana and, possibly, Heracles of Macedon from his mistress Barsine.
He lost another child when Roxana miscarried at Babylon.
Alexander also had a close relationship with his friend, general, and bodyguard Hephaestion, the son of a Macedonian noble.
Hephaestion's death devastated Alexander.
This event may have contributed to Alexander's failing health and detached mental state during his final months.
Alexander's sexuality has been the subject of speculation and controversy.
No ancient sources stated that Alexander had homosexual relationships, or that Alexander's relationship with Hephaestion was sexual.
Aelian, however, writes of Alexander's visit to Troy where "Alexander garlanded the tomb of Achilles, and Hephaestion that of Patroclus, the latter hinting that he was a beloved of Alexander, in just the same way as Patroclus was of Achilles."
Noting that the word "eromenos" (ancient Greek for beloved) does not necessarily bear sexual meaning.
Alexander may have been bisexual, in keeping with Greek upper class custom.
Green argues that there is little evidence in ancient sources that Alexander had much carnal interest in women; he did not produce an heir until the very end of his life.
However, he was relatively young when he died, and Ogden suggests that Alexander's matrimonial record is more impressive than his father's at the same age.
Apart from wives, Alexander had many more female companions.
Alexander accumulated a harem in the style of Persian kings, but he used it rather sparingly, showing great self-control in "pleasures of the body".
Nevertheless, Plutarch described how Alexander was infatuated by Roxana while complimenting him on not forcing himself on her.
Green suggested that, in the context of the period, Alexander formed quite strong friendships with women, including Ada of Caria, who adopted him, and even Darius' mother Sisygambis, who supposedly died from grief upon hearing of Alexander's death.
Alexander's legacy extended beyond his military conquests.
His campaigns greatly increased contacts and trade between East and West, and vast areas to the east were significantly exposed to Greek civilization and influence.
Some of the cities he founded became major cultural centers, many surviving into the 21st century.
His chroniclers recorded valuable information about the areas through which he marched, while the Greeks themselves got a sense of belonging to a world beyond the Mediterranean.
Alexander's most immediate legacy was the introduction of Macedonian rule to huge new swathes of Asia.
At the time of his death, Alexander's empire covered some , and was the largest state of its time.
Many of these areas remained in Macedonian hands or under Greek influence for the next 200–300 years.
The successor states that emerged were, at least initially, dominant forces, and these 300 years are often referred to as the Hellenistic period.
The eastern borders of Alexander's empire began to collapse even during his lifetime.
However, the power vacuum he left in the northwest of the Indian subcontinent directly gave rise to one of the most powerful Indian dynasties in history, the Maurya Empire.
Taking advantage of this power vacuum, Chandragupta Maurya (referred to in Greek sources as "Sandrokottos"), of relatively humble origin, took control of the Punjab, and with that power base proceeded to conquer the Nanda Empire.
Over the course of his conquests, Alexander founded some twenty cities that bore his name, most of them east of the Tigris.
The first, and greatest, was Alexandria in Egypt, which would become one of the leading Mediterranean cities.
The cities' locations reflected trade routes as well as defensive positions.
At first, the cities must have been inhospitable, little more than defensive garrisons.
Following Alexander's death, many Greeks who had settled there tried to return to Greece.
However, a century or so after Alexander's death, many of the Alexandrias were thriving, with elaborate public buildings and substantial populations that included both Greek and local peoples.
In 334 BC, Alexander the Great donated funds for the completion of the new temple of Athena Polias in Priene.
An inscription from the temple, now housed in the British Museum, declares: "King Alexander dedicated [this temple] to Athena Polias."
This inscription is one of the few independent archaeological discoveries confirming an episode from Alexander's life.
The temple was designed by Pytheos, one of the architects of the Mausoleum at Halicarnassus.
"Hellenization" was coined by the German historian Johann Gustav Droysen to denote the spread of Greek language, culture, and population into the former Persian empire after Alexander's conquest.
That this export took place is undoubted, and can be seen in the great Hellenistic cities of, for instance, Alexandria, Antioch and Seleucia (south of modern Baghdad).
Alexander sought to insert Greek elements into Persian culture and attempted to hybridize Greek and Persian culture.
This culminated in his aspiration to homogenize the populations of Asia and Europe.
However, his successors explicitly rejected such policies.
Nevertheless, Hellenization occurred throughout the region, accompanied by a distinct and opposite 'Orientalization' of the successor states.
The core of the Hellenistic culture promulgated by the conquests was essentially Athenian.
The close association of men from across Greece in Alexander's army directly led to the emergence of the largely Attic-based "koine", or "common" Greek dialect.
Koine spread throughout the Hellenistic world, becoming the lingua franca of Hellenistic lands and eventually the ancestor of modern Greek.
Furthermore, town planning, education, local government, and art current in the Hellenistic period were all based on Classical Greek ideals, evolving into distinct new forms commonly grouped as Hellenistic.
Aspects of Hellenistic culture were still evident in the traditions of the Byzantine Empire in the mid-15th century.
Some of the most pronounced effects of Hellenization can be seen in Afghanistan and India, in the region of the relatively late-rising Greco-Bactrian Kingdom (250–125 BC) (in modern Afghanistan, Pakistan, and Tajikistan) and the Indo-Greek Kingdom (180 BC – 10 AD) in modern Afghanistan and India.
There on the newly formed Silk Road Greek culture apparently hybridized with Indian, and especially Buddhist culture.
The resulting syncretism known as Greco-Buddhism heavily influenced the development of Buddhism and created a culture of Greco-Buddhist art.
These Greco-Buddhist kingdoms sent some of the first Buddhist missionaries to China, Sri Lanka, and the Mediterranean (Greco-Buddhist monasticism).
Some of the first and most influential figurative portrayals of the Buddha appeared at this time, perhaps modeled on Greek statues of Apollo in the Greco-Buddhist style.
Several Buddhist traditions may have been influenced by the ancient Greek religion: the concept of Boddhisatvas is reminiscent of Greek divine heroes, and some Mahayana ceremonial practices (burning incense, gifts of flowers, and food placed on altars) are similar to those practiced by the ancient Greeks; however, similar practices were also observed amongst the native Indic culture.
One Greek king, Menander I, probably became Buddhist, and was immortalized in Buddhist literature as 'Milinda'.
The process of Hellenization also spurred trade between the east and west.
For example, Greek astronomical instruments dating to the 3rd century BC were found in the Greco-Bactrian city of Ai Khanoum in modern-day Afghanistan, while the Greek concept of a spherical earth surrounded by the spheres of planets eventually supplanted the long-standing Indian cosmological belief of a disc consisting of four continents grouped around a central mountain (Mount Meru) like the petals of a flower.
The Yavanajataka (lit.
Greek astronomical treatise) and Paulisa Siddhanta texts depict the influence of Greek astronomical ideas on Indian astronomy.
Following the conquests of Alexander the Great in the east, Hellenistic influence on Indian art was far-ranging.
In the area of architecture, a few examples of the Ionic order can be found as far as Pakistan with the Jandial temple near Taxila.
Several examples of capitals displaying Ionic influences can be seen as far as Patna, especially with the Pataliputra capital, dated to the 3rd century BC.
The Corinthian order is also heavily represented in the art of Gandhara, especially through Indo-Corinthian capitals.
Alexander and his exploits were admired by many Romans, especially generals, who wanted to associate themselves with his achievements.
Polybius began his "Histories" by reminding Romans of Alexander's achievements, and thereafter Roman leaders saw him as a role model.
Pompey the Great adopted the epithet "Magnus" and even Alexander's anastole-type haircut, and searched the conquered lands of the east for Alexander's 260-year-old cloak, which he then wore as a sign of greatness.
Julius Caesar dedicated a Lysippean equestrian bronze statue but replaced Alexander's head with his own, while Octavian visited Alexander's tomb in Alexandria and temporarily changed his seal from a sphinx to Alexander's profile.
The emperor Trajan also admired Alexander, as did Nero and Caracalla.
The Macriani, a Roman family that in the person of Macrinus briefly ascended to the imperial throne, kept images of Alexander on their persons, either on jewelry, or embroidered into their clothes.
On the other hand, some Roman writers, particularly Republican figures, used Alexander as a cautionary tale of how autocratic tendencies can be kept in check by republican values.
Alexander was used by these writers as an example of ruler values such as (friendship) and (clemency), but also (anger) and (over-desire for glory).
Legendary accounts surround the life of Alexander the Great, many deriving from his own lifetime, probably encouraged by Alexander himself.
His court historian Callisthenes portrayed the sea in Cilicia as drawing back from him in proskynesis.
Writing shortly after Alexander's death, another participant, Onesicritus, invented a tryst between Alexander and Thalestris, queen of the mythical Amazons.
When Onesicritus read this passage to his patron, Alexander's general and later King Lysimachus reportedly quipped, "I wonder where I was at the time."
In the first centuries after Alexander's death, probably in Alexandria, a quantity of the legendary material coalesced into a text known as the "Alexander Romance", later falsely ascribed to Callisthenes and therefore known as "Pseudo-Callisthenes".
This text underwent numerous expansions and revisions throughout Antiquity and the Middle Ages, containing many dubious stories, and was translated into numerous languages.
Alexander the Great's accomplishments and legacy have been depicted in many cultures.
Alexander has figured in both high and popular culture beginning in his own era to the present day.
The "Alexander Romance", in particular, has had a significant impact on portrayals of Alexander in later cultures, from Persian to medieval European to modern Greek.
Alexander features prominently in modern Greek folklore, more so than any other ancient figure.
The colloquial form of his name in modern Greek ("O Megalexandros") is a household name, and he is the only ancient hero to appear in the Karagiozis shadow play.
One well-known fable among Greek seamen involves a solitary mermaid who would grasp a ship's prow during a storm and ask the captain "Is King Alexander alive?"
The correct answer is "He is alive and well and rules the world!"
causing the mermaid to vanish and the sea to calm.
Any other answer would cause the mermaid to turn into a raging Gorgon who would drag the ship to the bottom of the sea, all hands aboard.
In pre-Islamic Middle Persian (Zoroastrian) literature, Alexander is referred to by the epithet "gujastak", meaning "accursed", and is accused of destroying temples and burning the sacred texts of Zoroastrianism.
In Sunni Islamic Persia, under the influence of the "Alexander Romance" (in "Iskandarnamah"), a more positive portrayal of Alexander emerges.
Firdausi's "Shahnameh" ("The Book of Kings") includes Alexander in a line of legitimate Persian shahs, a mythical figure who explored the far reaches of the world in search of the Fountain of Youth.
Later Persian writers associate him with philosophy, portraying him at a symposium with figures such as Socrates, Plato and Aristotle, in search of immortality.
The figure of Dhul-Qarnayn (literally "the Two-Horned One") mentioned in the Quran is believed by some scholars to represent Alexander, due to parallels with the "Alexander Romance".
In this tradition, he was a heroic figure who built a wall to defend against the nations of Gog and Magog.
He then travelled the known world in search of the Water of Life and Immortality, eventually becoming a prophet.
The Syriac version of the "Alexander Romance" portrays him as an ideal Christian world conqueror who prayed to "the one true God".
In Egypt, Alexander was portrayed as the son of Nectanebo II, the last pharaoh before the Persian conquest.
His defeat of Darius was depicted as Egypt's salvation, "proving" Egypt was still ruled by an Egyptian.
According to Josephus, Alexander was shown the Book of Daniel when he entered Jerusalem, which described a mighty Greek king who would conquer the Persian Empire.
This is cited as a reason for sparing Jerusalem.
In Hindi and Urdu, the name "Sikandar", derived from Persian, denotes a rising young talent.
In medieval Europe, Alexander the Great was revered as a member of the Nine Worthies, a group of heroes whose lives were believed to encapsulate all the ideal qualities of chivalry.
Irish playwright Aubrey Thomas de Vere wrote "Alexander the Great, a Dramatic Poem".
Apart from a few inscriptions and fragments, texts written by people who actually knew Alexander or who gathered information from men who served with Alexander were all lost.
Contemporaries who wrote accounts of his life included Alexander's campaign historian Callisthenes; Alexander's generals Ptolemy and Nearchus; Aristobulus, a junior officer on the campaigns; and Onesicritus, Alexander's chief helmsman.
Their works are lost, but later works based on these original sources have survived.
The earliest of these is Diodorus Siculus (1st century BC), followed by Quintus Curtius Rufus (mid-to-late 1st century AD), Arrian (1st to 2nd century AD), the biographer Plutarch (1st to 2nd century AD), and finally Justin, whose work dated as late as the 4th century.
Of these, Arrian is generally considered the most reliable, given that he used Ptolemy and Aristobulus as his sources, closely followed by Diodorus.
</doc>
<doc id="784" url="https://en.wikipedia.org/wiki?curid=784" title="Alfred Korzybski">
Alfred Korzybski

Alfred Habdank Skarbek Korzybski (; July 3, 1879 – March 1, 1950) was a Polish-American independent scholar who developed a field called general semantics, which he viewed as both distinct from, and more encompassing than, the field of semantics.
He argued that human knowledge of the world is limited both by the human nervous system and the languages humans have developed, and thus no one can have direct access to reality, given that the most we can know is that which is filtered through the brain's responses to reality.
His best known dictum is "The map is not the territory".
Born in Warsaw, Poland, then part of the Russian Empire, Korzybski belonged to an aristocratic Polish family whose members had worked as mathematicians, scientists, and engineers for generations.
He learned the Polish language at home and the Russian language in schools; and having a French and German governess, he became fluent in four languages as a child.
Korzybski studied engineering at the Warsaw University of Technology.
During the First World War (1914-1918) Korzybski served as an intelligence officer in the Russian Army.
After being wounded in a leg and suffering other injuries, he moved to North America in 1916 (first to Canada, then to the United States) to coordinate the shipment of artillery to Russia.
He also lectured to Polish-American audiences about the conflict, promoting the sale of war bonds.
After the war he decided to remain in the United States, becoming a naturalized citizen in 1940.
He met Mira Edgerly,
a painter of portraits on ivory, shortly after the1918 Armistice; They married in January 1919; the marriage lasted until his death.
E. P. Dutton published Korzybski's first book, "Manhood of Humanity", in 1921.
In this work he proposed and explained in detail a new theory of humankind: mankind as a "time-binding" class of life (humans perform time binding by the transmission of knowledge and abstractions through time which become accreted in cultures).
Korzybski's work culminated in the initiation of a discipline that he named general semantics (GS).
This should not be confused with semantics.
The basic principles of general semantics, which include time-binding, are described in the publication "Science and Sanity", published in 1933.
In 1938 Korzybski founded the Institute of General Semantics in Chicago.
The post-World War II housing shortage in Chicago cost him the Institute's building lease, so in 1946 he moved the Institute to Lakeville, Connecticut, U.S., where he directed it until his death in 1950.
Korzybski maintained that humans are limited in what they know by (1) the structure of their nervous systems, and (2) the structure of their languages.
Humans cannot experience the world directly, but only through their "abstractions" (nonverbal impressions or "gleanings" derived from the nervous system, and verbal indicators expressed and derived from language).
These sometimes mislead us about what is the truth.
Our understanding sometimes lacks "similarity of structure" with what is actually happening.
He sought to train our awareness of abstracting, using techniques he had derived from his study of mathematics and science.
He called this awareness, this goal of his system, "consciousness of abstracting".
His system included the promotion of attitudes such as "I don't know; let's see," in order that we may better discover or reflect on its realities as revealed by modern science.
Another technique involved becoming inwardly and outwardly quiet, an experience he termed, "silence on the objective levels".
Many devotees and critics of Korzybski reduced his rather complex system to a simple matter of what he said about the verb form "is" of the general verb "to be."
His system, however, is based primarily on such terminology as the different "orders of abstraction," and formulations such as "consciousness of abstracting."
The contention that Korzybski "opposed" the use of the verb "to be" would be a profound exaggeration.
He thought that "certain uses" of the verb "to be", called the "is of identity" and the "is of predication", were faulty in structure, e.g., a statement such as, "Elizabeth is a fool" (said of a person named "Elizabeth" who has done something that we regard as foolish).
In Korzybski's system, one's assessment of Elizabeth belongs to a higher order of abstraction than Elizabeth herself.
Korzybski's remedy was to "deny" identity; in this example, to be aware continually that "Elizabeth" is "not" what we "call" her.
We find Elizabeth not in the verbal domain, the world of words, but the nonverbal domain (the two, he said, amount to different orders of abstraction).
This was expressed by Korzybski's most famous premise, "the map is not the territory".
Note that this premise uses the phrase "is not", a form of "to be"; this and many other examples show that he did not intend to abandon "to be" as such.
In fact, he said explicitly that there were no structural problems with the verb "to be" when used as an auxiliary verb or when used to state existence or location.
It was even acceptable at times to use the faulty forms of the verb "to be," as long as one was aware of their structural limitations.
One day, Korzybski was giving a lecture to a group of students, and he interrupted the lesson suddenly in order to retrieve a packet of biscuits, wrapped in white paper, from his briefcase.
He muttered that he just had to eat something, and he asked the students on the seats in the front row if they would also like a biscuit.
A few students took a biscuit.
"Nice biscuit, don't you think," said Korzybski, while he took a second one.
The students were chewing vigorously.
Then he tore the white paper from the biscuits, in order to reveal the original packaging.
On it was a big picture of a dog's head and the words "Dog Cookies."
The students looked at the package, and were shocked.
Two of them wanted to vomit, put their hands in front of their mouths, and ran out of the lecture hall to the toilet.
"You see," Korzybski remarked, "I have just demonstrated that people don't just eat food, but also words, and that the taste of the former is often outdone by the taste of the latter."
William Burroughs went to a Korzybski workshop in the Autumn of 1939.
He was 25 years old, and paid $40.
His fellow students—there were 38 in all—included young Samuel I. Hayakawa (later to become a Republican member of the U.S.
Senate), Ralph Moriarty deBit (later to become the spiritual teacher Vitvan) and Wendell Johnson (founder of the Monster Study).
Korzybski was well received in numerous disciplines, as evidenced by the positive reactions from leading figures in the sciences and humanities in the 1940s and 1950s.
As reported in the third edition of "Science and Sanity", in World War II the US Army used Korzybski's system to treat battle fatigue in Europe, under the supervision of Dr. Douglas M. Kelley, who went on to become the psychiatrist in charge of the Nazi war criminals at Nuremberg.
Some of the General Semantics tradition was continued by Samuel I. Hayakawa.
</doc>
<doc id="785" url="https://en.wikipedia.org/wiki?curid=785" title="Asteroids (video game)">
Asteroids (video game)

Asteroids is an arcade space shooter released in November 1979 by Atari, Inc. and designed by Lyle Rains, Ed Logg, and Dominic Walsh.
The player controls a spaceship in an asteroid field which is periodically traversed by flying saucers.
The object of the game is to shoot and destroy asteroids and saucers while not colliding with either or being hit by the saucers' counter-fire.
The game becomes harder as the number of asteroids increases.
"Asteroids" was one of the first major hits of the golden age of arcade games.
The game sold over 70,000 arcade cabinets and proved both popular with players and influential with developers.
It has since been ported to multiple platforms.
"Asteroids" was widely imitated and directly influenced "Defender", "Gravitar", and many other video games.
"Asteroids" was conceived during a meeting between Logg and Rains and used hardware developed by Howard Delman previously used for "Lunar Lander".
Based on an unfinished game titled "Cosmos" and inspired by "Spacewar!
", "Computer Space", and "Space Invaders", "Asteroids" physics model, control scheme and gameplay theme were derived from these earlier games and refined through trial and error.
The game is rendered on a vector display in a two-dimensional view that wraps around in both screen axes.
The objective of "Asteroids" is to destroy asteroids and saucers.
The player controls a triangular ship that can rotate left and right, fire shots straight forward, and thrust forward.
Once the ship begins moving in a direction, it will continue in that direction for a time without player intervention unless the player applies thrust in a different direction.
The ship eventually comes to a stop when not thrusting.
The player can also send the ship into hyperspace, causing it to disappear and reappear in a random location on the screen, at the risk of self-destructing or appearing on top of an asteroid.
Each level starts with a few large asteroids drifting in various directions on the screen.
Objects wrap around screen edges – for instance, an asteroid that drifts off the top edge of the screen reappears at the bottom and continues moving in the same direction.
As the player shoots asteroids, they break into smaller asteroids that move faster and are more difficult to hit.
Smaller asteroids are also worth more points.
Two flying saucers appear periodically on the screen; the "big saucer" shoots randomly and poorly, while the "small saucer" fires frequently at the ship.
After reaching a score of 40,000, only the small saucer appears.
As the player's score increases, the angle range of the shots from the small saucer diminishes until the saucer fires extremely accurately.
Once the screen has been cleared of all asteroids and flying saucers, a new set of large asteroids appears, thus starting the next level.
The game gets harder as the number of asteroids increases until after the score reaches a range between 40,000 and 60,000.
The player starts with 3-5 lives upon game start and gains an extra life per 10,000 points.
When the player loses all their lives, the game ends.
Machine "turns over" at 99,990 points, which is the maximum high score that can be achieved.
"Asteroids" contains several bugs.
The game slows down as the player gains 50-100 lives, due to a programming error in that there is no limit for the permitted number of lives.
The player can "lose" the game after more than 250 lives are collected.
"Asteroids" was conceived by Lyle Rains and programmed by Ed Logg with collaborations from other Atari staff.
Logg was impressed with the Atari Video Computer System (later called the Atari 2600), and joined Atari's coin-op division and worked on "Dirt Bike", which was never released due to an unsuccessful field test.
Paul Mancuso joined the development team as "Asteroids" technician and engineer Howard Delman contributed to the hardware.
During a meeting in April 1979, Rains discussed "Planet Grab", a multiplayer arcade game later renamed to "Cosmos".
Logg did not know the name of the game, thinking "Computer Space" as "the inspiration for the two-dimensional approach".
Rains conceived of "Asteroids" as a mixture of "Computer Space" and "Space Invaders", combining the two-dimensional approach of "Computer Space" with "Space Invaders" addictive gameplay of "completion" and "eliminate all threats".
The unfinished game featured a giant, indestructible asteroid, so Rains asked Logg: "Well, why don’t we have a game where you shoot the rocks and blow them up?"
In response, Logg described a similar concept where the player selectively shoots at rocks that break into smaller pieces.
Both agreed on the concept.
"Asteroids" was implemented on hardware developed by Delman and is a vector game, in which the graphics are composed of lines drawn on a vector monitor.
Rains initially wanted the game done in raster graphics, but Logg, experienced in vector graphics, suggested an XY monitor because the high image quality would permit precise aiming.
The hardware is chiefly a MOS 6502 executing the game program, and QuadraScan, a high-resolution vector graphics processor developed by Atari and referred to as an "XY display system" and the "Digital Vector Generator (DVG)".
The original design concepts for QuadraScan came out of Cyan Engineering, Atari's off-campus research lab in Grass Valley, California, in 1978.
Cyan gave it to Delman, who finished the design and first used it for "Lunar Lander".
Logg received Delman's modified board with five buttons, 13 sound effects, and additional RAM, and used it to develop "Asteroids".
The size of the board was 4 by 4 inches, and it was "linked up" to a monitor.
Logg modeled the player's ship, the five-button control scheme, and the game physics after "Spacewar!
", which he had played as a student at the University of California, Berkeley, but made several changes to improve playability.
The ship was programmed into the hardware and rendered by the monitor, and was configured to move with thrust and inertia.
The hyperspace button was not placed near Logg's right thumb, which he was dissatisfied with, as he had a problem "tak[ing] his hand off the thrust button".
Drawings of asteroids in various shapes were incorporated into the game.
Logg copied the idea of a high score table with initials from Exidy's "Star Fire".
The two saucers were formulated to be different from each other.
A steadily decreasing timer that shortens intervals between saucer attacks was employed to keep the player from not shooting asteroids and saucers.
The minimalist soundtrack features a "heartbeat" sound effect, which quickens as the game progresses.
The game did not have a sound chip, so Delman created a hardware circuit for 13 sound effects by hand which was wired onto the board.
A prototype of "Asteroids" was well received by several Atari staff and engineers, who would "wander between labs, passing comment and 
stopping to play as they went".
Logg was often asked when he would be leaving by employees eager to play the prototype, so he created a second prototype specifically for staff to play.
Atari went to Sacramento, California for testing, setting up prototypes of the game in local arcades to measure its potential success.
The company also observed veteran players and younger players during focus group sessions at Atari itself.
A group of old players familiar with "Spacewar!"
struggled to maintain grip on the thrust button and requested a joystick, whereas younger players accustomed to "Space Invaders" noted they get no break in the game.
Logg and other Atari engineers observed proceedings and documented comments in four pages.
"Asteroids" was released for theAtari 2600 and Atari 8-bit family in 1981 and Atari 7800 in 1986.
Released in 1981, the 2600 port was the first game to use bank switching, a technique developed by Carl Nielsen's group of engineers that increased available ROM space from 4 KB to 8 KB.
Brad Stewart, the programmer tasked to work on the port, used bank switching to complete the game.
A port for the Atari 5200, identical to the Atari 8-bit computer version, was in development in 1982, but was not published.
The Atari 7800 version was a launch title and features co-operative play.
The asteroids receive colorful textures, and the "heartbeat" sound effect remains intact.
"Asteroids" was immediately successful upon release.
It displaced "Space Invaders" by popularity in the United States and became Atari's best selling arcade game of all time, with over 70,000 units sold.
Atari earned an estimated $150 million in sales from the game, and arcade operators earned a further $500 million from coin drops.
Atari had been in the process of manufacturing another vector game, "Lunar Lander", but demand for "Asteroids" was so high "that several hundred "Asteroids" games were shipped in "Lunar Lander" cabinets".
"Asteroids" was so popular that some video arcade operators had to install large boxes to hold the number of coins spent by players.
The saucer in the original game design was supposed to take a shot as soon as it appeared.
This action was altered so there would be a delay before the saucer shoots, leading to "lurking" from players.
Lurking is a strategy in which the player uses thrust to keep the ship in motion, leaves 1 or 2 asteroids undamaged, and hunts for saucers, allowing the player to pick off as many 1,000-point UFOs as possible and play indefinitely on a single credit.
Since the saucer could only shoot directly at the player's position on the screen, the player could "hide" at the opposite end of the screen and shoot across the screen boundary, while remaining relatively safe.
Complaints from operators losing revenue due to lurking led to the creation of an EPROM restricting such chances.
Usage of the names of "Saturday Night Live" characters "Mr. Bill" and "Sluggo" to refer to the saucers in an "Esquire" article about the game led to Logg receiving a cease and desist letter from a lawyer with the "Mr. Bill Trademark."
"Asteroids" received positive reviews from video game critics and has been regarded as Logg's magnum opus.
William Cassidy, writing for GameSpy's "Classic Gaming", noticed its innovations, including being one of the first video games to track initials and allow players to enter their initials for appearing in the top 10 high scores, and commented, "the vector graphics fit the futuristic outer space theme very well."
In 1996, "Next Generation" listed it as number 39 on their "Top 100 Games of All Time", particularly lauding the control dynamics which require "the constant juggling of speed, positioning, and direction."
"Asteroids" was ranked fourth on "Retro Gamer"s list of "Top 25 Arcade Games"; the "Retro Gamer" staff cited its simplicity and the lack of a proper ending as allowances of revisiting the game.
In 2012, "Asteroids" was listed on Time's All-TIME 100 greatest video games list.
"Entertainment Weekly" named "Asteroids" one of the top ten games for the Atari 2600 in 2013.
It was added to the Museum of Modern Art's collection of video games.
By contrast, in March 1983 the Atari 8-bit port won sixth place in "Softline"s Dog of the Year awards "for badness in computer games", Atari division, based on reader submissions.
Richard A. Edwards reviewed the 1981 "Asteroids" home cartridge in "The Space Gamer" No.
46.
Edwards commented that "This home cartridge is a virtual duplicate of the ever-popular Atari arcade game.
[...] If blasting asteroids is the thing you want to do then this is the game, but at this price I can't wholeheartedly recommend it."
Released in 1981, "Asteroids Deluxe" is the first sequel to "Asteroids".
Dave Shepperd edited the code and made enhancements to the game without Logg's involvement.
The onscreen objects were tinted blue, and hyperspace was replaced by a shield that depleted if used.
The asteroids rotate, and the added "killer satellite" enemy breaks apart into three smaller ships when hit that home in on the player's position.
The arcade machine's monitor displays vector graphics overlaying a holographic backdrop.
The game is much harder than the original and does not allow saucers to be hunted—a common strategy for "Asteroids" high scores.
It was followed by Owen Rubin's "Space Duel" in 1982, featuring colorful geometric shapes and co-op multiplayer gameplay.
In 1987's "Blasteroids", Ed Rotberg added "power-ups, ship morphing, branching levels, bosses, and the ability to dock your ships in multiplayer for added firepower".
"Blasteroids" uses raster graphics instead of vectors.
The game was included as part of the Atari Lynx title "Super Asteroids & Missile Command", and featured in the original "Microsoft Arcade" compilation in 1993, the latter with four other Atari video games: "Missile Command", "Tempest", "Centipede", and "Battlezone".
Activision made an enhanced version of "Asteroids" for PlayStation, Nintendo 64, Microsoft Windows, and the Game Boy Color in 1998.
Doug Perry, writing for entertainment and video game journalism website IGN, praised the high-end graphics – with realistic space object models, backgrounds, and special effects – for making "Asteroids" "a pleasure to look at" while being a homage to the original arcade version.
The Atari Flashback series of dedicated video game consoles have included both the 2600 and the arcade versions of "Asteroids".
Published by Crave Entertainment on December 14, 1999, "Asteroids Hyper 64" is the Nintendo 64 port of "Asteroids".
The game's graphics were upgraded to 3D, with both the ship and asteroids receiving polygon models along static backgrounds, and it was supplemented with weapons and a multiplayer mode.
IGN writer Matt Casamassina was pleased that the gameplay was faithful to the original but felt the minor additions and constant "repetition" was not enough to make the port "warrant a $50 purchase".
He was disappointed about the lack of music and found the sound effects to be of poor quality.
In 2001, Infogrames released "Atari Anniversary Edition" for the Sega Dreamcast, PlayStation, and PC compatibles.
Developed by Digital Eclipse, it included emulated versions of Asteroids and other old Atari games.
Jeff Gerstmann of GameSpot criticized the Dreamcast version for its limitations, such as the presentation of vector graphics on a low resolution television set, which obscures the copyright text in "Asteroids".
The arcade and Atari 2600 versions of "Asteroids", along with "Asteroids Deluxe", were included in "Atari Anthology" for both Xbox and PlayStation 2.
Released on November 28, 2007, the Xbox Live Arcade port of "Asteroids" has revamped HD graphics along with an added intense "throttle monkey" mode.
Both "Asteroids" in its arcade and 2600 versions and "Asteroids Deluxe" were ported to Microsofts "Game Room" download service in 2010.
Glu Mobile released a mobile phone port of the game with supplementary features as well as the original arcade version.
"Asteroids" was included on "Atari Greatest Hits Volume 1" for the Nintendo DS.
Craig Harris, writing for IGN, noted that the Nintendo DS's small screen can not properly display details of games with vector graphics.
A technical demo of "Asteroids" was developed by iThink for the Atari Jaguar but it was never released, though a prototype exists in the hands of video game collector Richard Turner, owner of the JustClaws website who demonstrated it during E-JagFest 2000 and its also referred unofficially as "Asteroids 2000".
In 2017, a ROM image of the prototype was released online.
"Asteroids" inspired many direct clones."
By December 1981 "BYTE" observed that "If imitation is the sincerest form of flattery, then [Atari's "Asteroids" has] a lot to be proud of ... Its popularity has inspired numerous imitations", including eight for personal computers.
Quality Software's "Asteroids in Space" (1980) was one of the best selling games for the Apple II and was voted one of the most popular software titles of 1978-80 by "Softalk" magazine.
Others clones include Acornsoft's "Meteors", "Moons of Jupiter" for the VIC-20, "MineStorm" for the Vectrex, and "Apple-Oids" is a 1980 clone for the Apple II, with asteroids in the shape of apples.
The Mattel Intellivision title "Meteor!"
, an "Asteroids" clone, was cancelled to avoid a lawsuit, and was reworked as "Astrosmash".
The resultant game borrows elements from "Asteroids" and "Space Invaders".
On November 13, 1982, 15-year-old Scott Safran of Cherry Hill, New Jersey, set a world record of 41,336,440 points on the arcade game "Asteroids", beating the 40,101,910 point score set by Leo Daniels of Carolina Beach on February 6, 1982.
In 1998, to congratulate Safran on his accomplishment, the Twin Galaxies Intergalactic Scoreboard searched for him for four years until 2002, when it was discovered that he had died in an accident in 1989.
In a ceremony in Philadelphia on April 27, 2002, Walter Day of Twin Galaxies presented an award to the surviving members of Safran's family, commemorating the Asteroid Champion's achievement.
On April 5, 2010, John McAllister broke Safran's record with a high score of 41,838,740 in a 58-hour Internet livestream.
</doc>
<doc id="786" url="https://en.wikipedia.org/wiki?curid=786" title="Asparagales">
Asparagales

Asparagales (asparagoid lilies) is an order of plants in modern classification systems such as the Angiosperm Phylogeny Group (APG) and the Angiosperm Phylogeny Web.
The order takes its name from the type family Asparagaceae and is placed in the monocots amongst the lilioid monocots.
The order has only recently been recognized in classification systems.
It was first put forward by Huber in 1977 and later taken up in the Dahlgren system of 1985 and then the APG in 1998, 2003 and 2009.
Before this, many of its families were assigned to the old order Liliales, a very large order containing almost all monocots with colourful tepals and lacking starch in their endosperm.
DNA sequence analysis indicated that many of the taxa previously included in Liliales should actually be redistributed over three orders, Liliales, Asparagales and Dioscoreales.
The boundaries of the Asparagales and of its families have undergone a series of changes in recent years; future research may lead to further changes and ultimately greater stability.
In the APG circumscription, Asparagales is the largest order of monocots with 14 families, 1,122 genera, and about 36,000 species.
The order is clearly circumscribed on the basis of molecular phylogenetics, but is difficult to define morphologically, since its members are structurally diverse.
Most species of Asparagales are herbaceous perennials, although some are climbers and some are tree-like.
The order also contains many geophytes (bulbs, corms and various kinds of tuber).
According to telomere sequence, at least two evolutionary switch-points happened within the order.
Basal sequence is formed by TTTAGGG like in majority of higher plants.
Basal motif was changed to vertebrate-like TTAGGG and finally the most divergent motif CTCGGTTATGGG appears in "Allium".
One of the defining characteristics (synapomorphies) of the order is the presence of phytomelanin, a black pigment present in the seed coat, creating a dark crust.
Phytomelanin is found in most families of the Asparagales (although not in Orchidaceae, thought to be a sister to the rest of the group).
The leaves of almost all species form a tight rosette, either at the base of the plant or at the end of the stem, but occasionally along the stem.
The flowers are not particularly distinctive, being 'lily type', with six tepals and up to six stamina.
The order is thought to have first diverged from other related monocots some 120–130 million years ago (early in the Cretaceous period), although given the difficulty in classifying the families involved, estimates are likely to be uncertain.
From an economic point of view, the order Asparagales is second in importance within the monocots to the order Poales (which includes grasses and cereals).
Species are used as food and flavourings (e.g.
onion, garlic, leek, asparagus, vanilla), as cut flowers (e.g.
freesia, gladiolus, iris, orchids), and as garden ornamentals (e.g.
day lilies, lily of the valley, "Agapanthus").
Thus although most species in the order are herbaceous, some no more than 15 cm high, there are a number of climbers ("e.g.
", some species of "Asparagus"), as well as several genera forming trees (e.g.
"Agave", "Cordyline", "Yucca", "Dracaena", "Aloe" ), which can exceed 10 m in height.
Succulent genera occur in several families (e.g.
"Aloe").
Almost all species have a tight cluster of leaves (a rosette), either at the base of the plant or at the end of a more-or-less woody stem as with "Yucca".
In some cases the leaves are produced along the stem.
The flowers are in the main not particularly distinctive, being of a general 'lily type', with six tepals, either free or fused from the base and up to six stamina.
They are frequently clustered at the end of the plant stem.
The Asparagales are generally distinguished from the Liliales by the lack of markings on the tepals, the presence of septal nectaries in the ovaries, rather than the bases of the tepals or stamen filaments, and the presence of secondary growth.
They are generally geophytes, but with linear leaves, and a lack of fine reticular venation.
The seeds characteristically have the external epidermis either obliterated (in most species bearing fleshy fruit), or if present, have a layer of black carbonaceous phytomelanin in species with dry fruits (nuts).
The inner part of the seed coat is generally collapsed, in contrast to Liliales whose seeds have a well developed outer epidermis, lack phytomelanin, and usually display a cellular inner layer.
The orders which have been separated from the old Liliales are difficult to characterize.
No single morphological character appears to be diagnostic of the order Asparagales.
As circumscribed within the Angiosperm Phylogeny Group system Asparagales is the largest order within the monocotyledons, with 14 families, 1,122 genera and about 25,000–42,000 species, thus accounting for about 50% of all monocots and 10–15% of the flowering plants (angiosperms).
The attribution of botanical authority for the name Asparagales belongs to Johann Heinrich Friedrich Link (1767 – 1851) who coined the word 'Asparaginae' in 1829 for a higher order taxon that included "Asparagus" although Adanson and Jussieau had also done so earlier (see History).
Earlier circumscriptions of Asparagales attributed the name to Bromhead (1838), who had been the first to use the term 'Asparagales'.
The type genus, "Asparagus", from which the name of the order is derived, was described by Carl Linnaeus in 1753, with ten species.
He placed "Asparagus" within the "Hexandria Monogynia" (six stamens, one carpel) in his sexual classification in the "Species Plantarum".
The majority of taxa now considered to constitute Asparagales have historically been placed within the very large and diverse family, Liliaceae.
The Liliaceae family was first described by Michel Adanson in 1763, and in his taxonomic scheme he created eight sections within it, including the Asparagi with "Asparagus" and three other genera.
The system of organising genera into families is generally credited to Antoine Laurent de Jussieu who formally described both the Liliaceae and the type family of Asparagales, the Asparagaceae, as Lilia and Asparagi, respectively, in 1789.
Jussieu established the hierarchical system of taxonomy (phylogeny), placing "Asparagus" and related genera within a division of Monocotyledons, a class (III) of "Stamina Perigynia" and 'order' Asparagi, divided into three subfamilies.
The use of the term "Ordo" (order) at that time was closer to what we now understand as Family, rather than Order.
In creating his scheme he used a modified form of Linnaeus' sexual classification but using the respective topography of stamens to carpels rather than just their numbers.
While De Jussieu's "Stamina Perigynia" also included a number of 'orders' that would eventually form families within the Asparagales such as the Asphodeli (Asphodelaceae), Narcissi (Amaryllidaceae) and Irides (Iridaceae), the remainder are now allocated to other orders.
Jussieu's Asparagi soon came to be referred to as "Asparagacées" in the French literature (Latin: Asparagaceae).
Meanwhile, the 'Narcissi' had been renamed as the 'Amaryllidées' (Amaryllideae) in 1805, by Jean Henri Jaume Saint-Hilaire, using "Amaryllis" as the type species rather than "Narcissus", and thus has the authority attribution for Amaryllidaceae.
In 1810 Brown proposed that a subgroup of Liliaceae be distinguished on the basis of the position of the ovaries and be referred to as Amaryllideae and in 1813 de Candolle described Liliacées Juss.
and Amaryllidées Brown as two quite separate families.
The literature on the organisation of genera into families and higher ranks became available in the English language with Samuel Frederick Gray's "A natural arrangement of British plants" (1821).
Gray used a combination of Linnaeus' sexual classification and Jussieu's natural classification to group together a number of families having in common six equal stamens, a single style and a perianth that was simple and petaloid, but did not use formal names for these higher ranks.
Within the grouping he separated families by the characteristics of their fruit and seed.
He treated groups of genera with these characteristics as separate families, such as Amaryllideae, Liliaceae, Asphodeleae and Asparageae.
The circumscription of Asparagales has been a source of difficulty for many botanists from the time of John Lindley (1846), the other important British taxonomist of the early nineteenth century.
In his first taxonomic work, "An Introduction to the Natural System of Botany" (1830) he partly followed Jussieu by describing a subclass he called Endogenae, or Monocotyledonous Plants (preserving de Candolle's "Endogenæ phanerogamæ") divided into two tribes, the Petaloidea and Glumaceae.
He divided the former, often referred to as petaloid monocots, into 32 orders, including the Liliaceae (defined narrowly), but also most of the families considered to make up the Asparagales today, including the Amaryllideae.
By 1846, in his final scheme Lindley had greatly expanded and refined the treatment of the monocots, introducing both an intermediate ranking (Alliances) and tribes within orders ("i.e." families).
Lindley placed the Liliaceae within the Liliales, but saw it as a paraphyletic ("catch-all") family, being all Liliales not included in the other orders, but hoped that the future would reveal some characteristic that would group them better.
The order Liliales was very large and had become a used to include almost all monocotyledons with colourful tepals and without starch in their endosperm (the lilioid monocots).
The Liliales was difficult to divide into families because morphological characters were not present in patterns that clearly demarcated groups.
This kept the Liliaceae separate from the Amaryllidaceae (Narcissales).
Of these Liliaceae was divided into eleven tribes (with 133 genera) and Amaryllidaceae into four tribes (with 68 genera), yet both contained many genera that would eventually segregate to each other's contemporary orders (Liliales and Asparagales respectively).
The Liliaceae would be reduced to a small 'core' represented by the Tulipae tribe, while large groups such Scilleae and Asparagae would become part of Asparagales either as part of the Amaryllidaceae or as separate families.
While of the Amaryllidaceae, the Agaveae would be part of Asparagaceae but the Alstroemeriae would become a family within the Liliales.
The number of known genera (and species) continued to grow and by the time of the next major British classification, that of Bentham and Hooker in 1883 (published in Latin) several of Lindley's other families had been absorbed into the Liliaceae.
They used the term 'series' to indicate suprafamilial rank, with seven series of monocotyledons (including Glumaceae), but did not use Lindley's terms for these.
However they did place the Liliaceous and Amaryllidaceous genera into separate series.
The Liliaceae were placed in series Coronariae, while the Amaryllideae were placed in series Epigynae.
The Liliaceae now consisted of twenty tribes (including Tulipeae, Scilleae and Asparageae), and the Amaryllideae of five (including Agaveae and Alstroemerieae).
An important addition to the treatment of the Liliaceae was the recognition of the Allieae as a distinct tribe that would eventually find its way to the Asparagales as the Allioideae subfamily of the Amaryllidaceae.
The appearance of Charles Darwin's Origin of Species in 1859 changed the way that taxonomists considered plant classification, incorporating evolutionary information into their schemata.
The Darwinian approach led to the concept of phylogeny (tree-like structure) in assembling classification systems, starting with Eichler.
Eichler, having established a hierarchical system in which the flowering plants (angiosperms) were divided into monocotyledons and dicotyledons, further divided into former into seven orders.
Within the Liliiflorae were seven families, including Liliaceae and Amaryllidaceae.
Liliaceae included "Allium" and "Ornithogalum" (modern Allioideae) and "Asparagus".
Engler, in his system developed Eichler's ideas into a much more elaborate scheme which he treated in a number of works including "Die Natürlichen Pflanzenfamilien" (Engler and Prantl 1888) and "Syllabus der Pflanzenfamilien" (1892–1924).
In his treatment of Liliiflorae the Liliineae were a suborder which included both Liliaceae and Amaryllidaceae families.
The Liliaceae had eight subfamilies and the Amaryllidaceae four.
In this rearrangement of Liliaceae, with fewer subdivisions, the core Liliales were represented as subfamily Lilioideae (with Tulipae and Scilleae as tribes), the Asparagae were represented as Asparagoideae and the Allioideae was preserved, representing the alliaceous genera.
Allieae, Agapantheae and Gilliesieae were the three tribes within this subfamily.
In the Amaryllidacea, there was little change from Bentham and Hooker.
A similar approach was adopted by Wettstein.
In the twentieth century the Wettstein system (1901–1935) placed many of the taxa in an order called 'Liliiflorae'.
Next Johannes Paulus Lotsy (1911) proposed dividing the Liliiflorae into a number of smaller families including Asparagaceae.
Then Herbert Huber (1969, 1977), following Lotsy's example, proposed that the Liliiflorae be split into four groups including the 'Asparagoid' Liliiflorae.
The widely used Cronquist system (1968–1988) used the very broadly defined order Liliales.
These various proposals to separate small groups of genera into more homogeneous families, made little impact till that of Dahlgren (1985) incorporating new information including synapomorphy.
Dahlgren developed Huber's ideas further and popularised them, with a major deconstruction of existing families into smaller units.
They created a new order, calling it Asparagales.
This was one of five orders within the superorder Liliiflorae.
Where Cronquist saw one family, Dahlgren saw forty distributed over three orders (predominantly Liliales and Asparagales).
Over the 1980s, in the context of a more general review of the classification of angiosperms, the Liliaceae were subjected to more intense scrutiny.
By the end of that decade, the Royal Botanic Gardens at Kew, the British Museum of Natural History and the Edinburgh Botanical Gardens formed a committee to examine the possibility of separating the family at least for the organization of their herbaria.
That committee finally recommended that 24 new families be created in the place of the original broad Liliaceae, largely by elevating subfamilies to the rank of separate families.
The order Asparagales as currently circumscribed has only recently been recognized in classification systems, through the advent of phylogenetics.
The 1990s saw considerable progress in plant phylogeny and phylogenetic theory, enabling a phylogenetic tree to be constructed for all of the flowering plants.
The establishment of major new clades necessitated a departure from the older but widely used classifications such as Cronquist and Thorne based largely on morphology rather than genetic data.
This complicated discussion about plant evolution and necessitated a major restructuring.
"rbc"L gene sequencing and cladistic analysis of monocots had redefined the Liliales in 1995. from four morphological orders "sensu" Dahlgren.
The largest clade representing the Liliaceae, all previously included in Liliales, but including both the Calochortaceae and Liliaceae "sensu" Tamura.
This redefined family, that became referred to as core Liliales, but corresponded to the emerging circumscription of the Angiosperm Phylogeny Group (1998).
The 2009 revision of the Angiosperm Phylogeny Group system, APG III, places the order in the clade monocots.
From the Dahlgren system of 1985 onwards, studies based mainly on morphology had identified the Asparagales as a distinct group, but had also included groups now located in Liliales, Pandanales and Zingiberales.
Research in the 21st century has supported the monophyly of Asparagales, based on morphology, 18S rDNA, and other DNA sequences, although some phylogenetic reconstructions based on molecular data have suggested that Asparagales may be paraphyletic, with Orchidaceae separated from the rest.
Within the monocots, Asparagales is the sister group of the commelinid clade.
This cladogram shows the placement of Asparagales within the orders of Lilianae "sensu" Chase & Reveal (monocots) based on molecular phylogenetic evidence.
The lilioid monocot orders are bracketed, namely Petrosaviales, Dioscoreales, Pandanales, Liliales and Asparagales.
These constitute a paraphyletic assemblage, that is groups with a common ancestor that do not include all direct descendants (in this case commelinids as the sister group to Asparagales); to form a clade, all the groups joined by thick lines would need to be included.
While Acorales and Alismatales have been collectively referred to as "alismatid monocots" (basal or early branching monocots), the remaining clades (lilioid and commelinid monocots) have been referred to as the "core monocots".
The relationship between the orders (with the exception of the two sister orders) is pectinate, that is diverging in succession from the line that leads to the commelinids.
Numbers indicate crown group (most recent common ancestor of the sampled species of the clade of interest) divergence times in mya (million years ago).
A phylogenetic tree for the Asparagales, generally to family level, but including groups which were recently and widely treated as families but which are now reduced to subfamily rank, is shown below.
The tree shown above can be divided into a basal paraphyletic group, the 'lower Asparagales (asparagoids)', from Orchidaceae to Asphodelaceae, and a well-supported monophyletic group of 'core Asparagales' (higher asparagoids), comprising the two largest families, Amaryllidaceae "sensu lato" and Asparagaceae "sensu lato".
Two differences between these two groups (although with exceptions) are: the mode of microsporogenesis and the position of the ovary.
The 'lower Asparagales' typically have simultaneous microsporogenesis (i.e. cell walls develop only after both meiotic divisions), which appears to be an apomorphy within the monocots, whereas the 'core Asparagales' have reverted to successive microsporogenesis (i.e. cell walls develop after each division).
The 'lower Asparagales' typically have an inferior ovary, whereas the 'core Asparagales' have reverted to a superior ovary.
A 2002 morphological study by Rudall treated possessing an inferior ovary as a synapomorphy of the Asparagales, stating that reversions to a superior ovary in the 'core Asparagales' could be associated with the presence of nectaries below the ovaries.
However, Stevens notes that superior ovaries are distributed among the 'lower Asparagales' in such a way that it is not clear where to place the evolution of different ovary morphologies.
The position of the ovary seems a much more flexible character (here and in other angiosperms) than previously thought.
The APG III system when it was published in 2009, greatly expanded the families Xanthorrhoeaceae, Amaryllidaceae, and Asparagaceae.
Thirteen of the families of the earlier APG II system were thereby reduced to subfamilies within these three families.
The expanded Xanthorrhoeaceae is now called "Asphodelaceae".
The APG II families (left) and their equivalent APG III subfamilies (right) are as follows:

Orchidaceae is the largest family of all angiosperms and hence by far the largest in the order.
The Dahlgren system recognized three families of orchids, but DNA sequence analysis later showed that these families are polyphyletic and so should be combined.
Several studies suggest (with high bootstrap support) that Orchidaceae is the sister of the rest of the Asparagales.
Other studies have placed the orchids differently in the phylogenetic tree, generally among the Boryaceae-Hypoxidaceae clade.
The position of Orchidaceae shown above seems the best current hypothesis, but cannot be taken as confirmed.
Orchids have simultaneous microsporogenesis and inferior ovaries, two characters that are typical of the 'lower Asparagales'.
However, their nectaries are rarely in the septa of the ovaries, and most orchids have dust-like seeds, atypical of the rest of the order.
(Some members of Vanilloideae and Cypripedioideae have crustose seeds, probably associated with dispersal by birds and mammals that are attracted by fermenting fleshy fruit releasing fragrant compounds, e.g.
vanilla.)
In terms of the number of species, Orchidaceae diversification is remarkable.
However, although the other Asparagales may be less rich in species, they are more variable morphologically, including tree-like forms.
The four families excluding Boryaceae form a well-supported clade in studies based on DNA sequence analysis.
All four contain relatively few species, and it has been suggested that they be combined into one family under the name Hypoxidaceae "sensu lato".
The relationship between Boryaceae (which includes only two genera, "Borya" and "Alania"), and other Asparagales has remained unclear for a long time.
The Boryaceae are mycorrhizal, but not in the same way as orchids.
Morphological studies have suggested a close relationship between Boryaceae and Blandfordiaceae.
There is relatively low support for the position of Boryaceae in the tree shown above.
The relationship shown between Ixioliriaceae and Tecophilaeaceae is still unclear.
Some studies have supported a clade of these two families, others have not.
The position of Doryanthaceae has also varied, with support for the position shown above, but also support for other positions.
The clade from Iridaceae upwards appears to have stronger support.
All have some genetic characteristics in common, having lost Arabidopsis-type telomeres.
Iridaceae is distinctive among the Asparagales in the unique structure of the inflorescence (a rhipidium), the combination of an inferior ovary and three stamens, and the common occurrence of unifacial leaves whereas bifacial leaves are the norm in other Asparagales.
Members of the clade from Iridaceae upwards have infra-locular septal nectaries, which Rudall interpreted as a driver towards secondarily superior ovaries.
The next node in the tree (Xanthorrhoeaceae "sensu lato" + the 'core Asparagales') has strong support.
'Anomalous' secondary thickening occurs among this clade, e.g.
in "Xanthorrhoea" (family Asphodelaceae) and "Dracaena" (family Asparagaceae "sensu lato"), with species reaching tree-like proportions.
The 'core Asparagales', comprising Amaryllidaceae "sensu lato" and Asparagaceae "sensu lato", are a strongly supported clade, as are clades for each of the families.
Relationships within these broadly defined families appear less clear, particularly within the Asparagaceae "sensu lato".
Stevens notes that most of its subfamilies are difficult to recognize, and that significantly different divisions have been used in the past, so that the use of a broadly defined family to refer to the entire clade is justified.
Thus the relationships among subfamilies shown above, based on APWeb , is somewhat uncertain.
Several studies have attempted to date the evolution of the Asparagales, based on phylogenetic evidence.
Earlier studies generally give younger dates than more recent studies, which have been preferred in the table below.
A 2009 study suggests that the Asparagales have the highest diversification rate in the monocots, about the same as the order Poales, although in both orders the rate is little over half that of the eudicot order Lamiales, the clade with the highest rate.
The taxonomic diversity of the monocotyledons is described in detail by Kubitzki.
Up-to-date information on the Asparagales can be found on the Angiosperm Phylogeny Website.
The APG III system's family circumscriptions are being used as the basis of the Kew-hosted "World Checklist of Selected Plant Families".
With this circumscription, the order consists of 14 families (Dahlgren had 31) with approximately 1120 genera and 26000 species.
Order Asparagales 

The earlier 2003 version, APG II, allowed 'bracketed' families, i.e. families which could either be segregated from more comprehensive families or could be included in them.
These are the families given under "including" in the list above.
APG III does not allow bracketed families, requiring the use of the more comprehensive family; otherwise the circumscription of the Asparagales is unchanged.
A separate paper accompanying the publication of the 2009 APG III system provided subfamilies to accommodate the families which were discontinued.
The first APG system of 1998 contained some extra families, included in square brackets in the list above.
Two older systems which use the order Asparagales are the Dahlgren system and the Kubitzki system.
The families included in the circumscriptions of the order in these two systems are shown in the first and second columns of the table below.
The equivalent family in the modern APG III system (see below) is shown in the third column.
Note that although these systems may use the same name for a family, the genera which it includes may be different, so the equivalence between systems is only approximate in some cases.
The Asparagales include many important crop plants and ornamental plants.
Crops include Allium, Asparagus and Vanilla, while ornamentals include irises, hyacinths and orchids.
</doc>
<doc id="787" url="https://en.wikipedia.org/wiki?curid=787" title="Alismatales">
Alismatales

The Alismatales (alismatids) are an order of flowering plants including about 4500 species.
Plants assigned to this order are mostly tropical or aquatic.
Some grow in fresh water, some in marine habitats.
The Alismatales comprise herbaceous flowering plants of aquatic and marshy habitats, and the only monocots known to have green embryos other than the Amaryllidaceae.
They also include the only marine angiosperms growing completely submerged, the seagrasses.
The flowers are usually arranged in inflorescences, and the mature seeds lack endosperm.
Both marine and freshwater forms include those with staminate flowers that detach from the parent plant and float to the surface where they become pollinated.
In others, pollination occurs underwater, where pollen may form elongated strands, increasing chance of success.
Most aquatic species have a totally submerged juvenile phase, and flowers are either floating or emergent.
Vegetation may be totally submersed, have floating leaves, or protrude from the water.
Collectively, they are commonly known as "water plantain".
The Alismatales contain about 165 genera in 13 families, with a cosmopolitan distribution.
Phylogenetically, they are basal monocots, diverging early in evolution relative to the lilioid and commelinid monocot lineages.
Together with the Acorales, the Alismatales are referred to informally as the alismatid monocots.
The Cronquist system (1981) places the Alismatales in subclass Alismatidae, class Liliopsida [= monocotyledons] and includes only three families as shown:
Cronquist's subclass Alismatidae conformed fairly closely to the order Alismatales as defined by APG, minus the Araceae.
The Dahlgren system places the Alismatales in the superorder Alismatanae in the subclass Liliidae [= monocotyledons] in the class Magnoliopsida [= angiosperms] with the following families included:

In Tahktajan's classification (1997), the order Alismatales contains only the Alismataceae and Limnocharitaceae, making it equivalent to the Alismataceae as revised in APG-III.
Other families included in the Alismatates as currently defined are here distributed among 10 additional orders, all of which are assigned, with the following exception, to the Subclass Alismatidae.
Araceae in Tahktajan 1997 is assigned to the Arales and placed in the Subclass Aridae; Tofieldiaceae to the Melanthiales and placed in the Liliidae.
The Angiosperm Phylogeny Group system (APG) of 1998 and APG II (2003) assigned the Alismatales to the monocots, which may be thought of as an unranked clade containing the families listed below.
The biggest departure from earlier systems (see below) is the inclusion of family Araceae.
By its inclusion, the order has grown enormously in number of species.
The family Araceae alone accounts for about a hundred genera, totaling over two thousand species.
The rest of the families together contain only about five hundred species, many of which are in very small families.
The APG III system (2009) differs only in that the Limnocharitaceae are combined with the Alismataceae; it was also suggested that the genus "Maundia" (of the Juncaginaceae) could be separated into a monogeneric family, the Maundiaceae, but the authors noted that more study was necessary before the Maundiaceae could be recognized.
In APG IV (2016), it was decided that evidence was sufficient to elevate "Maundia" to family level as the monogeneric Maundiaceae.
The authors considered including a number of the smaller orders within the Juncaginaceae, but an online survey of botanists and other users found little support for this "lumping" approach.
Consequently, the family structure for APG IV is:


Cladogram showing the orders of monocots (Lilianae "sensu" Chase & Reveal) based on molecular phylogenetic evidence:


</doc>
<doc id="788" url="https://en.wikipedia.org/wiki?curid=788" title="Apiales">
Apiales

The Apiales are an order of flowering plants.
The families are those recognized in the APG III system.
This is typical of the newer classifications, though there is some slight variation and in particular, the Torriceliaceae may be divided.
Under this definition, well-known members include carrots, celery, parsley, and "Hedera helix" (English ivy).
The order Apiales is placed within the asterid group of eudicots as circumscribed by the APG III system.
Within the asterids, Apiales belongs to an unranked group called the campanulids, and within the campanulids, it belongs to a clade known in phylogenetic nomenclature as Apiidae.
In 2010, a subclade of Apiidae named Dipsapiidae was defined to consist of the three orders: Apiales, Paracryphiales, and Dipsacales.
Under the Cronquist system, only the Apiaceae and Araliaceae were included here, and the restricted order was placed among the rosids rather than the asterids.
The Pittosporaceae were placed within the Rosales, and many of the other forms within the family Cornaceae.
"Pennantia" was in the family Icacinaceae.
In the classification system of Dahlgren the Apiaceae and Araliaceae families were placed in the order Ariales, in the superorder Araliiflorae (also called Aralianae).
The present understanding of the Apiales is fairly recent and is based upon comparison of DNA sequences by phylogenetic methods.
The circumscriptions of some of the families have changed.
In 2009, one of the subfamilies of Araliaceae was shown to be polyphyletic.
The largest and obviously closely related families of Apiales are Araliaceae, Myodocarpaceae and
Apiaceae, which resemble each other in the structure of their gynoecia.
In this respect however, the Pittosporaceae is notably distinct from them.
Typical syncarpous gynoecia exhibit four vertical zones, determined by the extent of fusion of the carpels.
In most plants the synascidiate (i.e. "united bottle-shaped") and symplicate zones are fertile and bear the ovules.
Each of the first three families possess mainly bi- or multilocular ovaries in a gynoecium with a long synascidiate, but very short symplicate zone, where the ovules are inserted at their transition, the so-called cross-zone (or "Querzone").
In gynoecia of the Pittosporaceae, the symplicate is much longer than the synascidiate zone, and the ovules are arranged along the first.
Members of the latter family consequently have unilocular ovaries with a single cavity between adjacent carpels.
</doc>
<doc id="789" url="https://en.wikipedia.org/wiki?curid=789" title="Asterales">
Asterales

Asterales is an order of dicotyledonous flowering plants that includes the large family Asteraceae (or Compositae) known for composite flowers made of florets, and ten families related to the Asteraceae.
The order is a cosmopolite (plants found throughout most of the world including desert and frigid zones), and includes mostly herbaceous species, although a small number of trees (such as the giant Lobelia and the giant Senecio) and shrubs are also present.
Asterales are organisms that seem to have evolved from one common ancestor.
Asterales share characteristics on morphological and biochemical levels.
Synapomorphies (a character that is shared by two or more groups through evolutionary development) include the presence in the plants of oligosaccharide inulin, a nutrient storage molecule used instead of starch; and unique stamen morphology.
The stamens are usually found around the style, either aggregated densely or fused into a tube, probably an adaptation in association with the plunger (brush; or secondary) pollination that is common among the families of the order, wherein pollen is collected and stored on the length of the pistil.
The name and order Asterales is botanically venerable, dating back to at least 1926 in the Hutchinson system of plant taxonomy when it contained only five families, of which only two are retained in the APG III classification.
Under the Cronquist system of taxonomic classification of flowering plants, Asteraceae was the only family in the group, but newer systems (such as APG II and APG III) have expanded it to 11.
In the classification system of Dahlgren the Asterales were in the superorder Asteriflorae (also called Asteranae).
The order Asterales currently includes 11 families, the largest of which are the Asteraceae, with about 25,000 species, and the Campanulaceae ("bellflowers"), with about 2,000 species.
The remaining families count together for less than 1500 species.
The two large families are cosmopolitan, with many of their species found in the Northern Hemisphere, and the smaller families are usually confined to Australia and the adjacent areas, or sometimes South America.
Only the Asteraceae have composite flower heads; the other families do not, but share other characteristics such as storage of inulin that define the 11 families as more closely related to each other than to other plant families or orders such as the rosids.
The phylogenetic tree according to APG III for the Campanulid clade is as below.
The core Asterales are Stylidiaceae (six genera), APA clade (Alseuosmiaceae, Phellinaceae and Argophyllaceae, together 7 genera), MGCA clade (Menyanthaceae, Goodeniaceae, Calyceraceae, in total twenty genera), and Asteraceae (about sixteen hundred genera).
Other Asterales are Rousseaceae (four genera), Campanulaceae (eighty four genera) and Pentaphragmataceae (one genus).
All Asterales families are represented in the Southern Hemisphere; however, Asteraceae and Campanulaceae are cosmopolitan and Menyanthaceae nearly so.
Although most extant species of Asteraceae are herbaceous, the examination of the basal members in the family suggests that the common ancestor of the family was an arborescent plant, a tree or shrub, perhaps adapted to dry conditions, radiating from South America.
Less can be said about the Asterales themselves with certainty, although since several families in Asterales contain trees, the ancestral member is most likely to have been a tree or shrub.
Because all clades are represented in the southern hemisphere but many not in the northern hemisphere, it is natural to conjecture that there is a common southern origin to them.
Asterales are angiosperms, flowering plants that appeared about 140 million years ago.
The Asterales order probably originated in the Cretaceous (145 – 66 Mya) on the supercontinent Gondwana which broke up from 184 – 80 Mya, forming the area that is now Australia, South America, Africa, India and Antarctica.
Asterales contain about 14% of eudicot diversity.
From an analysis of relationships and diversities within the Asterales and with their superorders, estimates of the age of the beginning of the Asterales have been made, which range from 116 Mya to 82Mya.
However few fossils have been found, of the Menyanthaceae-Asteraceae clade in the Oligocene, about 29 Mya.
Fossil evidence of the Asterales is rare and belongs to rather recent epochs, so the precise estimation of the order's age is quite difficult.
An Oligocene (34 – 23 Mya) pollen is known for Asteraceae and Goodeniaceae, and seeds from Oligocene and Miocene (23 – 5.3 Mya) are known for Menyanthaceae and Campanulaceae respectively.
The Asterales, by dint of being a super-set of the family Asteraceae, include some species grown for food, including the sunflower ("Helianthus annuus"), lettuce ("Lactuca sativa") and chicory ("Cichorium").
Many are also used as spices and traditional medicines.
Asterales are common plants and have many known uses.
For example, pyrethrum (derived from Old World members of the genus "Chrysanthemum") is a natural insecticide with minimal environmental impact.
Wormwood, derived from a genus that includes the sagebrush, is used as a source of flavoring for absinthe, a bitter classical liquor of European origin.
Despite the large number of species in order Asterales, they do not compare in economic benefit for mankind to the Poales or to the Fabaceae.
The Asteraceae include many invasive plant species in North America.
</doc>
<doc id="791" url="https://en.wikipedia.org/wiki?curid=791" title="Asteroid">
Asteroid

Asteroids are minor planets, especially of the inner Solar System.
Larger asteroids have also been called planetoids.
These terms have historically been applied to any astronomical object orbiting the Sun that did not resemble a planet-like disc and was not observed to have characteristics of an active comet such as a tail.
As minor planets in the outer Solar System were discovered they were typically found to have volatile-rich surfaces similar to comets.
As a result, they were often distinguished from objects found in the main asteroid belt.
In this article, the term "asteroid" refers to the minor planets of the inner Solar System including those co-orbital with Jupiter.
There exist millions of asteroids, many thought to be the shattered remnants of planetesimals, bodies within the young Sun's solar nebula that never grew large enough to become planets.
The vast majority of known asteroids orbit within the main asteroid belt located between the orbits of Mars and Jupiter, or are co-orbital with Jupiter (the Jupiter trojans).
However, other orbital families exist with significant populations, including the near-Earth objects.
Individual asteroids are classified by their characteristic spectra, with the majority falling into three main groups: C-type, M-type, and S-type.
These were named after and are generally identified with carbon-rich, metallic, and silicate (stony) compositions, respectively.
The sizes of asteroids varies greatly; the largest, Ceres, is almost across.
Asteroids are differentiated from comets and meteoroids.
In the case of comets, the difference is one of composition: while asteroids are mainly composed of mineral and rock, comets are primarily composed of dust and ice.
Furthermore, asteroids formed closer to the sun, preventing the development of cometary ice.
The difference between asteroids and meteoroids is mainly one of size: meteoroids have a diameter of less than one meter, whereas asteroids have a diameter of greater than one meter.
Finally, meteoroids can be composed of either cometary or asteroidal materials.
Only one asteroid, 4 Vesta, which has a relatively reflective surface, is normally visible to the naked eye, and this only in very dark skies when it is favorably positioned.
Rarely, small asteroids passing close to Earth may be visible to the naked eye for a short time.
, the Minor Planet Center had data on almost 745,000 objects in the inner and outer Solar System, of which almost 504,000 had enough information to be given numbered designations.
The United Nations declared 30 June as International Asteroid Day to educate the public about asteroids.
The date of International Asteroid Day commemorates the anniversary of the Tunguska asteroid impact over Siberia, Russian Federation, on 30 June 1908.
In April 2018, the B612 Foundation reported "It's a 100 percent certain we'll be hit [by a devastating asteroid], but we're not 100 percent sure when."
The first asteroid to be discovered, Ceres, was originally considered to be a new planet.
This was followed by the discovery of other similar bodies, which, with the equipment of the time, appeared to be points of light, like stars, showing little or no planetary disc, though readily distinguishable from stars due to their apparent motions.
This prompted the astronomer Sir William Herschel to propose the term "asteroid", coined in Greek as ἀστεροειδής, or "asteroeidēs", meaning 'star-like, star-shaped', and derived from the Ancient Greek "astēr" 'star, planet'.
In the early second half of the nineteenth century, the terms "asteroid" and "planet" (not always qualified as "minor") were still used interchangeably.
Overview of discovery timeline:

Asteroid discovery methods have dramatically improved over the past two centuries.
In the last years of the 18th century, Baron Franz Xaver von Zach organized a group of 24 astronomers to search the sky for the missing planet predicted at about 2.8 AU from the Sun by the Titius-Bode law, partly because of the discovery, by Sir William Herschel in 1781, of the planet Uranus at the distance predicted by the law.
This task required that hand-drawn sky charts be prepared for all stars in the zodiacal band down to an agreed-upon limit of faintness.
On subsequent nights, the sky would be charted again and any moving object would, hopefully, be spotted.
The expected motion of the missing planet was about 30 seconds of arc per hour, readily discernible by observers.
The first object, Ceres, was not discovered by a member of the group, but rather by accident in 1801 by Giuseppe Piazzi, director of the observatory of Palermo in Sicily.
He discovered a new star-like object in Taurus and followed the displacement of this object during several nights.
Later that year, Carl Friedrich Gauss used these observations to calculate the orbit of this unknown object, which was found to be between the planets Mars and Jupiter.
Piazzi named it after Ceres, the Roman goddess of agriculture.
Three other asteroids (2 Pallas, 3 Juno, and 4 Vesta) were discovered over the next few years, with Vesta found in 1807.
After eight more years of fruitless searches, most astronomers assumed that there were no more and abandoned any further searches.
However, Karl Ludwig Hencke persisted, and began searching for more asteroids in 1830.
Fifteen years later, he found 5 Astraea, the first new asteroid in 38 years.
He also found 6 Hebe less than two years later.
After this, other astronomers joined in the search and at least one new asteroid was discovered every year after that (except the wartime years 1944 and 1945).
Notable asteroid hunters of this early era were J. R. Hind, Annibale de Gasparis, Robert Luther, H. M. S. Goldschmidt, Jean Chacornac, James Ferguson, Norman Robert Pogson, E. W. Tempel, J. C. Watson, C. H. F. Peters, A. Borrelly, J. Palisa, the Henry brothers and Auguste Charlois.
In 1891, Max Wolf pioneered the use of astrophotography to detect asteroids, which appeared as short streaks on long-exposure photographic plates.
This dramatically increased the rate of detection compared with earlier visual methods: Wolf alone discovered 248 asteroids, beginning with 323 Brucia, whereas only slightly more than 300 had been discovered up to that point.
It was known that there were many more, but most astronomers did not bother with them, calling them "vermin of the skies", a phrase variously attributed to Eduard Suess and Edmund Weiss.
Even a century later, only a few thousand asteroids were identified, numbered and named.
Until 1998, asteroids were discovered by a four-step process.
First, a region of the sky was photographed by a wide-field telescope, or astrograph.
Pairs of photographs were taken, typically one hour apart.
Multiple pairs could be taken over a series of days.
Second, the two films or plates of the same region were viewed under a stereoscope.
Any body in orbit around the Sun would move slightly between the pair of films.
Under the stereoscope, the image of the body would seem to float slightly above the background of stars.
Third, once a moving body was identified, its location would be measured precisely using a digitizing microscope.
The location would be measured relative to known star locations.
These first three steps do not constitute asteroid discovery: the observer has only found an apparition, which gets a provisional designation, made up of the year of discovery, a letter representing the half-month of discovery, and finally a letter and a number indicating the discovery's sequential number (example: ).
The last step of discovery is to send the locations and time of observations to the Minor Planet Center, where computer programs determine whether an apparition ties together earlier apparitions into a single orbit.
If so, the object receives a catalogue number and the observer of the first apparition with a calculated orbit is declared the discoverer, and granted the honor of naming the object subject to the approval of the International Astronomical Union.
There is increasing interest in identifying asteroids whose orbits cross Earth's, and that could, given enough time, collide with Earth "(see Earth-crosser asteroids)".
The three most important groups of near-Earth asteroids are the Apollos, Amors, and Atens.
Various asteroid deflection strategies have been proposed, as early as the 1960s.
The near-Earth asteroid 433 Eros had been discovered as long ago as 1898, and the 1930s brought a flurry of similar objects.
In order of discovery, these were: 1221 Amor, 1862 Apollo, 2101 Adonis, and finally 69230 Hermes, which approached within 0.005 AU of Earth in 1937.
Astronomers began to realize the possibilities of Earth impact.
Two events in later decades increased the alarm: the increasing acceptance of the Alvarez hypothesis that an impact event resulted in the Cretaceous–Paleogene extinction, and the 1994 observation of Comet Shoemaker-Levy 9 crashing into Jupiter.
The U.S.
military also declassified the information that its military satellites, built to detect nuclear explosions, had detected hundreds of upper-atmosphere impacts by objects ranging from one to ten meters across.
All these considerations helped spur the launch of highly efficient surveys that consist of charge-coupled device (CCD) cameras and computers directly connected to telescopes.
, it was estimated that 89% to 96% of near-Earth asteroids one kilometer or larger in diameter had been discovered.
A list of teams using such systems includes:

, the LINEAR system alone has discovered 138,393 asteroids.
Among all the surveys, 4711 near-Earth asteroids have been discovered including over 600 more than in diameter.
Traditionally, small bodies orbiting the Sun were classified as comets, asteroids, or meteoroids, with anything smaller than one meter across being called a meteoroid.
Beech and Steel's 1995 paper proposed a meteoroid definition including size limits.
The term "asteroid", from the Greek word for "star-like", never had a formal definition, with the broader term minor planet being preferred by the International Astronomical Union.
However, following the discovery of asteroids below ten meters in size, Rubin and Grossman's 2010 paper revised the previous definition of meteoroid to objects between 10 µm and 1 meter in size in order to maintain the distinction between asteroids and meteoroids.
The smallest asteroids discovered (based on absolute magnitude "H") are with "H" = 33.2 and with "H" = 32.1 both with an estimated size of about 1 meter.
In 2006, the term "small Solar System body" was also introduced to cover both most minor planets and comets.
Other languages prefer "planetoid" (Greek for "planet-like"), and this term is occasionally used in English especially for larger minor planets such as the dwarf planets as well as an alternative for asteroids since they are not star-like.
The word "planetesimal" has a similar meaning, but refers specifically to the small building blocks of the planets that existed when the Solar System was forming.
The term "planetule" was coined by the geologist William Daniel Conybeare to describe minor planets, but is not in common use.
The three largest objects in the asteroid belt, Ceres, Pallas, and Vesta, grew to the stage of protoplanets.
Ceres is a dwarf planet, the only one in the inner Solar System.
When found, asteroids were seen as a class of objects distinct from comets, and there was no unified term for the two until "small Solar System body" was coined in 2006.
The main difference between an asteroid and a comet is that a comet shows a coma due to sublimation of near surface ices by solar radiation.
A few objects have ended up being dual-listed because they were first classified as minor planets but later showed evidence of cometary activity.
Conversely, some (perhaps all) comets are eventually depleted of their surface volatile ices and become asteroid-like.
A further distinction is that comets typically have more eccentric orbits than most asteroids; most "asteroids" with notably eccentric orbits are probably dormant or extinct comets.
For almost two centuries, from the discovery of Ceres in 1801 until the discovery of the first centaur, Chiron in 1977, all known asteroids spent most of their time at or within the orbit of Jupiter, though a few such as Hidalgo ventured far beyond Jupiter for part of their orbit.
Those located between the orbits of Mars and Jupiter were known for many years simply as The Asteroids.
When astronomers started finding more small bodies that permanently resided further out than Jupiter, now called centaurs, they numbered them among the traditional asteroids, though there was debate over whether they should be considered asteroids or as a new type of object.
Then, when the first trans-Neptunian object (other than Pluto), Albion, was discovered in 1992, and especially when large numbers of similar objects started turning up, new terms were invented to sidestep the issue: Kuiper-belt object, trans-Neptunian object, scattered-disc object, and so on.
These inhabit the cold outer reaches of the Solar System where ices remain solid and comet-like bodies are not expected to exhibit much cometary activity; if centaurs or trans-Neptunian objects were to venture close to the Sun, their volatile ices would sublimate, and traditional approaches would classify them as comets and not asteroids.
The innermost of these are the Kuiper-belt objects, called "objects" partly to avoid the need to classify them as asteroids or comets.
They are thought to be predominantly comet-like in composition, though some may be more akin to asteroids.
Furthermore, most do not have the highly eccentric orbits associated with comets, and the ones so far discovered are larger than traditional comet nuclei.
(The much more distant Oort cloud is hypothesized to be the main reservoir of dormant comets.)
Other recent observations, such as the analysis of the cometary dust collected by the "Stardust" probe, are increasingly blurring the distinction between comets and asteroids, suggesting "a continuum between asteroids and comets" rather than a sharp dividing line.
The minor planets beyond Jupiter's orbit are sometimes also called "asteroids", especially in popular presentations.
However, it is becoming increasingly common for the term "asteroid" to be restricted to minor planets of the inner Solar System.
Therefore, this article will restrict itself for the most part to the classical asteroids: objects of the asteroid belt, Jupiter trojans, and near-Earth objects.
When the IAU introduced the class small Solar System bodies in 2006 to include most objects previously classified as minor planets and comets, they created the class of dwarf planets for the largest minor planets—those that have enough mass to have become ellipsoidal under their own gravity.
According to the IAU, "the term 'minor planet' may still be used, but generally the term 'Small Solar System Body' will be preferred."
Currently only the largest object in the asteroid belt, Ceres, at about across, has been placed in the dwarf planet category.
It is thought that planetesimals in the asteroid belt evolved much like the rest of the solar nebula until Jupiter neared its current mass, at which point excitation from orbital resonances with Jupiter ejected over 99% of planetesimals in the belt.
Simulations and a discontinuity in spin rate and spectral properties suggest that asteroids larger than approximately in diameter accreted during that early era, whereas smaller bodies are fragments from collisions between asteroids during or after the Jovian disruption.
Ceres and Vesta grew large enough to melt and differentiate, with heavy metallic elements sinking to the core, leaving rocky minerals in the crust.
In the Nice model, many Kuiper-belt objects are captured in the outer asteroid belt, at distances greater than 2.6 AU.
Most were later ejected by Jupiter, but those that remained may be the D-type asteroids, and possibly include Ceres.
Various dynamical groups of asteroids have been discovered orbiting in the inner Solar System.
Their orbits are perturbed by the gravity of other bodies in the Solar System and by the Yarkovsky effect.
Significant populations include:

The majority of known asteroids orbit within the asteroid belt between the orbits of Mars and Jupiter, generally in relatively low-eccentricity (i.e. not very elongated) orbits.
This belt is now estimated to contain between 1.1 and 1.9 million asteroids larger than in diameter, and millions of smaller ones.
These asteroids may be remnants of the protoplanetary disk, and in this region the accretion of planetesimals into planets during the formative period of the Solar System was prevented by large gravitational perturbations by Jupiter.
Trojans are populations that share an orbit with a larger planet or moon, but do not collide with it because they orbit in one of the two Lagrangian points of stability, L4 and L5, which lie 60° ahead of and behind the larger body.
The most significant population of trojans are the Jupiter trojans.
Although fewer Jupiter trojans have been discovered (), it is thought that they are as numerous as the asteroids in the asteroid belt.
Trojans have been found in the orbits of other planets, including Venus, Earth, Mars, Uranus, and Neptune.
Near-Earth asteroids, or NEAs, are asteroids that have orbits that pass close to that of Earth.
Asteroids that actually cross Earth's orbital path are known as "Earth-crossers".
, 14,464 near-Earth asteroids are known and the number over one kilometer in diameter is estimated to be 900–1,000.
Asteroids vary greatly in size, from almost for the largest down to rocks just 1 meter across.
The three largest are very much like miniature planets: they are roughly spherical, have at least partly differentiated interiors, and are thought to be surviving protoplanets.
The vast majority, however, are much smaller and are irregularly shaped; they are thought to be either surviving planetesimals or fragments of larger bodies.
The dwarf planet Ceres is by far the largest asteroid, with a diameter of .
The next largest are 4 Vesta and 2 Pallas, both with diameters of just over .
Vesta is the only main-belt asteroid that can, on occasion, be visible to the naked eye.
On some rare occasions, a near-Earth asteroid may briefly become visible without technical aid; see 99942 Apophis.
The mass of all the objects of the asteroid belt, lying between the orbits of Mars and Jupiter, is estimated to be about 2.8–, or about 4% of the mass of the Moon.
Of this, Ceres comprises , a third of the total.
Adding in the next three most massive objects, Vesta (9%), Pallas (7%), and Hygiea (3%), brings this figure up to 51%; whereas the three after that, 511 Davida (1.2%), 704 Interamnia (1.0%), and 52 Europa (0.9%), only add another 3% to the total mass.
The number of asteroids then increases rapidly as their individual masses decrease.
The number of asteroids decreases markedly with size.
Although this generally follows a power law, there are 'bumps' at and , where more asteroids than expected from a logarithmic distribution are found.
Although their location in the asteroid belt excludes them from planet status, the three largest objects, Ceres, Vesta, and Pallas, are intact protoplanets that share many characteristics common to planets, and are atypical compared to the majority of "potato"-shaped asteroids.
The fourth largest asteroid, Hygiea, has an undifferentiated interior, like the majority of asteroids.
Between them, the four largest asteroids constitute half the mass of the asteroid belt.
Ceres is the only asteroid with a fully ellipsoidal shape and hence the only one that is a dwarf planet.
It has a much higher absolute magnitude than the other asteroids, of around 3.32, and may possess a surface layer of ice.
Like the planets, Ceres is differentiated: it has a crust, a mantle and a core.
No meteorites from Ceres have been found on Earth.
Vesta, too, has a differentiated interior, though it formed inside the Solar System's frost line, and so is devoid of water; its composition is mainly of basaltic rock such as olivine.
Aside from the large crater at its southern pole, Rheasilvia, Vesta also has an ellipsoidal shape.
Vesta is the parent body of the Vestian family and other V-type asteroids, and is the source of the HED meteorites, which constitute 5% of all meteorites on Earth.
Pallas is unusual in that, like Uranus, it rotates on its side, with its axis of rotation tilted at high angles to its orbital plane.
Its composition is similar to that of Ceres: high in carbon and silicon, and perhaps partially differentiated.
Pallas is the parent body of the Palladian family of asteroids.
Hygiea is the largest carbonaceous asteroid and, unlike the other largest asteroids, lies relatively close to the plane of the ecliptic.
It is the largest member and presumed parent body of the Hygiean family of asteroids.
Measurements of the rotation rates of large asteroids in the asteroid belt show that there is an upper limit.
No asteroid with a diameter larger than 100 meters has a rotation period smaller than 2.2 hours.
For asteroids rotating faster than approximately this rate, the inertial force at the surface is greater than the gravitational force, so any loose surface material would be flung out.
However, a solid object should be able to rotate much more rapidly.
This suggests that most asteroids with a diameter over 100 meters are rubble piles formed through accumulation of debris after collisions between asteroids.
The physical composition of asteroids is varied and in most cases poorly understood.
Ceres appears to be composed of a rocky core covered by an icy mantle, where Vesta is thought to have a nickel-iron core, olivine mantle, and basaltic crust.
10 Hygiea, however, which appears to have a uniformly primitive composition of carbonaceous chondrite, is thought to be the largest undifferentiated asteroid.
Most of the smaller asteroids are thought to be piles of rubble held together loosely by gravity, though the largest are probably solid.
Some asteroids have moons or are co-orbiting binaries: Rubble piles, moons, binaries, and scattered asteroid families are thought to be the results of collisions that disrupted a parent asteroid, or, possibly, a planet.
Asteroids contain traces of amino acids and other organic compounds, and some speculate that asteroid impacts may have seeded the early Earth with the chemicals necessary to initiate life, or may have even brought life itself to Earth "(also see panspermia)".
In August 2011, a report, based on NASA studies with meteorites found on Earth, was published suggesting DNA and RNA components (adenine, guanine and related organic molecules) may have been formed on asteroids and comets in outer space.
Composition is calculated from three primary sources: albedo, surface spectrum, and density.
The last can only be determined accurately by observing the orbits of moons the asteroid might have.
So far, every asteroid with moons has turned out to be a rubble pile, a loose conglomeration of rock and metal that may be half empty space by volume.
The investigated asteroids are as large as 280 km in diameter, and include 121 Hermione (268×186×183 km), and 87 Sylvia (384×262×232 km).
Only half a dozen asteroids are larger than 87 Sylvia, though none of them have moons; however, some smaller asteroids are thought to be more massive, suggesting they may not have been disrupted, and indeed 511 Davida, the same size as Sylvia to within measurement error, is estimated to be two and a half times as massive, though this is highly uncertain.
The fact that such large asteroids as Sylvia can be rubble piles, presumably due to disruptive impacts, has important consequences for the formation of the Solar System: Computer simulations of collisions involving solid bodies show them destroying each other as often as merging, but colliding rubble piles are more likely to merge.
This means that the cores of the planets could have formed relatively quickly.
On 7 October 2009, the presence of water ice was confirmed on the surface of 24 Themis using NASA’s Infrared Telescope Facility.
The surface of the asteroid appears completely covered in ice.
As this ice layer is sublimated, it may be getting replenished by a reservoir of ice under the surface.
Organic compounds were also detected on the surface.
Scientists hypothesize that some of the first water brought to Earth was delivered by asteroid impacts after the collision that produced the Moon.
The presence of ice on 24 Themis supports this theory.
In October 2013, water was detected on an extrasolar body for the first time, on an asteroid orbiting the white dwarf GD 61.
On 22 January 2014, European Space Agency (ESA) scientists reported the detection, for the first definitive time, of water vapor on Ceres, the largest object in the asteroid belt.
The detection was made by using the far-infrared abilities of the Herschel Space Observatory.
The finding is unexpected because comets, not asteroids, are typically considered to "sprout jets and plumes".
According to one of the scientists, "The lines are becoming more and more blurred between comets and asteroids."
In May 2016, significant asteroid data arising from the Wide-field Infrared Survey Explorer and NEOWISE missions have been questioned.
Although the early original criticism had not undergone peer review, a more recent peer-reviewed study was subsequently published.
Most asteroids outside the "big four" (Ceres, Pallas, Vesta, and Hygiea) are likely to be broadly similar in appearance, if irregular in shape.
50-km (31-mi) 253 Mathilde is a rubble pile saturated with craters with diameters the size of the asteroid's radius, and Earth-based observations of 300-km (186-mi) 511 Davida, one of the largest asteroids after the big four, reveal a similarly angular profile, suggesting it is also saturated with radius-size craters.
Medium-sized asteroids such as Mathilde and 243 Ida that have been observed up close also reveal a deep regolith covering the surface.
Of the big four, Pallas and Hygiea are practically unknown.
Vesta has compression fractures encircling a radius-size crater at its south pole but is otherwise a spheroid.
Ceres seems quite different in the glimpses Hubble has provided, with surface features that are unlikely to be due to simple craters and impact basins, but details will be expanded with the "Dawn spacecraft", which entered Ceres orbit on 6 March 2015.
Asteroids become darker and redder with age due to space weathering.
However evidence suggests most of the color change occurs rapidly, in the first hundred thousands years, limiting the usefulness of spectral measurement for determining the age of asteroids.
Asteroids are commonly classified according to two criteria: the characteristics of their orbits, and features of their reflectance spectrum.
Many asteroids have been placed in groups and families based on their orbital characteristics.
Apart from the broadest divisions, it is customary to name a group of asteroids after the first member of that group to be discovered.
Groups are relatively loose dynamical associations, whereas families are tighter and result from the catastrophic break-up of a large parent asteroid sometime in the past.
Families are more common and easier to identify within the main asteroid belt, but several small families have been reported among the Jupiter trojans.
Main belt families were first recognized by Kiyotsugu Hirayama in 1918 and are often called Hirayama families in his honor.
About 30–35% of the bodies in the asteroid belt belong to dynamical families each thought to have a common origin in a past collision between asteroids.
A family has also been associated with the plutoid dwarf planet .
Some asteroids have unusual horseshoe orbits that are co-orbital with Earth or some other planet.
Examples are 3753 Cruithne and .
The first instance of this type of orbital arrangement was discovered between Saturn's moons Epimetheus and Janus.
Sometimes these horseshoe objects temporarily become quasi-satellites for a few decades or a few hundred years, before returning to their earlier status.
Both Earth and Venus are known to have quasi-satellites.
Such objects, if associated with Earth or Venus or even hypothetically Mercury, are a special class of Aten asteroids.
However, such objects could be associated with outer planets as well.
In 1975, an asteroid taxonomic system based on color, albedo, and spectral shape was developed by Clark R. Chapman, David Morrison, and Ben Zellner.
These properties are thought to correspond to the composition of the asteroid's surface material.
The original classification system had three categories: C-types for dark carbonaceous objects (75% of known asteroids), S-types for stony (silicaceous) objects (17% of known asteroids) and U for those that did not fit into either C or S. This classification has since been expanded to include many other asteroid types.
The number of types continues to grow as more asteroids are studied.
The two most widely used taxonomies now used are the Tholen classification and SMASS classification.
The former was proposed in 1984 by David J. Tholen, and was based on data collected from an eight-color asteroid survey performed in the 1980s.
This resulted in 14 asteroid categories.
In 2002, the Small Main-Belt Asteroid Spectroscopic Survey resulted in a modified version of the Tholen taxonomy with 24 different types.
Both systems have three broad categories of C, S, and X asteroids, where X consists of mostly metallic asteroids, such as the M-type.
There are also several smaller classes.
The proportion of known asteroids falling into the various spectral types does not necessarily reflect the proportion of all asteroids that are of that type; some types are easier to detect than others, biasing the totals.
Originally, spectral designations were based on inferences of an asteroid's composition.
However, the correspondence between spectral class and composition is not always very good, and a variety of classifications are in use.
This has led to significant confusion.
Although asteroids of different spectral classifications are likely to be composed of different materials, there are no assurances that asteroids within the same taxonomic class are composed of similar materials.
A newly discovered asteroid is given a provisional designation (such as ) consisting of the year of discovery and an alphanumeric code indicating the half-month of discovery and the sequence within that half-month.
Once an asteroid's orbit has been confirmed, it is given a number, and later may also be given a name (e.g.
433 Eros).
The formal naming convention uses parentheses around the number (e.g.
(433) Eros), but dropping the parentheses is quite common.
Informally, it is common to drop the number altogether, or to drop it after the first mention when a name is repeated in running text.
In addition, names can be proposed by the asteroid's discoverer, within guidelines established by the International Astronomical Union.
The first asteroids to be discovered were assigned iconic symbols like the ones traditionally used to designate the planets.
By 1855 there were two dozen asteroid symbols, which often occurred in multiple variants.
In 1851, after the fifteenth asteroid (Eunomia) had been discovered, Johann Franz Encke made a major change in the upcoming 1854 edition of the "Berliner Astronomisches Jahrbuch" (BAJ, "Berlin Astronomical Yearbook").
He introduced a disk (circle), a traditional symbol for a star, as the generic symbol for an asteroid.
The circle was then numbered in order of discovery to indicate a specific asteroid (although he assigned ① to the fifth, Astraea, while continuing to designate the first four only with their existing iconic symbols).
The numbered-circle convention was quickly adopted by astronomers, and the next asteroid to be discovered (16 Psyche, in 1852) was the first to be designated in that way at the time of its discovery.
However, Psyche was given an iconic symbol as well, as were a few other asteroids discovered over the next few years (see chart above).
20 Massalia was the first asteroid that was not assigned an iconic symbol, and no iconic symbols were created after the 1855 discovery of 37 Fides.
That year Astraea's number was increased to ⑤, but the first four asteroids, Ceres to Vesta, were not listed by their numbers until the 1867 edition.
The circle was soon abbreviated to a pair of parentheses, which were easier to typeset and sometimes omitted altogether over the next few decades, leading to the modern convention.
Until the age of space travel, objects in the asteroid belt were merely pinpricks of light in even the largest telescopes and their shapes and terrain remained a mystery.
The best modern ground-based telescopes and the Earth-orbiting Hubble Space Telescope can resolve a small amount of detail on the surfaces of the largest asteroids, but even these mostly remain little more than fuzzy blobs.
Limited information about the shapes and compositions of asteroids can be inferred from their light curves (their variation in brightness as they rotate) and their spectral properties, and asteroid sizes can be estimated by timing the lengths of star occulations (when an asteroid passes directly in front of a star).
Radar imaging can yield good information about asteroid shapes and orbital and rotational parameters, especially for near-Earth asteroids.
In terms of delta-v and propellant requirements, NEOs are more easily accessible than the Moon.
The first close-up photographs of asteroid-like objects were taken in 1971, when the "Mariner 9" probe imaged Phobos and Deimos, the two small moons of Mars, which are probably captured asteroids.
These images revealed the irregular, potato-like shapes of most asteroids, as did later images from the Voyager probes of the small moons of the gas giants.
The first true asteroid to be photographed in close-up was 951 Gaspra in 1991, followed in 1993 by 243 Ida and its moon Dactyl, all of which were imaged by the "Galileo" probe en route to Jupiter.
The first dedicated asteroid probe was "NEAR Shoemaker", which photographed 253 Mathilde in 1997, before entering into orbit around 433 Eros, finally landing on its surface in 2001.
Other asteroids briefly visited by spacecraft en route to other destinations include 9969 Braille (by "Deep Space 1" in 1999), and 5535 Annefrank (by "Stardust" in 2002).
From September to November 2005, the Japanese "Hayabusa" probe studied 25143 Itokawa in detail and was plagued with difficulties, but returned samples of its surface to Earth on 13 June 2010.
The European "Rosetta" probe (launched in 2004) flew by 2867 Šteins in 2008 and 21 Lutetia, the third-largest asteroid visited to date, in 2010.
In September 2007, NASA launched the "Dawn" spacecraft, which orbited 4 Vesta from July 2011 to September 2012, and has been orbiting the dwarf planet 1 Ceres since 2015.
4 Vesta is the second-largest asteroid visited to date.
On 13 December 2012, China's lunar orbiter "Chang'e 2" flew within of the asteroid 4179 Toutatis on an extended mission.
The Japan Aerospace Exploration Agency (JAXA) launched the "Hayabusa2" probe in December 2014, and plans to return samples from 162173 Ryugu in December 2020.
In June 2018, the US National Science and Technology Council warned that America is unprepared for an asteroid impact event, and has developed and released the ""National Near-Earth Object Preparedness Strategy Action Plan"" to better prepare.
In May 2011, NASA selected the OSIRIS-REx sample return mission to asteroid 101955 Bennu; it launched on September 8, 2016.
Its arrival at Bennu is planned for December 2018, but by that fall it was close enough to image the asteroid in reasonable detail.
In early 2013, NASA announced the planning stages of a mission to capture a near-Earth asteroid and move it into lunar orbit where it could possibly be visited by astronauts and later impacted into the Moon.
On 19 June 2014, NASA reported that asteroid 2011 MD was a prime candidate for capture by a robotic mission, perhaps in the early 2020s.
It has been suggested that asteroids might be used as a source of materials that may be rare or exhausted on Earth (asteroid mining), or materials for constructing space habitats "(see Colonization of the asteroids)".
Materials that are heavy and expensive to launch from Earth may someday be mined from asteroids and used for space manufacturing and construction.
In the U.S.
Discovery program the "Psyche" spacecraft proposal to 16 Psyche and "Lucy" spacecraft to Jupiter trojans made it to the semifinalist stage of mission selection.
In January 2017, "Lucy"and "Psyche" mission were both selected as NASA's Discovery Program missions 13 and 14 respectively.
Location of Ceres (within asteroid belt) compared to other bodies of the Solar System
Asteroids and the asteroid belt are a staple of science fiction stories.
Asteroids play several potential roles in science fiction: as places human beings might colonize, resources for extracting minerals, hazards encountered by spacecraft traveling between two other points, and as a threat to life on Earth or other inhabited planets, dwarf planets and natural satellites by potential impact.
"Further information about asteroids"



</doc>
<doc id="794" url="https://en.wikipedia.org/wiki?curid=794" title="Allocution">
Allocution

An allocution, or allocutus, is a formal statement made to the court by the defendant who has been found guilty prior to being sentenced.
It is part of the criminal procedure in some jurisdictions using common law.
An allocution allows the defendant to explain why the sentence should be lenient.
In plea bargains, an allocution may be required of the defendant.
The defendant explicitly admits specifically and in detail the actions and their reasons in exchange for a reduced sentence.
In principle, that removes any doubt as to the exact nature of the defendant's guilt in the matter.
The term "allocution" is used generally only in jurisdictions in the United States, but there are vaguely similar processes in other common law countries.
In many other jurisdictions, it is for the defense lawyer to mitigate on his client's behalf, and the defendant rarely has the opportunity to speak.
The right of victims to speak at sentencing is also sometimes referred to as allocution.
In Australia, the term "allocutus" is used by the Clerk of Arraigns or another formal associate of the Court.
It is generally phrased as, "Prisoner at the Bar, you have been found Guilty by a jury of your peers of the offense of XYZ.
Do you have anything to say as to why the sentence of this Court should not now be passed upon you?"
The defense counsel will then make a "plea in mitigation" (also called "submissions on penalty") in an attempt to mitigate the relative seriousness of the offense and heavily refer to and rely upon the defendant's previous good character and good works, if any.
The right to make a plea in mitigation is absolute.
If a judge or magistrate refuses to hear such a plea or does not properly consider it, the sentence can be overturned on appeal.
In most of the United States, defendants are allowed the opportunity to allocute before a sentence is passed.
Some jurisdictions hold that as an absolute right.
In its absence, a sentence but not the conviction may be overturned, resulting in the need for a new sentencing hearing.
In the federal system, Federal Rule of Criminal Procedure 32(i)(4) provides that the court must "address the defendant personally in order to permit the defendant to speak or present any information to mitigate the sentence."
The Federal Public Defender recommends that defendants speak in terms of how a lenient sentence will be sufficient but not greater than necessary to comply with the statutory directives set forth in .
</doc>
<doc id="795" url="https://en.wikipedia.org/wiki?curid=795" title="Affidavit">
Affidavit

An affidavit ( ) is a written sworn statement of fact voluntarily made by an "affiant" or "deponent" under an oath or affirmation administered by a person authorized to do so by law.
Such statement is witnessed as to the authenticity of the affiant's signature by a taker of oaths, such as a notary public or commissioner of oaths.
The name is Medieval Latin for "he/she has declared upon oath".
An affidavit is a type of verified statement or showing, or in other words, it contains a verification, meaning it is under oath or penalty of perjury, and this serves as evidence to its veracity and is required for court proceedings.
Affidavits may be written in the first or third person, depending on who drafted the document.
If in the first person, the document's component parts are typically as follows:

If an affidavit is notarized or authenticated, it will also include a caption with a venue and title in reference to judicial proceedings.
In some cases, an introductory clause, called a "preamble", is added attesting that the affiant personally appeared before the authenticating authority.
On 2 March 2016, the High Court of Australia held that the ACT Uniform Evidence Legislation is neutral in the way sworn evidence and unsworn evidence is treated as being of equal weight.
In Indian law, although an affidavit may be taken as proof of the facts stated therein, the Courts have no jurisdiction to admit evidence by way of affidavit.
Affidavit is treated as "evidence" within the meaning of Section 3 of the Evidence Act.
However, it was held by the Supreme Court that an affidavit can be used as evidence only if the Court so orders for sufficient reasons, namely, the right of the opposite party to have the deponent produced for cross-examination (Khandesh Spg & Wvg Mills CO.
Ltd.
Vs Rashtriya Girni Kamgar Sangh, citation 1960 AIR571, 1960 SCR(2) 841).
Therefore, an affidavit cannot ordinarily be used as evidence in absence of a specific order of the Court.
In Sri Lanka, under the Oaths Ordinance, with the exception of court marshals, a person may submit an affidavit signed in the presence of a Commissioner for Oaths or a justice of the peace.
Affidavits are made in a similar way as to England and Wales, although "make oath" is sometimes omitted.
A declaration may be substituted for an affidavit in most cases for those opposed to swearing oaths.
The person making the affidavit is known as the deponent but does not sign the affidavit.
The affidavit concludes in the standard format "sworn (declared) before me, [name of commissioner for oaths/solicitor], a commissioner for oaths (solicitor), on the [date] at [location] in the county/city of [county/city], and I know the deponent (declarant)", and it is signed and stamped by the commissioner for oaths.
In American jurisprudence, under the rules for hearsay, admission of an unsupported affidavit as evidence is unusual (especially if the affiant is not available for cross-examination) with regard to material facts which may be dispositive of the matter at bar.
Affidavits from persons who are dead or otherwise incapacitated, or who cannot be located or made to appear, may be accepted by the court, but usually only in the presence of corroborating evidence.
An affidavit which reflected a better grasp of the facts close in time to the actual events may be used to refresh a witness's recollection.
Materials used to refresh recollection are admissible as evidence.
If the affiant is a party in the case, the affiant's opponent may be successful in having the affidavit admitted as evidence, as statements by a party-opponent are admissible through an exception to the hearsay rule.
Affidavits are typically included in the response to interrogatories.
Requests for admissions under Federal Rule of Civil Procedure 36, however, are not required to be sworn.
Some types of motions will not be accepted by the court unless accompanied by an independent sworn statement or other evidence, in support of the need for the motion.
In such a case, a court will accept an affidavit from the filing attorney in support of the motion, as certain assumptions are made, to wit: The affidavit in place of sworn testimony promotes judicial economy.
The lawyer is an officer of the court and knows that a false swearing by him, if found out, could be grounds for severe penalty up to and including disbarment.
The lawyer if called upon would be able to present independent and more detailed evidence to prove the facts set forth in his affidavit.
The acceptance of an affidavit by one society does not confirm its acceptance as a legal document in other jurisdictions.
Equally, the acceptance that a lawyer is an officer of the court (for swearing the affidavit) is not a given.
This matter is addressed by the use of the apostille, a means of certifying the legalization of a document for international use under the terms of the 1961 Hague Convention Abolishing the Requirement of Legalization for Foreign Public Documents.
Documents which have been notarized by a notary public, and certain other documents, and then certified with a conformant apostille, are accepted for legal use in all the nations that have signed the Hague Convention.
Thus most affidavits now require to be apostilled if used for cross border issues.
</doc>
<doc id="798" url="https://en.wikipedia.org/wiki?curid=798" title="Aries (constellation)">
Aries (constellation)

Aries is one of the constellations of the zodiac.
It is located in the northern celestial hemisphere between Pisces to the west and Taurus to the east.
The name Aries is Latin for ram, and its symbol is (Unicode ♈), representing a ram's horns.
It is one of the 48 constellations described by the 2nd century astronomer Ptolemy, and remains one of the 88 modern constellations.
It is a mid-sized constellation, ranking 39th overall size, with an area of 441 square degrees (1.1% of the celestial sphere).
Although Aries came to represent specifically the ram whose fleece became the Golden Fleece of Ancient Greek mythology, it has represented a ram since late Babylonian times.
Before that, the stars of Aries formed a farmhand.
Different cultures have incorporated the stars of Aries into different constellations including twin inspectors in China and a porpoise in the Marshall Islands.
Aries is a relatively dim constellation, possessing only four bright stars: Hamal (Alpha Arietis, second magnitude), Sheratan (Beta Arietis, third magnitude), Mesarthim (Gamma Arietis, fourth magnitude), and 41 Arietis (also fourth magnitude).
The few deep-sky objects within the constellation are quite faint and include several pairs of interacting galaxies.
Several meteor showers appear to radiate from Aries, including the Daytime Arietids and the Epsilon Arietids.
Aries is now recognized as an official constellation, albeit as a specific region of the sky, by the International Astronomical Union.
It was originally defined in ancient texts as a specific pattern of stars, and has remained a constellation since ancient times; it now includes the ancient pattern as well as the surrounding stars.
In the description of the Babylonian zodiac given in the clay tablets known as the MUL.APIN, the constellation now known as Aries was the final station along the ecliptic.
The MUL.APIN was a comprehensive table of the risings and settings of stars, which likely served as an agricultural calendar.
Modern-day Aries was known as , "The Agrarian Worker" or "The Hired Man".
Although likely compiled in the 12th or 11th century BC, the MUL.APIN reflects a tradition which marks the Pleiades as the vernal equinox, which was the case with some precision at the beginning of the Middle Bronze Age.
The earliest identifiable reference to Aries as a distinct constellation comes from the boundary stones that date from 1350 to 1000 BC.
On several boundary stones, a zodiacal ram figure is distinct from the other characters present.
The shift in identification from the constellation as the Agrarian Worker to the Ram likely occurred in later Babylonian tradition because of its growing association with Dumuzi the Shepherd.
By the time the MUL.APIN was created—by 1000 BC—modern Aries was identified with both Dumuzi's ram and a hired laborer.
The exact timing of this shift is difficult to determine due to the lack of images of Aries or other ram figures.
In ancient Egyptian astronomy, Aries was associated with the god Amon-Ra, who was depicted as a man with a ram's head and represented fertility and creativity.
Because it was the location of the vernal equinox, it was called the "Indicator of the Reborn Sun".
During the times of the year when Aries was prominent, priests would process statues of Amon-Ra to temples, a practice that was modified by Persian astronomers centuries later.
Aries acquired the title of "Lord of the Head" in Egypt, referring to its symbolic and mythological importance.
Aries was not fully accepted as a constellation until classical times.
In Hellenistic astrology, the constellation of Aries is associated with the golden ram of Greek mythology that rescued Phrixus and Helle on orders from Hermes, taking Phrixus to the land of Colchis.
Phrixos and Helle were the son and daughter of King Athamas and his first wife Nephele.
The king's second wife, Ino, was jealous and wished to kill his children.
To accomplish this, she induced a famine in Boeotia, then falsified a message from the Oracle of Delphi that said Phrixos must be sacrificed to end the famine.
Athamas was about to sacrifice his son atop Mount Laphystium when Aries, sent by Nephele, arrived.
Helle fell off of Aries's back in flight and drowned in the Dardanelles, also called the Hellespont in her honor.
After arriving, Phrixus sacrificed the ram to Zeus and gave the Fleece to Aeëtes of Colchis, who rewarded him with an engagement to his daughter Chalciope.
Aeëtes hung its skin in a sacred place where it became known as the Golden Fleece and was guarded by a dragon.
In a later myth, this Golden Fleece was stolen by Jason and the Argonauts.
Historically, Aries has been depicted as a crouched, wingless ram with its head turned towards Taurus.
Ptolemy asserted in his "Almagest" that Hipparchus depicted Alpha Arietis as the ram's muzzle, though Ptolemy did not include it in his constellation figure.
Instead, it was listed as an "unformed star", and denoted as "the star over the head".
John Flamsteed, in his "Atlas Coelestis", followed Ptolemy's description by mapping it above the figure's head.
Flamsteed followed the general convention of maps by depicting Aries lying down.
Astrologically, Aries has been associated with the head and its humors.
It was strongly associated with Mars, both the planet and the god.
It was considered to govern Western Europe and Syria, and to indicate a strong temper in a person.
The First Point of Aries, the location of the vernal equinox, is named for the constellation.
This is because the Sun crossed the celestial equator from south to north in Aries more than two millennia ago.
Hipparchus defined it in 130 BC.
as a point south of Gamma Arietis.
Because of the precession of the equinoxes, the First Point of Aries has since moved into Pisces and will move into Aquarius by around 2600 AD.
The Sun now appears in Aries from late April through mid May, though the constellation is still associated with the beginning of spring.
Medieval Muslim astronomers depicted Aries in various ways.
Astronomers like al-Sufi saw the constellation as a ram, modeled on the precedent of Ptolemy.
However, some Islamic celestial globes depicted Aries as a nondescript four-legged animal with what may be antlers instead of horns.
Some early Bedouin observers saw a ram elsewhere in the sky; this constellation featured the Pleiades as the ram's tail.
The generally accepted Arabic formation of Aries consisted of thirteen stars in a figure along with five "unformed" stars, four of which were over the animal's hindquarters and one of which was the disputed star over Aries's head.
Al-Sufi's depiction differed from both other Arab astronomers' and Flamsteed's, in that his Aries was running and looking behind itself.
The obsolete constellations introduced in Aries (Musca Borealis, Lilium, Vespa, and Apes) have all been composed of the northern stars.
Musca Borealis was created from the stars 33 Arietis, 35 Arietis, 39 Arietis, and 41 Arietis.
In 1612, Petrus Plancius introduced Apes, a constellation representing a bee.
In 1624, the same stars were used by Jakob Bartsch to create a constellation called Vespa, representing a wasp.
In 1679 Augustin Royer used these stars for his constellation Lilium, representing the fleur-de-lis.
None of these constellation became widely accepted.
Johann Hevelius renamed the constellation "Musca" in 1690 in his "Firmamentum Sobiescianum".
To differentiate it from Musca, the southern fly, it was later renamed Musca Borealis but it did not gain acceptance and its stars were ultimately officially reabsorbed into Aries.
In 1922, the International Astronomical Union defined its recommended three-letter abbreviation, "Ari".
The official boundaries of Aries were defined in 1930 by Eugène Delporte as a polygon of 12 segments.
Its right ascension is between 1 46.4 and 3 29.4 and its declination is between 10.36° and 31.22° in the equatorial coordinate system.
In traditional Chinese astronomy, stars from Aries were used in several constellations.
The brightest stars—Alpha, Beta, and Gamma Arietis—formed a constellation called "Lou", variously translated as "bond", "lasso", and "sickle", which was associated with the ritual sacrifice of cattle.
This name was shared by the 16th lunar mansion, the location of the full moon closest to the autumnal equinox.
The lunar mansion represented the area where animals were gathered before sacrifice around that time.
This constellation has also been associated with harvest-time as it could represent a woman carrying a basket of food on her head.
35, 39, and 41 Arietis were part of a constellation called "Wei", which represented a fat abdomen and was the namesake of the 17th lunar mansion, which represented granaries.
Delta and Zeta Arietis were a part of the constellation "Tianyin", thought to represent the Emperor's hunting partner.
"Zuogeng" ("Tso-kang"), a constellation depicting a marsh and pond inspector, was composed of Mu, Nu, Omicron, Pi, and Sigma Arietis.
He was accompanied by "Yeou-kang", a constellation depicting an official in charge of pasture distribution.
In a similar system to the Chinese, the first lunar mansion in Hindu astronomy was called "Aswini", after the traditional names for Beta and Gamma Arietis, the Aswins.
Because the Hindu new year began with the vernal equinox, the Rig Veda contains over 50 new-year's related hymns to the twins, making them some of the most prominent characters in the work.
Aries itself was known as ""Aja"" and ""Mesha"".
In Hebrew astronomy Aries was named ""Teli""; it signified either Simeon or Gad, and generally symbolizes the "Lamb of the World".
The neighboring Syrians named the constellation "Amru", and the bordering Turks named it "Kuzi".
Half a world away, in the Marshall Islands, several stars from Aries were incorporated into a constellation depicting a porpoise, along with stars from Cassiopeia, Andromeda, and Triangulum.
Alpha, Beta, and Gamma Arietis formed the head of the porpoise, while stars from Andromeda formed the body and the bright stars of Cassiopeia formed the tail.
Other Polynesian peoples recognized Aries as a constellation.
The Marquesas islanders called it "Na-pai-ka"; the Māori constellation "Pipiri" may correspond to modern Aries as well.
In indigenous Peruvian astronomy, a constellation with most of the same stars as Aries existed.
It was called the "Market Moon" and the "Kneeling Terrace", as a reminder for when to hold the annual harvest festival, Ayri Huay.
Aries has three prominent stars forming an asterism, designated Alpha, Beta, and Gamma Arietis by Johann Bayer.
All three are commonly used for navigation.
There is also one other star above the fourth magnitude, 41 Arietis (Bharani).
α Arietis, called Hamal, is the brightest star in Aries.
Its traditional name is derived from the Arabic word for "lamb" or "head of the ram" ("ras al-hamal"), which references Aries's mythological background.
With a spectral class of K2 and a luminosity class of III, it is an orange giant with an apparent visual magnitude of 2.00, which lies 66 light-years from Earth.
Hamal has a luminosity of and its absolute magnitude is −0.1.
β Arietis, also known as Sheratan, is a blue-white star with an apparent visual magnitude of 2.64.
Its traditional name is derived from ""sharatayn"", the Arabic word for "the two signs", referring to both Beta and Gamma Arietis in their position as heralds of the vernal equinox.
The two stars were known to the Bedouin as ""qarna al-hamal"", "horns of the ram".
It is 59 light-years from Earth.
It has a luminosity of and its absolute magnitude is 2.1.
It is a spectroscopic binary star, one in which the companion star is only known through analysis of the spectra.
The spectral class of the primary is A5.
Hermann Carl Vogel determined that Sheratan was a spectroscopic binary in 1903; its orbit was determined by Hans Ludendorff in 1907.
It has since been studied for its eccentric orbit.
γ Arietis, with a common name of Mesarthim, is a binary star with two white-hued components, located in a rich field of magnitude 8–12 stars.
Its traditional name has conflicting derivations.
It may be derived from a corruption of "al-sharatan", the Arabic word meaning "pair" or a word for "fat ram".
However, it may also come from the Sanskrit for "first star of Aries" or the Hebrew for "ministerial servants", both of which are unusual languages of origin for star names.
Along with Beta Arietis, it was known to the Bedouin as ""qarna al-hamal"".
The primary is of magnitude 4.59 and the secondary is of magnitude 4.68.
The system is 164 light-years from Earth.
The two components are separated by 7.8 arcseconds, and the system as a whole has an apparent magnitude of 3.9.
The primary has a luminosity of and the secondary has a luminosity of ; the primary is an A-type star with an absolute magnitude of 0.2 and the secondary is a B9-type star with an absolute magnitude of 0.4.
The angle between the two components is 1°.
Mesarthim was discovered to be a double star by Robert Hooke in 1664, one of the earliest such telescopic discoveries.
The primary, γ Arietis, is an Alpha² Canum Venaticorum variable star that has a range of 0.02 magnitudes and a period of 2.607 days.
It is unusual because of its strong silicon emission lines.
The constellation is home to several double stars, including Epsilon, Lambda, and Pi Arietis.
ε Arietis is a binary star with two white components.
The primary is of magnitude 5.2 and the secondary is of magnitude 5.5.
The system is 290 light-years from Earth.
Its overall magnitude is 4.63, and the primary has an absolute magnitude of 1.4.
Its spectral class is A2.
The two components are separated by 1.5 arcseconds.
λ Arietis is a wide double star with a white-hued primary and a yellow-hued secondary.
The primary is of magnitude 4.8 and the secondary is of magnitude 7.3.
The primary is 129 light-years from Earth.
It has an absolute magnitude of 1.7 and a spectral class of F0.
The two components are separated by 36 arcseconds at an angle of 50°; the two stars are located 0.5° east of 7 Arietis.
π Arietis is a close binary star with a blue-white primary and a white secondary.
The primary is of magnitude 5.3 and the secondary is of magnitude 8.5.
The primary is 776 light-years from Earth.
The primary itself is a wide double star with a separation of 25.2 arcseconds; the tertiary has a magnitude of 10.8.
The primary and secondary are separated by 3.2 arcseconds.
Most of the other stars in Aries visible to the naked eye have magnitudes between 3 and 5. δ Ari, called Boteïn, is a star of magnitude 4.35, 170 light-years away.
It has an absolute magnitude of −0.1 and a spectral class of K2.
ζ Arietis is a star of magnitude 4.89, 263 light-years away.
Its spectral class is A0 and its absolute magnitude is 0.0.
14 Arietis is a star of magnitude 4.98, 288 light-years away.
Its spectral class is F2 and its absolute magnitude is 0.6.
39 Arietis (Lilii Borea) is a similar star of magnitude 4.51, 172 light-years away.
Its spectral class is K1 and its absolute magnitude is 0.0.
35 Arietis is a dim star of magnitude 4.55, 343 light-years away.
Its spectral class is B3 and its absolute magnitude is −1.7.
41 Arietis, known both as c Arietis and Nair al Butain, is a brighter star of magnitude 3.63, 165 light-years away.
Its spectral class is B8 and it has a luminosity of .
Its absolute magnitude is −0.2.
53 Arietis is a runaway star of magnitude 6.09, 815 light-years away.
Its spectral class is B2.
It was likely ejected from the Orion Nebula approximately five million years ago, possibly due to supernovae.
Finally, Teegarden's Star is the closest star to Earth in Aries.
It is a brown dwarf of magnitude 15.14 and spectral class M6.5V.
With a proper motion of 5.1 arcseconds per year, it is the 24th closest star to Earth overall.
Aries has its share of variable stars, including R and U Arietis, Mira-type variable stars, and T Arietis, a semi-regular variable star.
R Arietis is a Mira variable star that ranges in magnitude from a minimum of 13.7 to a maximum of 7.4 with a period of 186.8 days.
It is 4,080 light-years away.
U Arietis is another Mira variable star that ranges in magnitude from a minimum of 15.2 to a maximum of 7.2 with a period of 371.1 days.
T Arietis is a semiregular variable star that ranges in magnitude from a minimum of 11.3 to a maximum of 7.5 with a period of 317 days.
It is 1,630 light-years away.
One particularly interesting variable in Aries is SX Arietis, a rotating variable star considered to be the prototype of its class, helium variable stars.
SX Arietis stars have very prominent emission lines of Helium I and Silicon III.
They are normally main-sequence B0p—B9p stars, and their variations are not usually visible to the naked eye.
Therefore, they are observed photometrically, usually having periods that fit in the course of one night.
Similar to Alpha² Canum Venaticorum variables, SX Arietis stars have periodic changes in their light and magnetic field, which correspond to the periodic rotation; they differ from the Alpha² Canum Venaticorum variables in their higher temperature.
There are between 39 and 49 SX Arietis variable stars currently known; ten are noted as being "uncertain" in the General Catalog of Variable Stars.
NGC 772 is a spiral galaxy with an integrated magnitude of 10.3, located southeast of β Arietis and 15 arcminutes west of 15 Arietis.
It is a relatively bright galaxy and shows obvious nebulosity and ellipticity in an amateur telescope.
It is 7.2 by 4.2 arcminutes, meaning that its surface brightness, magnitude 13.6, is significantly lower than its integrated magnitude.
NGC 772 is a class SA(s)b galaxy, which means that it is an unbarred spiral galaxy without a ring that possesses a somewhat prominent bulge and spiral arms that are wound somewhat tightly.
The main arm, on the northwest side of the galaxy, is home to many star forming regions; this is due to previous gravitational interactions with other galaxies.
NGC 772 has a small companion galaxy, NGC 770, that is about 113,000 light-years away from the larger galaxy.
The two galaxies together are also classified as Arp 78 in the Arp peculiar galaxy catalog.
NGC 772 has a diameter of 240,000 light-years and the system is 114 million light-years from Earth.
Another spiral galaxy in Aries is NGC 673, a face-on class SAB(s)c galaxy.
It is a weakly barred spiral galaxy with loosely wound arms.
It has no ring and a faint bulge and is 2.5 by 1.9 arcminutes.
It has two primary arms with fragments located farther from the core.
171,000 light-years in diameter, NGC 673 is 235 million light-years from Earth.
NGC 678 and NGC 680 are a pair of galaxies in Aries that are only about 200,000 light-years apart.
Part of the NGC 691 group of galaxies, both are at a distance of approximately 130 million light-years.
NGC 678 is an edge-on spiral galaxy that is 4.5 by 0.8 arcminutes.
NGC 680, an elliptical galaxy with an asymmetrical boundary, is the brighter of the two at magnitude 12.9; NGC 678 has a magnitude of 13.35.
Both galaxies have bright cores, but NGC 678 is the larger galaxy at a diameter of 171,000 light-years; NGC 680 has a diameter of 72,000 light-years.
NGC 678 is further distinguished by its prominent dust lane.
NGC 691 itself is a spiral galaxy slightly inclined to our line of sight.
It has multiple spiral arms and a bright core.
Because it is so diffuse, it has a low surface brightness.
It has a diameter of 126,000 light-years and is 124 million light-years away.
NGC 877 is the brightest member of an 8-galaxy group that also includes NGC 870, NGC 871, and NGC 876, with a magnitude of 12.53.
It is 2.4 by 1.8 arcminutes and is 178 million light-years away with a diameter of 124,000 light-years.
Its companion is NGC 876, which is about 103,000 light-years from the core of NGC 877.
They are interacting gravitationally, as they are connected by a faint stream of gas and dust.
Arp 276 is a different pair of interacting galaxies in Aries, consisting of NGC 935 and IC 1801.
NGC 821 is an E6 elliptical galaxy.
It is unusual because it has hints of an early spiral structure, which is normally only found in lenticular and spiral galaxies.
NGC 821 is 2.6 by 2.0 arcminutes and has a visual magnitude of 11.3.
Its diameter is 61,000 light-years and it is 80 million light-years away.
Another unusual galaxy in Aries is Segue 2.
Segue 2 is a dwarf galaxy that is a satellite galaxy of the Milky Way, recently discovered to be a potential relic of the epoch of reionization.
Aries is home to several meteor showers.
The Daytime Arietid meteor shower is one of the strongest meteor showers that occurs during the day, lasting from 22 May to 2 July.
It is an annual shower associated with the Marsden group of comets that peaks on 7 June with a maximum zenithal hourly rate of 54 meteors.
Its parent body may be the asteroid Icarus.
The meteors are sometimes visible before dawn, because the radiant is 32 degrees away from the Sun.
They usually appear at a rate of 1–2 per hour as "earthgrazers", meteors that last several seconds and often begin at the horizon.
Because most of the Daytime Arietids are not visible to the naked eye, they are observed in the radio spectrum.
This is possible because of the ionized gas they leave in their wake.
Other meteor showers radiate from Aries during the day; these include the Daytime Epsilon Arietids and the Northern and Southern Daytime May Arietids.
The Jodrell Bank Observatory discovered the Daytime Arietids in 1947 when James Hey and G. S. Stewart adapted the World War II-era radar systems for meteor observations.
The Delta Arietids are another meteor shower radiating from Aries.
Peaking on 9 December with a low peak rate, the shower lasts from 8 December to 14 January, with the highest rates visible from 8 to 14 December.
The average Delta Aquarid meteor is very slow, with an average velocity of per second.
However, this shower sometimes produces bright fireballs.
This meteor shower has northern and southern components, both of which are likely associated with 1990 HA, a near-Earth asteroid.
The Autumn Arietids also radiate from Aries.
The shower lasts from 7 September to 27 October and peaks on 9 October.
Its peak rate is low.
The Epsilon Arietids appear from 12 to 23 October.
Other meteor showers radiating from Aries include the October Delta Arietids, Daytime Epsilon Arietids, Daytime May Arietids, Sigma Arietids, Nu Arietids, and Beta Arietids.
The Sigma Arietids, a class IV meteor shower, are visible from 12 to 19 October, with a maximum zenithal hourly rate of less than two meteors per hour on 19 October.
Aries contains several stars with extrasolar planets.
HIP 14810, a G5 type star, is orbited by three giant planets (those more than ten times the mass of Earth).
HD 12661, like HIP 14810, is a G-type main sequence star, slightly larger than the Sun, with two orbiting planets.
One planet is 2.3 times the mass of Jupiter, and the other is 1.57 times the mass of Jupiter.
HD 20367 is a G0 type star, approximately the size of the Sun, with one orbiting planet.
The planet, discovered in 2002, has a mass 1.07 times that of Jupiter and orbits every 500 days.
Explanatory notes

Citations

Bibliography

Online sources


"SIMBAD"



</doc>
<doc id="799" url="https://en.wikipedia.org/wiki?curid=799" title="Aquarius (constellation)">
Aquarius (constellation)

Aquarius is a constellation of the zodiac, situated between Capricornus and Pisces.
Its name is Latin for "water-carrier" or "cup-carrier", and its symbol is , a representation of water.
Aquarius is one of the oldest of the recognized constellations along the zodiac (the Sun's apparent path).
It was one of the 48 constellations listed by the 2nd century astronomer Ptolemy, and it remains one of the 88 modern constellations.
It is found in a region often called the Sea due to its profusion of constellations with watery associations such as Cetus the whale, Pisces the fish, and Eridanus the river.
At apparent magnitude 2.9, Beta Aquarii is the brightest star in the constellation.
Aquarius is identified as "The Great One" in the Babylonian star catalogues and represents the god Ea himself, who is commonly depicted holding an overflowing vase.
The Babylonian star-figure appears on entitlement stones and cylinder seals from the second millennium.
It contained the winter solstice in the Early Bronze Age.
In Old Babylonian astronomy, Ea was the ruler of the southernmost quarter of the Sun's path, the "Way of Ea", corresponding to the period of 45 days on either side of winter solstice.
Aquarius was also associated with the destructive floods that the Babylonians regularly experienced, and thus was negatively connoted.
In Ancient Egypt astronomy, Aquarius was associated with the annual flood of the Nile; the banks were said to flood when Aquarius put his jar into the river, beginning spring.
In the Greek tradition, the constellation came to be represented simply as a single vase from which a stream poured down to Piscis Austrinus.
The name in the Hindu zodiac is likewise "kumbha" "water-pitcher".
In Greek mythology, Aquarius is sometimes associated with Deucalion, the son of Prometheus who built a ship with his wife Pyrrha to survive an imminent flood.
They sailed for nine days before washing ashore on Mount Parnassus.
Aquarius is also sometimes identified with beautiful Ganymede, a youth in Greek mythology and the son of Trojan king Tros, who was taken to Mount Olympus by Zeus to act as cup-carrier to the gods.
Neighboring Aquila represents the eagle, under Zeus' command, that snatched the young boy; some versions of the myth indicate that the eagle was in fact Zeus transformed.
An alternative version of the tale recounts Ganymede's kidnapping by the goddess of the dawn, Eos, motivated by her affection for young men; Zeus then stole him from Eos and employed him as cup-bearer.
Yet another figure associated with the water bearer is Cecrops I, a king of Athens who sacrificed water instead of wine to the gods.
In the first century, Ptolemy's "Almagest" established the common Western depiction of Aquarius.
His water jar, an asterism itself, consists of Gamma, Pi, Eta, and Zeta Aquarii; it pours water in a stream of more than 20 stars terminating with Fomalhaut, now assigned solely to Piscis Austrinus.
The water bearer's head is represented by 5th magnitude 25 Aquarii while his left shoulder is Beta Aquarii; his right shoulder and forearm are represented by Alpha and Gamma Aquarii respectively.
In Chinese astronomy, the stream of water flowing from the Water Jar was depicted as the "Army of Yu-Lin" ("Yu-lin-kiun" or "Yulinjun").
The name "Yu-lin" means "feathers and forests", referring to the numerous light-footed soldiers from the northern reaches of the empire represented by these faint stars.
The constellation's stars were the most numerous of any Chinese constellation, numbering 45, the majority of which were located in modern Aquarius.
The celestial army was protected by the wall "Leibizhen", which counted Iota, Lambda, Phi, and Sigma Aquarii among its 12 stars.
88, 89, and 98 Aquarii represent "Fou-youe", the axes used as weapons and for hostage executions.
Also in Aquarius is "Loui-pi-tchin", the ramparts that stretch from 29 and 27 Piscium and 33 and 30 Aquarii through Phi, Lambda, Sigma, and Iota Aquarii to Delta, Gamma, Kappa, and Epsilon Capricorni.
Near the border with Cetus, the axe "Fuyue" was represented by three stars; its position is disputed and may have instead been located in Sculptor.
"Tienliecheng" also has a disputed position; the 13-star castle replete with ramparts may have possessed Nu and Xi Aquarii but may instead have been located south in Piscis Austrinus.
The Water Jar asterism was seen to the ancient Chinese as the tomb, "Fenmu".
Nearby, the emperors' mausoleum "Xiuliang" stood, demarcated by Kappa Aquarii and three other collinear stars.
"Ku" ("crying") and "Qi" ("weeping"), each composed of two stars, were located in the same region.
Three of the Chinese lunar mansions shared their name with constellations.
"Nu", also the name for the 10th lunar mansion, was a handmaiden represented by Epsilon, Mu, 3, and 4 Aquarii.
The 11th lunar mansion shared its name with the constellation "Xu" ("emptiness"), formed by Beta Aquarii and Alpha Equulei; it represented a bleak place associated with death and funerals.
"Wei", the rooftop and 12th lunar mansion, was a V-shaped constellation formed by Alpha Aquarii, Theta Pegasi, and Epsilon Pegasi; it shared its name with two other Chinese constellations, in modern-day Scorpius and Aries.
Despite both its prominent position on the zodiac and its large size, Aquarius has no particularly bright stars, its four brightest stars being less than magnitude 2.
However, recent research has shown that there are several stars lying within its borders that possess planetary systems.
The two brightest stars, Alpha and Beta Aquarii, are luminous yellow supergiants, of spectral types G0Ib and G2Ib respectively, that were once hot blue-white B-class main sequence stars 5 to 9 times as massive as the Sun.
The two are also moving through space perpendicular to the plane of the Milky Way.
Just shading Alpha, Beta Aquarii is the brightest star in Aquarius with an apparent magnitude of 2.91.
It also has the proper name of Sadalsuud.
Having cooled and swollen to around 50 times the Sun's diameter, it is around 2200 times as luminous as the Sun.
It is around 6.4 times as massive as the Sun and around 56 million years old.
Sadalsuud is 540 ± 20 light-years from Earth.
Alpha Aquarii, also known as Sadalmelik, has an apparent magnitude of 2.94.
It is 520 ± 20 light-years distant from Earth, and is around 6.5 times as massive as the Sun and 3000 times as luminous.
It is 53 million years old.
γ Aquarii, also called Sadachbia, is a white main sequence star of spectral type star of spectral type A0V that is between 158 and 315 million years old and is around two and a half times the Sun's mass, and double its radius.
Of magnitude 3.85, it is 164 ± 9 light years away.
It has a luminosity of .
The name Sadachbia comes from the Arabic for "lucky stars of the tents", "sa'd al-akhbiya".
δ Aquarii, also known as Skat or Scheat is a blue-white A2 spectral type star of magnitude 3.27 and luminosity of .
ε Aquarii, also known as Albali, is a blue-white A1 spectral type star with an apparent magnitude of 3.77, an absolute magnitude of 1.2, and a luminosity of .
ζ Aquarii is an F2 spectral type double star; both stars are white.
Overall, it appears to be of magnitude 3.6 and luminosity of .
The primary has a magnitude of 4.53 and the secondary a magnitude of 4.31, but both have an absolute magnitude of 0.6.
Its orbital period is 760 years; the two components are currently moving farther apart.
θ Aquarii, sometimes called Ancha, is a G8 spectral type star with an apparent magnitude of 4.16 and an absolute magnitude of 1.4.

λ Aquarii, also called Hudoor or Ekchusis, is an M2 spectral type star of magnitude 3.74 and luminosity of .
ξ Aquarii, also called Bunda, is an A7 spectral type star with an apparent magnitude of 4.69 and an absolute magnitude of 2.4.

π Aquarii, also called Seat, is a B0 spectral type star with an apparent magnitude of 4.66 and an absolute magnitude of -4.1.
Twelve exoplanet systems have been found in Aquarius as of 2013.
Gliese 876, one of the nearest stars to Earth at a distance of 15 light-years, was the first red dwarf star to be found to possess a planetary system.
It is orbited by four planets, including one terrestrial planet 6.6 times the mass of Earth.
The planets vary in orbital period from 2 days to 124 days.
91 Aquarii is an orange giant star orbited by one planet, 91 Aquarii b. The planet's mass is 2.9 times the mass of Jupiter, and its orbital period is 182 days.
Gliese 849 is a red dwarf star orbited by the first known long-period Jupiter-like planet, Gliese 849 b. The planet's mass is 0.99 times that of Jupiter and its orbital period is 1,852 days.
There are also less-prominent systems in Aquarius.
WASP-6, a type G8 star of magnitude 12.4, is host to one exoplanet, WASP-6 b. The star is 307 parsecs from Earth and has a mass of 0.888 solar masses and a radius of 0.87 solar radii.
WASP-6 b was discovered in 2008 by the transit method.
It orbits its parent star every 3.36 days at a distance of 0.042 astronomical units (AU).
It is 0.503 Jupiter masses but has a proportionally larger radius of 1.224 Jupiter radii.
HD 206610, a K0 star located 194 parsecs from Earth, is host to one planet, HD 206610 b. The host star is larger than the Sun; more massive at 1.56 solar masses and larger at 6.1 solar radii.
The planet was discovered by the radial velocity method in 2010 and has a mass of 2.2 Jupiter masses.
It orbits every 610 days at a distance of 1.68 AU.
Much closer to its sun is WASP-47 b, which orbits every 4.15 days only 0.052 AU from its sun, yellow dwarf (G9V) WASP-47.
WASP-47 is close in size to the Sun, having a radius of 1.15 solar radii and a mass even closer at 1.08 solar masses.
WASP-47 b was discovered in 2011 by the transit method, like WASP-6 b. It is slightly larger than Jupiter with a mass of 1.14 Jupiter masses and a radius of 1.15 Jupiter masses.
There are several more single-planet systems in Aquarius.
HD 210277, a magnitude 6.63 yellow star located 21.29 parsecs from Earth, is host to one known planet: HD 210277 b. The 1.23 Jupiter mass planet orbits at nearly the same distance as Earth orbits the Sun1.1 AU, though its orbital period is significantly longer at around 442 days.
HD 210277 b was discovered earlier than most of the other planets in Aquarius, detected by the radial velocity method in 1998.
The star it orbits resembles the Sun beyond their similar spectral class; it has a radius of 1.1 solar radii and a mass of 1.09 solar masses.
HD 212771 b, a larger planet at 2.3 Jupiter masses, orbits host star HD 212771 at a distance of 1.22 AU.
The star itself, barely below the threshold of naked-eye visibility at magnitude 7.6, is a G8IV (yellow subgiant) star located 131 parsecs from Earth.
Though it has a similar mass to the Sun1.15 solar massesit is significantly less dense with its radius of 5 solar radii.
Its lone planet was discovered in 2010 by the radial velocity method, like several other exoplanets in the constellation.
As of 2013, there were only two known multiple-planet systems within the bounds of Aquarius: the Gliese 876 and HD 215152 systems.
The former is quite prominent; the latter has only two planets and has a host star farther away at 21.5 parsecs.
The HD 215152 system consists of the planets HD 215152 b and HD 215152 c orbiting their K0-type, magnitude 8.13 sun.
Both discovered in 2011 by the radial velocity method, the two tiny planets orbit very close to their host star.
HD 215152 c is the larger at 0.0097 Jupiter masses (still significantly larger than the Earth, which weighs in at 0.00315 Jupiter masses); its smaller sibling is barely smaller at 0.0087 Jupiter masses.
The error in the mass measurements (0.0032 and respectively) is large enough to make this discrepancy statistically insignificant.
HD 215152 c also orbits further from the star than HD 215152 b, 0.0852 AU compared to 0.0652.
On 23 February 2017, NASA announced that ultracool dwarf star TRAPPIST-1 in Aquarius has seven Earth-like rocky planets.
Of these, three are in the system's habitable zone, and may contain water.
The discovery of the TRAPPIST-1 system is seen by astronomers as a significant step toward finding life beyond Earth.
Because of its position away from the galactic plane, the majority of deep-sky objects in Aquarius are galaxies, globular clusters, and planetary nebulae.
Aquarius contains three deep sky objects that are in the Messier catalog: the globular clusters Messier 2, Messier 72, and the open cluster Messier 73.
Two well-known planetary nebulae are also located in Aquarius: the Saturn Nebula (NGC 7009), to the southeast of μ Aquarii; and the famous Helix Nebula (NGC 7293), southwest of δ Aquarii.
M2, also catalogued as NGC 7089, is a rich globular cluster located approximately 37,000 light-years from Earth.
At magnitude 6.5, it is viewable in small-aperture instruments, but a 100 mm aperture telescope is needed to resolve any stars.
M72, also catalogued as NGC 6981, is a small 9th magnitude globular cluster located approximately 56,000 light-years from Earth.
M73, also catalogued as NGC 6994, is an open cluster with highly disputed status.
Aquarius is also home to several planetary nebulae.
NGC 7009, also known as the Saturn Nebula, is an 8th magnitude planetary nebula located 3,000 light-years from Earth.
It was given its moniker by the 19th century astronomer Lord Rosse for its resemblance to the planet Saturn in a telescope; it has faint protrusions on either side that resemble Saturn's rings.
It appears blue-green in a telescope and has a central star of magnitude 11.3.
Compared to the Helix Nebula, another planetary nebula in Aquarius, it is quite small.
NGC 7293, also known as the Helix Nebula, is the closest planetary nebula to Earth at a distance of 650 light-years.
It covers 0.25 square degrees, making it also the largest planetary nebula as seen from Earth.
However, because it is so large, it is only viewable as a very faint object, though it has a fairly high integrated magnitude of 6.0.
One of the visible galaxies in Aquarius is NGC 7727, of particular interest for amateur astronomers who wish to discover or observe supernovae.
A spiral galaxy (type S), it has an integrated magnitude of 10.7 and is 3 by 3 arcseconds.
NGC 7252 is a tangle of stars resulting from the collision of two large galaxies and is known as the Atoms-for-Peace galaxy because of its resemblance to a cartoon atom.
There are three major meteor showers with radiants in Aquarius: the Eta Aquariids, the Delta Aquariids, and the Iota Aquariids.
The Eta Aquariids are the strongest meteor shower radiating from Aquarius.
It peaks between 5 and 6 May with a rate of approximately 35 meteors per hour.
Originally discovered by Chinese astronomers in 401, Eta Aquariids can be seen coming from the Water Jar beginning on April 21 and as late as May 12.
The parent body of the shower is Halley's Comet, a periodic comet.
Fireballs are common shortly after the peak, approximately between May 9 and May 11.
The normal meteors appear to have yellow trails.
The Delta Aquariids is a double radiant meteor shower that peaks first on 29 July and second on 6 August.
The first radiant is located in the south of the constellation, while the second radiant is located in the northern circlet of Pisces asterism.
The southern radiant's peak rate is about 20 meteors per hour, while the northern radiant's peak rate is about 10 meteors per hour.
The Iota Aquariids is a fairly weak meteor shower that peaks on 6 August, with a rate of approximately 8 meteors per hour.
, the Sun appears in the constellation Aquarius from 16 February to 11 March.
In tropical astrology, the Sun is considered to be in the sign Aquarius from 20 January to 19 February, and in sidereal astrology, from 15 February to 14 March.
Aquarius is also associated with the Age of Aquarius, a concept popular in 1960s counterculture.
Despite this prominence, the Age of Aquarius will not dawn until the year 2597, as an astrological age does not begin until the Sun is in a particular constellation on the vernal equinox.
</doc>
<doc id="800" url="https://en.wikipedia.org/wiki?curid=800" title="Anime">
Anime

Anime () is hand-drawn and computer animation originating from or associated with Japan.
The word "anime" is the Japanese term for "animation", which means all forms of animated media.
Outside Japan, "anime" refers specifically to animation from Japan or as a Japanese-disseminated animation style often characterized by colorful graphics, vibrant characters and fantastical themes.
The culturally abstract approach to the word's meaning may open up the possibility of anime produced in countries other than Japan.
For simplicity, many Westerners strictly view anime as a Japanese animation product.
Some scholars suggest defining anime as specifically or quintessentially Japanese may be related to a new form of Orientalism.
The earliest commercial Japanese animation dates to 1917, and Japanese anime production has since continued to increase steadily.
The characteristic anime art style emerged in the 1960s with the works of Osamu Tezuka and spread internationally in the late twentieth century, developing a large domestic and international audience.
Anime is distributed theatrically, by way of television broadcasts, directly to home media, and over the Internet.
It is classified into numerous genres targeting diverse broad and niche audiences.
Anime is a diverse art form with distinctive production methods and techniques that have been adapted over time in response to emergent technologies.
It consists of an ideal story-telling mechanism, combining graphic art, characterization, cinematography, and other forms of imaginative and individualistic techniques.
The production of anime focuses less on the animation of movement and more on the realism of settings as well as the use of camera effects, including panning, zooming, and angle shots.
Being hand-drawn, anime is separated from reality by a crucial gap of fiction that provides an ideal path for escapism that audiences can immerse themselves into with relative ease.
Diverse art styles are used and character proportions and features can be quite varied, including characteristically large emotive or realistically sized eyes.
The anime industry consists of over 430 production studios, including major names like Studio Ghibli, Gainax, and Toei Animation.
Despite comprising only a fraction of Japan's domestic film market, anime makes up a majority of Japanese DVD sales.
It has also seen international success after the rise of English-dubbed programming.
This rise in international popularity has resulted in non-Japanese productions using the anime art style.
Whether these works are anime-influenced animation or proper anime is a subject for debate amongst fans.
Anime is an art form, specifically animation, that includes all genres found in cinema, but it can be mistakenly classified as a genre.
In Japanese, the term "anime" is used as a blanket term to refer to all forms of animation from around the world.
In English, "anime" () is more restrictively used to denote a "Japanese-style animated film or television entertainment" or as "a style of animation created in Japan".
The etymology of the word "anime" is disputed.
The English term "animation" is written in Japanese "katakana" as ("animēshon", ) and is ("anime") in its shortened form.
The pronunciation of "anime" in Japanese differs from pronunciations in other languages such as Standard English (pronunciation: ), which has different vowels and stress with regards to Japanese, where each mora carries equal stress.
As with a few other Japanese words such as "saké", "Pokémon", and "Kobo Abé," English-language texts sometimes spell "anime" as "animé" (as in French), with an acute accent over the final "e", to cue the reader to pronounce the letter, not to leave it silent as Standard English orthography may suggest.
Some sources claim that "anime" derives from the French term for animation "dessin animé", but others believe this to be a myth derived from the French popularity of the medium in the late 1970s and 1980s.
In English, "anime"—when used as a common noun—normally functions as a mass noun.
(For example: "Do you watch anime?"
or "How much anime have you collected?")
Prior to the widespread use of "anime", the term "Japanimation" was prevalent throughout the 1970s and 1980s.
In the mid-1980s, the term "anime" began to supplant "Japanimation".
In general, the latter term now only appears in period works where it is used to distinguish and identify Japanese animation.
The word "anime" has also been criticised, e.g.
in 1987, when Hayao Miyazaki stated that he despised the truncated word "anime" because to him it represented the desolation of the Japanese animation industry.
He equated the desolation with animators lacking motivation and with mass-produced, overly expressionistic products relying upon a fixed iconography of facial expressions and protracted and exaggerated action scenes but lacking depth and sophistication in that they do not attempt to convey emotion or thought.
The first format of anime was theatrical viewing which originally began with commercial productions in 1917.
Originally the animated flips were crude and required played musical components before adding sound and vocal components to the production.
On July 14, 1958, Nippon Television aired "Mogura no Abanchūru" ("Mole's Adventure"), both the first televised and first color anime to debut.
It wasn't until the 1960s when the first televised series were broadcast and it has remained a popular medium since.
Works released in a direct to video format are called "original video animation" (OVA) or "original animation video" (OAV); and are typically not released theatrically or televised prior to home media release.
The emergence of the Internet has led some animators to distribute works online in a format called "original net anime" (ONA).
The home distribution of anime releases were popularized in the 1980s with the VHS and LaserDisc formats.
The VHS NTSC video format used in both Japan and the United States is credited as aiding the rising popularity of anime in the 1990s.
The Laser Disc and VHS formats were transcended by the DVD format which offered the unique advantages; including multiple subtitling and dubbing tracks on the same disc.
The DVD format also has its drawbacks in the its usage of region coding; adopted by the industry to solve licensing, piracy and export problems and restricted region indicated on the DVD player.
The Video CD (VCD) format was popular in Hong Kong and Taiwan, but became only a minor format in the United States that was closely associated with bootleg copies.
Japanese animation began in the early 20th century, when Japanese filmmakers experimented with the animation techniques also pioneered in France, Germany, the United States and Russia.
A claim for the earliest Japanese animation is "Katsudō Shashin", an undated and private work by an unknown creator.
In 1917, the first professional and publicly displayed works began to appear.
Animators such as Ōten Shimokawa and Seitarou Kitayama produced numerous works, with the oldest surviving film being Kouchi's "Namakura Gatana", a two-minute clip of a samurai trying to test a new sword on his target only to suffer defeat.
The 1923 Great Kantō earthquake resulted in widespread destruction to Japan's infrastructure and the destruction of Shimokawa's warehouse, destroying most of these early works.
By the 1930s animation was well established in Japan as an alternative format to the live-action industry.
It suffered competition from foreign producers and many animators, Noburō Ōfuji and Yasuji Murata, who still worked in cheaper cutout animation rather than cel animation.
Other creators, Kenzō Masaoka and Mitsuyo Seo, nonetheless made great strides in animation technique; they benefited from the patronage of the government, which employed animators to produce educational shorts and propaganda.
The first talkie anime was "Chikara to Onna no Yo no Naka", produced by Masaoka in 1933.
By 1940, numerous anime artists' organizations had risen, including the Shin Mangaha Shudan and Shin Nippon Mangaka.
The first feature-length animated film was "Momotaro's Divine Sea Warriors" directed by Seo in 1944 with sponsorship by the Imperial Japanese Navy.
The success of The Walt Disney Company's 1937 feature film "Snow White and the Seven Dwarfs" profoundly influenced many Japanese animators.
In the 1960s, manga artist and animator Osamu Tezuka adapted and simplified many Disney animation techniques to reduce costs and to limit the number of frames in productions.
He intended this as a temporary measure to allow him to produce material on a tight schedule with inexperienced animation staff.
"Three Tales", aired in 1960, was the first anime shown on television.
The first anime television series was "Otogi Manga Calendar", aired from 1961 to 1964.
The 1970s saw a surge of growth in the popularity of "manga", Japanese comic books and graphic novels, many of which were later animated.
The work of Osamu Tezuka drew particular attention: he has been called a "legend" and the "god of manga".
His work—and that of other pioneers in the field—inspired characteristics and genres that remain fundamental elements of anime today.
The giant robot genre (known as "mecha" outside Japan), for instance, took shape under Tezuka, developed into the Super Robot genre under Go Nagai and others, and was revolutionized at the end of the decade by Yoshiyuki Tomino who developed the Real Robot genre.
Robot anime like the "Gundam" and "The Super Dimension Fortress Macross" series became instant classics in the 1980s, and the robot genre of anime is still one of the most common in Japan and worldwide today.
In the 1980s, anime became more accepted in the mainstream in Japan (although less than manga), and experienced a boom in production.
Following a few successful adaptations of anime in overseas markets in the 1980s, anime gained increased acceptance in those markets in the 1990s and even more at the turn of the 21st century.
In 2002, "Spirited Away", a Studio Ghibli production directed by Hayao Miyazaki won the Golden Bear at the Berlin International Film Festival and in 2003 at the 75th Academy Awards it won the Academy Award for Best Animated Feature.
Anime are often classified by target demographic, including , , and a diverse range of genres targeting an adult audience.
Shoujo and shounen anime sometimes contain elements popular with children of both sexes in an attempt to gain crossover appeal.
Adult anime may feature a slower pace or greater plot complexity that younger audiences may typically find unappealing, as well as adult themes and situations.
A subset of adult anime works featuring pornographic elements are labeled "R18" in Japan, and are internationally known as "hentai" (originating from ).
By contrast, some anime subgenres incorporate "ecchi", sexual themes or undertones without depictions of sexual intercourse, as typified in the comedic or harem genres; due to its popularity among adolescent and adult anime enthusiasts, the inclusion of such elements is considered a form of fan service.
Some genres explore homosexual romances, such as "yaoi" (male homosexuality) and "yuri" (female homosexuality).
While often used in a pornographic context, the terms can also be used broadly in a wider context to describe or focus on the themes or the development of the relationships themselves.
Anime's genre classification differs from other types of animation and does not lend itself to simple classification.
Gilles Poitras compared the labeling "Gundam 0080" and its complex depiction of war as a "giant robot" anime akin to simply labeling "War and Peace" a "war novel".
Science fiction is a major anime genre and includes important historical works like Tezuka's "Astro Boy" and Yokoyama's "Tetsujin 28-go".
A major subgenre of science fiction is mecha, with the "Gundam" metaseries being iconic.
The diverse fantasy genre includes works based on Asian and Western traditions and folklore; examples include the Japanese feudal fairytale "InuYasha", and the depiction of Scandinavian goddesses who move to Japan to maintain a computer called Yggdrasil in "Ah!
My Goddess".
Genre crossing in anime is also prevalent, such as the blend of fantasy and comedy in "Dragon Half", and the incorporation of slapstick humor in the crime anime film "Castle of Cagliostro".
Other subgenres found in anime include magical girl, harem, sports, martial arts, literary adaptations, medievalism, and war.
Anime differs greatly from other forms of animation by its diverse art styles, methods of animation, its production, and its process.
Visually, anime is a diverse art form that contains a wide variety of art styles, differing from one creator, artist, and studio.
While no one art style predominates anime as a whole, they do share some similar attributes in terms of animation technique and character design.
Anime follows the typical production of animation, including storyboarding, voice acting, character design, and cel production ("Shirobako", itself a series, highlights many of the aspects involved in anime production).
Since the 1990s, animators have increasingly used computer animation to improve the efficiency of the production process.
Artists like Noburō Ōfuji pioneered the earliest anime works, which were experimental and consisted of images drawn on blackboards, stop motion animation of paper cutouts, and silhouette animation.
Cel animation grew in popularity until it came to dominate the medium.
In the 21st century, the use of other animation techniques is mostly limited to independent short films, including the stop motion puppet animation work produced by Tadahito Mochinaga, Kihachirō Kawamoto and Tomoyasu Murata.
Computers were integrated into the animation process in the 1990s, with works such as "Ghost in the Shell" and "Princess Mononoke" mixing cel animation with computer-generated images.
Fuji Film, a major cel production company, announced it would stop cel production, producing an industry panic to procure cel imports and hastening the switch to digital processes.
Prior to the digital era, anime was produced with traditional animation methods using a pose to pose approach.
The majority of mainstream anime uses fewer expressive key frames and more in-between animation.
Japanese animation studios were pioneers of many limited animation techniques, and have given anime a distinct set of conventions.
Unlike Disney animation, where the emphasis is on the movement, anime emphasizes the art quality and let limited animation techniques make up for the lack of time spent on movement.
Such techniques are often used not only to meet deadlines but also as artistic devices.
Anime scenes place emphasis on achieving three-dimensional views, and backgrounds are instrumental in creating the atmosphere of the work.
The backgrounds are not always invented and are occasionally based on real locations, as exemplified in "Howl's Moving Castle" and "The Melancholy of Haruhi Suzumiya".
Oppliger stated that anime is one of the rare mediums where putting together an all-star cast usually comes out looking "tremendously impressive".
The cinematic effects of anime differentiates itself from the stage plays found in American animation.
Anime is cinematically shot as if by camera, including panning, zooming, distance and angle shots to more complex dynamic shots that would be difficult to produce in reality.
In anime, the animation is produced before the voice acting, contrary to American animation which does the voice acting first; this can cause lip sync errors in the Japanese version.
Body proportions of human anime characters tend to accurately reflect the proportions of the human body in reality.
The height of the head is considered by the artist as the base unit of proportion.
Head heights can vary, but most anime characters are about seven to eight heads tall.
Anime artists occasionally make deliberate modifications to body proportions to produce super deformed characters that feature a disproportionately small body compared to the head; many super deformed characters are two to four heads tall.
Some anime works like "Crayon Shin-chan" completely disregard these proportions, in such a way that they resemble cariacatured Western cartoons.
A common anime character design convention is exaggerated eye size.
The animation of characters with large eyes in anime can be traced back to Osamu Tezuka, who was deeply influenced by such early animation characters as Betty Boop, who was drawn with disproportionately large eyes.
Tezuka is a central figure in anime and manga history, whose iconic art style and character designs allowed for the entire range of human emotions to be depicted solely through the eyes.
The artist adds variable color shading to the eyes and particularly to the cornea to give them greater depth.
Generally, a mixture of a light shade, the tone color, and a dark shade is used.
Cultural anthropologist Matt Thorn argues that Japanese animators and audiences do not perceive such stylized eyes as inherently more or less foreign.
However, not all anime have large eyes.
For example, the works of Hayao Miyazaki are known for having realistically proportioned eyes, as well as realistic hair colors on their characters.
Hair in anime is often unnaturally lively and colorful or uniquely styled.
The movement of hair in anime is exaggerated and "hair action" is used to emphasize the action and emotions of characters for added visual effect.
Poitras traces hairstyle color to cover illustrations on manga, where eye-catching artwork and colorful tones are attractive for children's manga.
Despite being produced for a domestic market, anime features characters whose race or nationality is not always defined, and this is often a deliberate decision, such as in the "Pokémon" animated series.
Anime and manga artists often draw from a common canon of iconic facial expression illustrations to denote particular moods and thoughts.
These techniques are often different in form than their counterparts in Western animation, and they include a fixed iconography that is used as shorthand for certain emotions and moods.
For example, a male character may develop a nosebleed when aroused.
A variety of visual symbols are employed, including sweat drops to depict nervousness, visible blushing for embarrassment, or glowing eyes for an intense glare.
The opening and credits sequences of most anime television episodes are accompanied by Japanese pop or rock songs, often by reputed bands.
They may be written with the series in mind, but are also aimed at the general music market, and therefore often allude only vaguely or not at all to the themes or plot of the series.
Pop and rock songs are also sometimes used as incidental music ("insert songs") in an episode, often to highlight particularly important scenes.
The animation industry consists of more than 430 production companies with some of the major studios including Toei Animation, Gainax, Madhouse, Gonzo, Sunrise, Bones, TMS Entertainment, Nippon Animation, P.A.Works, Studio Pierrot and Studio Ghibli.
Many of the studios are organized into a trade association, The Association of Japanese Animations.
There is also a labor union for workers in the industry, the Japanese Animation Creators Association.
Studios will often work together to produce more complex and costly projects, as done with Studio Ghibli's "Spirited Away".
An anime episode can cost between US$100,000 and US$300,000 to produce.
In 2001, animation accounted for 7% of the Japanese film market, above the 4.6% market share for live-action works.
The popularity and success of anime is seen through the profitability of the DVD market, contributing nearly 70% of total sales.
According to a 2016 article on Nikkei Asian Review, Japanese television stations have bought over worth of anime from production companies "over the past few years", compared with under from overseas.
There has been a rise in sales of shows to television stations in Japan, caused by late night anime with adults as the target demographic.
This type of anime is less popular outside Japan, being considered "more of a niche product".
"Spirited Away" (2001) is the all-time highest-grossing film in Japan.
It was also the highest-grossing anime film worldwide until it was overtaken by Makoto Shinkai's 2016 film "Your Name".
Anime films represent a large part of the highest-grossing Japanese films yearly in Japan, with 6 out of the top 10 in 2014, in 2015 and also in 2016.
Anime has to be licensed by companies in other countries in order to be legally released.
While anime has been licensed by its Japanese owners for use outside Japan since at least the 1960s, the practice became well-established in the United States in the late 1970s to early 1980s, when such TV series as "Gatchaman" and "Captain Harlock" were licensed from their Japanese parent companies for distribution in the US market.
The trend towards American distribution of anime continued into the 1980s with the licensing of titles such as "Voltron" and the 'creation' of new series such as "Robotech" through use of source material from several original series.
In the early 1990s, several companies began to experiment with the licensing of less children-oriented material.
Some, such as A.D.
Vision, and Central Park Media and its imprints, achieved fairly substantial commercial success and went on to become major players in the now very lucrative American anime market.
Others, such as AnimEigo, achieved limited success.
Many companies created directly by Japanese parent companies did not do as well, most releasing only one or two titles before completing their American operations.
Licenses are expensive, often hundreds of thousands of dollars for one series and tens of thousands for one movie.
The prices vary widely; for example, "" cost only $91,000 to license while "Kurau Phantom Memory" cost $960,000.
Simulcast Internet streaming rights can be less expensive, with prices around $1,000-$2,000 an episode, but can also be more expensive, with some series costing more than per episode.
The anime market for the United States was worth approximately $2.74 billion in 2009.
Dubbed animation began airing in the United States in 2000 on networks like The WB and Cartoon Network's Adult Swim.
In 2005, this resulted in five of the top ten anime titles having previously aired on Cartoon Network.
As a part of localization, some editing of cultural references may occur to better follow the references of the non-Japanese culture.
The cost of English localization averages US $10,000 per episode.
The industry has been subject to both praise and condemnation for fansubs, the addition of unlicensed and unauthorized subtitled translations of anime series or films.
Fansubs, which were originally distributed on VHS bootlegged cassettes in the 1980s, have been freely available and disseminated online since the 1990s.
Since this practice raises concerns for copyright and piracy issues, fansubbers tend to adhere to an unwritten moral code to destroy or no longer distribute an anime once an official translated or subtitled version becomes licensed.
They also try to encourage viewers to buy an official copy of the release once it comes out in English, although fansubs typically continue to circulate through file sharing networks.
Even so, the laid back regulations of the Japanese animation industry tends to overlook these issues, allowing it to grow underground and thus increasing the popularity until there is a demand for official high quality releases for animation companies.
This has led to an increase in global popularity with Japanese animations, reaching $40 million in sales in 2004.
Legal international availability of anime on the Internet has changed in recent years, with simulcasts of series available on websites like Crunchyroll.
Japan External Trade Organization (JETRO) valued the domestic anime market in Japan at (), including from licensed products, in 2005.
JETRO reported sales of overseas anime exports in 2004 to be ().
JETRO valued the anime market in the United States at (), including in home video sales and over from licensed products, in 2005.
JETRO projected in 2005 that the worldwide anime market, including sales of licensed products, would grow to ().
The anime market in China was valued at in 2017, and is projected to reach by 2020.
The anime industry has several annual awards which honor the year's best works.
Major annual awards in Japan include the Ōfuji Noburō Award, the Mainichi Film Award for Best Animation Film, the Animation Kobe Awards, the Japan Media Arts Festival animation awards, the Tokyo Anime Award and the Japan Academy Prize for Animation of the Year.
In the United States, anime films compete in the ICv2.com Anime Awards There were also the American Anime Awards, which were designed to recognize excellence in anime titles nominated by the industry, and were held only once in 2006.
Anime productions have also been nominated and won awards not exclusively for anime, like the Academy Award for Best Animated Feature or the Golden Bear.
Anime has become commercially profitable in Western countries, as demonstrated by early commercially successful Western adaptations of anime, such as "Astro Boy" and "Speed Racer".
Early American adaptions in the 1960s made Japan expand into the continental European market, first with productions aimed at European and Japanese children, such as "Heidi", "Vicky the Viking" and "Barbapapa", which aired in various countries.
Particularly Italy, Spain and France grew an interest into Japan's output, due to its cheap selling price and productive output.
In fact, Italy imported the most anime outside of Japan.
These mass imports influenced anime popularity in South American, Arabic and German markets.
The beginning of 1980 saw the introduction of Japanese anime series into the American culture.
In the 1990s, Japanese animation slowly gained popularity in America.
Media companies such as Viz and Mixx began publishing and releasing animation into the American market.
The 1988 film "Akira" is largely credited with popularizing anime in the Western world during the early 1990s, before anime was further popularized by television shows such "Pokémon" and "Dragon Ball" in the late 1990s.
The growth of the Internet later provided Western audiences an easy way to access Japanese content.
This is especially the case with net services such as Netflix and Crunchyroll.
As a direct result, various interests surrounding Japan has increased.
Anime clubs gave rise to anime conventions in the 1990s with the "anime boom", a period marked by increased popularity of anime.
These conventions are dedicated to anime and manga and include elements like cosplay contests and industry talk panels.
Cosplay, a portmanteau for "costume play", is not unique to anime and has become popular in contests and masquerades at anime conventions.
Japanese culture and words have entered English usage through the popularity of the medium, including "otaku", an unflattering Japanese term commonly used in English to denote a fan of anime and manga.
Another word that has arisen describing fans in the United States is "wapanese" meaning White individuals who desire to be Japanese, or later known as "weeaboo" for individuals who demonstrate a strong interest in Japanese anime subculture, which is a term that originated from abusive content posted on the popular bulletin board website 4chan.org.
Anime enthusiasts have produced fan fiction and fan art, including computer wallpaper and anime music videos.
As of the 2010s, many anime fans use online communities and databases such as MyAnimeList to discuss anime and track their progress watching respective series.
One of the key points that made anime different from a handful of the Western cartoons is the potential for visceral content.
Once the expectation that the aspects of visual intrigue or animation being just for children is put aside, the audience can realize that themes involving violence, suffering, sexuality, pain, and death can all be storytelling elements utilized in anime as much as other types of media.
However, as anime itself became increasingly popular, its styling has been inevitably the subject of both satire and serious creative productions.
"South Park"s "Chinpokomon" and "Good Times with Weapons" episodes, Adult Swim's "Perfect Hair Forever", and Nickelodeon's "Kappa Mikey" are examples of satirical depictions of Japanese culture and anime.
Some works have sparked debate for blurring the lines between satire and serious "anime style" productions, such as the American anime style production "".
These anime styled works have become defined as anime-influenced animation, in an attempt to classify all anime styled works of non-Japanese origin.
Some creators of these works cite anime as a source of inspiration and like the French production team for "Ōban Star-Racers" moved to Tokyo to collaborate with a Japanese production team.
When anime is defined as a "style" rather than as a national product it leaves open the possibility of anime being produced in other countries.
A U.A.E.-Filipino produced TV series called "Torkaizer" is dubbed as the "Middle East's First Anime Show", and is currently in production, which is currently looking for funding.
The web-based series "RWBY" is produced using an anime art style and has been declared to be anime.
In addition, the series will be released in Japan, under the label of "anime" per the Japanese definition of the term and referenced as an "American-made anime".
Netflix declared the company's intention to produce anime.
In doing so, the company is offering a more accessible channel for distribution to Western markets.
Defining anime as style has been contentious amongst fans, with John Oppliger stating, "The insistence on referring to original American art as Japanese "anime" or "manga" robs the work of its cultural identity."
A number of anime media franchises have gained considerable global popularity, and are among the world's highest-grossing media franchises.
"Pokémon" in particular is the highest-grossing media franchise of all time, bigger than "Star Wars" and "Marvel Cinematic Universe".
Other anime media franchises among the world's top 10 highest-grossing media franchises include "Hello Kitty" and "Dragon Ball", while the top 20 also includes "Fist of the North Star", "Yu-Gi-Oh", "Gundam" and "Evangelion".
</doc>
<doc id="801" url="https://en.wikipedia.org/wiki?curid=801" title="Asterism">
Asterism

Asterism may refer to:




</doc>
<doc id="802" url="https://en.wikipedia.org/wiki?curid=802" title="Ankara">
Ankara

Ankara (; ), historically known as Ancyra and Angora, is the capital of the Republic of Turkey.
With a population of 4,587,558 in the urban center and 5,150,072 in its province , it is Turkey's second largest city after Istanbul (the former imperial capital), having outranked İzmir in the 20th century.
On 23 April 1920 the Grand National Assembly of Turkey was established in Ankara, which became the headquarters of Atatürk and the Turkish National Movement during the Turkish War of Independence.
Ankara became the new Turkish capital upon the establishment of the Republic on 29 October 1923, succeeding in this role the former Turkish capital Istanbul (Constantinople) following the fall of the Ottoman Empire.
The government is a prominent employer, but Ankara is also an important commercial and industrial city, located at the center of Turkey's road and railway networks.
The city gave its name to the Angora wool shorn from Angora rabbits, the long-haired Angora goat (the source of mohair), and the Angora cat.
The area is also known for its pears, honey and muscat grapes.
Although situated in one of the driest places of Turkey and surrounded mostly by steppe vegetation except for the forested areas on the southern periphery, Ankara can be considered a green city in terms of green areas per inhabitant, at per head.
Ankara is a very old city with various Hittite, Phrygian, Hellenistic, Roman, Byzantine, and Ottoman archaeological sites.
The historical center of town is a rocky hill rising over the left bank of the Ankara Çayı, a tributary of the Sakarya River, the classical Sangarius.
The hill remains crowned by the ruins of the old citadel.
Although few of its outworks have survived, there are well-preserved examples of Roman and Ottoman architecture throughout the city, the most remarkable being the 20  Temple of Augustus and Rome that boasts the Monumentum Ancyranum, the inscription recording the "Res Gestae Divi Augusti".
The orthography of the name Ankara has varied over the ages.
It has been identified with the Hittite cult center "Ankuwaš", although this remains a matter of debate.
In classical antiquity and during the medieval period, the city was known as "Ánkyra" (,  "anchor") in Greek and "Ancyra" in Latin; the Galatian Celtic name was probably a similar variant.
Following its annexation by the Seljuk Turks in 1073, the city became known in many European languages as "Angora"; it was also known in Ottoman Turkish as "Engürü".
The form "Angora" is preserved in the names of breeds of many different kinds of animals, and in the names of several locations in the US (see Angora).
Ankara has a hot-summer Mediterranean climate (Köppen "Csa") which closely borders a hot summer Mediterranean continental climate (Köppen "Dsa").
Under the Trewartha climate classification, Ankara has a middle latitude steppe climate ("BSk").
Due to its elevation and inland location, Ankara has cold, somewhat snowy winters and hot, dry summers.
Rainfall occurs mostly during the spring and autumn.
Ankara lies in USDA Hardiness zone 7b, and its annual average precipitation is fairly low at , nevertheless precipitation can be observed throughout the year.
Monthly mean temperatures range from in January to in July, with an annual mean of .
Ankara had a population of 75,000 in 1927.
In 2013, Ankara Province had a population of 5,045,083.
When Ankara became the capital of the Republic of Turkey in 1923, it was designated as a planned city for 500,000 future inhabitants.
During the 1920s, 1930s and 1940s, the city grew in a planned and orderly pace.
However, from the 1950s onward, the city grew much faster than envisioned, because unemployment and poverty forced people to migrate from the countryside into the city in order to seek a better standard of living.
As a result, many illegal houses called gecekondu were built around the city, causing the unplanned and uncontrolled urban landscape of Ankara, as not enough planned housing could be built fast enough.
Although precariously built, the vast majority of them have electricity, running water and modern household amenities.
Nevertheless, many of these gecekondus have been replaced by huge public housing projects in the form of tower blocks such as Elvankent, Eryaman and Güzelkent; and also as mass housing compounds for military and civil service accommodation.
Although many gecekondus still remain, they too are gradually being replaced by mass housing compounds, as empty land plots in the city of Ankara for new construction projects are becoming impossible to find.
The region's history can be traced back to the Bronze Age Hattic civilization, which was succeeded in the 2nd millennium BC by the Hittites, in the 10th century BC by the Phrygians, and later by the Lydians, Persians, Greeks, Galatians, Romans, Byzantines, and Turks (the Seljuk Sultanate of Rûm, the Ottoman Empire and finally republican Turkey).
The oldest settlements in and around the city center of Ankara belonged to the Hattic civilization which existed during the Bronze Age and was gradually absorbed c.
2000–1700 BC by the Indo-European Hittites.
The city grew significantly in size and importance under the Phrygians starting around 1000 BC, and experienced a large expansion following the mass migration from Gordion, (the capital of Phrygia), after an earthquake which severely damaged that city around that time.
In Phrygian tradition, King Midas was venerated as the founder of Ancyra, but Pausanias mentions that the city was actually far older, which accords with present archaeological knowledge.
Phrygian rule was succeeded first by Lydian and later by Persian rule, though the strongly Phrygian character of the peasantry remained, as evidenced by the gravestones of the much later Roman period.
Persian sovereignty lasted until the Persians' defeat at the hands of Alexander the Great who conquered the city in 333 BC.
Alexander came from Gordion to Ankara and stayed in the city for a short period.
After his death at Babylon in 323 BC and the subsequent division of his empire among his generals, Ankara and its environs fell into the share of Antigonus.
Another important expansion took place under the Greeks of Pontos who came there around 300 BC and developed the city as a trading center for the commerce of goods between the Black Sea ports and Crimea to the north; Assyria, Cyprus, and Lebanon to the south; and Georgia, Armenia and Persia to the east.
By that time the city also took its name Ἄγκυρα ("Ánkyra", meaning "anchor" in Greek) which, in slightly modified form, provides the modern name of "Ankara".
In 278 BC, the city, along with the rest of central Anatolia, was occupied by a Celtic group, the Galatians, who were the first to make Ankara one of their main tribal centers, the headquarters of the Tectosages tribe.
Other centers were Pessinos, today's "Balhisar", for the Trocmi tribe, and Tavium, to the east of Ankara, for the "Tolstibogii" tribe.
The city was then known as "Ancyra".
The Celtic element was probably relatively small in numbers; a warrior aristocracy which ruled over Phrygian-speaking peasants.
However, the Celtic language continued to be spoken in Galatia for many centuries.
At the end of the 4th century, St.
Jerome, a native of Dalmatia, observed that the language spoken around Ankara was very similar to that being spoken in the northwest of the Roman world near Trier.
The city was subsequently passed under the control of the Roman Empire.
In 25 BC, Emperor Augustus raised it to the status of a "polis" and made it the capital city of the Roman province of Galatia.
Ankara is famous for the "Monumentum Ancyranum" ("Temple of Augustus and Rome") which contains the official record of the "Acts of Augustus", known as the "Res Gestae Divi Augusti", an inscription cut in marble on the walls of this temple.
The ruins of Ancyra still furnish today valuable bas-reliefs, inscriptions and other architectural fragments.
Two other Galatian tribal centers, Tavium near Yozgat, and Pessinus (Balhisar) to the west, near Sivrihisar, continued to be reasonably important settlements in the Roman period, but it was Ancyra that grew into a grand metropolis.
An estimated 200,000 people lived in Ancyra in good times during the Roman Empire, a far greater number than was to be the case from after the fall of the Roman Empire until the early 20th century.
A small river, the Ankara Çayı, ran through the center of the Roman town.
It has now been covered and diverted, but it formed the northern boundary of the old town during the Roman, Byzantine and Ottoman periods.
Çankaya, the rim of the majestic hill to the south of the present city center, stood well outside the Roman city, but may have been a summer resort.
In the 19th century, the remains of at least one Roman villa or large house were still standing not far from where the Çankaya Presidential Residence stands today.
To the west, the Roman city extended until the area of the Gençlik Park and Railway Station, while on the southern side of the hill, it may have extended downwards as far as the site presently occupied by Hacettepe University.
It was thus a sizeable city by any standards and much larger than the Roman towns of Gaul or Britannia.
Ancyra's importance rested on the fact that it was the junction point where the roads in northern Anatolia running north-south and east-west intersected, giving it major strategic importance for Rome's eastern frontier.
The great imperial road running east passed through Ankara and a succession of emperors and their armies came this way.
They were not the only ones to use the Roman highway network, which was equally convenient for invaders.
In the second half of the 3rd century, Ancyra was invaded in rapid succession by the Goths coming from the west (who rode far into the heart of Cappadocia, taking slaves and pillaging) and later by the Arabs.
For about a decade, the town was one of the western outposts of one of Palmyrean empress Zenobia in the Syrian Desert, who took advantage of a period of weakness and disorder in the Roman Empire to set up a short-lived state of her own.
The town was reincorporated into the Roman Empire under Emperor Aurelian in 272.
The tetrarchy, a system of multiple (up to four) emperors introduced by Diocletian (284–305), seems to have engaged in a substantial programme of rebuilding and of road construction from Ankara westwards to Germe and Dorylaeum (now Eskişehir).
In its heyday, Roman Ankara was a large market and trading center but it also functioned as a major administrative capital, where a high official ruled from the city's Praetorium, a large administrative palace or office.
During the 3rd century, life in Ancyra, as in other Anatolian towns, seems to have become somewhat militarized in response to the invasions and instability of the town.
The city is well known during the 4th century as a centre of Christian activity (see also below), due to frequent imperial visits, and through the letters of the pagan scholar Libanius.
Bishop Marcellus of Ancyra and Basil of Ancyra were active in the theological controversies of their day, and the city was the site of no less than three church synods in 314, 358 and 375, the latter two in favour of Arianism.
The city was visited by Emperor Constans I (r.
337–350) in 347 and 350, Julian (r.
361–363) during his Persian campaign in 362, and Julian's successor Jovian (r.
363–364) in winter 363/364 (he entered his consulship while in the city).
After Jovian's death soon after, Valentinian I (r.
364–375) was acclaimed emperor at Ancyra, and in the next year his brother Valens (r.
364–378) used Ancyra as his base against the usurper Procopius.
When the province of Galatia was divided sometime in 396/99, Ancyra remained the civil capital of Galatia I, as well as its ecclesiastical centre (metropolitan see).
Emperor Arcadius (r.
395–408) frequently used the city as his summer residence, and some information about the ecclesiastical affairs of the city during the early 5th century is found in the works of Palladius of Galatia and Nilus of Galatia.
In 479, the rebel Marcian attacked the city, without being able to capture it.
In 610/11, Comentiolus, brother of Emperor Phocas (r.
602–610), launched his own unsuccessful rebellion in the city against Heraclius (r.
610–641).
Ten years later, in 620 or more likely 622, it was captured by the Sassanid Persians during the Byzantine–Sassanid War of 602–628.
Although the city returned to Byzantine hands after the end of the war, the Persian presence left traces in the city's archaeology, and likely began the process of its transformation from a late antique city to a medieval fortified settlement.
In 654, the city was captured for the first time by the Arabs of the Rashidun Caliphate, under Muawiyah, the future founder of the Umayyad Caliphate.
At about the same time, the themes were established in Anatolia, and Ancyra became capital of the Opsician Theme, which was the largest and most important theme until it was split up under Emperor Constantine V (r.
741–775); Ancyra then became the capital of the new Bucellarian Theme.
The city was attacked without success by Abbasid forces in 776 and in 798/99.
In 805, Emperor Nikephoros I (r.
802–811) strengthened its fortifications, a fact which probably saved it from sack during the large-scale invasion of Anatolia by Caliph Harun al-Rashid in the next year.
Arab sources report that Harun and his successor al-Ma'mun (r.
813–833) took the city, but this information is later invention.
In 838, however, during the Amorium campaign, the armies of Caliph al-Mu'tasim (r.
833–842) converged and met at the city; abandoned by its inhabitants, Ancara was razed to the ground, before the Arab armies went on to besiege and destroy Amorium.
In 859, Emperor Michael III (r.
842–867) came to the city during a campaign against the Arabs, and ordered its fortifications restored.
In 872, the city was menaced, but not taken, by the Paulicians under Chrysocheir.
The last Arab raid to reach the city was undertaken in 931, by the Abbasid governor of Tarsus, Thamal al-Dulafi, but the city again was not captured.
After the Battle of Manzikert in 1071, the Seljuk Turks overran much of Anatolia.
By 1073, the Turkish settlers had reached the vicinity of Ancyra, and the city was captured shortly after, at the latest by the time of the rebellion of Nikephoros Melissenos in 1081.
In 1101, when the Crusade under Raymond IV of Toulouse arrived, the city had been under Danishmend control for some time.
The Crusaders captured the city, and handed it over to the Byzantine emperor Alexios I Komnenos (r.
1081–1118).
Byzantine rule did not last long, and the city was captured by the Seljuk Sultanate of Rum at some unknown point; in 1127, it returned to Danishmend control until 1143, when the Seljuks of Rum retook it.
After the Battle of Köse Dağ in 1243, in which the Mongols defeated the Seljuks, most of Anatolia became part of the dominion of the Mongols.
Taking advantage of Seljuk decline, a semi-religious cast of craftsmen and trade people named "Ahiler" chose Ankara as their independent city-state in 1290.
Orhan I, the second Bey of the Ottoman Empire, captured the city in 1356.
Timur defeated Bayezid I at the Battle of Ankara in 1402 and took the city, but in 1403 Ankara was again under Ottoman control.
The Levant Company maintained a factory in the town from 1639 to 1768.
In the 19th century, its population was estimated at 20,000 to 60,000.
It was sacked by Egyptians under Ibrahim Pasha in 1832.
Prior to World War I, the town had a British consulate and a population of around 28,000, roughly ⅓ of whom were Christian.
Following the Ottoman defeat at World War I, the Ottoman capital Constantinople (modern Istanbul) and much of Anatolia were occupied by the Allies, who planned to share these lands between Armenia, France, Greece, Italy and the United Kingdom, leaving for the Turks the core piece of land in central Anatolia.
In response, the leader of the Turkish nationalist movement, Mustafa Kemal Atatürk, established the headquarters of his resistance movement in Ankara in 1920.
After the Turkish War of Independence was won and the Treaty of Sèvres was superseded by the Treaty of Lausanne (1923), the Turkish nationalists replaced the Ottoman Empire with the Republic of Turkey on 29 October 1923.
A few days earlier, Ankara had officially replaced Constantinople as the new Turkish capital city, on 13 October 1923.
After Ankara became the capital of the newly founded Republic of Turkey, new development divided the city into an old section, called "Ulus", and a new section, called "Yenişehir".
Ancient buildings reflecting Roman, Byzantine, and Ottoman history and narrow winding streets mark the old section.
The new section, now centered on Kızılay Square, has the trappings of a more modern city: wide streets, hotels, theaters, shopping malls, and high-rises.
Government offices and foreign embassies are also located in the new section.
Ankara has experienced a phenomenal growth since it was made Turkey's capital in 1923, when it was "a small town of no importance".
In 1924, the year after the government had moved there, Ankara had about 35,000 residents.
By 1927 there were 44,553 residents and by 1950 the population had grown to 286,781.
Ankara continued to grow rapidly during the latter half of the 20th century and eventually outranked Izmir as Turkey's second largest city, after Istanbul.
Ankara's urban population reached 4,587,558 in 2014, while the population of Ankara Province reached 5,150,072 in 2015.
Early Christian martyrs of Ancyra, about whom little is known, included Proklos and Hilarios who were natives of the otherwise unknown nearby village of Kallippi, and suffered repression under the emperor Trajan (98–117).
In the 280s we hear of Philumenos, a Christian corn merchant from southern Anatolia, being captured and martyred in Ankara, and Eustathius.
As in other Roman towns, the reign of Diocletian marked the culmination of the persecution of the Christians.
In 303, Ancyra was one of the towns where the co-Emperors Diocletian and his deputy Galerius launched their anti-Christian persecution.
In Ancyra, their first target was the 38-year-old Bishop of the town, whose name was Clement.
Clement's life describes how he was taken to Rome, then sent back, and forced to undergo many interrogations and hardship before he, and his brother, and various companions were put to death.
The remains of the church of St.
Clement can be found today in a building just off Işıklar Caddesi in the Ulus district.
Quite possibly this marks the site where Clement was originally buried.
Four years later, a doctor of the town named Plato and his brother Antiochus also became celebrated martyrs under Galerius.
Theodotus of Ancyra is also venerated as a saint.
However, the persecution proved unsuccessful and in 314 Ancyra was the center of an important council of the early church; its 25 disciplinary canons constitute one of the most important documents in the early history of the administration of the Sacrament of Penance.
The synod also considered ecclesiastical policy for the reconstruction of the Christian Church after the persecutions, and in particular the treatment of "lapsi"—Christians who had given in to forced paganism (sacrifices) to avoid martyrdom during these persecutions.
Though paganism was probably tottering in Ancyra in Clement's day, it may still have been the majority religion.
Twenty years later, Christianity and monotheism had taken its place.
Ancyra quickly turned into a Christian city, with a life dominated by monks and priests and theological disputes.
The town council or senate gave way to the bishop as the main local figurehead.
During the middle of the 4th century, Ancyra was involved in the complex theological disputes over the nature of Christ, and a form of Arianism seems to have originated there.
In 362–363, the Emperor Julian passed through Ancyra on his way to an ill-fated campaign against the Persians, and according to Christian sources, engaged in a persecution of various holy men.
The stone base for a statue, with an inscription describing Julian as "Lord of the whole world from the British Ocean to the barbarian nations", can still be seen, built into the eastern side of the inner circuit of the walls of Ankara Castle.
The Column of Julian which was erected in honor of the emperor's visit to the city in 362 still stands today.
In 375, Arian bishops met at Ancyra and deposed several bishops, among them St.
Gregory of Nyssa.
In the late 4th century, Ancyra became something of an imperial holiday resort.
After Constantinople became the East Roman capital, emperors in the 4th and 5th centuries would retire from the humid summer weather on the Bosporus to the drier mountain atmosphere of Ancyra.
Theodosius II (408–450) kept his court in Ancyra in the summers.
Laws issued in Ancyra testify to the time they spent there.
The Metropolis of Ancyra continued to be a residential see of the Eastern Orthodox Church until the 20th century, with about 40,000 faithful, mostly Turkish-speaking, but that situation ended as a result of the 1923 Convention Concerning the Exchange of Greek and Turkish Populations.
The earlier Armenian Genocide put an end to the residential eparchy of Ancyra of the Armenian Catholic Church, which had been established in 1850.
It is also a titular metropolis of the Ecumenical Patriarchate of Constantinople.
Both the Ancient Byzantine Metropolitan archbishopric and the 'modern' Armenian eparchy are now listed by the Catholic Church as titular sees, with separate apostolic successions.
In 1735 an Armenian Catholic diocese was established (Curiate Italian: "Ancira degli Ameni").
Having fallen into disuse, it was restored on 30 April 1850.
The Armenian Genocide brought an effective end to the residential diocese, which was only formally suppressed in 1972 and instantly transformed into an Armenian Catholic titular bishopric.
The titular see has had a single occupant:

No later than 1696, the Catholic Church also established a Latin Rite titular archbishopric of Ancyra.
The last incumbent died in 1976.
The Saint Clement Church is the only structure survived from the Byzantine era in Ankara.
The church is believed to have been built between the 4th and 9th centuries.
At the time of the Ottoman Sultan Murad II, a mosque and madrasah were built on top of the church.
Today, only the interior facade of a wall and marble blocks from the church have survived.
The city has exported mohair (from the Angora goat) and Angora wool (from the Angora rabbit) internationally for centuries.
In the 19th century, the city also exported substantial amounts of goat and cat skins, gum, wax, honey, berries, and madder root.
It was connected to Istanbul by railway before the First World War, continuing to export mohair, wool, berries, and grain.
The Central Anatolia Region is one of the primary locations of grape and wine production in Turkey, and Ankara is particularly famous for its Kalecik Karası and Muscat grapes; and its Kavaklıdere wine, which is produced in the Kavaklıdere neighbourhood within the Çankaya district of the city.
Ankara is also famous for its pears.
Another renowned natural product of Ankara is its indigenous type of honey ("Ankara Balı") which is known for its light color and is mostly produced by the Atatürk Forest Farm and Zoo in the Gazi district, and by other facilities in the Elmadağ, Çubuk and Beypazarı districts.
Ankara is the center of the state-owned and private Turkish defence and aerospace companies, where the industrial plants and headquarters of the Turkish Aerospace Industries, MKE, ASELSAN, Havelsan, Roketsan, FNSS, Nurol Makina, and numerous other firms are located.
Exports to foreign countries from these defence and aerospace firms have steadily increased in the past decades.
The IDEF in Ankara is one of the largest international expositions of the global arms industry.
A number of the global automotive companies also have production facilities in Ankara, such as the German bus and truck manufacturer MAN SE.
Ankara hosts the OSTIM Industrial Zone, Turkey's largest industrial park.
A large percentage of the complicated employment in Ankara is provided by the state institutions; such as the ministries, undersecretariats, and other administrative bodies of the Turkish government.
There are also many foreign citizens working as diplomats or clerks in the embassies of their respective countries.
The "Electricity, Gas, Bus General Directorate" (EGO) operates the Ankara Metro and other forms of public transportation.
Ankara is currently served by a suburban rail named Ankaray (A1) and three subway lines (M1, M2, M3) of the Ankara Metro with about 300,000 total daily commuters, while an additional subway line (M4) is currently under construction.
A long gondola lift with four stations connects the district of Şentepe to the Yenimahalle metro station.
The Ankara Central Station is a major rail hub in Turkey.
The Turkish State Railways operates passenger train service from Ankara to other major cities, such as: Istanbul, Eskişehir, Balıkesir, Kütahya, İzmir, Kayseri, Adana, Kars, Elâzığ, Malatya, Diyarbakır, Karabük, Zonguldak and Sivas.
Commuter rail also runs between the stations of Sincan and Kayaş.
On 13 March 2009, the new Yüksek Hızlı Tren (YHT) high-speed rail service began operation between Ankara and Eskişehir.
On 23 August 2011, another YHT high-speed line commercially started its service between Ankara and Konya.
On 25 July 2014, the Ankara–Istanbul high-speed line of YHT entered service.
Esenboğa International Airport, located in the north-east of the city, is Ankara's main airport.
The average amount of time people spend commuting on public transit in Ankara on a weekday is 71 minutes.
17% of public transit passengers, ride for more than two hours every day.
The average amount of time people wait at a stop or station for public transit is sixteen minutes, while 28% of users wait for over twenty minutes on average every day.
The average distance people usually ride in a single trip with public transit is , while 27% travel for over in a single direction.
Ankara is politically a triple battleground between the ruling conservative Justice and Development Party (AKP), the opposition Kemalist centre-left Republican People's Party (CHP) and the nationalist far-right Nationalist Movement Party (MHP).
The province of Ankara is divided into 25 districts.
The CHP's key and almost only political stronghold in Ankara lies within the central area of Çankaya, which is the city's most populous district.
While the CHP has always gained between 60 and 70% of the vote in Çankaya since 2002, political support elsewhere throughout Ankara is minimal.
The high population within Çankaya, as well as Yenimahalle to an extent, has allowed the CHP to take overall second place behind the AKP in both local and general elections, with the MHP a close third, despite the fact that the MHP is politically stronger than the CHP in almost every other district.
Overall, the AKP enjoys the most support throughout the city.
The electorate of Ankara thus tend to vote in favour of the political right, far more so than the other main cities of Istanbul and İzmir.
In retrospect, the 2013–14 protests against the AKP government were particularly strong in Ankara, proving to be fatal on multiple occasions.
Melih Gökçek has been the Metropolitan Mayor of Ankara since 1994 as a politician from the Welfare Party.
He later joined the Virtue Party and then the AKP.
Initially elected in the 1994 local elections, he was re-elected in 1999, 2004 and 2009.
In the 2014 local election, Gökçek stood for a fifth term.
The MHP metropolitan mayoral candidate for the 2009 local elections, conservative politician Mansur Yavaş, stood as the CHP candidate against Gökçek.
In a heavily controversial election, Gökçek was declared the winner by just 1% ahead of Yavaş amid allegations of systematic electoral fraud.
With the Supreme Electoral Council and courts rejecting Yavaş's appeals, he has declared intention to take the irregularities to the European Court of Human Rights.
Although Gökçek was inaugurated for a fifth term, most election observers believe that Yavaş was the winner of the election.
Gökçek is resigned on 28 October 2017, replaced by former mayor of Sincan, Mustafa Tuna.
The city suffered from a series of terrorist attacks in 2015 and 2016, most notably on 10 October 2015; 17 February 2016; 13 March 2016; and 15 July 2016.
The foundations of the Ankara castle and citadel were laid by the Galatians on a prominent lava outcrop (), and the rest was completed by the Romans.
The Byzantines and Seljuks further made restorations and additions.
The area around and inside the citadel, being the oldest part of Ankara, contains many fine examples of traditional architecture.
There are also recreational areas to relax.
Many restored traditional Turkish houses inside the citadel area have found new life as restaurants, serving local cuisine.
The citadel was depicted in various Turkish banknotes during 1927–1952 and 1983–1989.
The remains, the stage, and the backstage of the Roman theatre can be seen outside the castle.
Roman statues that were found here are exhibited in the Museum of Anatolian Civilizations.
The seating area is still under excavation.
The Augusteum, now known as the Temple of Augustus and Rome, was built 25  20  following the conquest of Central Anatolia by the Roman Empire.
Ancyra then formed the capital of the new province of Galatia.
After the death of Augustus in  14, a copy of the text of the "Res Gestae Divi Augusti" (the "Monumentum Ancyranum") was inscribed on the interior of the temple's ' in Latin and a Greek translation on an exterior wall of the '.
The temple on the ancient acropolis of Ancyra was enlarged in the 2nd century and converted into a church in the 5th century.
It is located in the Ulus quarter of the city.
It was subsequently publicized by the Austrian ambassador Ogier Ghiselin de Busbecq in the 16th century.
The Roman Baths of Ankara have all the typical features of a classical Roman bath complex: a "frigidarium" (cold room), a "tepidarium" (warm room) and a "caldarium" (hot room).
The baths were built during the reign of the Roman emperor Caracalla in the early 3rd century to honor Asclepios, the God of Medicine.
Today, only the basement and first floors remain.
It is situated in the Ulus quarter.
The Roman Road of Ankara or "Cardo Maximus" was found in 1995 by Turkish archaeologist Cevdet Bayburtluoğlu.
It is long and wide.
Many ancient artifacts were discovered during the excavations along the road and most of them are currently displayed at the Museum of Anatolian Civilizations.
The Column of Julian or Julianus, now in the Ulus district, was erected in honor of the Roman emperor Julian the Apostate's visit to Ancyra in 362.
Kocatepe Mosque is the largest mosque in the city.
Located in the Kocatepe quarter, it was constructed between 1967 and 1987 in classical Ottoman style with four minarets.
Its size and prominent location have made it a landmark for the city.
Ahmet Hamdi Akseki Mosque is located near the Presidency of Religious Affairs on the Eskişehir Road.
Built in the Turkish neoclassical style, it is one of the largest new mosques in the city, completed and opened in 2013.
It can accommodate 6 thousand people during general prayers, and up to 30 thousand people during funeral prayers.
The mosque was decorated with Anatolian Seljuk style patterns.
It is the largest Ottoman mosque in Ankara and was built by the famous architect Sinan in the 16th century.
The mimber (pulpit) and mihrap (prayer niche) are of white marble, and the mosque itself is of Ankara stone, an example of very fine workmanship.
This mosque, in the Ulus quarter next to the Temple of Augustus, was built in the early 15th century in Seljuk style by an unknown architect.
It was subsequently restored by architect Mimar Sinan in the 16th century, with Kütahya tiles being added in the 18th century.
The mosque was built in honor of Hacı Bayram-ı Veli, whose tomb is next to the mosque, two years before his death (1427–28).
The usable space inside this mosque is on the first floor and on the second floor.
It was founded in the Ulus quarter near the Ankara Citadel and was constructed by the Ahi fraternity during the late 14th and early 15th centuries.
The finely carved walnut mimber (pulpit) is of particular interest.
The Alâeddin Mosque is the oldest mosque in Ankara.
It has a carved walnut mimber, the inscription on which records that the mosque was completed in early AH 574 (which corresponds to the summer of 1178 AD) and was built by the Seljuk prince Muhiddin Mesud Şah (d.
1204), the Bey of Ankara, who was the son of the Anatolian Seljuk sultan Kılıç Arslan II (reigned 1156–1192.)
The " Victory Monument" (Turkish: "") was crafted by Austrian sculptor Heinrich Krippel in 1925 and was erected in 1927 at Ulus Square.
The monument is made of marble and bronze and features an equestrian statue of Mustafa Kemal Atatürk, who wears a Republic era modern military uniform, with the rank Field Marshal.
Located at Zafer Square (Turkish: "Zafer Meydanı"), the marble and bronze statue was crafted by the renowned Italian sculptor Pietro Canonica in 1927 and depicts a standing Atatürk who wears a Republic era modern military uniform, with the rank Field Marshal.
This monument, located in Güven Park near Kızılay Square, was erected in 1935 and bears Atatürk's advice to his people: "Turk!
Be proud, work hard, and believe in yourself."
The monument was depicted on the reverse of the Turkish 5 lira banknote of 1937–1952 and of the 1000 lira banknotes of 1939–1946.
Erected in 1978 at Sıhhiye Square, this impressive monument symbolizes the Hatti Sun Disc (which was later adopted by the Hittites) and commemorates Anatolia's earliest known civilization.
The Hatti Sun Disc has been used in the previous logo of Ankara Metropolitan Municipality.
It was also used in the previous logo of the Ministry of Culture & Tourism.
Suluhan is a historical Inn in Ankara.
It is also called the "Hasanpaşa Han".
It is about southeast of Ulus Square and situated in the Hacıdoğan neighbourhood.
According to the "vakfiye" (inscription) of the building, the Ottoman era "han" was commissioned by Hasan Pasha, a regional beylerbey, and was constructed between 1508 and 1511, during the final years of the reign of Sultan Bayezid II.
There are 102 rooms (now shops) which face the two yards.
In each room there is a window, a niche and a chimney.
Çengelhan Rahmi Koç Museum is a museum of industrial technology situated in , an Ottoman era Inn which was completed in 1523, during the early years of the reign of Sultan Suleiman the Magnificent.
The exhibits include industrial/technological artifacts from the 1850s onwards.
There are also sections about Mustafa Kemal Atatürk, the founder of modern Turkey; Vehbi Koç, Rahmi Koç's father and one of the first industrialists of Turkey, and Ankara city.
Foreign visitors to Ankara usually like to visit the old shops in "Çıkrıkçılar Yokuşu" (Weavers' Road) near Ulus, where myriad things ranging from traditional fabrics, hand-woven carpets and leather products can be found at bargain prices.
"Bakırcılar Çarşısı" (Bazaar of Coppersmiths) is particularly popular, and many interesting items, not just of copper, can be found here like jewelry, carpets, costumes, antiques and embroidery.
Up the hill to the castle gate, there are many shops selling a huge and fresh collection of spices, dried fruits, nuts, and other produce.
Modern shopping areas are mostly found in Kızılay, or on Tunalı Hilmi Avenue, including the modern mall of Karum (named after the ancient Assyrian merchant colonies called "Kârum" that were established in central Anatolia at the beginning of the 2nd millennium BC) which is located towards the end of the Avenue; and in Çankaya, the quarter with the highest elevation in the city.
Atakule Tower next to Atrium Mall in Çankaya has views over Ankara and also has a revolving restaurant at the top.
The symbol of the Armada Shopping Mall is an anchor, and there's a large anchor monument at its entrance, as a reference to the ancient Greek name of the city, Ἄγκυρα (Ánkyra), which means anchor.
Likewise, the anchor monument is also related with the Spanish name of the mall, Armada, which means naval fleet.
As Ankara started expanding westward in the 1970s, several modern, suburbia-style developments and mini-cities began to rise along the western highway, also known as the Eskişehir Road.
The "Armada","CEPA" and "Kentpark" malls on the highway, the "Galleria", "Arcadium" and "Gordion" in Ümitköy, and a huge mall, "Real" in Bilkent Center, offer North American and European style shopping opportunities (these places can be reached through the Eskişehir Highway.)
There is also the newly expanded "ANKAmall" at the outskirts, on the Istanbul Highway, which houses most of the well-known international brands.
This mall is the largest throughout the Ankara region.
In 2014 a few more shopping malls were open in Ankara.
They are "Next Level" and "Taurus" on the Boulevard of Mevlana (also known as Konya Road).
Turkish State Opera and Ballet, the national directorate of opera and ballet companies of Turkey, has its headquarters in Ankara, and serves the city with three venues:


Ankara is host to five classical music orchestras:

There are four concert halls in the city:

The city has been host to several well-established, annual theatre, music, film festivals:


Ankara also has a number of concert venues such as "Eskiyeni", "IF Performance Hall", "Jolly Joker", "Kite", "Nefes Bar", "Noxus Pub", "Passage Pub" and "Route", which host the live performances and events of popular musicians.
The Turkish State Theatres also has its head office in Ankara and runs the following stages in the city:

In addition, the city is served by several private theatre companies, among which , who have their own stage in the city center, is a notable example.
There are about 50 museums in the city.
The Museum of Anatolian Civilizations ("Anadolu Medeniyetleri Müzesi") is situated at the entrance of the Ankara Castle.
It is an old 15th century bedesten (covered bazaar) that has been restored and now houses a collection of Paleolithic, Neolithic, Hatti, Hittite, Phrygian, Urartian and Roman works as well as a major section dedicated to Lydian treasures.
Anıtkabir is located on an imposing hill, which forms the "Anıttepe" quarter of the city, where the mausoleum of Mustafa Kemal Atatürk, founder of the Republic of Turkey, stands.
Completed in 1953, it is an impressive fusion of ancient and modern architectural styles.
An adjacent museum houses a wax statue of Atatürk, his writings, letters and personal items, as well as an exhibition of photographs recording important moments in his life and during the establishment of the Republic.
Anıtkabir is open every day, while the adjacent museum is open every day except Mondays.
Ankara Ethnography Museum ("Etnoğrafya Müzesi") is located opposite to the Ankara Opera House on Talat Paşa Boulevard, in the Ulus district.
There is a fine collection of folkloric items, as well as artifacts from the Seljuk and Ottoman periods.
In front of the museum building, there is a marble and bronze equestrian statue of Mustafa Kemal Atatürk (who wears a Republic era modern military uniform, with the rank Field Marshal) which was crafted in 1927 by the renowned Italian sculptor Pietro Canonica.
The State Art and Sculpture Museum ("Resim-Heykel Müzesi") which opened to the public in 1980 is close to the Ethnography Museum and houses a rich collection of Turkish art from the late 19th century to the present day.
There are also galleries which host guest exhibitions.
Cer Modern is the modern-arts museum of Ankara, inaugurated on 1 April 2010.
It is situated in the renovated building of the historic TCDD Cer Atölyeleri, formerly a workshop of the Turkish State Railways.
The museum incorporates the largest exhibition hall in Turkey.
The museum holds periodic exhibitions of modern and contemporary art as well as hosting other contemporary arts events.
The War of Independence Museum ("Kurtuluş Savaşı Müzesi") is located on Ulus Square.
It was originally the first Parliament building (TBMM) of the Republic of Turkey.
The War of Independence was planned and directed here as recorded in various photographs and items presently on exhibition.
In another display, wax figures of former presidents of the Republic of Turkey are on exhibit.
The Mehmet Akif Literature Museum Library is an important literary museum and archive opened in 2011 and dedicated to Mehmet Akif Ersoy (1873–1936), the poet of the Turkish National Anthem.
The TCDD Open Air Steam Locomotive Museum is an open-air museum which traces the history of steam locomotives.
Ankara Aviation Museum ("Hava Kuvvetleri Müzesi Komutanlığı") is located near the Istanbul Road in Etimesgut.
The museum opened to the public in September 1998.
It is home to various missiles, avionics, aviation materials and aircraft that have served in the Turkish Air Force (e.g.
combat aircraft such as the F-86 Sabre, F-100 Super Sabre, F-102 Delta Dagger, F-104 Starfighter, F-5 Freedom Fighter, F-4 Phantom; and cargo planes such as the Transall C-160.)
Also a Hungarian MiG-21, a Pakistani MiG-19, and a Bulgarian MiG-17 are on display at the museum.
The METU Science and Technology Museum ("ODTÜ Bilim ve Teknoloji Müzesi") is located inside the Middle East Technical University campus.
As with all other cities of Turkey, football is the most popular sport in Ankara.
The city has one football club currently competing in the Turkish Super League: Ankaragücü, founded in 1910, is the oldest club in Ankara and is associated with Ankara's military arsenal manufacturing company MKE.
They were the Turkish Cup winners in 1972 and 1981.
Gençlerbirliği, founded in 1923, currently play in the TFF First League and are known as the "Ankara Gale" or the "Poppies" because of their colors: red and black.
They were the Turkish Cup winners in 1987 and 2001.
Gençlerbirliği's B team, Hacettepe S.K.
(formerly known as Gençlerbirliği OFTAŞ) played in the Turkish Super League but currently plays in the TFF Second League.
A fourth team, Büyükşehir Belediye Ankaraspor, played in the Turkish Super League until 2010, when they were expelled.
The club was reconstituted in 2014 as Osmanlıspor and currently play in the TFF First League at the Osmanlı Stadyumu in the Sincan district of Yenikent, outside the city center.
Ankara has a large number of minor teams, playing at regional levels.
In the TFF Second League: Bugsaşspor in Sincan, Ankara Demirspor in Çankaya, Etimesgut Belediyespor in Etimesgut, Keçiörengücü in Keçiören; in the TFF Third League: Ankara Adliyespor in Keçiören; Altındağ Belediyespor in Altındağ; in the Amateur League: Turanspor in Etimesgut, Türk Telekomspor owned by the phone company in Yenimahalle, Çubukspor in Çubuk, and Bağlumspor in Keçiören.
In the Turkish Basketball League, Ankara is represented by Türk Telekom, whose home is the Ankara Arena, and CASA TED Kolejliler, whose home is the TOBB Sports Hall.
Halkbank Ankara is currently the leading domestic powerhouse in Men's Volleyball, having won many championships and cups in the Turkish Men's Volleyball League and even the CEV Cup in 2013.
Ankara Buz Pateni Sarayı is where the ice skating and ice hockey competitions take place in the city.
There are many popular spots for skateboarding which is active in the city since the 1980s.
Skaters in Ankara usually meet in the park near the Grand National Assembly of Turkey.
The 2012-built THF Sport Hall hosts the Handball Super League and Women's Handball Super League matches scheduled in Ankara.
Ankara has many parks and open spaces mainly established in the early years of the Republic and well maintained and expanded thereafter.
The most important of these parks are: Gençlik Parkı (houses an amusement park with a large pond for rowing), the Botanical garden, Seğmenler Park, Anayasa Park, Kuğulu Park (famous for the swans received as a gift from the Chinese government), Abdi İpekçi Park, Esertepe Parkı, Güven Park (see above for the monument), Kurtuluş Park (has an ice-skating rink), Altınpark (also a prominent exposition/fair area), Harikalar Diyarı (claimed to be Biggest Park of Europe inside city borders) and Göksu Park.
Gençlik Park was depicted on the reverse of the Turkish 100 lira banknotes of 1952–1976.
Atatürk Forest Farm and Zoo ("Atatürk Orman Çiftliği") is an expansive recreational farming area which houses a zoo, several small agricultural farms, greenhouses, restaurants, a dairy farm and a brewery.
It is a pleasant place to spend a day with family, be it for having picnics, hiking, biking or simply enjoying good food and nature.
There is also an exact replica of the house where Atatürk was born in 1881, in Thessaloniki, Greece.
Visitors to the "Çiftlik" (farm) as it is affectionately called by Ankarans, can sample such famous products of the farm such as old-fashioned beer and ice cream, fresh dairy products and meat rolls/kebaps made on charcoal, at a traditional restaurant ("Merkez Lokantası", Central Restaurant), cafés and other establishments scattered around the farm.
Ankara is noted, within Turkey, for the multitude of universities it is home to.
These include the following, several of them being among the most reputable in the country:

Ankara is home to a world-famous domestic cat breed – the Turkish Angora, called "Ankara kedisi" (Ankara cat) in Turkish.
Turkish Angoras are one of the ancient, naturally occurring cat breeds, having originated in Ankara and its surrounding region in central Anatolia.
They mostly have a white, silky, medium to long length coat, no undercoat and a fine bone structure.
There seems to be a connection between the Angora Cats and Persians, and the Turkish Angora is also a distant cousin of the Turkish Van.
Although they are known for their shimmery white coat, currently there are more than twenty varieties including black, blue and reddish fur.
They come in tabby and tabby-white, along with smoke varieties, and are in every color other than pointed, lavender, and cinnamon (all of which would indicate breeding to an outcross.)
Eyes may be blue, green, or amber, or even one blue and one amber or green.
The W gene which is responsible for the white coat and blue eye is closely related to the hearing ability, and the presence of a blue eye can indicate that the cat is deaf to the side the blue eye is located.
However, a great many blue and odd-eyed white cats have normal hearing, and even deaf cats lead a very normal life if kept indoors.
Ears are pointed and large, eyes are almond shaped and the head is massive with a two plane profile.
Another characteristic is the tail, which is often kept parallel to the back.
The Angora rabbit () is a variety of domestic rabbit bred for its long, soft hair.
The Angora is one of the oldest types of domestic rabbit, originating in Ankara and its surrounding region in central Anatolia, along with the Angora cat and Angora goat.
The rabbits were popular pets with French royalty in the mid-18th century, and spread to other parts of Europe by the end of the century.
They first appeared in the United States in the early 20th century.
They are bred largely for their long Angora wool, which may be removed by shearing, combing, or plucking (gently pulling loose wool.)
Angoras are bred mainly for their wool because it is silky and soft.
They have a humorous appearance, as they oddly resemble a fur ball.
Most are calm and docile but should be handled carefully.
Grooming is necessary to prevent the fiber from matting and felting on the rabbit.
A condition called "wool block" is common in Angora rabbits and should be treated quickly.
Sometimes they are shorn in the summer as the long fur can cause the rabbits to overheat.
The Angora goat () is a breed of domestic goat that originated in Ankara and its surrounding region in central Anatolia.
This breed was first mentioned in the time of Moses, roughly in 1500 BC.
The first Angora goats were brought to Europe by Charles V, Holy Roman Emperor, about 1554, but, like later imports, were not very successful.
Angora goats were first introduced in the United States in 1849 by Dr. James P. Davis.
Seven adult goats were a gift from Sultan Abdülmecid I in appreciation for his services and advice on the raising of cotton.
The fleece taken from an Angora goat is called mohair.
A single goat produces between of hair per year.
Angoras are shorn twice a year, unlike sheep, which are shorn only once.
Angoras have high nutritional requirements due to their rapid hair growth.
A poor quality diet will curtail mohair development.
The United States, Turkey, and South Africa are the top producers of mohair.
For a long period of time, Angora goats were bred for their white coat.
In 1998, the Colored Angora Goat Breeders Association was set up to promote breeding of colored Angoras.
Today, Angora goats produce white, black (deep black to greys and silver), red (the color fades significantly as the goat gets older), and brownish fiber.
Angora goats were depicted on the reverse of the Turkish 50 lira banknotes of 1938–1952.
Ankara is twinned with:

43. ilişki durumu evli izle

Attribution



</doc>
<doc id="803" url="https://en.wikipedia.org/wiki?curid=803" title="Arabic">
Arabic

Arabic () ' or () ' or ) is a Central Semitic language that first emerged in Iron Age northwestern Arabia and is now the of the Arab world.
It is named after the Arabs, a term initially used to describe peoples living from Mesopotamia in the east to the Anti-Lebanon mountains in the west, in northwestern Arabia, and in the Sinai Peninsula.
Arabic is classified as a macrolanguage comprising 30 modern varieties, including its standard form, Modern Standard Arabic, which is derived from Classical Arabic.
As the modern written language, Modern Standard Arabic is widely taught in schools and universities, and is used to varying degrees in workplaces, government, and the media.
The two formal varieties are grouped together as Literary Arabic ("), which is the official language of 26 states and the liturgical language of Islam.
Modern Standard Arabic largely follows the grammatical standards of Classical Arabic, and uses much of the same vocabulary.
However, it has discarded some grammatical constructions and vocabulary that no longer have any counterpart in the spoken varieties, and has adopted certain new constructions and vocabulary from the spoken varieties.
Much of the new vocabulary is used to denote concepts that have arisen in the post-classical era, especially in modern times.
During the Middle Ages, Literary Arabic was a major vehicle of culture in Europe, especially in science, mathematics and philosophy.
As a result, many European languages have also borrowed many words from it.
Arabic influence, mainly in vocabulary, is seen in European languages, mainly Spanish and to a lesser extent Portuguese, Galician, and Catalan, owing to both the proximity of Christian European and Muslim Arab civilizations and 800 years of Arabic culture and language in the Iberian Peninsula, referred to in Arabic as ".
Sicilian has about 500 Arabic words as result of Sicily being progressively conquered by Arabs from North Africa, from the mid-9th to mid=10th centuries.
Many of these words relate to agriculture and related activities (Hull and Ruffino).
Balkan languages, including Greek and Bulgarian, have also acquired a significant number of Arabic words through contact with Ottoman Turkish.
Arabic has influenced many languages around the globe throughout its history.
Some of the most influenced languages are Persian, Turkish, Spanish, Urdu, Kashmiri, Kurdish, Bosnian, Kazakh, Bengali, Hindi, Malay, Maldivian, Indonesian, Pashto, Punjabi, Tagalog, Sindhi, and Hausa, and some languages in parts of Africa.
Conversely, Arabic has borrowed words from other languages, including Greek and Persian in medieval times, and contemporary European languages such as English and French in modern times.
Classical Arabic is the liturgical language of 1.8 billion Muslims, and Modern Standard Arabic is one of six official languages of the United Nations.
All varieties of Arabic combined are spoken by perhaps as many as 422 million speakers (native and non-native) in the Arab world, making it the fifth most spoken language in the world.
Arabic is written with the Arabic alphabet, which is an abjad script and is written from right to left, although the spoken varieties are sometimes written in ASCII Latin from left to right with no standardized orthography.
Arabic is a Central Semitic language, closely related to the Northwest Semitic languages (Aramaic, Hebrew, Ugaritic, and Phoenician), the Ancient South Arabian languages, and various other Semitic languages of Arabia such as Dadanitic.
The Semitic languages changed a great deal between Proto-Semitic and the establishment of the Central Semitic languages, particularly in grammar.
Innovations of the Central Semitic languages—all maintained in Arabic—include:
There are several features which Classical Arabic, the modern Arabic varieties, as well as the Safaitic and Hismaic inscriptions share which are unattested in any other Central Semitic language variety, including the Dadanitic and Taymanitic languages of the northern Hejaz.
These features are evidence of common descent from a hypothetical ancestor, Proto-Arabic.
The following features can be reconstructed with confidence for Proto-Arabic:

Arabia boasted a wide variety of Semitic languages in antiquity.
In the southwest, various Central Semitic languages both belonging to and outside of the Ancient South Arabian family (e.g.
Southern Thamudic) were spoken.
It is also believed that the ancestors of the Modern South Arabian languages (non-Central Semitic languages) were also spoken in southern Arabia at this time.
To the north, in the oases of northern Hejaz, Dadanitic and Taymanitic held some prestige as inscriptional languages.
In Najd and parts of western Arabia, a language known to scholars as Thamudic C is attested.
In eastern Arabia, inscriptions in a script derived from ASA attest to a language known as Hasaitic.
Finally, on the northwestern frontier of Arabia, various languages known to scholars as Thamudic B, Thamudic D, Safaitic, and Hismaic are attested.
The last two share important isoglosses with later forms of Arabic, leading scholars to theorize that Safaitic and Hismaic are in fact early forms of Arabic and that they should be considered Old Arabic.
Beginning in the 1st century CE, fragments of Northern Old Arabic are attested in the Nabataean script across northern Arabia.
By the 4th century CE, the Nabataean Aramaic writing system had come to express varieties of Arabic other than that of the Nabataeans.
In late pre-Islamic times, a transdialectal and transcommunal variety of Arabic emerged in the Hejaz which continued living its parallel life after literary Arabic had been institutionally standardized in the 2nd and 3rd century of the Hijra, most strongly in Judeo-Christian texts, keeping alive ancient features eliminated from the "learned" tradition (Classical Arabic).
This variety and both its classicizing and "lay" iterations have been termed Middle Arabic in the past, but they are thought to continue an Old Higazi register.
It is clear that the orthography of the Qur'an was not developed for the standardized form of Classical Arabic; rather, it shows the attempt on the part of writers to record an archaic form of Old Higazi.
In the late 6th century AD, a relatively uniform intertribal "poetic koine" distinct from the spoken vernaculars developed based on the Bedouin dialects of Najd, probably in connection with the court of al-Ḥīra.
During the first Islamic century, the majority of Arabic poets and Arabic-writing persons spoke Arabic as their mother tongue.
Their texts, although mainly preserved in far later manuscripts, contain traces of non-standardized Classical Arabic elements in morphology and syntax.
The standardization of Classical Arabic reached completion around the end of the 8th century.
The first comprehensive description of the "ʿarabiyya" "Arabic", Sībawayhi's "al"-"Kitāb", is based first of all upon a corpus of poetic texts, in addition to Qur'an usage and Bedouin informants whom he considered to be reliable speakers of the "ʿarabiyya".
By the 8th century, knowledge of Classical Arabic had become an essential prerequisite for rising into the higher classes throughout the Islamic world.
Charles Ferguson's koine theory (Ferguson 1959) claims that the modern Arabic dialects collectively descend from a single military koine that sprang up during the Islamic conquests; this view has been challenged in recent times.
Ahmad al-Jallad proposes that there were at least two considerably distinct types of Arabic on the eve of the conquests: Northern and Central (Al-Jallad 2009).
The modern dialects emerged from a new contact situation produced following the conquests.
Instead of the emergence of a single or multiple koines, the dialects contain several sedimentary layers of borrowed and areal features, which they absorbed at different points in their linguistic histories.
According to Veersteegh and Bickerton, colloquial Arabic dialects arose from pidginized Arabic formed from contact between Arabs and conquered peoples.
Pidginization and subsequent creolization among Arabs and arabized peoples could explain relative morphological and phonological simplicity of vernacular Arabic compared to Classical and MSA.
"Arabic" usually designates one of three main variants: Classical Arabic, Modern Standard Arabic and "colloquial" or "dialectal" Arabic.
Classical Arabic is the language found in the Quran, used from the period of Pre-Islamic Arabia to that of the Abbasid Caliphate.
Theoretically, Classical Arabic is considered normative, according to the syntactic and grammatical norms laid down by classical grammarians (such as Sibawayh) and the vocabulary defined in classical dictionaries (such as the Lisān al-ʻArab).
In practice, however, modern authors almost never write in pure Classical Arabic, instead using a literary language with its own grammatical norms and vocabulary, commonly known as Modern Standard Arabic (MSA).
MSA is the variety used in most current, printed Arabic publications, spoken by some of the Arabic media across North Africa and the Middle East, and understood by most educated Arabic speakers.
"Literary Arabic" and "Standard Arabic" ( ") are less strictly defined terms that may refer to Modern Standard Arabic or Classical Arabic.
Some of the differences between Classical Arabic (CA) and Modern Standard Arabic (MSA) are as follows:

MSA uses much Classical vocabulary (e.g., ' 'to go') that is not present in the spoken varieties, but deletes Classical words that sound obsolete in MSA.
In addition, MSA has borrowed or coined a large number of terms for concepts that did not exist in Quranic times, and MSA continues to evolve.
Some words have been borrowed from other languages—notice that transliteration mainly indicates spelling and not real pronunciation (e.g., ' 'film' or " 'democracy').
However, the current preference is to avoid direct borrowings, preferring to either use loan translations (e.g., ' 'branch', also used for the branch of a company or organization; ' 'wing', is also used for the wing of an airplane, building, air force, etc.
), or to coin new words using forms within existing roots ( ' 'apoptosis', using the root "m/w/t" 'death' put into the Xth form, or ' 'university', based on ' 'to gather, unite'; ' 'republic', based on ' 'multitude').
An earlier tendency was to redefine an older word although this has fallen into disuse (e.g., ' 'telephone' < 'invisible caller (in Sufism)'; "" 'newspaper' < 'palm-leaf stalk').
"Colloquial" or "dialectal" Arabic refers to the many national or regional varieties which constitute the everyday spoken language and evolved from Classical Arabic.
Colloquial Arabic has many regional variants; geographically distant varieties usually differ enough to be mutually unintelligible, and some linguists consider them distinct languages.
The varieties are typically unwritten.
They are often used in informal spoken media, such as soap operas and talk shows, as well as occasionally in certain forms of written media such as poetry and printed advertising.
The only variety of modern Arabic to have acquired official language status is Maltese, which is spoken in (predominantly Catholic) Malta and written with the Latin script.
It is descended from Classical Arabic through Siculo-Arabic, but is not mutually intelligible with any other variety of Arabic.
Most linguists list it as a separate language rather than as a dialect of Arabic.
Even during Muhammad's lifetime, there were dialects of spoken Arabic.
Muhammad spoke in the dialect of Mecca, in the western Arabian peninsula, and it was in this dialect that the Quran was written down.
However, the dialects of the eastern Arabian peninsula were considered the most prestigious at the time, so the language of the Quran was ultimately converted to follow the eastern phonology.
It is this phonology that underlies the modern pronunciation of Classical Arabic.
The phonological differences between these two dialects account for some of the complexities of Arabic writing, most notably the writing of the glottal stop or "hamzah" (which was preserved in the eastern dialects but lost in western speech) and the use of ' (representing a sound preserved in the western dialects but merged with ' in eastern speech).
The sociolinguistic situation of Arabic in modern times provides a prime example of the linguistic phenomenon of diglossia, which is the normal use of two separate varieties of the same language, usually in different social situations.
In the case of Arabic, educated Arabs of any nationality can be assumed to speak both their school-taught Standard Arabic as well as their native, mutually unintelligible "dialects"; these dialects linguistically constitute separate languages which may have dialects of their own.
When educated Arabs of different dialects engage in conversation (for example, a Moroccan speaking with a Lebanese), many speakers code-switch back and forth between the dialectal and standard varieties of the language, sometimes even within the same sentence.
Arabic speakers often improve their familiarity with other dialects via music or film.
The issue of whether Arabic is one language or many languages is politically charged, in the same way it is for the varieties of Chinese, Hindi and Urdu, Serbian and Croatian, Scots and English, etc.
In contrast to speakers of Hindi and Urdu who claim they cannot understand each other even when they can, speakers of the varieties of Arabic will claim they can all understand each other even when they cannot.
The issue of diglossia between spoken and written language is a significant complicating factor: A single written form, significantly different from any of the spoken varieties learned natively, unites a number of sometimes divergent spoken forms.
For political reasons, Arabs mostly assert that they all speak a single language, despite significant issues of mutual incomprehensibility among differing spoken versions.
From a linguistic standpoint, it is often said that the various spoken varieties of Arabic differ among each other collectively about as much as the Romance languages.
This is an apt comparison in a number of ways.
The period of divergence from a single spoken form is similar—perhaps 1500 years for Arabic, 2000 years for the Romance languages.
Also, while it is comprehensible to people from the Maghreb, a linguistically innovative variety such as Moroccan Arabic is essentially incomprehensible to Arabs from the Mashriq, much as French is incomprehensible to Spanish or Italian speakers but relatively easily learned by them.
This suggests that the spoken varieties may linguistically be considered separate languages.
The influence of Arabic has been most important in Islamic countries, because it is the language of the Islamic sacred book, the Quran.
Arabic is also an important source of vocabulary for languages such as Amharic, Baluchi, Bengali, Berber, Bosnian, Chaldean, Chechen, Croatian, Dagestani, English, German, Gujarati, Hausa, Hindi, Kazakh, Kurdish, Kutchi, Kyrgyz, Malay (Malaysian and Indonesian), Pashto, Persian, Punjabi, Rohingya, Romance languages (French, Catalan, Italian, Portuguese, Sicilian, Spanish, etc.)
Saraiki, Sindhi, Somali, Sylheti, Swahili, Tagalog, Tigrinya, Turkish, Turkmen, Urdu, Uyghur, Uzbek, Visayan and Wolof, as well as other languages in countries where these languages are spoken.
France has recently been emphasizing the learning and usage of Arabic in their classroom(s)/school(s).
Arabic is considered to be a popular second-language choice in France.
In addition, English has many Arabic loanwords, some directly, but most via other Mediterranean languages.
Examples of such words include admiral, adobe, alchemy, alcohol, algebra, algorithm, alkaline, almanac, amber, arsenal, assassin, candy, carat, cipher, coffee, cotton, ghoul, hazard, jar, kismet, lemon, loofah, magazine, mattress, sherbet, sofa, sumac, tariff, and zenith.
Other languages such as Maltese and Kinubi derive ultimately from Arabic, rather than merely borrowing vocabulary or grammatical rules.
Terms borrowed range from religious terminology (like Berber "taẓallit", "prayer", from "salat" ( "")), academic terms (like Uyghur "mentiq", "logic"), and economic items (like English "coffee") to placeholders (like Spanish "fulano", "so-and-so"), everyday terms (like Hindustani "lekin", "but", or Spanish "taza" and French "tasse", meaning "cup"), and expressions (like Catalan "a betzef", "galore, in quantity").
Most Berber varieties (such as Kabyle), along with Swahili, borrow some numbers from Arabic.
Most Islamic religious terms are direct borrowings from Arabic, such as ("salat"), "prayer", and ("imam"), "prayer leader."
In languages not directly in contact with the Arab world, Arabic loanwords are often transferred indirectly via other languages rather than being transferred directly from Arabic.
For example, most Arabic loanwords in Hindustani and Turkish entered through Persian though Persian is an Indo-Iranian language.
Older Arabic loanwords in Hausa were borrowed from Kanuri.
Arabic words also made their way into several West African languages as Islam spread across the Sahara.
Variants of Arabic words such as "kitāb" ("book") have spread to the languages of African groups who had no direct contact with Arab traders.
Since throughout the Islamic world, Arabic occupied a position similar to that of Latin in Europe, many of the Arabic concepts in the fields of science, philosophy, commerce, etc.
were coined from Arabic roots by non-native Arabic speakers, notably by Aramaic and Persian translators, and then found their way into other languages.
This process of using Arabic roots, especially in Kurdish and Persian, to translate foreign concepts continued through to the 18th and 19th centuries, when swaths of Arab-inhabited lands were under Ottoman rule.
The most important sources of borrowings into (pre-Islamic) Arabic are from the related (Semitic) languages Aramaic, which used to be the principal, international language of communication throughout the ancient Near and Middle East, Ethiopic, and to a lesser degree Hebrew (mainly religious concepts).
In addition, many cultural, religious and political terms have entered Arabic from Iranian languages, notably Middle Persian, Parthian, and (Classical) Persian, and Hellenistic Greek ("kīmiyāʼ" has as origin the Greek "khymia", meaning in that language the melting of metals; see Roger Dachez, "Histoire de la Médecine de l'Antiquité au XXe siècle", Tallandier, 2008, p.
251), "alembic" (distiller) from "ambix" (cup), "almanac" (climate) from "almenichiakon" (calendar).
(For the origin of the last three borrowed words, see Alfred-Louis de Prémare, "Foundations of Islam", Seuil, L'Univers Historique, 2002.)
Some Arabic borrowings from Semitic or Persian languages are, as presented in De Prémare's above-cited book:

There have been many instances of national movements to convert Arabic script into Latin script or to Romanize the language.
Currently, the only language derived from Classical Arabic to use Latin script is Maltese.
The Beirut newspaper "La Syrie" pushed for the change from Arabic script to Latin letters in 1922.
The major head of this movement was Louis Massignon, a French Orientalist, who brought his concern before the Arabic Language Academy in Damacus in 1928.
Massignon's attempt at Romanization failed as the Academy and population viewed the proposal as an attempt from the Western world to take over their country.
Sa'id Afghani, a member of the Academy, mentioned that the movement to Romanize the script was a Zionist plan to dominate Lebanon.
After the period of colonialism in Egypt, Egyptians were looking for a way to reclaim and re-emphasize Egyptian culture.
As a result, some Egyptians pushed for an Egyptianization of the Arabic language in which the formal Arabic and the colloquial Arabic would be combined into one language and the Latin alphabet would be used.
There was also the idea of finding a way to use Hieroglyphics instead of the Latin alphabet, but this was seen as too complicated to use.
A scholar, Salama Musa agreed with the idea of applying a Latin alphabet to Arabic, as he believed that would allow Egypt to have a closer relationship with the West.
He also believed that Latin script was key to the success of Egypt as it would allow for more advances in science and technology.
This change in alphabet, he believed, would solve the problems inherent with Arabic, such as a lack of written vowels and difficulties writing foreign words that made it difficult for non-native speakers to learn.
Ahmad Lutfi As Sayid and Muhammad Azmi, two Egyptian intellectuals, agreed with Musa and supported the push for Romanization.
The idea that Romanization was necessary for modernization and growth in Egypt continued with Abd Al-Aziz Fahmi in 1944.
He was the chairman for the Writing and Grammar Committee for the Arabic Language Academy of Cairo.
However, this effort failed as the Egyptian people felt a strong cultural tie to the Arabic alphabet.
In particular, the older Egyptian generations believed that the Arabic alphabet had strong connections to Arab values and history, due to the long history of the Arabic alphabet (Shrivtiel, 189) in Muslim societies.
The Quran introduced a new way of writing to the world.
People began studying and applying the unique styles they learned from the Quran to not only their own writing, but also their culture.
Writers studied the unique structure and format of the Quran in order to identify and apply the figurative devices and their impact on the reader.
The Quran inspired musicality in poetry through the internal rhythm of the verses.
The arrangement of words, how certain sounds create harmony, and the agreement of rhymes create the sense of rhythm within each verse.
At times, the chapters of the Quran only have the rhythm in common.
The repetition in the Quran introduced the true power and impact repetition can have in poetry.
The repetition of certain words and phrases made them appear more firm and explicit in the Quran.
The Quran uses constant metaphors of blindness and deafness to imply unbelief.
Metaphors were not a new concept to poetry, however the strength of extended metaphors was.
The explicit imagery in the Quran inspired many poets to include and focus on the feature in their own work.
The poet ibn al Mu'tazz wrote a book regarding the figures of speech inspired by his study of the Quran.
Poets such as badr Shakir al sayyab expresses his political opinion in his work through imagery inspired by the forms of more harsher imagery used in the Quran.
The Quran uses figurative devices in order to express the meaning in the most beautiful form possible.
The study of the pauses in the Quran as well as other rhetoric allow it to be approached in a multiple ways.
Although the Quran is known for its fluency and harmony, the structure can be best described as chaotic.
The suras also known as chapters of the Quran are not placed in chronological order.
The only constant in their structure is that the longest are placed first and shorter ones follow.
The topics discussed in the chapter often have no relation to each other and only share their sense of rhyme.
The Quran introduces to poetry the idea of abandoning order and scattering narratives throughout the text.
Harmony is also present in the sound of the Quran.
The elongations and accents present in the Quran create a harmonious flow within the writing.
Unique sound of the Quran recited, due to the accents, create a deeper level of understanding through a deeper emotional connection.
The Quran is written in a language that is simple and understandable by people.
The simplicity of the writing inspired later poets to write in a more clear and clear-cut style.
The words of the Quran, although unchanged, are to this day understandable and frequently used in both formal and informal Arabic.
The simplicity of the language makes memorizing and reciting the Quran a slightly easier task.
The writer al-Khattabi explains how culture is a required element to create a sense of art in work as well as understand it.
He believes that fluency and harmony the Quran possess are not the only elements that make it beautiful and create a bond between the reader and the text.
While a lot of poetry was deemed comparable to the Quran in that it is equal to or better than the composition of the Quran, a debate rose that such statements are not possible because humans are incapable of composing work comparable to the Quran.
Because the structure of the Quran made it difficult for a clear timeline to be seen, Hadith were the main source of chronological order.
The Hadith were passed down from generation to generation and this tradition became a large resource for understanding the context.
Poetry after the Quran began possessing this element of tradition by including ambiguity and background information to be required to understand the meaning.
After the Quran came down to the people, the tradition of memorizing the verses became present.
It is believed that the greater the amount of the Quran memorized, the greater the faith.
As technology improved over time, hearing recitations of Quran became more available as well as more tools to help memorize the verses.
The tradition of Love Poetry served as a symbolic representation of a Muslim's desire for a closer contact with their Lord.
While the influence of the Quran on Arabic poetry is explained and defended by numerous writers, some writers such as Al- Baqillani believe that poetry and the Quran are in no conceivable way related due to the uniqueness of the Quran.
Poetry's imperfections prove his points that they cannot be compared with the fluency the Quran holds.
Classical Arabic is the language of poetry and literature (including news); it is also mainly the language of the Quran.
Classical Arabic is closely associated with the religion of Islam because the Quran was written in it.
Most of the world's Muslims do not speak Classical Arabic as their native language, but many can read the Quranic script and recite the Quran.
Among non-Arab Muslims, translations of the Quran are most often accompanied by the original text.
At present, Modern Standard Arabic (MSA) is also used in modernized versions of literary forms of the Quran.
Some Muslims present a monogenesis of languages and claim that the Arabic language was the language revealed by God for the benefit of mankind and the original language as a prototype system of symbolic communication, based upon its system of triconsonantal roots, spoken by man from which all other languages were derived, having first been corrupted.
Judaism has a similar account with the Tower of Babel.
"Colloquial Arabic" is a collective term for the spoken dialects of Arabic used throughout the Arab world, which differ radically from the literary language.
The main dialectal division is between the varieties within and outside of the Arabian peninsula, followed by that between sedentary varieties and the much more conservative Bedouin varieties.
All of the varieties outside of the Arabian peninsula (which include the large majority of speakers) have a large number of features in common with each other that are not found in Classical Arabic.
This has led researchers to postulate the existence of a prestige koine dialect in the one or two centuries immediately following the Arab conquest, whose features eventually spread to all of the newly conquered areas.
(These features are present to varying degrees inside the Arabian peninsula.
Generally, the Arabian peninsula varieties have much more diversity than the non-peninsula varieties, but have been understudied.)
Within the non-peninsula varieties, the largest difference is between the non-Egyptian North African dialects (especially Moroccan Arabic) and the others.
Moroccan Arabic in particular is hardly comprehensible to Arabic speakers east of Libya (although the converse is not true, in part due to the popularity of Egyptian films and other media).
One factor in the differentiation of the dialects is influence from the languages previously spoken in the areas, which have typically provided a significant number of new words and have sometimes also influenced pronunciation or word order; however, a much more significant factor for most dialects is, as among Romance languages, retention (or change of meaning) of different classical forms.
Thus Iraqi "aku", Levantine "fīh" and North African "kayən" all mean 'there is', and all come from Classical Arabic forms ("yakūn", "fīhi", "kā'in" respectively), but now sound very different.
Transcription is a broad IPA transcription, so minor differences were ignored for easier comparison.
Also, the pronunciation of Modern Standard Arabic differs significantly from region to region.
According to Charles A. Ferguson, the following are some of the characteristic features of the koine that underlies all of the modern dialects outside the Arabian peninsula.
Although many other features are common to most or all of these varieties, Ferguson believes that these features in particular are unlikely to have evolved independently more than once or twice and together suggest the existence of the koine:


Of the 29 Proto-Semitic consonants, only one has been lost: , which merged with .
But the consonant is still found in many colloquial Arabic dialects.
Various other consonants have changed their sound too, but have remained distinct.
An original lenited to , and - consistently attested in pre-Islamic Greek transcription of Arabic languages - became palatalized to or by the time of the Quran and , , or after early Muslim conquests and in MSA (see Arabic phonology#Local variations for more detail).
An original voiceless alveolar lateral fricative became .
Its emphatic counterpart was considered by Arabs to be the most unusual sound in Arabic (Hence the Classical Arabic's appellation ' or "language of the '"); for most modern dialects, it has become an emphatic stop with loss of the laterality or with complete loss of any pharyngealization or velarization, .
(The classical "" pronunciation of pharyngealization still occurs in the Mehri language and the similar sound without velarization, , exists in other Modern South Arabian languages.)
Other changes may also have happened.
Classical Arabic pronunciation is not thoroughly recorded and different reconstructions of the sound system of Proto-Semitic propose different phonetic values.
One example is the emphatic consonants, which are pharyngealized in modern pronunciations but may have been velarized in the eighth century and glottalized in Proto-Semitic.
Reduction of and between vowels occurs in a number of circumstances and is responsible for much of the complexity of third-weak ("defective") verbs.
Early Akkadian transcriptions of Arabic names shows that this reduction had not yet occurred as of the early part of the 1st millennium BC.
The Classical Arabic language as recorded was a poetic koine that reflected a consciously archaizing dialect, chosen based on the tribes of the western part of the Arabian Peninsula, who spoke the most conservative variants of Arabic.
Even at the time of Muhammed and before, other dialects existed with many more changes, including the loss of most glottal stops, the loss of case endings, the reduction of the diphthongs and into monophthongs , etc.
Most of these changes are present in most or all modern varieties of Arabic.
An interesting feature of the writing system of the Quran (and hence of Classical Arabic) is that it contains certain features of Muhammad's native dialect of Mecca, corrected through diacritics into the forms of standard Classical Arabic.
Among these features visible under the corrections are the loss of the glottal stop and a differing development of the reduction of certain final sequences containing : Evidently, final became as in the Classical language, but final became a different sound, possibly (rather than again in the Classical language).
This is the apparent source of the "alif maqṣūrah" 'restricted alif' where a final is reconstructed: a letter that would normally indicate or some similar high-vowel sound, but is taken in this context to be a logical variant of "alif" and represent the sound .
Although Classical Arabic was a unitary language and is now used in Quran, its pronunciation varies somewhat from country to country and from region to region within a country.
It is influenced by colloquial dialects.
The "colloquial" spoken dialects of Arabic are learned at home and constitute the native languages of Arabic speakers.
"Formal" Literary Arabic (usually specifically Modern Standard Arabic) is learned at school; although many speakers have a native-like command of the language, it is technically not the native language of any speakers.
Both varieties can be both written and spoken, although the colloquial varieties are rarely written down and the formal variety is spoken mostly in formal circumstances, e.g., in radio and TV broadcasts, formal lectures, parliamentary discussions and to some extent between speakers of different colloquial dialects.
Even when the literary language is spoken, however, it is normally only spoken in its pure form when reading a prepared text out loud and communication between speakers of different colloquial dialects.
When speaking extemporaneously (i.e. making up the language on the spot, as in a normal discussion among people), speakers tend to deviate somewhat from the strict literary language in the direction of the colloquial varieties.
In fact, there is a continuous range of "in-between" spoken varieties: from nearly pure Modern Standard Arabic (MSA), to a form that still uses MSA grammar and vocabulary but with significant colloquial influence, to a form of the colloquial language that imports a number of words and grammatical constructions in MSA, to a form that is close to pure colloquial but with the "rough edges" (the most noticeably "vulgar" or non-Classical aspects) smoothed out, to pure colloquial.
The particular variant (or "register") used depends on the social class and education level of the speakers involved and the level of formality of the speech situation.
Often it will vary within a single encounter, e.g., moving from nearly pure MSA to a more mixed language in the process of a radio interview, as the interviewee becomes more comfortable with the interviewer.
This type of variation is characteristic of the diglossia that exists throughout the Arabic-speaking world.
Although Modern Standard Arabic (MSA) is a unitary language, its pronunciation varies somewhat from country to country and from region to region within a country.
The variation in individual "accents" of MSA speakers tends to mirror corresponding variations in the colloquial speech of the speakers in question, but with the distinguishing characteristics moderated somewhat.
Note that it is important in descriptions of "Arabic" phonology to distinguish between pronunciation of a given colloquial (spoken) dialect and the pronunciation of MSA by these same speakers.
Although they are related, they are not the same.
For example, the phoneme that derives from Classical Arabic has many different pronunciations in the modern spoken varieties, e.g., including the proposed original .
Speakers whose native variety has either or will use the same pronunciation when speaking MSA.
Even speakers from Cairo, whose native Egyptian Arabic has , normally use when speaking MSA.
The of Persian Gulf speakers is the only variant pronunciation which isn't found in MSA; is used instead, but may use [j] in MSA for comfortable pronunciation.
Another reason of different pronunciations is influence of colloquial dialects.
The differentiation of pronunciation of colloquial dialects is the influence from other languages previously spoken and some still presently spoken in the regions, such as Coptic in Egypt, Berber, Punic or Phoenician in North Africa, Himyaritic, Modern South Arabian and Old South Arabian in Yemen and Oman, Aramaic and Canaanite languages (including Phoenician) in the Levant and Mesopotamia.
Another example: Many colloquial varieties are known for a type of vowel harmony in which the presence of an "emphatic consonant" triggers backed allophones of nearby vowels (especially of the low vowels , which are backed to in these circumstances and very often fronted to in all other circumstances).
In many spoken varieties, the backed or "emphatic" vowel allophones spread a fair distance in both directions from the triggering consonant; in some varieties (most notably Egyptian Arabic), the "emphatic" allophones spread throughout the entire word, usually including prefixes and suffixes, even at a distance of several syllables from the triggering consonant.
Speakers of colloquial varieties with this vowel harmony tend to introduce it into their MSA pronunciation as well, but usually with a lesser degree of spreading than in the colloquial varieties.
(For example, speakers of colloquial varieties with extremely long-distance harmony may allow a moderate, but not extreme, amount of spreading of the harmonic allophones in their MSA speech, while speakers of colloquial varieties with moderate-distance harmony may only harmonize immediately adjacent vowels in MSA.)
Modern Standard Arabic has six pure vowels (while most modern dialects have eight pure vowels which includes the long vowels ), with short and corresponding long vowels .
There are also two diphthongs: and .
The pronunciation of the vowels differs from speaker to speaker, in a way that tends to reflect the pronunciation of the corresponding colloquial variety.
Nonetheless, there are some common trends.
Most noticeable is the differing pronunciation of and , which tend towards fronted , or in most situations, but a back in the neighborhood of emphatic consonants.
Some accents and dialects, such as those of the Hejaz region, have an open or a central in all situations.
The vowel varies towards too.
Listen to the final vowel in the recording of " at the beginning of this article, for example.
The point is, Arabic has only three short vowel phonemes, so those phonemes can have a very wide range of allophones.
The vowels and are often affected somewhat in emphatic neighborhoods as well, with generally more back or centralized allophones, but the differences are less great than for the low vowels.
The pronunciation of short and tends towards and , respectively, in many dialects.
The definition of both "emphatic" and "neighborhood" vary in ways that reflect (to some extent) corresponding variations in the spoken dialects.
Generally, the consonants triggering "emphatic" allophones are the pharyngealized consonants ; ; and , if not followed immediately by .
Frequently, the fricatives also trigger emphatic allophones; occasionally also the pharyngeal consonants (the former more than the latter).
Many dialects have multiple emphatic allophones of each vowel, depending on the particular nearby consonants.
In most MSA accents, emphatic coloring of vowels is limited to vowels immediately adjacent to a triggering consonant, although in some it spreads a bit farther: e.g., ' 'time'; ' 'homeland'; " 'downtown' (sometimes or similar).
In a non-emphatic environment, the vowel in the diphthong tends to be fronted even more than elsewhere, often pronounced or : hence ' 'sword' but ' 'summer'.
However, in accents with no emphatic allophones of (e.g., in the Hejaz), the pronunciation or occurs in all situations.
The phoneme is represented by the Arabic letter ' () and has many standard pronunciations.
is characteristic of north Algeria, Iraq, also in most of the Arabian peninsula but with an allophonic in some positions; occurs in most of the Levant and most North Africa; and is used in most of Egypt and some regions in Yemen and Oman.
Generally this corresponds with the pronunciation in the colloquial dialects.
In some regions in Sudan and Yemen, as well as in some Sudanese and Yemeni dialects, it may be either or , representing the original pronunciation of Classical Arabic.
Foreign words containing may be transcribed with , , , , , or , mainly depending on the regional spoken variety of Arabic or the commonly diacriticized Arabic letter.
Note also that in northern Egypt, where the Arabic letter ' () is normally pronounced , a separate phoneme , which may be transcribed with , occurs in a small number of mostly non-Arabic loanwords, e.g., 'jacket'.
In many varieties, () are actually epiglottal (despite what is reported in many earlier works).
The emphatic consonant was actually pronounced , or possibly —either way, a highly unusual sound.
The medieval Arabs actually termed their language " 'the language of the Ḍād' (the name of the letter used for this sound), since they thought the sound was unique to their language.
(In fact, it also exists in a few other minority Semitic languages, e.g., Mehri.)
Arabic has consonants traditionally termed "emphatic" (), which exhibit simultaneous pharyngealization as well as varying degrees of velarization , so they may be written with the "Velarized or pharyngealized" diacritic () as: .
This simultaneous articulation is described as "Retracted Tongue Root" by phonologists.
In some transcription systems, emphasis is shown by capitalizing the letter, for example, is written ; in others the letter is underlined or has a dot below it, for example, .
Vowels and consonants can be phonologically short or long.
Long (geminate) consonants are normally written doubled in Latin transcription (i.e. bb, dd, etc.
), reflecting the presence of the Arabic diacritic mark ', which indicates doubled consonants.
In actual pronunciation, doubled consonants are held twice as long as short consonants.
This consonant lengthening is phonemically contrastive: ' 'he accepted' vs. " 'he kissed'.
Arabic has two kinds of syllables: open syllables (CV) and (CVV)—and closed syllables (CVC), (CVVC) and (CVCC).
The syllable types with two morae (units of time), i.e. CVC and CVV, are termed "heavy syllables", while those with three morae, i.e. CVVC and CVCC, are "superheavy syllables".
Superheavy syllables in Classical Arabic occur in only two places: at the end of the sentence (due to pausal pronunciation) and in words such as ' 'hot', ' 'stuff, substance', ' 'they disputed with each other', where a long ' occurs before two identical consonants (a former short vowel between the consonants has been lost).
(In less formal pronunciations of Modern Standard Arabic, superheavy syllables are common at the end of words or before clitic suffixes such as "" 'us, our', due to the deletion of final short vowels.)
In surface pronunciation, every vowel must be preceded by a consonant (which may include the glottal stop ).
There are no cases of hiatus within a word (where two vowels occur next to each other, without an intervening consonant).
Some words do have an underlying vowel at the beginning, such as the definite article "al-" or words such as ' 'he bought', ' 'meeting'.
When actually pronounced, one of three things happens:

Word stress is not phonemically contrastive in Standard Arabic.
It bears a strong relationship to vowel length.
The basic rules for Modern Standard Arabic are:

Examples:' 'book', ' 'writer', ' 'desk', ' 'desks', ' 'library' (but ' 'library' in short pronunciation), ' (Modern Standard Arabic) 'they wrote' = ' (dialect), ' (Modern Standard Arabic) 'they wrote it' = ' (dialect), ' (Modern Standard Arabic) 'they (dual, fem) wrote', ' (Modern Standard Arabic) 'I wrote' = ' (short form or dialect).
Doubled consonants count as two consonants: ' 'magazine', "" "place".
These rules may result in differently stressed syllables when final case endings are pronounced, vs. the normal situation where they are not pronounced, as in the above example of ' 'library' in full pronunciation, but ' 'library' in short pronunciation.
The restriction on final long vowels does not apply to the spoken dialects, where original final long vowels have been shortened and secondary final long vowels have arisen from loss of original final "-hu/hi".
Some dialects have different stress rules.
In the Cairo (Egyptian Arabic) dialect a heavy syllable may not carry stress more than two syllables from the end of a word, hence ' 'school', ' 'Cairo'.
This also affects the way that Modern Standard Arabic is pronounced in Egypt.
In the Arabic of Sanaa, stress is often retracted: ' 'two houses', ' 'their table', ' 'desks', ' 'sometimes', "" 'their school'.
(In this dialect, only syllables with long vowels or diphthongs are considered heavy; in a two-syllable word, the final syllable can be stressed only if the preceding syllable is light; and in longer words, the final syllable cannot be stressed.)
The final short vowels (e.g., the case endings "-a -i -u" and mood endings "-u -a") are often not pronounced in this language, despite forming part of the formal paradigm of nouns and verbs.
The following levels of pronunciation exist:

This is the most formal level actually used in speech.
All endings are pronounced as written, except at the end of an utterance, where the following changes occur:

This is a formal level of pronunciation sometimes seen.
It is somewhat like pronouncing all words as if they were in pausal position (with influence from the colloquial varieties).
The following changes occur:

This is the pronunciation used by speakers of Modern Standard Arabic in extemporaneous speech, i.e. when producing new sentences rather than simply reading a prepared text.
It is similar to formal short pronunciation except that the rules for dropping final vowels apply "even" when a clitic suffix is added.
Basically, short-vowel case and mood endings are never pronounced and certain other changes occur that echo the corresponding colloquial pronunciations.
Specifically:

As mentioned above, many spoken dialects have a process of "emphasis spreading", where the "emphasis" (pharyngealization) of emphatic consonants spreads forward and back through adjacent syllables, pharyngealizing all nearby consonants and triggering the back allophone in all nearby low vowels.
The extent of emphasis spreading varies.
For example, in Moroccan Arabic, it spreads as far as the first full vowel (i.e. sound derived from a long vowel or diphthong) on either side; in many Levantine dialects, it spreads indefinitely, but is blocked by any or ; while in Egyptian Arabic, it usually spreads throughout the entire word, including prefixes and suffixes.
In Moroccan Arabic, also have emphatic allophones and , respectively.
Unstressed short vowels, especially , are deleted in many contexts.
Many sporadic examples of short vowel change have occurred (especially → and interchange ↔).
Most Levantine dialects merge short /i u/ into in most contexts (all except directly before a single final consonant).
In Moroccan Arabic, on the other hand, short triggers labialization of nearby consonants (especially velar consonants and uvular consonants), and then short /a i u/ all merge into , which is deleted in many contexts.
(The labialization plus is sometimes interpreted as an underlying phoneme .)
This essentially causes the wholesale loss of the short-long vowel distinction, with the original long vowels remaining as half-long , phonemically , which are used to represent "both" short and long vowels in borrowings from Literary Arabic.
Most spoken dialects have monophthongized original to in most circumstances, including adjacent to emphatic consonants, while keeping them as the original diphthongs in others e.g.
.
